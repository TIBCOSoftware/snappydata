{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 SnappyData\u2122 (aka TIBCO ComputeDB\u2122) is a distributed, in-memory optimized, analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workloads. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources/formats and stream processing all in one unified cluster. One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs. SnappyData enables complex processing on large data sets in sub-second timeframes. Note SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes. Important Capabilities \u00b6 Easily discover and catalog big data sets You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more. Rich connectivity SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See Apache Spark Packages . You can also dynamically deploy connectors to a running SnappyData cluster. Virtual or in-memory data You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed. When speed is essential, applications can selectively copy the external data into memory using a single SQL command. In-memory Columnar + Row store You can choose in-memory data to be stored in any of the following forms: Columnar : The form that is compressed and designed for scanning/aggregating large data sets. Row store : The form that has an extremely fast key access or highly selective access. The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store. High performance When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent. Flexible rich data transformations External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.* Prepares data for data science Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone. Stream ingestion and liveness While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated \u2014 for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple. In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer Stream Processing with SnappyData Approximate Query Processing(AQP) When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer AQP . Access from anywhere You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1 to 2.1.3. SnappyData natively supports modern visualization tools such as TIBCO Spotfire , Tableau , and Qlikview . Downloading and Installing SnappyData \u00b6 You can download and install the latest version of SnappyData from github or you can download the enterprise version that is TIBCO ComputeDB from here . Refer to the documentation for installation steps. Getting Started \u00b6 Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options: On-premise clusters AWS Docker Kubernetes You can find more information on options for running SnappyData here . Quick Performance Comparison of SnappyData vs Apache Spark \u00b6 If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this test using the Spark Shell. Other Relevant content \u00b6 Paper on Snappydata (Community Edition of TIBCO ComputeDB) at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems. Another early Paper that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016. TPC-H benchmark comparing Apache Spark with SnappyData Checkout the SnappyData blog for developer content TIBCO community page for the latest info. Community Support \u00b6 We monitor the following channels comments/questions: Stackoverflow Slack Gitter Mailing List Reddit JIRA Link with SnappyData Distribution \u00b6 Using Maven Dependency \u00b6 SnappyData's artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates: groupId: io.snappydata artifactId: snappydata-cluster_2.11 version: 1.3.1 Using SBT Dependency \u00b6 If you are using SBT, add this line to your build.sbt for core SnappyData artifacts: libraryDependencies += \"io.snappydata\" % \"snappydata-core_2.11\" % \"1.3.1\" For additions related to SnappyData cluster, use: libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" You can find more specific SnappyData artifacts here Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1 ), it may be due an issue with its pom file. As a workaround, you can add the below code to your build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Building from Source \u00b6 If you would like to build SnappyData from source, refer to the documentation on building from source . How is SnappyData Different than Apache Spark? \u00b6 Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight. The SnappyData Approach \u00b6 Snappy Architecture \u00b6 SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources. Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark. Streaming Example - Ad Analytics \u00b6 Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - Ad Analytics code example . Here is a screencast that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra. Contributing to SnappyData \u00b6 If you are interested in contributing, please visit the community page for ways in which you can help.","title":"Overview"},{"location":"#introduction","text":"SnappyData\u2122 (aka TIBCO ComputeDB\u2122) is a distributed, in-memory optimized, analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workloads. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources/formats and stream processing all in one unified cluster. One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs. SnappyData enables complex processing on large data sets in sub-second timeframes. Note SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes.","title":"Introduction"},{"location":"#important-capabilities","text":"Easily discover and catalog big data sets You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more. Rich connectivity SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See Apache Spark Packages . You can also dynamically deploy connectors to a running SnappyData cluster. Virtual or in-memory data You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed. When speed is essential, applications can selectively copy the external data into memory using a single SQL command. In-memory Columnar + Row store You can choose in-memory data to be stored in any of the following forms: Columnar : The form that is compressed and designed for scanning/aggregating large data sets. Row store : The form that has an extremely fast key access or highly selective access. The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store. High performance When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent. Flexible rich data transformations External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.* Prepares data for data science Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone. Stream ingestion and liveness While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated \u2014 for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple. In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer Stream Processing with SnappyData Approximate Query Processing(AQP) When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer AQP . Access from anywhere You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1 to 2.1.3. SnappyData natively supports modern visualization tools such as TIBCO Spotfire , Tableau , and Qlikview .","title":"Important Capabilities"},{"location":"#downloading-and-installing-snappydata","text":"You can download and install the latest version of SnappyData from github or you can download the enterprise version that is TIBCO ComputeDB from here . Refer to the documentation for installation steps.","title":"Downloading and Installing SnappyData"},{"location":"#getting-started","text":"Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options: On-premise clusters AWS Docker Kubernetes You can find more information on options for running SnappyData here .","title":"Getting Started"},{"location":"#quick-performance-comparison-of-snappydata-vs-apache-spark","text":"If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this test using the Spark Shell.","title":"Quick Performance Comparison of SnappyData vs Apache Spark"},{"location":"#other-relevant-content","text":"Paper on Snappydata (Community Edition of TIBCO ComputeDB) at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems. Another early Paper that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016. TPC-H benchmark comparing Apache Spark with SnappyData Checkout the SnappyData blog for developer content TIBCO community page for the latest info.","title":"Other Relevant content"},{"location":"#community-support","text":"We monitor the following channels comments/questions: Stackoverflow Slack Gitter Mailing List Reddit JIRA","title":"Community Support"},{"location":"#link-with-snappydata-distribution","text":"","title":"Link with SnappyData Distribution"},{"location":"#using-maven-dependency","text":"SnappyData's artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates: groupId: io.snappydata artifactId: snappydata-cluster_2.11 version: 1.3.1","title":"Using Maven Dependency"},{"location":"#using-sbt-dependency","text":"If you are using SBT, add this line to your build.sbt for core SnappyData artifacts: libraryDependencies += \"io.snappydata\" % \"snappydata-core_2.11\" % \"1.3.1\" For additions related to SnappyData cluster, use: libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" You can find more specific SnappyData artifacts here Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1 ), it may be due an issue with its pom file. As a workaround, you can add the below code to your build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 .","title":"Using SBT Dependency"},{"location":"#building-from-source","text":"If you would like to build SnappyData from source, refer to the documentation on building from source .","title":"Building from Source"},{"location":"#how-is-snappydata-different-than-apache-spark","text":"Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight.","title":"How is SnappyData Different than Apache Spark?"},{"location":"#the-snappydata-approach","text":"","title":"The SnappyData Approach"},{"location":"#snappy-architecture","text":"SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources. Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark.","title":"Snappy Architecture"},{"location":"#streaming-example-ad-analytics","text":"Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - Ad Analytics code example . Here is a screencast that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra.","title":"Streaming Example - Ad Analytics"},{"location":"#contributing-to-snappydata","text":"If you are interested in contributing, please visit the community page for ways in which you can help.","title":"Contributing to SnappyData"},{"location":"GettingStarted/","text":"Introduction \u00b6 SnappyData\u2122 (aka TIBCO ComputeDB\u2122) is a distributed, in-memory optimized, analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workloads. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources/formats and stream processing all in one unified cluster. One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs. SnappyData enables complex processing on large data sets in sub-second timeframes. Note SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes. Important Capabilities \u00b6 Easily discover and catalog big data sets You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more. Rich connectivity SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See Apache Spark Packages . You can also dynamically deploy connectors to a running SnappyData cluster. Virtual or in-memory data You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed. When speed is essential, applications can selectively copy the external data into memory using a single SQL command. In-memory Columnar + Row store You can choose in-memory data to be stored in any of the following forms: Columnar : The form that is compressed and designed for scanning/aggregating large data sets. Row store : The form that has an extremely fast key access or highly selective access. The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store. High performance When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent. Flexible rich data transformations External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.* Prepares data for data science Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone. Stream ingestion and liveness While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated \u2014 for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple. In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer Stream Processing with SnappyData Approximate Query Processing(AQP) When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer AQP . Access from anywhere You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1 to 2.1.3. SnappyData natively supports modern visualization tools such as TIBCO Spotfire , Tableau , and Qlikview . Downloading and Installing SnappyData \u00b6 You can download and install the latest version of SnappyData from github or you can download the enterprise version that is TIBCO ComputeDB from here . Refer to the documentation for installation steps. Getting Started \u00b6 Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options: On-premise clusters AWS Docker Kubernetes You can find more information on options for running SnappyData here . Quick Performance Comparison of SnappyData vs Apache Spark \u00b6 If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this test using the Spark Shell. Other Relevant content \u00b6 Paper on Snappydata (Community Edition of TIBCO ComputeDB) at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems. Another early Paper that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016. TPC-H benchmark comparing Apache Spark with SnappyData Checkout the SnappyData blog for developer content TIBCO community page for the latest info. Community Support \u00b6 We monitor the following channels comments/questions: Stackoverflow Slack Gitter Mailing List Reddit JIRA Link with SnappyData Distribution \u00b6 Using Maven Dependency \u00b6 SnappyData's artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates: groupId: io.snappydata artifactId: snappydata-cluster_2.11 version: 1.3.1 Using SBT Dependency \u00b6 If you are using SBT, add this line to your build.sbt for core SnappyData artifacts: libraryDependencies += \"io.snappydata\" % \"snappydata-core_2.11\" % \"1.3.1\" For additions related to SnappyData cluster, use: libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" You can find more specific SnappyData artifacts here Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1 ), it may be due an issue with its pom file. As a workaround, you can add the below code to your build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Building from Source \u00b6 If you would like to build SnappyData from source, refer to the documentation on building from source . How is SnappyData Different than Apache Spark? \u00b6 Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight. The SnappyData Approach \u00b6 Snappy Architecture \u00b6 SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources. Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark. Streaming Example - Ad Analytics \u00b6 Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - Ad Analytics code example . Here is a screencast that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra. Contributing to SnappyData \u00b6 If you are interested in contributing, please visit the community page for ways in which you can help.","title":"Introduction"},{"location":"GettingStarted/#introduction","text":"SnappyData\u2122 (aka TIBCO ComputeDB\u2122) is a distributed, in-memory optimized, analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workloads. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources/formats and stream processing all in one unified cluster. One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs. SnappyData enables complex processing on large data sets in sub-second timeframes. Note SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes.","title":"Introduction"},{"location":"GettingStarted/#important-capabilities","text":"Easily discover and catalog big data sets You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more. Rich connectivity SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See Apache Spark Packages . You can also dynamically deploy connectors to a running SnappyData cluster. Virtual or in-memory data You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed. When speed is essential, applications can selectively copy the external data into memory using a single SQL command. In-memory Columnar + Row store You can choose in-memory data to be stored in any of the following forms: Columnar : The form that is compressed and designed for scanning/aggregating large data sets. Row store : The form that has an extremely fast key access or highly selective access. The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store. High performance When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent. Flexible rich data transformations External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.* Prepares data for data science Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone. Stream ingestion and liveness While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated \u2014 for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple. In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer Stream Processing with SnappyData Approximate Query Processing(AQP) When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer AQP . Access from anywhere You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1 to 2.1.3. SnappyData natively supports modern visualization tools such as TIBCO Spotfire , Tableau , and Qlikview .","title":"Important Capabilities"},{"location":"GettingStarted/#downloading-and-installing-snappydata","text":"You can download and install the latest version of SnappyData from github or you can download the enterprise version that is TIBCO ComputeDB from here . Refer to the documentation for installation steps.","title":"Downloading and Installing SnappyData"},{"location":"GettingStarted/#getting-started","text":"Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options: On-premise clusters AWS Docker Kubernetes You can find more information on options for running SnappyData here .","title":"Getting Started"},{"location":"GettingStarted/#quick-performance-comparison-of-snappydata-vs-apache-spark","text":"If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this test using the Spark Shell.","title":"Quick Performance Comparison of SnappyData vs Apache Spark"},{"location":"GettingStarted/#other-relevant-content","text":"Paper on Snappydata (Community Edition of TIBCO ComputeDB) at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems. Another early Paper that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016. TPC-H benchmark comparing Apache Spark with SnappyData Checkout the SnappyData blog for developer content TIBCO community page for the latest info.","title":"Other Relevant content"},{"location":"GettingStarted/#community-support","text":"We monitor the following channels comments/questions: Stackoverflow Slack Gitter Mailing List Reddit JIRA","title":"Community Support"},{"location":"GettingStarted/#link-with-snappydata-distribution","text":"","title":"Link with SnappyData Distribution"},{"location":"GettingStarted/#using-maven-dependency","text":"SnappyData's artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates: groupId: io.snappydata artifactId: snappydata-cluster_2.11 version: 1.3.1","title":"Using Maven Dependency"},{"location":"GettingStarted/#using-sbt-dependency","text":"If you are using SBT, add this line to your build.sbt for core SnappyData artifacts: libraryDependencies += \"io.snappydata\" % \"snappydata-core_2.11\" % \"1.3.1\" For additions related to SnappyData cluster, use: libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" You can find more specific SnappyData artifacts here Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1 ), it may be due an issue with its pom file. As a workaround, you can add the below code to your build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 .","title":"Using SBT Dependency"},{"location":"GettingStarted/#building-from-source","text":"If you would like to build SnappyData from source, refer to the documentation on building from source .","title":"Building from Source"},{"location":"GettingStarted/#how-is-snappydata-different-than-apache-spark","text":"Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight.","title":"How is SnappyData Different than Apache Spark?"},{"location":"GettingStarted/#the-snappydata-approach","text":"","title":"The SnappyData Approach"},{"location":"GettingStarted/#snappy-architecture","text":"SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources. Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark.","title":"Snappy Architecture"},{"location":"GettingStarted/#streaming-example-ad-analytics","text":"Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - Ad Analytics code example . Here is a screencast that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra.","title":"Streaming Example - Ad Analytics"},{"location":"GettingStarted/#contributing-to-snappydata","text":"If you are interested in contributing, please visit the community page for ways in which you can help.","title":"Contributing to SnappyData"},{"location":"LICENSE/","text":"License \u00b6 The source code is distributed with Apache License 2.0. Users can download and deploy it in production. Full text of the license is below. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright \u00a9 2017-2022 TIBCO Software Inc. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"LICENSE/#license","text":"The source code is distributed with Apache License 2.0. Users can download and deploy it in production. Full text of the license is below. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright \u00a9 2017-2022 TIBCO Software Inc. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"additional_docs/","text":"RowStore User's Guide \u00b6 The RowStore User's Guide is intended for existing SQLFire or GemFire XD users who want to upgrade to SnappyData RowStore. This guide contains the migration steps for such users along with other product details. The latest document is available at http://rowstore.docs.snappydata.io . SnappyData Resources \u00b6 You can view videos, presentations, slideshows and refer to important resources about SnappyData at http://www.snappydata.io/resources .","title":"Additional docs"},{"location":"additional_docs/#rowstore-users-guide","text":"The RowStore User's Guide is intended for existing SQLFire or GemFire XD users who want to upgrade to SnappyData RowStore. This guide contains the migration steps for such users along with other product details. The latest document is available at http://rowstore.docs.snappydata.io .","title":"RowStore User's Guide"},{"location":"additional_docs/#snappydata-resources","text":"You can view videos, presentations, slideshows and refer to important resources about SnappyData at http://www.snappydata.io/resources .","title":"SnappyData Resources"},{"location":"apidocsintro/","text":"API Documentation \u00b6 Details about SnappyData Spark Extension APIs can be found here . Details of all the other API reference for SnappyData can be found here .","title":"Full API Reference Guides"},{"location":"apidocsintro/#api-documentation","text":"Details about SnappyData Spark Extension APIs can be found here . Details of all the other API reference for SnappyData can be found here .","title":"API Documentation"},{"location":"aqp_aws/","text":"Using SnappyData CloudBuilder \u00b6 CloudBuilder is a cloud-based service that allows for instant visualization of analytic query results on large datasets. Powered by the SnappyData Synopsis Data Engine ( SDE ), users interact with CloudBuilder to populate the synopsis engine with the right data sets and accelerate SQL queries by using the engine to provide latency bounded responses to large complex aggregate queries. CloudBuilder uses Apache Zeppelin as the front end notebook to display results and allows users to build powerful notebooks representing key elements of their business in a matter of minutes. The service provides a web URL that spins up a cluster instance on AWS or users can download the CloudBuilder EC2 script to configure a custom sized cluster, to create and render powerful visualizations of their big data sets with the click of a button. With CloudBuilder, you can speed up the process of understanding what your data is telling you, and move on to the task of organizing your business around those insights rapidly. In this document, the features provided by SnappyData for analyzing your data is described. It also provides details for deploying a SnappyData Cloud cluster on AWS using either the CloudFormation service or by using the EC2 scripts. Refer to the examples and guidelines provided in this document to help you create notebooks using which, you can execute SQL queries or data frame API to analyze your data. The following topics are covered in this section: Key Components Quick Start Steps The Technology Powering SnappyData CloudBuilder","title":"Using <!--iSight-Cloud-->SnappyData CloudBuilder"},{"location":"aqp_aws/#using-snappydata-cloudbuilder","text":"CloudBuilder is a cloud-based service that allows for instant visualization of analytic query results on large datasets. Powered by the SnappyData Synopsis Data Engine ( SDE ), users interact with CloudBuilder to populate the synopsis engine with the right data sets and accelerate SQL queries by using the engine to provide latency bounded responses to large complex aggregate queries. CloudBuilder uses Apache Zeppelin as the front end notebook to display results and allows users to build powerful notebooks representing key elements of their business in a matter of minutes. The service provides a web URL that spins up a cluster instance on AWS or users can download the CloudBuilder EC2 script to configure a custom sized cluster, to create and render powerful visualizations of their big data sets with the click of a button. With CloudBuilder, you can speed up the process of understanding what your data is telling you, and move on to the task of organizing your business around those insights rapidly. In this document, the features provided by SnappyData for analyzing your data is described. It also provides details for deploying a SnappyData Cloud cluster on AWS using either the CloudFormation service or by using the EC2 scripts. Refer to the examples and guidelines provided in this document to help you create notebooks using which, you can execute SQL queries or data frame API to analyze your data. The following topics are covered in this section: Key Components Quick Start Steps The Technology Powering SnappyData CloudBuilder","title":"Using SnappyData CloudBuilder"},{"location":"architecture/","text":"Architecture Overview \u00b6 This section presents a high-level overview of SnappyData\u2019s core components. It also explains how our data pipeline (as streams) are ingested into our in-memory store and subsequently interacted with and analyzed. The following topics are covered in this section: Core Components Data Ingestion Pipeline Hybrid Cluster Manager SnappyData Cluster Architecture","title":"Architecture Overview"},{"location":"architecture/#architecture-overview","text":"This section presents a high-level overview of SnappyData\u2019s core components. It also explains how our data pipeline (as streams) are ingested into our in-memory store and subsequently interacted with and analyzed. The following topics are covered in this section: Core Components Data Ingestion Pipeline Hybrid Cluster Manager SnappyData Cluster Architecture","title":"Architecture Overview"},{"location":"configuration/","text":"Configuring the Cluster \u00b6 SnappyData has three main components - Locator, Server, and Lead. The Lead node embeds a Spark driver and the Server node embeds a Spark Executor. The server node also embeds a SnappyData store. SnappyData cluster can be started with the default configurations using script sbin/snappy-start-all.sh . This script starts up a locator, one data server, and one lead node. However, SnappyData can be configured to start multiple components on different nodes. Also, each component can be configured individually using configuration files. In this section, you can learn how the components can be individually configured and also learn about various other configurations of SnappyData. The following topics are covered in this section: Configuring and Launching a SnappyData Cluster Configuring and Launching a Multi-node SnappyData Cluster Configuration Files Configuring Locators Configuring Leads Configuring Data Servers Configuring SnappyData Smart Connector Environment Settings Hadoop Provided Settings SnappyData Command Line Utility Logging List of Properties Firewalls and Connections","title":"Configuring the Cluster"},{"location":"configuration/#configuring-the-cluster","text":"SnappyData has three main components - Locator, Server, and Lead. The Lead node embeds a Spark driver and the Server node embeds a Spark Executor. The server node also embeds a SnappyData store. SnappyData cluster can be started with the default configurations using script sbin/snappy-start-all.sh . This script starts up a locator, one data server, and one lead node. However, SnappyData can be configured to start multiple components on different nodes. Also, each component can be configured individually using configuration files. In this section, you can learn how the components can be individually configured and also learn about various other configurations of SnappyData. The following topics are covered in this section: Configuring and Launching a SnappyData Cluster Configuring and Launching a Multi-node SnappyData Cluster Configuration Files Configuring Locators Configuring Leads Configuring Data Servers Configuring SnappyData Smart Connector Environment Settings Hadoop Provided Settings SnappyData Command Line Utility Logging List of Properties Firewalls and Connections","title":"Configuring the Cluster"},{"location":"experimental/","text":"Experimental Features \u00b6 SnappyData 1.3.1 provides the following features on an experimental basis. These features are included only for testing purposes and are not yet supported officially: Authorization for External Tables \u00b6 You can enable authorization of external tables by setting the system property CHECK_EXTERNAL_TABLE_AUTHZ to true when the cluster's security is enabled. System admin or the schema owner can grant or revoke the permissions on external tables to other users. For example: GRANT ALL ON <external-table> to <user>;","title":"Experimental Features"},{"location":"experimental/#experimental-features","text":"SnappyData 1.3.1 provides the following features on an experimental basis. These features are included only for testing purposes and are not yet supported officially:","title":"Experimental Features"},{"location":"experimental/#authorization-for-external-tables","text":"You can enable authorization of external tables by setting the system property CHECK_EXTERNAL_TABLE_AUTHZ to true when the cluster's security is enabled. System admin or the schema owner can grant or revoke the permissions on external tables to other users. For example: GRANT ALL ON <external-table> to <user>;","title":"Authorization for External Tables"},{"location":"imp_info/","text":"Important Information \u00b6 Important Information","title":"Important Information"},{"location":"imp_info/#important-information","text":"Important Information","title":"Important Information"},{"location":"jobs/","text":"Building Snappy applications using Spark API \u00b6 SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames. So, we recommend getting to know the concepts in SparkSQL and the DataFrame API . And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced here . In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but, can also be reliably managed on disk. SnappyContext \u00b6 A SnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's SQLContext to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to HiveContext - integrates the SQLContext functionality with the Snappy store. Using SnappyContext to create table and query data \u00b6 Below are examples to create a SnappyContext from SparkContext. Scala \u00b6 val conf = new org.apache.spark.SparkConf() .setAppName(\"ExampleTest\") .setMaster(\"local[*]\") val sc = new org.apache.spark.SparkContext(conf) // get the SnappyContext val snc = org.apache.spark.sql.SnappyContext(sc) Java \u00b6 SparkConf conf = new org.apache.spark.SparkConf() .setAppName(\"ExampleTest\") .setMaster(\"local[*]\"); JavaSparkContext sc = new JavaSparkContext(conf); // get the SnappyContext SnappyContext snc = SnappyContext.getOrCreate(sc); Python \u00b6 from pyspark.sql.snappy import SnappyContext from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(\"ExampleTest\").setMaster(\"local[*]\") sc = SparkContext(conf=conf) # get the SnappyContext snc = SnappyContext(sc) Create columnar tables using API. Other than create and drop table, rest are all based on the Spark SQL Data Source APIs. Scala \u00b6 val props1 = Map(\"BUCKETS\" -> \"8\") // Number of partitions to use in the SnappyStore case class Data(COL1: Int, COL2: Int, COL3: Int) val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sc.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snc.createDataFrame(rdd) // create a column table snc.dropTable(\"COLUMN_TABLE\", ifExists = true) // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snc.createTable(\"COLUMN_TABLE\", \"column\", dataDF.schema, props1) // append dataDF into the table dataDF.write.insertInto(\"COLUMN_TABLE\") val results1 = snc.sql(\"SELECT * FROM COLUMN_TABLE\") println(\"contents of column table are:\") results1.foreach(println) Java \u00b6 Map<String, String> props1 = new HashMap<>(); props1.put(\"buckets\", \"16\"); JavaRDD<Row> jrdd = jsc.parallelize(Arrays.asList( RowFactory.create(1,2,3), RowFactory.create(7,8,9), RowFactory.create(9,2,3), RowFactory.create(4,2,3), RowFactory.create(5,6,7) )); StructType schema = new StructType(new StructField[]{ new StructField(\"col1\", DataTypes.IntegerType, false, Metadata.empty()), new StructField(\"col2\", DataTypes.IntegerType, false, Metadata.empty()), new StructField(\"col3\", DataTypes.IntegerType, false, Metadata.empty()), }); DataFrame dataDF = snc.createDataFrame(jrdd, schema); // create a column table snc.dropTable(\"COLUMN_TABLE\", true); // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snc.createTable(\"COLUMN_TABLE\", \"column\", dataDF.schema(), props1, false); // append dataDF into the table dataDF.write().insertInto(\"COLUMN_TABLE\"); DataFrame results1 = snc.sql(\"SELECT * FROM COLUMN_TABLE\"); System.out.println(\"contents of column table are:\"); for (Row r : results1.select(\"col1\", \"col2\", \"col3\"). collect()) { System.out.println(r); } Python \u00b6 from pyspark.sql.types import * data = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)] rdd = sc.parallelize(data) schema=StructType([StructField(\"col1\", IntegerType()), StructField(\"col2\", IntegerType()), StructField(\"col3\", IntegerType())]) dataDF = snc.createDataFrame(rdd, schema) # create a column table snc.dropTable(\"COLUMN_TABLE\", True) #\"column\" is the table format (that is row or column) #dataDF.schema provides the schema for table snc.createTable(\"COLUMN_TABLE\", \"column\", dataDF.schema, True, buckets=\"16\") #append dataDF into the table dataDF.write.insertInto(\"COLUMN_TABLE\") results1 = snc.sql(\"SELECT * FROM COLUMN_TABLE\") print(\"contents of column table are:\") results1.select(\"col1\", \"col2\", \"col3\"). show() The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster was expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to documentation for row and column tables Create row tables using API, update the contents of row table // create a row format table called ROW_TABLE snc.dropTable(\"ROW_TABLE\", ifExists = true) // \"row\" is the table format // dataDF.schema provides the schema for table val props2 = Map.empty[String, String] snc.createTable(\"ROW_TABLE\", \"row\", dataDF.schema, props2) // append dataDF into the data dataDF.write.insertInto(\"ROW_TABLE\") val results2 = snc.sql(\"select * from ROW_TABLE\") println(\"contents of row table are:\") results2.foreach(println) // row tables can be mutated // for example update \"ROW_TABLE\" and set col3 to 99 where // criteria \"col3 = 3\" is true using update API snc.update(\"ROW_TABLE\", \"COL3 = 3\", org.apache.spark.sql.Row(99), \"COL3\" ) val results3 = snc.sql(\"SELECT * FROM ROW_TABLE\") println(\"contents of row table are after setting col3 = 99 are:\") results3.foreach(println) // update rows using sql update statement snc.sql(\"UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\") val results4 = snc.sql(\"SELECT * FROM ROW_TABLE\") println(\"contents of row table are after setting col1 = 100 are:\") results4.foreach(println) SnappyStreamingContext \u00b6 SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL. Below example shows how to use the SnappyStreamingContext to apply a schema to existing DStream and then query the SchemaDStream with simple SQL. It also shows the SnappyStreamingContext ability to deal with sql queries. Scala \u00b6 import org.apache.spark.sql._ import org.apache.spark.streaming._ import scala.collection.mutable import org.apache.spark.rdd._ import org.apache.spark.sql.types._ import scala.collection.immutable.Map val snsc = new SnappyStreamingContext(sc, Duration(1)) val schema = StructType(List(StructField(\"id\", IntegerType) ,StructField(\"text\", StringType))) case class ShowCaseSchemaStream (loc:Int, text:String) snsc.snappyContext.dropTable(\"streamingExample\", ifExists = true) snsc.snappyContext.createTable(\"streamingExample\", \"column\", schema, Map.empty[String, String] , false) def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i => ShowCaseSchemaStream( i, s\"Text$i\")) val dstream = snsc.queueStream[ShowCaseSchemaStream]( mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30))) val schemaDStream = snsc.createSchemaDStream(dstream ) schemaDStream.foreachDataFrame(df => { df.write.format(\"column\"). mode(SaveMode.Append). options(Map.empty[String, String]). saveAsTable(\"streamingExample\") }) snsc.start() snsc.sql(\"select count(*) from streamingExample\").show Python \u00b6 from pyspark.streaming.snappy.context import SnappyStreamingContext from pyspark.sql.types import * def rddList(start, end): return sc.parallelize(range(start, end)).map(lambda i : ( i, \"Text\" + str(i))) def saveFunction(df): df.write.format(\"column\").mode(\"append\").saveAsTable(\"streamingExample\") schema=StructType([StructField(\"loc\", IntegerType()), StructField(\"text\", StringType())]) snsc = SnappyStreamingContext(sc, 1) snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)]) snsc._snappycontext.dropTable(\"streamingExample\" , True) snsc._snappycontext.createTable(\"streamingExample\", \"column\", schema) dstream = snsc.queueStream(getQueueOfRDDs()) schemadstream = snsc.createSchemaDStream(dstream, schema) schemadstream.foreachDataFrame(lambda df: saveFunction(df)) snsc.start() snsc.sql(\"select count(*) from streamingExample\").show() !!! Note: Currently Snappy dont have Python API's added for continuous queries and SDE/Sampling. Running Spark programs inside the database \u00b6 To create a job that can be submitted through the job server, the job must implement the SnappySQLJob or SnappyStreamingJob trait. Your job looks like: Scala \u00b6 class SnappySampleJob implements SnappySQLJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ def runSnappyJob(sc: SnappyContext, jobConfig: Config): Any /** SnappyData calls this function to validate the job input and reject invalid job requests **/ def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation } Java \u00b6 class SnappySampleJob extends SnappySQLJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ public Object runSnappyJob(SnappyContext snc, Config jobConfig) {//Implementation} /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(SnappyContext snc, Config config) {//validate} } Scala \u00b6 class SnappyStreamingSampleJob implements SnappyStreamingJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any /** SnappyData calls this function to validate the job input and reject invalid job requests **/ def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation } Java \u00b6 class SnappyStreamingSampleJob extends JavaSnappyStreamingJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig) {//validate} } The Job traits are simply extensions of the SparkJob implemented by Spark JobServer . \u2022 runSnappyJob contains the implementation of the Job. The SnappyContext / SnappyStreamingContext is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts. \u2022 isValidJob allows for an initial validation of the context and any provided configuration. If the context and configuration are OK to run the job, returning spark.jobserver.SnappyJobValid will let the job execute, otherwise returning spark.jobserver.SnappyJobInvalid(reason) prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code.\u2028validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources. See examples for Spark and spark streaming jobs. SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs Submitting Jobs \u00b6 Following command submits CreateAndLoadAirlineDataJob from the examples directory. This job creates dataframes from parquet files, loads the data from dataframe into column tables and row tables and creates sample table on column table in its runJob method. The program is compiled into a jar file (quickstart.jar) and submitted to jobs server as shown below. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.CreateAndLoadAirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The utility snappy-job.sh submits the job and returns a JSON that has a jobId of this job. --lead option specifies the host name of the lead node along with the port on which it accepts jobs (8090) --app-name option specifies the name given to the submitted app --class specifies the name of the class that contains implementation of the Spark job to be run --app-jar specifies the jar file that packages the code for Spark job The status returned by the utility is shown below: { \"status\": \"STARTED\", \"result\": { \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\", \"context\": \"snappyContext1452598154529305363\" } } This job ID can be used to query the status of the running job. $ ./bin/snappy-job.sh status \\ --lead localhost:8090 \\ --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 { \"duration\": \"17.53 secs\", \"classPath\": \"io.snappydata.examples.CreateAndLoadAirlineDataJob\", \"startTime\": \"2016-01-12T16:59:14.746+05:30\", \"context\": \"snappyContext1452598154529305363\", \"result\": \"See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\", \"status\": \"FINISHED\", \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\" } Once the tables are created, they can be queried by firing another job. Please refer to AirlineDataJob from examples for the implementation of the job. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.AirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results. Python users can also submit the python script using spark-submit in split cluster mode. For example below script can be used to read the data loaded by the CreateAndLoadAirlineDataJob. \"spark.snappydata.connection\" property denotes the locator url of the snappy cluster and it is used to connect to the snappy cluster. $ ./bin/spark-submit \\ --master spark://pnq-user02:7077 \\ --conf spark.snappydata.connection=localhost:10334 \\ --conf spark.ui.port=4042 python/examples/AirlineDataPythonApp.py Streaming Jobs \u00b6 An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying --stream as an option to the submit command. This option will cause creation of a new SnappyStreamingContext before the job is submitted. Alternatively, user may specify the name of an existing/pre-created streaming context as --context <context-name> with the submit command. For example, TwitterPopularTagsJob from the examples directory can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, top 10 popular tweets. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.TwitterPopularTagsJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\ --stream { \"status\": \"STARTED\", \"result\": { \"jobId\": \"982ac142-3550-41e1-aace-6987cb39fec8\", \"context\": \"snappyStreamingContext1463987084945028747\" } } User needs to stop the currently running streaming job followed by its streaming context if the user intends to submit another streaming job with a new streaming context. $ ./bin/snappy-job.sh stop \\ --lead localhost:8090 \\ --job-id 982ac142-3550-41e1-aace-6987cb39fec8 $ ./bin/snappy-job.sh listcontexts \\ --lead localhost:8090 [\"snappyContext1452598154529305363\", \"snappyStreamingContext1463987084945028747\", \"snappyStreamingContext\"] $ ./bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747 \\ --lead localhost:8090","title":"Jobs"},{"location":"jobs/#building-snappy-applications-using-spark-api","text":"SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames. So, we recommend getting to know the concepts in SparkSQL and the DataFrame API . And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced here . In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but, can also be reliably managed on disk.","title":"Building Snappy applications using Spark API"},{"location":"jobs/#snappycontext","text":"A SnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's SQLContext to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to HiveContext - integrates the SQLContext functionality with the Snappy store.","title":"SnappyContext"},{"location":"jobs/#using-snappycontext-to-create-table-and-query-data","text":"Below are examples to create a SnappyContext from SparkContext.","title":"Using SnappyContext to create table and query data"},{"location":"jobs/#scala","text":"val conf = new org.apache.spark.SparkConf() .setAppName(\"ExampleTest\") .setMaster(\"local[*]\") val sc = new org.apache.spark.SparkContext(conf) // get the SnappyContext val snc = org.apache.spark.sql.SnappyContext(sc)","title":"Scala"},{"location":"jobs/#java","text":"SparkConf conf = new org.apache.spark.SparkConf() .setAppName(\"ExampleTest\") .setMaster(\"local[*]\"); JavaSparkContext sc = new JavaSparkContext(conf); // get the SnappyContext SnappyContext snc = SnappyContext.getOrCreate(sc);","title":"Java"},{"location":"jobs/#python","text":"from pyspark.sql.snappy import SnappyContext from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(\"ExampleTest\").setMaster(\"local[*]\") sc = SparkContext(conf=conf) # get the SnappyContext snc = SnappyContext(sc) Create columnar tables using API. Other than create and drop table, rest are all based on the Spark SQL Data Source APIs.","title":"Python"},{"location":"jobs/#scala_1","text":"val props1 = Map(\"BUCKETS\" -> \"8\") // Number of partitions to use in the SnappyStore case class Data(COL1: Int, COL2: Int, COL3: Int) val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sc.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snc.createDataFrame(rdd) // create a column table snc.dropTable(\"COLUMN_TABLE\", ifExists = true) // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snc.createTable(\"COLUMN_TABLE\", \"column\", dataDF.schema, props1) // append dataDF into the table dataDF.write.insertInto(\"COLUMN_TABLE\") val results1 = snc.sql(\"SELECT * FROM COLUMN_TABLE\") println(\"contents of column table are:\") results1.foreach(println)","title":"Scala"},{"location":"jobs/#java_1","text":"Map<String, String> props1 = new HashMap<>(); props1.put(\"buckets\", \"16\"); JavaRDD<Row> jrdd = jsc.parallelize(Arrays.asList( RowFactory.create(1,2,3), RowFactory.create(7,8,9), RowFactory.create(9,2,3), RowFactory.create(4,2,3), RowFactory.create(5,6,7) )); StructType schema = new StructType(new StructField[]{ new StructField(\"col1\", DataTypes.IntegerType, false, Metadata.empty()), new StructField(\"col2\", DataTypes.IntegerType, false, Metadata.empty()), new StructField(\"col3\", DataTypes.IntegerType, false, Metadata.empty()), }); DataFrame dataDF = snc.createDataFrame(jrdd, schema); // create a column table snc.dropTable(\"COLUMN_TABLE\", true); // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snc.createTable(\"COLUMN_TABLE\", \"column\", dataDF.schema(), props1, false); // append dataDF into the table dataDF.write().insertInto(\"COLUMN_TABLE\"); DataFrame results1 = snc.sql(\"SELECT * FROM COLUMN_TABLE\"); System.out.println(\"contents of column table are:\"); for (Row r : results1.select(\"col1\", \"col2\", \"col3\"). collect()) { System.out.println(r); }","title":"Java"},{"location":"jobs/#python_1","text":"from pyspark.sql.types import * data = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)] rdd = sc.parallelize(data) schema=StructType([StructField(\"col1\", IntegerType()), StructField(\"col2\", IntegerType()), StructField(\"col3\", IntegerType())]) dataDF = snc.createDataFrame(rdd, schema) # create a column table snc.dropTable(\"COLUMN_TABLE\", True) #\"column\" is the table format (that is row or column) #dataDF.schema provides the schema for table snc.createTable(\"COLUMN_TABLE\", \"column\", dataDF.schema, True, buckets=\"16\") #append dataDF into the table dataDF.write.insertInto(\"COLUMN_TABLE\") results1 = snc.sql(\"SELECT * FROM COLUMN_TABLE\") print(\"contents of column table are:\") results1.select(\"col1\", \"col2\", \"col3\"). show() The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster was expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to documentation for row and column tables Create row tables using API, update the contents of row table // create a row format table called ROW_TABLE snc.dropTable(\"ROW_TABLE\", ifExists = true) // \"row\" is the table format // dataDF.schema provides the schema for table val props2 = Map.empty[String, String] snc.createTable(\"ROW_TABLE\", \"row\", dataDF.schema, props2) // append dataDF into the data dataDF.write.insertInto(\"ROW_TABLE\") val results2 = snc.sql(\"select * from ROW_TABLE\") println(\"contents of row table are:\") results2.foreach(println) // row tables can be mutated // for example update \"ROW_TABLE\" and set col3 to 99 where // criteria \"col3 = 3\" is true using update API snc.update(\"ROW_TABLE\", \"COL3 = 3\", org.apache.spark.sql.Row(99), \"COL3\" ) val results3 = snc.sql(\"SELECT * FROM ROW_TABLE\") println(\"contents of row table are after setting col3 = 99 are:\") results3.foreach(println) // update rows using sql update statement snc.sql(\"UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\") val results4 = snc.sql(\"SELECT * FROM ROW_TABLE\") println(\"contents of row table are after setting col1 = 100 are:\") results4.foreach(println)","title":"Python"},{"location":"jobs/#snappystreamingcontext","text":"SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL. Below example shows how to use the SnappyStreamingContext to apply a schema to existing DStream and then query the SchemaDStream with simple SQL. It also shows the SnappyStreamingContext ability to deal with sql queries.","title":"SnappyStreamingContext"},{"location":"jobs/#scala_2","text":"import org.apache.spark.sql._ import org.apache.spark.streaming._ import scala.collection.mutable import org.apache.spark.rdd._ import org.apache.spark.sql.types._ import scala.collection.immutable.Map val snsc = new SnappyStreamingContext(sc, Duration(1)) val schema = StructType(List(StructField(\"id\", IntegerType) ,StructField(\"text\", StringType))) case class ShowCaseSchemaStream (loc:Int, text:String) snsc.snappyContext.dropTable(\"streamingExample\", ifExists = true) snsc.snappyContext.createTable(\"streamingExample\", \"column\", schema, Map.empty[String, String] , false) def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i => ShowCaseSchemaStream( i, s\"Text$i\")) val dstream = snsc.queueStream[ShowCaseSchemaStream]( mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30))) val schemaDStream = snsc.createSchemaDStream(dstream ) schemaDStream.foreachDataFrame(df => { df.write.format(\"column\"). mode(SaveMode.Append). options(Map.empty[String, String]). saveAsTable(\"streamingExample\") }) snsc.start() snsc.sql(\"select count(*) from streamingExample\").show","title":"Scala"},{"location":"jobs/#python_2","text":"from pyspark.streaming.snappy.context import SnappyStreamingContext from pyspark.sql.types import * def rddList(start, end): return sc.parallelize(range(start, end)).map(lambda i : ( i, \"Text\" + str(i))) def saveFunction(df): df.write.format(\"column\").mode(\"append\").saveAsTable(\"streamingExample\") schema=StructType([StructField(\"loc\", IntegerType()), StructField(\"text\", StringType())]) snsc = SnappyStreamingContext(sc, 1) snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)]) snsc._snappycontext.dropTable(\"streamingExample\" , True) snsc._snappycontext.createTable(\"streamingExample\", \"column\", schema) dstream = snsc.queueStream(getQueueOfRDDs()) schemadstream = snsc.createSchemaDStream(dstream, schema) schemadstream.foreachDataFrame(lambda df: saveFunction(df)) snsc.start() snsc.sql(\"select count(*) from streamingExample\").show() !!! Note: Currently Snappy dont have Python API's added for continuous queries and SDE/Sampling.","title":"Python"},{"location":"jobs/#running-spark-programs-inside-the-database","text":"To create a job that can be submitted through the job server, the job must implement the SnappySQLJob or SnappyStreamingJob trait. Your job looks like:","title":"Running Spark programs inside the database"},{"location":"jobs/#scala_3","text":"class SnappySampleJob implements SnappySQLJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ def runSnappyJob(sc: SnappyContext, jobConfig: Config): Any /** SnappyData calls this function to validate the job input and reject invalid job requests **/ def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation }","title":"Scala"},{"location":"jobs/#java_2","text":"class SnappySampleJob extends SnappySQLJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ public Object runSnappyJob(SnappyContext snc, Config jobConfig) {//Implementation} /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(SnappyContext snc, Config config) {//validate} }","title":"Java"},{"location":"jobs/#scala_4","text":"class SnappyStreamingSampleJob implements SnappyStreamingJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any /** SnappyData calls this function to validate the job input and reject invalid job requests **/ def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation }","title":"Scala"},{"location":"jobs/#java_3","text":"class SnappyStreamingSampleJob extends JavaSnappyStreamingJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig) {//validate} } The Job traits are simply extensions of the SparkJob implemented by Spark JobServer . \u2022 runSnappyJob contains the implementation of the Job. The SnappyContext / SnappyStreamingContext is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts. \u2022 isValidJob allows for an initial validation of the context and any provided configuration. If the context and configuration are OK to run the job, returning spark.jobserver.SnappyJobValid will let the job execute, otherwise returning spark.jobserver.SnappyJobInvalid(reason) prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code.\u2028validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources. See examples for Spark and spark streaming jobs. SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs","title":"Java"},{"location":"jobs/#submitting-jobs","text":"Following command submits CreateAndLoadAirlineDataJob from the examples directory. This job creates dataframes from parquet files, loads the data from dataframe into column tables and row tables and creates sample table on column table in its runJob method. The program is compiled into a jar file (quickstart.jar) and submitted to jobs server as shown below. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.CreateAndLoadAirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The utility snappy-job.sh submits the job and returns a JSON that has a jobId of this job. --lead option specifies the host name of the lead node along with the port on which it accepts jobs (8090) --app-name option specifies the name given to the submitted app --class specifies the name of the class that contains implementation of the Spark job to be run --app-jar specifies the jar file that packages the code for Spark job The status returned by the utility is shown below: { \"status\": \"STARTED\", \"result\": { \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\", \"context\": \"snappyContext1452598154529305363\" } } This job ID can be used to query the status of the running job. $ ./bin/snappy-job.sh status \\ --lead localhost:8090 \\ --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 { \"duration\": \"17.53 secs\", \"classPath\": \"io.snappydata.examples.CreateAndLoadAirlineDataJob\", \"startTime\": \"2016-01-12T16:59:14.746+05:30\", \"context\": \"snappyContext1452598154529305363\", \"result\": \"See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\", \"status\": \"FINISHED\", \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\" } Once the tables are created, they can be queried by firing another job. Please refer to AirlineDataJob from examples for the implementation of the job. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.AirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results. Python users can also submit the python script using spark-submit in split cluster mode. For example below script can be used to read the data loaded by the CreateAndLoadAirlineDataJob. \"spark.snappydata.connection\" property denotes the locator url of the snappy cluster and it is used to connect to the snappy cluster. $ ./bin/spark-submit \\ --master spark://pnq-user02:7077 \\ --conf spark.snappydata.connection=localhost:10334 \\ --conf spark.ui.port=4042 python/examples/AirlineDataPythonApp.py","title":"Submitting Jobs"},{"location":"jobs/#streaming-jobs","text":"An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying --stream as an option to the submit command. This option will cause creation of a new SnappyStreamingContext before the job is submitted. Alternatively, user may specify the name of an existing/pre-created streaming context as --context <context-name> with the submit command. For example, TwitterPopularTagsJob from the examples directory can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, top 10 popular tweets. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.TwitterPopularTagsJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\ --stream { \"status\": \"STARTED\", \"result\": { \"jobId\": \"982ac142-3550-41e1-aace-6987cb39fec8\", \"context\": \"snappyStreamingContext1463987084945028747\" } } User needs to stop the currently running streaming job followed by its streaming context if the user intends to submit another streaming job with a new streaming context. $ ./bin/snappy-job.sh stop \\ --lead localhost:8090 \\ --job-id 982ac142-3550-41e1-aace-6987cb39fec8 $ ./bin/snappy-job.sh listcontexts \\ --lead localhost:8090 [\"snappyContext1452598154529305363\", \"snappyStreamingContext1463987084945028747\", \"snappyStreamingContext\"] $ ./bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747 \\ --lead localhost:8090","title":"Streaming Jobs"},{"location":"kubernetes/","text":"Setting up Cluster on Kubernetes \u00b6 Kubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes. The following sections are included in this topic: Prerequisites Getting Access to Kubernetes cluster Deploying SnappyData Chart on Kubernetes Setting up PKS Environment for Kubernetes Interacting with SnappyData Cluster on Kubernetes List of Configuration Parameters for SnappyData Chart Kubernetes Objects Used in SnappyData Chart Accessing Logs and Configuring Log Level Mounting ConfigMaps Prerequisites \u00b6 The following prerequisites must be met to deploy SnappyData on Kubernetes: Kubernetes cluster A running Kubernetes cluster of version 1.9 or higher. SnappyData has been tested on Google Container Engine(GKE) as well as on Pivotal Container Service (PKS). If Kubernetes cluster is not available, you can set it up as mentioned here . Helm tool Helm tool must be deployed in the Kubernetes environment. You can follow the instructions here to deploy Helm in your Kubernetes enviroment. Docker image Helm charts use Docker image to launch the SnappyData cluster on Kubernetes. You can refer to these steps to build and publish your Docker image for SnappyData. TIBCO does not provide a Docker image for SnappyData. Getting Access to Kubernetes Cluster \u00b6 If you would like to deploy Kubernetes on-premises, you can use any of the following options: Option 1 - PKS \u00b6 PKS on vSphere: Follow these instructions PKS on GCP: Follow these instructions Create a Kubernetes cluster using PKS CLI : After PKS is setup you will need to create a Kubernetes cluster as described here Option 2 - Google Cloud Platform (GCP) \u00b6 Login to your Google account and go to the Cloud console to launch a GKE cluster. Steps to perform after Kubernetes cluster is available: If using PKS, you must install the PKS command line tool. See instructions here . Install kubectl on your local development machine and configure access to the kubernetes/PKS cluster. See instructions for kubectl here . If you are using Google cloud, you will find instructions for setting up Google Cloud SDK ('gcloud') along with kubectl here . Deploying SnappyData on Kubernetes \u00b6 SnappyData Helm chart is used to deploy SnappyData on Kubernetes. It uses Kubernetes statefulsets to launch the locator, lead, and server members. To deploy SnappyData on Kubernetes: Clone the spark-on-k8s repository and change to charts directory. git clone https://github.com/TIBCOSoftware/spark-on-k8s cd spark-on-k8s/charts Edit the snappydata > values.yaml file to configure in the SnappyData chart. Specify the details of your SnappyData Docker image as mentioned in the example below. Replace values for image and tag appropriatly with your Dockerhub registry name, image name and tag . image: your-dockerhub-registry/snappydata-docker-image imageTag: 1.2 imagePullPolicy: IfNotPresent To pull a Docker image from a private registry, create a secret by following steps as mentioned here and specify the name of the secret in values.yaml as shown below. Note that, secret must be created in the namespace in which SnappyData will be deployed (namespace \"snappy\" in this case) imagePullSecrets: secretName Optionally, you can edit the snappydata > values.yaml file to change the default configurations in the SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer List of Configuration Parameters for SnappyData Chart Install the snappydata chart using the following command: helm install --name snappydata --namespace snappy ./snappydata/ The above command installs the SnappyData chart in a namespace called snappy and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console. By default, SnappyData Helm chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints. You can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions here . SnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted. Interacting with SnappyData Cluster on Kubernetes \u00b6 You can interact with the SnappyData cluster on Kuberenetes in the same manner as you interact with a SnappyData cluster that runs locally or on-premise. All you require is the host IP address of the locator and the lead with their respective ports numbers. To find the IP addresses and port numbers of the SnappyData processes, use command kubectl get svc --namespace=snappy . In the output , three services namely snappydata-leader-public , snappydata-locator-public and snappydata-server-public of type LoadBalancer are seen which expose the endpoints for locator, lead, and server respectively. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use. snappydata-leader-public service exposes port 5050 for SnappyData Monitoring Console, port 10000 for Hive Thrift server and port 8090 to accept SnappyData jobs . snappydata-locator-public service exposes port 1527 to accept JDBC/ODBC connections . You can do the following on the SnappyData cluster that is deployed on Kubernetes: Access SnappyData Monitoring Console Connect SnappyData using JDBC Driver Execute Queries Submit a SnappyData Job Stop SnappyData Cluster on Kubernetes Accessing SnappyData Monitoring Console \u00b6 The dashboards on the SnappyData Monitoring Console can be accessed using snappydata-leader-public service. To view the dashboard, type the URL in the web browser in the format: externalIp:5050 . Replace externalip with the external IP address of the snappydata-leader-public service. To access SnappyData Monitoring Console in Kubernetes: Check the SnappyData services running in the Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of the snappydata-leader-public service as shown in the following image: Type externalIp:5050 in the browser. Here you must replace externalip with the external IP address of the leader-public service. For example, 35.232.102.51:5050. Connecting SnappyData Using JDBC Driver \u00b6 For Kubernetes deployments, JDBC clients can connect to SnappyData cluster using the JDBC URL that is derived from the snappydata-locator-public service. To connect to SnappyData using JDBC driver in Kubernetes: Check the SnappyData services running in Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of the snappydata-locator-public service and the port number for external connections as shown in the following image: Use the external IP address and port of the snappydata-locator-public services to connect to SnappyData cluster using JDBC connections. For example, based on the above output, the JDBC URL to be used will be jdbc:snappydata://104.198.47.162:1527/ You can refer to SnappyData documentation for an example of JDBC program and for instructions on how to obtain JDBC driver using Maven/SBT co-ordinates. Executing Queries Using SnappyData Shell \u00b6 You can use SnappyData shell to connect to SnappyData and execute your queries. You can simply connect to one of the pods in the cluster and use the SnappyData Shell. Alternatively, you can download the SnappyData distribution from SnappyData github releases . SnappyData shell need not run within the Kubernetes cluster. To execute queries in Kubernetes deployment: Check the SnappyData services running in the Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of the snappydata-locator-public services and the port number for external connections as shown in the following image: Launch SnappyData shell and then create tables and execute queries. Following is an example of executing queries using SnappyData shell. # Connect to snappy-shell bin/snappy snappy> connect client '104.198.47.162:1527'; # Create tables and execute queries snappy> create table t1(col1 int, col2 int) using column; snappy> insert into t1 values(1, 1); 1 row inserted/updated/deleted Submitting a SnappyData Job \u00b6 Refer to the How Tos section in SnappyData documentation to understand how to submit SnappyData jobs. However, for submitting a SnappyData job in Kubernetes deployment, you need to use the snappydata-leader-public service that exposes port 8090 to run the jobs. To submit a SnappyData job in Kubernetes deployment: Check the SnappyData services running in Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of snappydata-leader-public service which must be noted. Change to SnappyData product directory. cd $SNAPPY_HOME Submit the job using the external IP of the snappydata-leader-public service and the port number 8090 in the --lead option. Following is an example of submitting a SnappyData Job: bin/snappy-job.sh submit --app-name CreatePartitionedRowTable --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable --app-jar examples/jars/quickstart.jar --lead 35.232.102.51:8090 Stopping the SnappyData Cluster on Kubernetes \u00b6 To stop the SnappyData cluster on Kubernetes, you must delete the SnappyData Helm chart using the helm delete command. $ helm delete --purge snappydata The dynamically provisioned volumes and the data in it is retained, even if the chart deployment is deleted. Note If the chart is deployed again with the same chart name and if the volume exists, then the existing volume is used instead of provisioning a new volume. List of Configuration Parameters for SnappyData Chart \u00b6 You can modify the values.yaml file to configure the SnappyData chart. The following table lists the configuration parameters available for this chart: Parameter Description Default image Docker repo from which the SnappyData Docker image is pulled. snappydatainc/snappydata imageTag Tag of the SnappyData Docker image that is pulled. imagePullPolicy Pull policy for the image. IfNotPresent imagePullSecrets Secret name to be used to pull image from a private registry locators.conf List of the configuration options that is passed to the locators. locators.resources Resource configuration for the locator Pods. User can configure CPU/memory requests and limit the usage. locators.requests.memory is set to 1024Mi . locators.persistence.storageClass Storage class that is used while dynamically provisioning a volume. Default value is not defined so default storage class for the cluster is chosen. locators.persistence.accessMode Access mode that is used for the dynamically provisioned volume. ReadWriteOnce locators.persistence.size Size of the dynamically provisioned volume. 10Gi servers.replicaCount Number of servers that are started in a SnappyData cluster. 2 servers.conf List of the configuration options that are passed to the servers. servers.resources Resource configuration for the server Pods. You can configure CPU/memory requests and limit the usage. servers.requests.memory is set to 4096Mi servers.persistence.storageClass Storage class that is used while dynamically provisioning a volume. Default value is not defined so default storage class for the cluster will be chosen. servers.persistence.accessMode Access mode for the dynamically provisioned volume. ReadWriteOnce servers.persistence.size Size of the dynamically provisioned volume. 10Gi leaders.conf List of configuration options that can be passed to the leaders. leaders.resources Resource configuration for the server pods. You can configure CPU/memory requests and limits the usage. leaders.requests.memory is set to 4096Mi leaders.persistence.storageClass Storage class that is used while dynamically provisioning a volume. Default value is not defined so default storage class for the cluster will be chosen. leaders.persistence.accessMode Access mode for the dynamically provisioned volume. ReadWriteOnce leaders.persistence.size Size of the dynamically provisioned volume. 10Gi The following sample shows the configuration used to start four servers each with a heap size of 2048 MB: servers: replicaCount: 4 ## config options for servers conf: \"-heap-size=2048m\" You can specify SnappyData configuration parameters in the servers.conf , locators.conf , and leaders.conf attributes for servers, locators, and leaders respectively. Kubernetes Objects Used in SnappyData Chart \u00b6 This section provides details about the following Kubernetes objects that are used in SnappyData Chart: Statefulsets for Servers, Leaders, and Locators Services that Expose External Endpoints Persistent Volumes Statefulsets for Servers, Leaders, and Locators \u00b6 Kubernetes statefulsets are used to manage stateful applications. Statefulsets provide many benefits such as stable and unique network identifiers, stable persistent storage, ordered deployment and scaling, graceful deletion, and rolling updates. SnappyData Helm chart deploys statefulsets for servers, leaders, and locators. By default the chart deploys two data servers, one locator, and one leader. Upon deletion of the Helm deployment, each pod gracefully terminates the SnappyData process that is running on it. Services that Expose External Endpoints \u00b6 SnappyData Helm chart creates services to allow you to make JDBC connections, execute Spark jobs, and access SnappyData Monitoring Console etc. Services of the type LoadBalancer have external IP address assigned and can be used to connect from outside of Kubernetes cluster. To check the service created for SnappyData deployment, use command kubectl get svc --namespace=snappy . The following output is displayed: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE snappydata-leader ClusterIP None <none> 5050/TCP 5m snappydata-leader-public LoadBalancer 10.51.255.175 35.232.102.51 5050:31964/TCP,8090:32700/TCP,3768:32575/TCP 5m snappydata-locator ClusterIP None <none> 10334/TCP,1527/TCP 5m snappydata-locator-public LoadBalancer 10.51.241.224 104.198.47.162 1527:31957/TCP 5m snappydata-server ClusterIP None <none> 1527/TCP 5m snappydata-server-public LoadBalancer 10.51.248.27 35.232.16.4 1527:31853/TCP 5m In the above output, three services namely snappydata-leader-public , snappydata-locator-public and snappydata-server-public of type LoadBalancer are created. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use. snappydata-leader-public service exposes port 5050 for SnappyData Monitoring Console and port 8090 to accept SnappyData jobs. snappydata-locator-public service exposes port 1527 to accept JDBC connections. Persistent Volumes \u00b6 A pod in a SnappyData deployment has a persistent volume mounted on it. This volume is dynamically provisioned and is used to store data directory for SnappyData. On each pod, the persistent volume is mounted on path /opt/snappydata/work . These volumes and the data in it is retained even if the chart deployment is deleted. Accessing Logs \u00b6 You can access the logs when the SnappyData cluster is running as well as when the SnappyData cluster is not running . Accessing Logs When SnappyData Cluster is Running \u00b6 When a SnappyData cluster is running, you can open a session for a pod using kubectl command and then view the logs. The following example shows how to access logs of snappydata-server-0 : # Connect to snappydata-server-0 pod and open a shell. $ kubectl exec -it snappydata-server-0 --namespace snappy -- /bin/bash # Switch to Snappydata work directory and view the logs. $ cd /opt/snappydata/work $ ls Accessing Logs When SnappyData Cluster is not Running \u00b6 When SnappyData cluster is not running, you can access the volumes used in SnappyData with a utility script snappy-debug-pod.sh located in the utils directory of Spark on k8s repository. This script launches a pod in the Kubernetes cluster with persistent volumes, specified via --pvc option, mounted on it and then returns a shell prompt. Volumes are mounted on the path starting with /data0 (volume1 on /data0 and so on) . In the following example, the names of the persistent volume claims used by the cluster are retrieved and passed to the snappy-debug-pod.sh script to be mounted on the pod. # Get the names of persistent volume claims used by SnappyData cluster installed in a namespace # called *snappy*. The PVCs used by SnappyData are prefixed with 'snappy-disk-claim-'. $ kubectl get pvc --namespace snappy NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE snappy-disk-claim-snappydata-leader-0 Bound pvc-17cf9834-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d snappy-disk-claim-snappydata-locator-0 Bound pvc-17d75411-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d snappy-disk-claim-snappydata-server-0 Bound pvc-17de4f1a-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d snappy-disk-claim-snappydata-server-1 Bound pvc-226d778d-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d # To view logs for server-0 and server-1, use PVCs 'snappy-disk-claim-snappydata-server-0' and snappy-disk-claim-snappydata-server-1' $ ./utils/snappy-debug-pod.sh --pvc snappy-disk-claim-snappydata-server-0,snappy-disk-claim-snappydata-server-1 --namespace snappy Volume for snappy-disk-claim-snappydata-server-0 will be mounted on /data0 Volume for snappy-disk-claim-snappydata-server-1 will be mounted on /data1 Launching the POD If you don't see a command prompt, try pressing enter. bash-4.1# In the above example, the second command opens a session with bash prompt for the pod on which the volumes corresponding to the mentioned PVCs are mounted on paths such as /data0 , /data1 and so on. You can then examine the logs in these mounted paths. For example: bash-4.1# ls /data1 lost+found members.txt snappydata-server-1 bash-4.1# ls /data0 lost+found members.txt snappydata-server-0 bash-4.1# ls /data0/snappydata-server-0/ bash-4.1# ls /data0/snappydata-server-0/*.*log Configuring the Log Level \u00b6 You can provide a log4j2.properties file while installing the SnappyData Helm chart. A template file log4j2.properties.template is provided in the charts/snappydata/conf/ directory. This template file can be renamed and used to configure log level as shown in the following example: $ cd charts/snappydata/conf/ # copy the template file and edit it to configure log level $ cp log4j2.properties.template log4j2.properties When SnappyData chart is installed, the log4.properties file will be used to configure the log level. Mounting ConfigMaps \u00b6 Files created in charts/snappydata/conf/ are mounted on SnappyData server, lead, and locator pods as configmaps and copied into the product's conf directory. This is useful to make configuration files (such as spark-env.sh, metrics.properties etc.) available to the product. Template files are provided in the charts/snappydata/conf/ . These template files can be renamed and edited to provide configuration. For example: $ cd snappydata/conf/ $ cp spark-env.sh.template spark-env.sh # now modify the spark-env.sh to specify configuration","title":"Setting-up SnappyData Cluster on Kubernetes"},{"location":"kubernetes/#setting-up-cluster-on-kubernetes","text":"Kubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes. The following sections are included in this topic: Prerequisites Getting Access to Kubernetes cluster Deploying SnappyData Chart on Kubernetes Setting up PKS Environment for Kubernetes Interacting with SnappyData Cluster on Kubernetes List of Configuration Parameters for SnappyData Chart Kubernetes Objects Used in SnappyData Chart Accessing Logs and Configuring Log Level Mounting ConfigMaps","title":"Setting up Cluster on Kubernetes"},{"location":"kubernetes/#prerequisites","text":"The following prerequisites must be met to deploy SnappyData on Kubernetes: Kubernetes cluster A running Kubernetes cluster of version 1.9 or higher. SnappyData has been tested on Google Container Engine(GKE) as well as on Pivotal Container Service (PKS). If Kubernetes cluster is not available, you can set it up as mentioned here . Helm tool Helm tool must be deployed in the Kubernetes environment. You can follow the instructions here to deploy Helm in your Kubernetes enviroment. Docker image Helm charts use Docker image to launch the SnappyData cluster on Kubernetes. You can refer to these steps to build and publish your Docker image for SnappyData. TIBCO does not provide a Docker image for SnappyData.","title":"Prerequisites"},{"location":"kubernetes/#getting-access-to-kubernetes-cluster","text":"If you would like to deploy Kubernetes on-premises, you can use any of the following options:","title":"Getting Access to Kubernetes Cluster"},{"location":"kubernetes/#option-1-pks","text":"PKS on vSphere: Follow these instructions PKS on GCP: Follow these instructions Create a Kubernetes cluster using PKS CLI : After PKS is setup you will need to create a Kubernetes cluster as described here","title":"Option 1 - PKS"},{"location":"kubernetes/#option-2-google-cloud-platform-gcp","text":"Login to your Google account and go to the Cloud console to launch a GKE cluster. Steps to perform after Kubernetes cluster is available: If using PKS, you must install the PKS command line tool. See instructions here . Install kubectl on your local development machine and configure access to the kubernetes/PKS cluster. See instructions for kubectl here . If you are using Google cloud, you will find instructions for setting up Google Cloud SDK ('gcloud') along with kubectl here .","title":"Option 2 - Google Cloud Platform (GCP)"},{"location":"kubernetes/#deploying-snappydata-on-kubernetes","text":"SnappyData Helm chart is used to deploy SnappyData on Kubernetes. It uses Kubernetes statefulsets to launch the locator, lead, and server members. To deploy SnappyData on Kubernetes: Clone the spark-on-k8s repository and change to charts directory. git clone https://github.com/TIBCOSoftware/spark-on-k8s cd spark-on-k8s/charts Edit the snappydata > values.yaml file to configure in the SnappyData chart. Specify the details of your SnappyData Docker image as mentioned in the example below. Replace values for image and tag appropriatly with your Dockerhub registry name, image name and tag . image: your-dockerhub-registry/snappydata-docker-image imageTag: 1.2 imagePullPolicy: IfNotPresent To pull a Docker image from a private registry, create a secret by following steps as mentioned here and specify the name of the secret in values.yaml as shown below. Note that, secret must be created in the namespace in which SnappyData will be deployed (namespace \"snappy\" in this case) imagePullSecrets: secretName Optionally, you can edit the snappydata > values.yaml file to change the default configurations in the SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer List of Configuration Parameters for SnappyData Chart Install the snappydata chart using the following command: helm install --name snappydata --namespace snappy ./snappydata/ The above command installs the SnappyData chart in a namespace called snappy and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console. By default, SnappyData Helm chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints. You can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions here . SnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted.","title":"Deploying SnappyData on Kubernetes"},{"location":"kubernetes/#interacting-with-snappydata-cluster-on-kubernetes","text":"You can interact with the SnappyData cluster on Kuberenetes in the same manner as you interact with a SnappyData cluster that runs locally or on-premise. All you require is the host IP address of the locator and the lead with their respective ports numbers. To find the IP addresses and port numbers of the SnappyData processes, use command kubectl get svc --namespace=snappy . In the output , three services namely snappydata-leader-public , snappydata-locator-public and snappydata-server-public of type LoadBalancer are seen which expose the endpoints for locator, lead, and server respectively. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use. snappydata-leader-public service exposes port 5050 for SnappyData Monitoring Console, port 10000 for Hive Thrift server and port 8090 to accept SnappyData jobs . snappydata-locator-public service exposes port 1527 to accept JDBC/ODBC connections . You can do the following on the SnappyData cluster that is deployed on Kubernetes: Access SnappyData Monitoring Console Connect SnappyData using JDBC Driver Execute Queries Submit a SnappyData Job Stop SnappyData Cluster on Kubernetes","title":"Interacting with SnappyData Cluster on Kubernetes"},{"location":"kubernetes/#accessing-snappydata-monitoring-console","text":"The dashboards on the SnappyData Monitoring Console can be accessed using snappydata-leader-public service. To view the dashboard, type the URL in the web browser in the format: externalIp:5050 . Replace externalip with the external IP address of the snappydata-leader-public service. To access SnappyData Monitoring Console in Kubernetes: Check the SnappyData services running in the Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of the snappydata-leader-public service as shown in the following image: Type externalIp:5050 in the browser. Here you must replace externalip with the external IP address of the leader-public service. For example, 35.232.102.51:5050.","title":"Accessing SnappyData Monitoring Console"},{"location":"kubernetes/#connecting-snappydata-using-jdbc-driver","text":"For Kubernetes deployments, JDBC clients can connect to SnappyData cluster using the JDBC URL that is derived from the snappydata-locator-public service. To connect to SnappyData using JDBC driver in Kubernetes: Check the SnappyData services running in Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of the snappydata-locator-public service and the port number for external connections as shown in the following image: Use the external IP address and port of the snappydata-locator-public services to connect to SnappyData cluster using JDBC connections. For example, based on the above output, the JDBC URL to be used will be jdbc:snappydata://104.198.47.162:1527/ You can refer to SnappyData documentation for an example of JDBC program and for instructions on how to obtain JDBC driver using Maven/SBT co-ordinates.","title":"Connecting SnappyData Using JDBC Driver"},{"location":"kubernetes/#executing-queries-using-snappydata-shell","text":"You can use SnappyData shell to connect to SnappyData and execute your queries. You can simply connect to one of the pods in the cluster and use the SnappyData Shell. Alternatively, you can download the SnappyData distribution from SnappyData github releases . SnappyData shell need not run within the Kubernetes cluster. To execute queries in Kubernetes deployment: Check the SnappyData services running in the Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of the snappydata-locator-public services and the port number for external connections as shown in the following image: Launch SnappyData shell and then create tables and execute queries. Following is an example of executing queries using SnappyData shell. # Connect to snappy-shell bin/snappy snappy> connect client '104.198.47.162:1527'; # Create tables and execute queries snappy> create table t1(col1 int, col2 int) using column; snappy> insert into t1 values(1, 1); 1 row inserted/updated/deleted","title":"Executing Queries Using SnappyData Shell"},{"location":"kubernetes/#submitting-a-snappydata-job","text":"Refer to the How Tos section in SnappyData documentation to understand how to submit SnappyData jobs. However, for submitting a SnappyData job in Kubernetes deployment, you need to use the snappydata-leader-public service that exposes port 8090 to run the jobs. To submit a SnappyData job in Kubernetes deployment: Check the SnappyData services running in Kubernetes cluster. kubectl get svc --namespace=snappy The output displays the external IP address of snappydata-leader-public service which must be noted. Change to SnappyData product directory. cd $SNAPPY_HOME Submit the job using the external IP of the snappydata-leader-public service and the port number 8090 in the --lead option. Following is an example of submitting a SnappyData Job: bin/snappy-job.sh submit --app-name CreatePartitionedRowTable --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable --app-jar examples/jars/quickstart.jar --lead 35.232.102.51:8090","title":"Submitting a SnappyData Job"},{"location":"kubernetes/#stopping-the-snappydata-cluster-on-kubernetes","text":"To stop the SnappyData cluster on Kubernetes, you must delete the SnappyData Helm chart using the helm delete command. $ helm delete --purge snappydata The dynamically provisioned volumes and the data in it is retained, even if the chart deployment is deleted. Note If the chart is deployed again with the same chart name and if the volume exists, then the existing volume is used instead of provisioning a new volume.","title":"Stopping the SnappyData Cluster on Kubernetes"},{"location":"kubernetes/#list-of-configuration-parameters-for-snappydata-chart","text":"You can modify the values.yaml file to configure the SnappyData chart. The following table lists the configuration parameters available for this chart: Parameter Description Default image Docker repo from which the SnappyData Docker image is pulled. snappydatainc/snappydata imageTag Tag of the SnappyData Docker image that is pulled. imagePullPolicy Pull policy for the image. IfNotPresent imagePullSecrets Secret name to be used to pull image from a private registry locators.conf List of the configuration options that is passed to the locators. locators.resources Resource configuration for the locator Pods. User can configure CPU/memory requests and limit the usage. locators.requests.memory is set to 1024Mi . locators.persistence.storageClass Storage class that is used while dynamically provisioning a volume. Default value is not defined so default storage class for the cluster is chosen. locators.persistence.accessMode Access mode that is used for the dynamically provisioned volume. ReadWriteOnce locators.persistence.size Size of the dynamically provisioned volume. 10Gi servers.replicaCount Number of servers that are started in a SnappyData cluster. 2 servers.conf List of the configuration options that are passed to the servers. servers.resources Resource configuration for the server Pods. You can configure CPU/memory requests and limit the usage. servers.requests.memory is set to 4096Mi servers.persistence.storageClass Storage class that is used while dynamically provisioning a volume. Default value is not defined so default storage class for the cluster will be chosen. servers.persistence.accessMode Access mode for the dynamically provisioned volume. ReadWriteOnce servers.persistence.size Size of the dynamically provisioned volume. 10Gi leaders.conf List of configuration options that can be passed to the leaders. leaders.resources Resource configuration for the server pods. You can configure CPU/memory requests and limits the usage. leaders.requests.memory is set to 4096Mi leaders.persistence.storageClass Storage class that is used while dynamically provisioning a volume. Default value is not defined so default storage class for the cluster will be chosen. leaders.persistence.accessMode Access mode for the dynamically provisioned volume. ReadWriteOnce leaders.persistence.size Size of the dynamically provisioned volume. 10Gi The following sample shows the configuration used to start four servers each with a heap size of 2048 MB: servers: replicaCount: 4 ## config options for servers conf: \"-heap-size=2048m\" You can specify SnappyData configuration parameters in the servers.conf , locators.conf , and leaders.conf attributes for servers, locators, and leaders respectively.","title":"List of Configuration Parameters for SnappyData Chart"},{"location":"kubernetes/#kubernetes-objects-used-in-snappydata-chart","text":"This section provides details about the following Kubernetes objects that are used in SnappyData Chart: Statefulsets for Servers, Leaders, and Locators Services that Expose External Endpoints Persistent Volumes","title":"Kubernetes Objects Used in SnappyData Chart"},{"location":"kubernetes/#statefulsets-for-servers-leaders-and-locators","text":"Kubernetes statefulsets are used to manage stateful applications. Statefulsets provide many benefits such as stable and unique network identifiers, stable persistent storage, ordered deployment and scaling, graceful deletion, and rolling updates. SnappyData Helm chart deploys statefulsets for servers, leaders, and locators. By default the chart deploys two data servers, one locator, and one leader. Upon deletion of the Helm deployment, each pod gracefully terminates the SnappyData process that is running on it.","title":"Statefulsets for Servers, Leaders, and Locators"},{"location":"kubernetes/#services-that-expose-external-endpoints","text":"SnappyData Helm chart creates services to allow you to make JDBC connections, execute Spark jobs, and access SnappyData Monitoring Console etc. Services of the type LoadBalancer have external IP address assigned and can be used to connect from outside of Kubernetes cluster. To check the service created for SnappyData deployment, use command kubectl get svc --namespace=snappy . The following output is displayed: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE snappydata-leader ClusterIP None <none> 5050/TCP 5m snappydata-leader-public LoadBalancer 10.51.255.175 35.232.102.51 5050:31964/TCP,8090:32700/TCP,3768:32575/TCP 5m snappydata-locator ClusterIP None <none> 10334/TCP,1527/TCP 5m snappydata-locator-public LoadBalancer 10.51.241.224 104.198.47.162 1527:31957/TCP 5m snappydata-server ClusterIP None <none> 1527/TCP 5m snappydata-server-public LoadBalancer 10.51.248.27 35.232.16.4 1527:31853/TCP 5m In the above output, three services namely snappydata-leader-public , snappydata-locator-public and snappydata-server-public of type LoadBalancer are created. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use. snappydata-leader-public service exposes port 5050 for SnappyData Monitoring Console and port 8090 to accept SnappyData jobs. snappydata-locator-public service exposes port 1527 to accept JDBC connections.","title":"Services that Expose External Endpoints"},{"location":"kubernetes/#persistent-volumes","text":"A pod in a SnappyData deployment has a persistent volume mounted on it. This volume is dynamically provisioned and is used to store data directory for SnappyData. On each pod, the persistent volume is mounted on path /opt/snappydata/work . These volumes and the data in it is retained even if the chart deployment is deleted.","title":"Persistent Volumes"},{"location":"kubernetes/#accessing-logs","text":"You can access the logs when the SnappyData cluster is running as well as when the SnappyData cluster is not running .","title":"Accessing Logs"},{"location":"kubernetes/#accessing-logs-when-snappydata-cluster-is-running","text":"When a SnappyData cluster is running, you can open a session for a pod using kubectl command and then view the logs. The following example shows how to access logs of snappydata-server-0 : # Connect to snappydata-server-0 pod and open a shell. $ kubectl exec -it snappydata-server-0 --namespace snappy -- /bin/bash # Switch to Snappydata work directory and view the logs. $ cd /opt/snappydata/work $ ls","title":"Accessing Logs When SnappyData Cluster is Running"},{"location":"kubernetes/#accessing-logs-when-snappydata-cluster-is-not-running","text":"When SnappyData cluster is not running, you can access the volumes used in SnappyData with a utility script snappy-debug-pod.sh located in the utils directory of Spark on k8s repository. This script launches a pod in the Kubernetes cluster with persistent volumes, specified via --pvc option, mounted on it and then returns a shell prompt. Volumes are mounted on the path starting with /data0 (volume1 on /data0 and so on) . In the following example, the names of the persistent volume claims used by the cluster are retrieved and passed to the snappy-debug-pod.sh script to be mounted on the pod. # Get the names of persistent volume claims used by SnappyData cluster installed in a namespace # called *snappy*. The PVCs used by SnappyData are prefixed with 'snappy-disk-claim-'. $ kubectl get pvc --namespace snappy NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE snappy-disk-claim-snappydata-leader-0 Bound pvc-17cf9834-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d snappy-disk-claim-snappydata-locator-0 Bound pvc-17d75411-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d snappy-disk-claim-snappydata-server-0 Bound pvc-17de4f1a-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d snappy-disk-claim-snappydata-server-1 Bound pvc-226d778d-68c3-11e8-ab38-42010a8001a3 10Gi RWO standard 50d # To view logs for server-0 and server-1, use PVCs 'snappy-disk-claim-snappydata-server-0' and snappy-disk-claim-snappydata-server-1' $ ./utils/snappy-debug-pod.sh --pvc snappy-disk-claim-snappydata-server-0,snappy-disk-claim-snappydata-server-1 --namespace snappy Volume for snappy-disk-claim-snappydata-server-0 will be mounted on /data0 Volume for snappy-disk-claim-snappydata-server-1 will be mounted on /data1 Launching the POD If you don't see a command prompt, try pressing enter. bash-4.1# In the above example, the second command opens a session with bash prompt for the pod on which the volumes corresponding to the mentioned PVCs are mounted on paths such as /data0 , /data1 and so on. You can then examine the logs in these mounted paths. For example: bash-4.1# ls /data1 lost+found members.txt snappydata-server-1 bash-4.1# ls /data0 lost+found members.txt snappydata-server-0 bash-4.1# ls /data0/snappydata-server-0/ bash-4.1# ls /data0/snappydata-server-0/*.*log","title":"Accessing Logs When SnappyData Cluster is not Running"},{"location":"kubernetes/#configuring-the-log-level","text":"You can provide a log4j2.properties file while installing the SnappyData Helm chart. A template file log4j2.properties.template is provided in the charts/snappydata/conf/ directory. This template file can be renamed and used to configure log level as shown in the following example: $ cd charts/snappydata/conf/ # copy the template file and edit it to configure log level $ cp log4j2.properties.template log4j2.properties When SnappyData chart is installed, the log4.properties file will be used to configure the log level.","title":"Configuring the Log Level"},{"location":"kubernetes/#mounting-configmaps","text":"Files created in charts/snappydata/conf/ are mounted on SnappyData server, lead, and locator pods as configmaps and copied into the product's conf directory. This is useful to make configuration files (such as spark-env.sh, metrics.properties etc.) available to the product. Template files are provided in the charts/snappydata/conf/ . These template files can be renamed and edited to provide configuration. For example: $ cd snappydata/conf/ $ cp spark-env.sh.template spark-env.sh # now modify the spark-env.sh to specify configuration","title":"Mounting ConfigMaps"},{"location":"new_features/","text":"New Features \u00b6 See the New Features section in the release notes .","title":"New Features"},{"location":"new_features/#new-features","text":"See the New Features section in the release notes .","title":"New Features"},{"location":"prev_doc_ver/","text":"Archived SnappyData Documentation \u00b6 Click a release to check the corresponding archived product documentation of SnappyData/ComputeDB: SnappyData 1.3.0 ComputeDB 1.2.0 SnappyData 1.1.1 SnappyData 1.1.0 SnappyData 1.0.2.1 SnappyData 1.0.2 SnappyData 1.0.1 SnappyData 1.0.0","title":"Doc Archives"},{"location":"prev_doc_ver/#archived-snappydata-documentation","text":"Click a release to check the corresponding archived product documentation of SnappyData/ComputeDB: SnappyData 1.3.0 ComputeDB 1.2.0 SnappyData 1.1.1 SnappyData 1.1.0 SnappyData 1.0.2.1 SnappyData 1.0.2 SnappyData 1.0.1 SnappyData 1.0.0","title":"Archived SnappyData Documentation"},{"location":"setting_up_jdbc_driver_qlikview/","text":"Setting Up SnappyData JDBC Client and QlikView \u00b6 Note Before using SnappyData JDBC Client, make sure Java 8 is installed. The following topics are covered in this section: Step 1: Download SnappyData JDBC Client Step 2: Download and Install QlikView Step 3: Download and Install JDBCConnector for QlikView Step 4: Configure the JDBCConnector to connect to SnappyData Step 5: Connecting from QlikView to SnappyData Step 1: Download SnappyData JDBC Client \u00b6 Download the SnappyData JDBC Client JAR . See also: How to connect using JDBC driver Step 2: Download and Install QlikView \u00b6 Download a QlikView installation package . Double-click the Setup.exe file to start the installation. For installation instructions refer to the QlikView documentation. Step 3: Download and Install JDBCConnector for QlikView \u00b6 To connect to SnappyData using JDBC Client from QlikView application, install the JDBCConnector. This connector integrates into the QlikView application Download JDBCConnector installer . Extract the contents of the compressed file, and double-clik on the installer to start the installation process. For installation instructions, refer to the documentation provided for the QlikView JDBC Connector. You may need to activate the product. Step 4: Configure the JDBCConnector to Connect to SnappyData \u00b6 After installing the JDBCConnector application, add the SnappyData profile in the JDBCConnector. Tip You can also create a profile from the QlikView application. Open JDBCConnector Application. In the Profiles tab, click Create Profile . Enter a profile name. For example, SnappyData. Click Set As Default to set it as the default profile. In the Java VM Options tab, click Select JVM , to set the path for the jvm.dll file. For example, C:\\Program Files\\Java\\jre1.8.0_121\\bi\\server\\jvm.dll. Click Add , to add/update option -Xmx1024M . In the JDBC Driver tab, select the path to the snappydata-jdbc_2.11-1.3.1.jar file. In the Advanced tab, add the JDBC Driver Classname io.snappydata.jdbc.ClientDriver . Click OK to save and apply your changes. Step 5: Connecting from QlikView to SnappyData \u00b6 Open the QlikView desktop application. Click File > New from the menu bar to create a QlikView workbook. The Getting Started Wizard is displayed. Close it to continue. Click File > Edit Script from the menu bar. In the Data tab, select JDBCConnector_x64.dll from the Database drop down. Click Configure . Verfiy that the following configuration is displayed: In the Java VM Options tab, the path to jvm.dll file is correct and also the add/update option displays -Xmx1024M . In the JDBC Driver tab, the path to the snappydata-jdbc_2.11-1.3.1.jar file is correct. In the Advanced tab, JDBC Driver class name is displayed as io.snappydata.jdbc.ClientDriver . Click Connect . The Connect through QlikView JDBC Connector window is displayed. In URL field enter the SnappyData JDBC URL in the format jdbc:snappydata:// : For example, jdbc:snappydata://192.168.1.200:1527. Enter both the Username and Password as app . Click OK to apply the changes. When the connection is successful, CONNECT TO script is added to the list of scripts in Edit Script panel. Click the Select button to add the Data Source Table (SELECT sql script) OR you can manually add the SELECT sql script. The SELECT script is added to the list of scripts. Click OK . After adding the desired Data Source Table, click OK on the Edit Script panel. From the menu bar, click File > Reload to load data from the data source. From the menu bar, click Tools > Quick Chart Wizard to add the required charts. The Selected charts wizard guides you for generating data visualizations. Refer to the QlikView documentation for more information on data visualization.","title":"Setting Up SnappyData JDBC Client and QlikView"},{"location":"setting_up_jdbc_driver_qlikview/#setting-up-snappydata-jdbc-client-and-qlikview","text":"Note Before using SnappyData JDBC Client, make sure Java 8 is installed. The following topics are covered in this section: Step 1: Download SnappyData JDBC Client Step 2: Download and Install QlikView Step 3: Download and Install JDBCConnector for QlikView Step 4: Configure the JDBCConnector to connect to SnappyData Step 5: Connecting from QlikView to SnappyData","title":"Setting Up SnappyData JDBC Client and QlikView"},{"location":"setting_up_jdbc_driver_qlikview/#step-1-download-snappydata-jdbc-client","text":"Download the SnappyData JDBC Client JAR . See also: How to connect using JDBC driver","title":"Step 1: Download SnappyData JDBC Client"},{"location":"setting_up_jdbc_driver_qlikview/#step-2-download-and-install-qlikview","text":"Download a QlikView installation package . Double-click the Setup.exe file to start the installation. For installation instructions refer to the QlikView documentation.","title":"Step 2: Download and Install QlikView"},{"location":"setting_up_jdbc_driver_qlikview/#step-3-download-and-install-jdbcconnector-for-qlikview","text":"To connect to SnappyData using JDBC Client from QlikView application, install the JDBCConnector. This connector integrates into the QlikView application Download JDBCConnector installer . Extract the contents of the compressed file, and double-clik on the installer to start the installation process. For installation instructions, refer to the documentation provided for the QlikView JDBC Connector. You may need to activate the product.","title":"Step 3: Download and Install JDBCConnector for QlikView"},{"location":"setting_up_jdbc_driver_qlikview/#step-4-configure-the-jdbcconnector-to-connect-to-snappydata","text":"After installing the JDBCConnector application, add the SnappyData profile in the JDBCConnector. Tip You can also create a profile from the QlikView application. Open JDBCConnector Application. In the Profiles tab, click Create Profile . Enter a profile name. For example, SnappyData. Click Set As Default to set it as the default profile. In the Java VM Options tab, click Select JVM , to set the path for the jvm.dll file. For example, C:\\Program Files\\Java\\jre1.8.0_121\\bi\\server\\jvm.dll. Click Add , to add/update option -Xmx1024M . In the JDBC Driver tab, select the path to the snappydata-jdbc_2.11-1.3.1.jar file. In the Advanced tab, add the JDBC Driver Classname io.snappydata.jdbc.ClientDriver . Click OK to save and apply your changes.","title":"Step 4: Configure the JDBCConnector to Connect to SnappyData"},{"location":"setting_up_jdbc_driver_qlikview/#step-5-connecting-from-qlikview-to-snappydata","text":"Open the QlikView desktop application. Click File > New from the menu bar to create a QlikView workbook. The Getting Started Wizard is displayed. Close it to continue. Click File > Edit Script from the menu bar. In the Data tab, select JDBCConnector_x64.dll from the Database drop down. Click Configure . Verfiy that the following configuration is displayed: In the Java VM Options tab, the path to jvm.dll file is correct and also the add/update option displays -Xmx1024M . In the JDBC Driver tab, the path to the snappydata-jdbc_2.11-1.3.1.jar file is correct. In the Advanced tab, JDBC Driver class name is displayed as io.snappydata.jdbc.ClientDriver . Click Connect . The Connect through QlikView JDBC Connector window is displayed. In URL field enter the SnappyData JDBC URL in the format jdbc:snappydata:// : For example, jdbc:snappydata://192.168.1.200:1527. Enter both the Username and Password as app . Click OK to apply the changes. When the connection is successful, CONNECT TO script is added to the list of scripts in Edit Script panel. Click the Select button to add the Data Source Table (SELECT sql script) OR you can manually add the SELECT sql script. The SELECT script is added to the list of scripts. Click OK . After adding the desired Data Source Table, click OK on the Edit Script panel. From the menu bar, click File > Reload to load data from the data source. From the menu bar, click Tools > Quick Chart Wizard to add the required charts. The Selected charts wizard guides you for generating data visualizations. Refer to the QlikView documentation for more information on data visualization.","title":"Step 5: Connecting from QlikView to SnappyData"},{"location":"setting_up_odbc_driver/","text":"Setting Up SnappyData ODBC Driver \u00b6 Note This is currently tested and supported only on Windows 10 (32-bit and 64-bit systems) but binaries for only Windows 10 64-bit are provided. Download and Install Visual C++ Redistributable for Visual Studio 2015 and above Step 1: Install the SnappyData ODBC Driver \u00b6 Download the SnappyData 1.3.1 Community Edition . Download the snappydata-odbc_1.3.0_win64.zip file. Follow steps 1 and 2 in the howto to install the SnappyData ODBC driver. Step 2: Create SnappyData DSN from ODBC Data Sources 64-bit/32-bit \u00b6 To create SnappyData DSN from ODBC Data Sources: Open the ODBC Data Source Administrator window: a. On the Start page, type ODBC Data Sources, and select Set up ODBC data sources from the list or select ODBC Data Sources in the Administrative Tools . b. Based on your Windows installation, open ODBC Data Sources (64-bit) or ODBC Data Sources (32-bit). In the ODBC Data Source Administrator window, select either the User DSN or System DSN tab. Click Add to view the list of installed ODBC drivers on your machine. From the list of drivers, select SnappyData ODBC Driver and click Finish . The SnappyData ODBC Configuration dialog is displayed. Enter the following details to create a DSN: Item Description Data Source Name of the Data Source. For example, snappydsn . Server Host name or IP address of the data server which is running in the SnappyData cluster. Port Port number of the server. By default, it is 1527 for the first locator in the cluster. User The user name required to connect to the server. For example, app Password The password required to connect to the server. For example, app Default Schema Optional argument to change the default schema set on the connection. By default it is the user name. Use Credential Manager Use the credential manager to get the password securely. If enabled, the password field contains the key of the Generic Windows Credentials in the Windows Credential Manager to be looked up. Load Balance Obtain a connection to some server in the cluster in a load balanced way. When this is enabled, the host/port provided should be that of a locator. Conversely this option must be enabled if the provided host/port are that of a locator. Auto Reconnect When this option is enabled, connections to the SnappyData cluster will attempt to reconnect on the next operation if an operation fails due to TCP connection failing for some reason (network or anything else). AQP Check the AQP checkbox for aqp queries: Error : Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. Confidence : Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. Behavior : The action to be taken if the error computed goes outside the error tolerance limit. Enable SSL If you are connecting to a SnappyData cluster that has Secure Sockets Layer (SSL) enabled, you can configure the driver for connecting. Enabling SSL \u00b6 The following instructions describe how to configure SSL in a DSN: Select the Enable SSL checkbox. To allow authentication using self-signed trusted certificates, specify the full path of the PEM file containing the self trusted certificate. For a self-signed trusted certificate, a common host name should match. To configure two-way SSL verification, select the Two Way SSL checkbox and then do the following: In the Trusted Certificate file field, specify the full path of the PEM file containing the CA-certificate. In the Client Certificate File field, specify the full path of the PEM file containing the client's certificate. In the Client Private Key File field, specify the full path of the file containing the client's private key. In the Client Private key password field, provide the private key password. Enter the ciphers that you want to use. This is an optional input. If left empty then default ciphers are \"ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH\" For information about connecting Tableau using SnappyData ODBC Driver, refer to Connect Tableau using ODBC Driver","title":"Setting Up SnappyData ODBC Driver"},{"location":"setting_up_odbc_driver/#setting-up-snappydata-odbc-driver","text":"Note This is currently tested and supported only on Windows 10 (32-bit and 64-bit systems) but binaries for only Windows 10 64-bit are provided. Download and Install Visual C++ Redistributable for Visual Studio 2015 and above","title":"Setting Up SnappyData ODBC Driver"},{"location":"setting_up_odbc_driver/#step-1-install-the-snappydata-odbc-driver","text":"Download the SnappyData 1.3.1 Community Edition . Download the snappydata-odbc_1.3.0_win64.zip file. Follow steps 1 and 2 in the howto to install the SnappyData ODBC driver.","title":"Step 1: Install the SnappyData ODBC Driver"},{"location":"setting_up_odbc_driver/#step-2-create-snappydata-dsn-from-odbc-data-sources-64-bit32-bit","text":"To create SnappyData DSN from ODBC Data Sources: Open the ODBC Data Source Administrator window: a. On the Start page, type ODBC Data Sources, and select Set up ODBC data sources from the list or select ODBC Data Sources in the Administrative Tools . b. Based on your Windows installation, open ODBC Data Sources (64-bit) or ODBC Data Sources (32-bit). In the ODBC Data Source Administrator window, select either the User DSN or System DSN tab. Click Add to view the list of installed ODBC drivers on your machine. From the list of drivers, select SnappyData ODBC Driver and click Finish . The SnappyData ODBC Configuration dialog is displayed. Enter the following details to create a DSN: Item Description Data Source Name of the Data Source. For example, snappydsn . Server Host name or IP address of the data server which is running in the SnappyData cluster. Port Port number of the server. By default, it is 1527 for the first locator in the cluster. User The user name required to connect to the server. For example, app Password The password required to connect to the server. For example, app Default Schema Optional argument to change the default schema set on the connection. By default it is the user name. Use Credential Manager Use the credential manager to get the password securely. If enabled, the password field contains the key of the Generic Windows Credentials in the Windows Credential Manager to be looked up. Load Balance Obtain a connection to some server in the cluster in a load balanced way. When this is enabled, the host/port provided should be that of a locator. Conversely this option must be enabled if the provided host/port are that of a locator. Auto Reconnect When this option is enabled, connections to the SnappyData cluster will attempt to reconnect on the next operation if an operation fails due to TCP connection failing for some reason (network or anything else). AQP Check the AQP checkbox for aqp queries: Error : Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. Confidence : Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. Behavior : The action to be taken if the error computed goes outside the error tolerance limit. Enable SSL If you are connecting to a SnappyData cluster that has Secure Sockets Layer (SSL) enabled, you can configure the driver for connecting.","title":"Step 2: Create SnappyData DSN from ODBC Data Sources 64-bit/32-bit"},{"location":"setting_up_odbc_driver/#enabling-ssl","text":"The following instructions describe how to configure SSL in a DSN: Select the Enable SSL checkbox. To allow authentication using self-signed trusted certificates, specify the full path of the PEM file containing the self trusted certificate. For a self-signed trusted certificate, a common host name should match. To configure two-way SSL verification, select the Two Way SSL checkbox and then do the following: In the Trusted Certificate file field, specify the full path of the PEM file containing the CA-certificate. In the Client Certificate File field, specify the full path of the PEM file containing the client's certificate. In the Client Private Key File field, specify the full path of the file containing the client's private key. In the Client Private key password field, provide the private key password. Enter the ciphers that you want to use. This is an optional input. If left empty then default ciphers are \"ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH\" For information about connecting Tableau using SnappyData ODBC Driver, refer to Connect Tableau using ODBC Driver","title":"Enabling SSL"},{"location":"techsupport/","text":"Mail Us \u00b6 You can contact the SnappyData support team using: support@tibco.com Community \u00b6 The following channels are monitored for comments/questions: Stackoverflow Gitter IRC Reddit JIRA","title":"Contact and Support"},{"location":"techsupport/#mail-us","text":"You can contact the SnappyData support team using: support@tibco.com","title":"Mail Us"},{"location":"techsupport/#community","text":"The following channels are monitored for comments/questions: Stackoverflow Gitter IRC Reddit JIRA","title":"Community"},{"location":"unsupported/","text":"Unsupported Third-Party Modules \u00b6 The following third-party modules are not supported by SnappyData 1.3.1, although they are shipped with the product: Spark RDD-based APIs: org.apache.spark.mllib GraphX (Graph Processing) Spark Streaming (DStreams) SnappyData does not support SparkR (R on Spark) .","title":"Unsupported Third-Party Modules"},{"location":"unsupported/#unsupported-third-party-modules","text":"The following third-party modules are not supported by SnappyData 1.3.1, although they are shipped with the product: Spark RDD-based APIs: org.apache.spark.mllib GraphX (Graph Processing) Spark Streaming (DStreams) SnappyData does not support SparkR (R on Spark) .","title":"Unsupported Third-Party Modules"},{"location":"additional_files/additional_docs/","text":"SnappyData Resources \u00b6 You can view videos, presentations, slideshows and refer to important resources about SnappyData at https://community.tibco.com/products/tibco-computedb .","title":"SnappyData Resources"},{"location":"additional_files/additional_docs/#snappydata-resources","text":"You can view videos, presentations, slideshows and refer to important resources about SnappyData at https://community.tibco.com/products/tibco-computedb .","title":"SnappyData Resources"},{"location":"additional_files/license_model/","text":"Licensing Model \u00b6 The source code is distributed with Apache License 2.0. Users can download the fully functional OSS version and deploy it in production.","title":"Licensing Model"},{"location":"additional_files/license_model/#licensing-model","text":"The source code is distributed with Apache License 2.0. Users can download the fully functional OSS version and deploy it in production.","title":"Licensing Model"},{"location":"additional_files/open_source_components/","text":"SnappyData Community Edition (Open Source) \u00b6 Starting with the 1.3.0 release, SnappyData Community Edition gets close to the erstwhile TIBCO ComputeDB Enterprise Edition in terms of the features. Apart from the GemFire connector (that depends on non-OSS Pivotal GemFire jars), the 1.3.0/1.3.1 Community Editions exceed the previous TIBCO ComputeDB Enterprise Edition 1.2.0 in both features and performance. You can find a list of new features and performance improvements in the release notes . The features hitherto available only in Enterprise edition - Off-heap storage for column tables, Approximate Query Processing, LDAP-based Authentication and Authorization, ODBC driver, to name a few - are now available in SnappyData (community edition) as well. The high level capabilities of the Community Edition are listed in the following table: Feature Available Mutable Row and Column Store X Compatibility with Spark X Shared Nothing Persistence and HA X REST API for Spark Job Submission X Fault Tolerance for Driver X Access to the system using JDBC Driver X CLI for backup, restore, and export data X Spark console extensions X System Performance/behavior statistics X Support for transactions in Row tables X Support for indexing in Row tables X Support for snapshot transactions in Column tables X Online compaction of column block data X Transparent disk overflow of large query results X Support for external Hive meta store X SQL extensions for stream processing X SnappyData sink for structured stream processing X Structured Streaming user interface X Runtime deployment of packages and jars X Scala code execution from SQL (EXEC SCALA) X Out of the box support for cloud storage X Support for Hadoop 3.2 X SnappyData Interpreter for Apache Zeppelin X Synopsis Data Engine for Approximate Querying X Support for Synopsis Data Engine from TIBCO Spotfire\u00ae X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X GemFire connector","title":"Community Edition Features"},{"location":"additional_files/open_source_components/#snappydata-community-edition-open-source","text":"Starting with the 1.3.0 release, SnappyData Community Edition gets close to the erstwhile TIBCO ComputeDB Enterprise Edition in terms of the features. Apart from the GemFire connector (that depends on non-OSS Pivotal GemFire jars), the 1.3.0/1.3.1 Community Editions exceed the previous TIBCO ComputeDB Enterprise Edition 1.2.0 in both features and performance. You can find a list of new features and performance improvements in the release notes . The features hitherto available only in Enterprise edition - Off-heap storage for column tables, Approximate Query Processing, LDAP-based Authentication and Authorization, ODBC driver, to name a few - are now available in SnappyData (community edition) as well. The high level capabilities of the Community Edition are listed in the following table: Feature Available Mutable Row and Column Store X Compatibility with Spark X Shared Nothing Persistence and HA X REST API for Spark Job Submission X Fault Tolerance for Driver X Access to the system using JDBC Driver X CLI for backup, restore, and export data X Spark console extensions X System Performance/behavior statistics X Support for transactions in Row tables X Support for indexing in Row tables X Support for snapshot transactions in Column tables X Online compaction of column block data X Transparent disk overflow of large query results X Support for external Hive meta store X SQL extensions for stream processing X SnappyData sink for structured stream processing X Structured Streaming user interface X Runtime deployment of packages and jars X Scala code execution from SQL (EXEC SCALA) X Out of the box support for cloud storage X Support for Hadoop 3.2 X SnappyData Interpreter for Apache Zeppelin X Synopsis Data Engine for Approximate Querying X Support for Synopsis Data Engine from TIBCO Spotfire\u00ae X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X GemFire connector","title":"SnappyData Community Edition (Open Source)"},{"location":"affinity_modes/","text":"Affinity Modes \u00b6 In this section, the various modes available for colocation of related data and computation is discussed. You can run the SnappyData store in the following modes: Local Mode : Used mainly for development, where the client application, the executors, and data store are all running in the same JVM Embedded SnappyData Store Mode : The Spark computations and in-memory data store run colocated in the same JVM SnappyData Smart Connector Mode : Allows you to work with the SnappyData store cluster from any compatible Spark distribution","title":"Affinity Modes"},{"location":"affinity_modes/#affinity-modes","text":"In this section, the various modes available for colocation of related data and computation is discussed. You can run the SnappyData store in the following modes: Local Mode : Used mainly for development, where the client application, the executors, and data store are all running in the same JVM Embedded SnappyData Store Mode : The Spark computations and in-memory data store run colocated in the same JVM SnappyData Smart Connector Mode : Allows you to work with the SnappyData store cluster from any compatible Spark distribution","title":"Affinity Modes"},{"location":"affinity_modes/connector_mode/","text":"SnappyData Smart Connector Mode \u00b6 In this mode, the Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. Conceptually, this is similar to how Spark applications work with stores like Cassandra, Redis etc. The Smart connector mode also implements several performance optimizations as described in this section. Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either local-only runners, Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run. Key Points: Can work with SnappyData store from a compatible Spark distribution (2.1.1, 2.1.2 or 2.1.3) Spark application executes in its own independent JVM processes The Spark application connects to SnappyData as a Spark Data source Supports any of the Spark supported resource managers (for example, local/local-cluster, Spark Standalone Manager, YARN or Mesos) Some of the advantages of this mode are: Performance When Spark partitions store data in column tables , the connector automatically attempts to localize the partitions into SnappyData store buckets on the local node. The connector uses the same column store format as well as compression techniques in Spark avoiding all data formatting related inefficiencies or unnecessary serialization costs. This is the fastest way to ingest data when Spark and the SnappyData cluster are operating as independent clusters. When storing to Row tables or when the partitioning in Spark is different from the partitioning configured on the table, data batches could be shuffled across nodes. Whenever Spark applications are writing to SnappyData tables, the data is always batched for the highest possible throughput. When queries are executed, while the entire query planning and execution is coordinated by the Spark engine (Catalyst), the smart connector still carries out a number of optimizations, which are listed below: Route jobs to same machines as SnappyData data nodes if the executor nodes are co-hosted on the same machines as the data nodes. Job for each partition tries to fetch only from same machine data store where possible. Data for column tables is fetched as it is stored in raw columnar byte data format, and optimized code generated plans read the columnar data directly which gives performance equivalent to and exceeding Spark's ColumnVectors while avoiding its most expensive step of having to copy external data to ColumnVectors. Colocated joins: If the underlying tables are colocated partition-wise, and executor nodes are co-hosting SnappyData data nodes, then the column batches are fetched from local machines and the join itself is partition-wise and does not require any exchange. Optimized column batch inserts like in the Embedded mode with job routing to same machines as data stores if possible. Features Allows creation of SnappySession which extends SparkSession in various ways: Enhanced SQL parser allowing for DML operations UPDATE , DELETE , PUT INTO , and DDL TRUNCATE operation Enhanced APIs to perform inline row-wise or bulk UPDATE , DELETE and PUT INTO operations New CREATE POLICY operation (and corresponding DROP POLICY ) to create a security access policy using SQL or API Tokenization and plan caching that allows reuse of plans and generated code for queries that differ only in constant values and thus substantially improves overall performance of the queries Automatically uses the embedded cluster for the hive meta-data by default, so any tables created will show up in the embedded cluster and vice-versa Ability to also attach to an external hive meta-store in addition to the embedded one Uniform APIs corresponding to DDL operations to CREATE , DROP tables, views and policies, and ALTER , TRUNCATE tables Example: Launch a Spark local mode cluster and use Smart Connector to access SnappyData cluster Step 1: Start the SnappyData cluster : You can either start SnappyData members using the snappy_start_all script or you can start them individually. Step 2: Launch the Apache Spark program In the Local mode ./bin/spark-shell --master local [ * ] --conf spark.snappydata.connection = localhost:1527 --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" Note The spark.snappydata.connection property points to the locator of a running SnappyData cluster. The value of this property is a combination of locator host and JDBC client port on which the locator listens for connections (default is 1527). In the Smart Connector mode, all snappydata.* SQL configuration properties should be prefixed with spark when running with upstream Apache Spark (not required if running bundled SnappyData's spark). For example, spark.snappydata.column.batchSize . This opens a Scala Shell. Step 3: Import any or all of the following: SnappySession SparkSession import org . apache . spark . sql .{ SnappySession , SparkSession } This starts the SnappyData cluster with Smart Connector mode. Create a SnappySession to interact with the SnappyData store. // Create a SnappySession to work with SnappyData store $scala > val snSession = new SnappySession ( spark . sparkContext ) The code example for writing a Smart Connector application program is located in SmartConnectorExample Using External Cluster Manager Cluster mode ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass --master spark://localhost:7077 --conf spark.snappydata.connection = localhost:1527 --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" Client mode ./bin/spark-submit --deploy-mode client --class somePackage.someClass --master spark://localhost:7077 --conf spark.snappydata.connection = localhost:1527 --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" Using YARN as a Cluster Manager Cluster mode ./spark-submit --master yarn --deploy-mode cluster --conf spark.driver.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --conf spark.executor.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --class MainClass SampleProjectYarn.jar Client mode ./spark-submit --master yarn --deploy-mode client --conf spark.driver.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --conf spark.executor.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --class MainClass SampleProjectYarn.jar","title":"SnappyData Smart Connector Mode"},{"location":"affinity_modes/connector_mode/#snappydata-smart-connector-mode","text":"In this mode, the Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. Conceptually, this is similar to how Spark applications work with stores like Cassandra, Redis etc. The Smart connector mode also implements several performance optimizations as described in this section. Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either local-only runners, Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run. Key Points: Can work with SnappyData store from a compatible Spark distribution (2.1.1, 2.1.2 or 2.1.3) Spark application executes in its own independent JVM processes The Spark application connects to SnappyData as a Spark Data source Supports any of the Spark supported resource managers (for example, local/local-cluster, Spark Standalone Manager, YARN or Mesos) Some of the advantages of this mode are: Performance When Spark partitions store data in column tables , the connector automatically attempts to localize the partitions into SnappyData store buckets on the local node. The connector uses the same column store format as well as compression techniques in Spark avoiding all data formatting related inefficiencies or unnecessary serialization costs. This is the fastest way to ingest data when Spark and the SnappyData cluster are operating as independent clusters. When storing to Row tables or when the partitioning in Spark is different from the partitioning configured on the table, data batches could be shuffled across nodes. Whenever Spark applications are writing to SnappyData tables, the data is always batched for the highest possible throughput. When queries are executed, while the entire query planning and execution is coordinated by the Spark engine (Catalyst), the smart connector still carries out a number of optimizations, which are listed below: Route jobs to same machines as SnappyData data nodes if the executor nodes are co-hosted on the same machines as the data nodes. Job for each partition tries to fetch only from same machine data store where possible. Data for column tables is fetched as it is stored in raw columnar byte data format, and optimized code generated plans read the columnar data directly which gives performance equivalent to and exceeding Spark's ColumnVectors while avoiding its most expensive step of having to copy external data to ColumnVectors. Colocated joins: If the underlying tables are colocated partition-wise, and executor nodes are co-hosting SnappyData data nodes, then the column batches are fetched from local machines and the join itself is partition-wise and does not require any exchange. Optimized column batch inserts like in the Embedded mode with job routing to same machines as data stores if possible. Features Allows creation of SnappySession which extends SparkSession in various ways: Enhanced SQL parser allowing for DML operations UPDATE , DELETE , PUT INTO , and DDL TRUNCATE operation Enhanced APIs to perform inline row-wise or bulk UPDATE , DELETE and PUT INTO operations New CREATE POLICY operation (and corresponding DROP POLICY ) to create a security access policy using SQL or API Tokenization and plan caching that allows reuse of plans and generated code for queries that differ only in constant values and thus substantially improves overall performance of the queries Automatically uses the embedded cluster for the hive meta-data by default, so any tables created will show up in the embedded cluster and vice-versa Ability to also attach to an external hive meta-store in addition to the embedded one Uniform APIs corresponding to DDL operations to CREATE , DROP tables, views and policies, and ALTER , TRUNCATE tables Example: Launch a Spark local mode cluster and use Smart Connector to access SnappyData cluster Step 1: Start the SnappyData cluster : You can either start SnappyData members using the snappy_start_all script or you can start them individually. Step 2: Launch the Apache Spark program In the Local mode ./bin/spark-shell --master local [ * ] --conf spark.snappydata.connection = localhost:1527 --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" Note The spark.snappydata.connection property points to the locator of a running SnappyData cluster. The value of this property is a combination of locator host and JDBC client port on which the locator listens for connections (default is 1527). In the Smart Connector mode, all snappydata.* SQL configuration properties should be prefixed with spark when running with upstream Apache Spark (not required if running bundled SnappyData's spark). For example, spark.snappydata.column.batchSize . This opens a Scala Shell. Step 3: Import any or all of the following: SnappySession SparkSession import org . apache . spark . sql .{ SnappySession , SparkSession } This starts the SnappyData cluster with Smart Connector mode. Create a SnappySession to interact with the SnappyData store. // Create a SnappySession to work with SnappyData store $scala > val snSession = new SnappySession ( spark . sparkContext ) The code example for writing a Smart Connector application program is located in SmartConnectorExample Using External Cluster Manager Cluster mode ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass --master spark://localhost:7077 --conf spark.snappydata.connection = localhost:1527 --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" Client mode ./bin/spark-submit --deploy-mode client --class somePackage.someClass --master spark://localhost:7077 --conf spark.snappydata.connection = localhost:1527 --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" Using YARN as a Cluster Manager Cluster mode ./spark-submit --master yarn --deploy-mode cluster --conf spark.driver.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --conf spark.executor.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --class MainClass SampleProjectYarn.jar Client mode ./spark-submit --master yarn --deploy-mode client --conf spark.driver.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --conf spark.executor.extraClassPath = /home/snappyuser/snappydata-1.3.1-bin/jars/* --class MainClass SampleProjectYarn.jar","title":"SnappyData Smart Connector Mode"},{"location":"affinity_modes/embedded_mode/","text":"Embedded SnappyData Store Mode \u00b6 In this mode, the Spark computations and in-memory data store run colocated in the same JVM. This is an out-of-the-box configuration and suitable for most SnappyData real-time production environments. You can launch SnappyData servers to bootstrap any data from disk, replicas or external data sources. Spark executors are launched when the SnappyData cluster starts up. Some of the advantages of this mode are: High performance : All your Spark applications access the table data locally, in-process. The query engine accesses all the data locally by reference and avoids copying which can be very expensive when working with large volumes. Driver High Availability : When Spark jobs are submitted, they can now run in an HA configuration. The submitted job becomes visible to a redundant \u201clead\u201d node that prevents the executors from going down when the Spark driver fails. Any submitted Spark job continues to run as long as there is at least one \u201clead\u201d node running. Less complex : There is only a single cluster to start, monitor, debug and tune. Enhanced SnappySession : Apart from all the features listed in connector mode , the embedded mode also offers APIs/SQL for the Synopsis Data Engine (SDE) In this mode, one can write Spark programs using jobs. For more details, refer to Snappy Jobs section. Example: Submit a Spark Job to the SnappyData Cluster ./bin/snappy-job.sh submit --app-name JsonApp --class org.apache.spark.examples.snappydata.WorkingWithJson --app-jar examples/jars/quickstart.jar --lead [ leadHost:port ] --conf json_resource_folder = ../../quickstart/src/main/resources Also, you can use Snappy SQL to create and query tables. You can either start SnappyData members using the snappy-start-all.sh script or you can start them individually. Having the Spark computation embedded in the same JVM allows us to do several optimizations at the query planning level. For example: If the join expression matches the partitioning scheme of tables, a partition to partition join instead of a shuffle based join is done. Moreover, if two tables are colocated (while defining the tables) costly data movement can be avoided. For replicated tables, that are present in all the data nodes, a simple local join (local lookup) is done instead of a broadcast join. Similarly, inserts into tables, groups the rows according to table partitioning keys, and routes to the JVM hosting the partition. This results in a higher ingestion rate.","title":"Embedded SnappyData Store Mode"},{"location":"affinity_modes/embedded_mode/#embedded-snappydata-store-mode","text":"In this mode, the Spark computations and in-memory data store run colocated in the same JVM. This is an out-of-the-box configuration and suitable for most SnappyData real-time production environments. You can launch SnappyData servers to bootstrap any data from disk, replicas or external data sources. Spark executors are launched when the SnappyData cluster starts up. Some of the advantages of this mode are: High performance : All your Spark applications access the table data locally, in-process. The query engine accesses all the data locally by reference and avoids copying which can be very expensive when working with large volumes. Driver High Availability : When Spark jobs are submitted, they can now run in an HA configuration. The submitted job becomes visible to a redundant \u201clead\u201d node that prevents the executors from going down when the Spark driver fails. Any submitted Spark job continues to run as long as there is at least one \u201clead\u201d node running. Less complex : There is only a single cluster to start, monitor, debug and tune. Enhanced SnappySession : Apart from all the features listed in connector mode , the embedded mode also offers APIs/SQL for the Synopsis Data Engine (SDE) In this mode, one can write Spark programs using jobs. For more details, refer to Snappy Jobs section. Example: Submit a Spark Job to the SnappyData Cluster ./bin/snappy-job.sh submit --app-name JsonApp --class org.apache.spark.examples.snappydata.WorkingWithJson --app-jar examples/jars/quickstart.jar --lead [ leadHost:port ] --conf json_resource_folder = ../../quickstart/src/main/resources Also, you can use Snappy SQL to create and query tables. You can either start SnappyData members using the snappy-start-all.sh script or you can start them individually. Having the Spark computation embedded in the same JVM allows us to do several optimizations at the query planning level. For example: If the join expression matches the partitioning scheme of tables, a partition to partition join instead of a shuffle based join is done. Moreover, if two tables are colocated (while defining the tables) costly data movement can be avoided. For replicated tables, that are present in all the data nodes, a simple local join (local lookup) is done instead of a broadcast join. Similarly, inserts into tables, groups the rows according to table partitioning keys, and routes to the JVM hosting the partition. This results in a higher ingestion rate.","title":"Embedded SnappyData Store Mode"},{"location":"affinity_modes/local_mode/","text":"Local Mode \u00b6 In this mode, you can execute all the components (client application, executors, and data store) locally in the application's JVM. It is the simplest way to start testing and using SnappyData, as you do not require a cluster, and the executor threads are launched locally for processing. Key Points No cluster required Launch Single JVM (Single-node Cluster) Launches executor threads locally for processing Embeds the SnappyData in-memory store in-process For development purposes only Example: Using the Local mode for developing SnappyData programs You can use an IDE of your choice, and provide the below dependency to get SnappyData binaries: Maven Gradle SBT <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-cluster_2.11 </artifactId> <version> 1.3.1 </version> </dependency> // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 compile 'io.snappydata:snappydata-cluster_2.11:1.3.1' // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" For Approximate Query Engine support: Maven Gradle SBT <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-aqp_2.11 --> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-aqp_2.11 </artifactId> <version> 1.3.1 </version> </dependency> // https://mvnrepository.com/artifact/io.snappydata/snappydata-aqp_2.11 compile 'io.snappydata:snappydata-aqp_2.11:1.3.1' // https://mvnrepository.com/artifact/io.snappydata/snappydata-aqp_2.11 libraryDependencies += \"io.snappydata\" % \"snappydata-aqp_2.11\" % \"1.3.1\" Note If your project fails when resolving the dependency with SBT (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1 ), it may be due an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Create SnappySession : To start SnappyData locally, you need to create a SnappySession in your program: val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"local[*]\" ) . getOrCreate val snappy = new SnappySession ( spark . sparkContext ) Example : Launch Apache Spark shell and provide SnappyData dependency as a Spark package : If you already have Spark 2.1.1 - 2.1.3 installed in your local machine you can directly use --packages option to download the SnappyData binaries. ./bin/spark-shell --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\"","title":"Local Mode"},{"location":"affinity_modes/local_mode/#local-mode","text":"In this mode, you can execute all the components (client application, executors, and data store) locally in the application's JVM. It is the simplest way to start testing and using SnappyData, as you do not require a cluster, and the executor threads are launched locally for processing. Key Points No cluster required Launch Single JVM (Single-node Cluster) Launches executor threads locally for processing Embeds the SnappyData in-memory store in-process For development purposes only Example: Using the Local mode for developing SnappyData programs You can use an IDE of your choice, and provide the below dependency to get SnappyData binaries: Maven Gradle SBT <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-cluster_2.11 </artifactId> <version> 1.3.1 </version> </dependency> // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 compile 'io.snappydata:snappydata-cluster_2.11:1.3.1' // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" For Approximate Query Engine support: Maven Gradle SBT <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-aqp_2.11 --> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-aqp_2.11 </artifactId> <version> 1.3.1 </version> </dependency> // https://mvnrepository.com/artifact/io.snappydata/snappydata-aqp_2.11 compile 'io.snappydata:snappydata-aqp_2.11:1.3.1' // https://mvnrepository.com/artifact/io.snappydata/snappydata-aqp_2.11 libraryDependencies += \"io.snappydata\" % \"snappydata-aqp_2.11\" % \"1.3.1\" Note If your project fails when resolving the dependency with SBT (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1 ), it may be due an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Create SnappySession : To start SnappyData locally, you need to create a SnappySession in your program: val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"local[*]\" ) . getOrCreate val snappy = new SnappySession ( spark . sparkContext ) Example : Launch Apache Spark shell and provide SnappyData dependency as a Spark package : If you already have Spark 2.1.1 - 2.1.3 installed in your local machine you can directly use --packages option to download the SnappyData binaries. ./bin/spark-shell --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\"","title":"Local Mode"},{"location":"architecture/","text":"SnappyData Concepts \u00b6 The topic explains the following fundamental concepts of SnappyData: Core Components SnappyData Cluster Architecture Hybrid Cluster Manager Distributed Transactions Affinity Modes","title":"SnappyData Concepts"},{"location":"architecture/#snappydata-concepts","text":"The topic explains the following fundamental concepts of SnappyData: Core Components SnappyData Cluster Architecture Hybrid Cluster Manager Distributed Transactions Affinity Modes","title":"SnappyData Concepts"},{"location":"architecture/cluster_architecture/","text":"SnappyData Cluster Architecture \u00b6 A SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members as represented in the below figure. Locator: Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons. Lead Node: The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members. Data Servers: A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or pass it to the lead node for execution by Spark SQL. SnappyData also has multiple deployment options. For more information refer to, Deployment Options . Interacting with SnappyData \u00b6 Note For the section on the Spark API, it is assumed that users have some familiarity with core Spark, Spark SQL, and Spark Streaming concepts . And, you can try out the Spark Quick Start . All the commands and programs listed in the Spark guides work in SnappyData as well. For the section on SQL, no Spark knowledge is necessary. To interact with SnappyData, interfaces are provided for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self-contained Spark application or can share the state with other jobs using the SnappyData store. Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways. Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster. Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver. When a driver fails, this can result in the executors getting shut down, taking down all cached state with it. Instead, SnappyData leverages the Spark JobServer to manage Jobs and queries within a \"lead\" node. Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). In this document, mostly the same set of features via the Spark API or using SQL is showcased. If you are familiar with Scala and understand Spark concepts you can choose to skip the SQL part go directly to the Spark API section . High Concurrency in SnappyData \u00b6 Thousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into low latency requests and high latency ones. For low latency operations, Spark\u2019s scheduling mechanism is completely bypassed and directly operate on the data. High latency operations (for example, compute intensive queries) are routed through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously. State Sharing in SnappyData \u00b6 A SnappyData cluster is designed to be a long-running clustered database. The state is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.","title":"SnappyData Cluster Architecture"},{"location":"architecture/cluster_architecture/#snappydata-cluster-architecture","text":"A SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members as represented in the below figure. Locator: Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons. Lead Node: The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members. Data Servers: A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or pass it to the lead node for execution by Spark SQL. SnappyData also has multiple deployment options. For more information refer to, Deployment Options .","title":"SnappyData Cluster Architecture"},{"location":"architecture/cluster_architecture/#interacting-with-snappydata","text":"Note For the section on the Spark API, it is assumed that users have some familiarity with core Spark, Spark SQL, and Spark Streaming concepts . And, you can try out the Spark Quick Start . All the commands and programs listed in the Spark guides work in SnappyData as well. For the section on SQL, no Spark knowledge is necessary. To interact with SnappyData, interfaces are provided for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self-contained Spark application or can share the state with other jobs using the SnappyData store. Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways. Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster. Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver. When a driver fails, this can result in the executors getting shut down, taking down all cached state with it. Instead, SnappyData leverages the Spark JobServer to manage Jobs and queries within a \"lead\" node. Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). In this document, mostly the same set of features via the Spark API or using SQL is showcased. If you are familiar with Scala and understand Spark concepts you can choose to skip the SQL part go directly to the Spark API section .","title":"Interacting with SnappyData"},{"location":"architecture/cluster_architecture/#high-concurrency-in-snappydata","text":"Thousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into low latency requests and high latency ones. For low latency operations, Spark\u2019s scheduling mechanism is completely bypassed and directly operate on the data. High latency operations (for example, compute intensive queries) are routed through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.","title":"High Concurrency in SnappyData"},{"location":"architecture/cluster_architecture/#state-sharing-in-snappydata","text":"A SnappyData cluster is designed to be a long-running clustered database. The state is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.","title":"State Sharing in SnappyData"},{"location":"architecture/core_components/","text":"Core Components \u00b6 The following figure depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, standard components, such as security and monitoring have been omitted. The storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row-oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. Refer to the Row/Column table section for details on the syntax and available features. Two primary programming models are supported by SnappyData \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions, to make the language compatible with the SQL standard. One could perceive SnappyData as an SQL database that uses Spark API as its language for stored procedures. Our stream processing is primarily through Spark Streaming, but it is integrated and runs within our store. The OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). All OLTP operations are routed immediately to appropriate data partitions without incurring any scheduling overhead. To support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, SnappyData uses a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster. In addition to the \u201cexact\u201d Dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Approximate Query Processing (AQP) and exploits appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance. To understand the data flow architecture, you are first walked through a real-time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.","title":"Core Components"},{"location":"architecture/core_components/#core-components","text":"The following figure depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, standard components, such as security and monitoring have been omitted. The storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row-oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. Refer to the Row/Column table section for details on the syntax and available features. Two primary programming models are supported by SnappyData \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions, to make the language compatible with the SQL standard. One could perceive SnappyData as an SQL database that uses Spark API as its language for stored procedures. Our stream processing is primarily through Spark Streaming, but it is integrated and runs within our store. The OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). All OLTP operations are routed immediately to appropriate data partitions without incurring any scheduling overhead. To support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, SnappyData uses a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster. In addition to the \u201cexact\u201d Dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Approximate Query Processing (AQP) and exploits appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance. To understand the data flow architecture, you are first walked through a real-time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.","title":"Core Components"},{"location":"architecture/data_ingestion_pipeline/","text":"Data Ingestion Pipeline \u00b6 The data pipeline involving analytics while streams are being ingested and the subsequent interactive analytics is the pervasive architecture for real-time applications. The steps to support these tasks are depicted in the following figure and explained below. After the SnappyData cluster is started and before any live streams can be processed, ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (for example, HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables. Spark Streaming\u2019s parallel receivers are relied on to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics. Next, SQL is used to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), these data structures are seamlessly combined in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high-velocity streams, SnappyData can still provide answers in real time by resorting to approximation. The stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (for example, maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution. To prevent the running out of memory situation, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory. Once ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance with users\u2019 requested accuracy.","title":"Data ingestion pipeline"},{"location":"architecture/data_ingestion_pipeline/#data-ingestion-pipeline","text":"The data pipeline involving analytics while streams are being ingested and the subsequent interactive analytics is the pervasive architecture for real-time applications. The steps to support these tasks are depicted in the following figure and explained below. After the SnappyData cluster is started and before any live streams can be processed, ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (for example, HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables. Spark Streaming\u2019s parallel receivers are relied on to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics. Next, SQL is used to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), these data structures are seamlessly combined in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high-velocity streams, SnappyData can still provide answers in real time by resorting to approximation. The stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (for example, maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution. To prevent the running out of memory situation, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory. Once ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance with users\u2019 requested accuracy.","title":"Data Ingestion Pipeline"},{"location":"architecture/hybrid_cluster_manager/","text":"Hybrid Cluster Manager \u00b6 Spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (for example, YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors. This is represented in the following figure. While Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet the following additional requirements as an operational database. High Concurrency : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations. State Sharing : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real-time data sharing. High Availability (HA) : As a highly concurrent distributed system that offers low latency access to data, applications must be protected from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations, therefore, become an important requirement for SnappyData. Consistency : As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data. After an overview of our cluster architecture, how SnappyData meets each of these requirements is explained in the subsequent sections.","title":"Hybrid Cluster Manager"},{"location":"architecture/hybrid_cluster_manager/#hybrid-cluster-manager","text":"Spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (for example, YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors. This is represented in the following figure. While Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet the following additional requirements as an operational database. High Concurrency : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations. State Sharing : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real-time data sharing. High Availability (HA) : As a highly concurrent distributed system that offers low latency access to data, applications must be protected from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations, therefore, become an important requirement for SnappyData. Consistency : As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data. After an overview of our cluster architecture, how SnappyData meets each of these requirements is explained in the subsequent sections.","title":"Hybrid Cluster Manager"},{"location":"best_practices/","text":"Best Practices \u00b6 The best practices section provides you guidelines for setting up your cluster and designing your database and the schema to use. The following topics are covered in this section: Tuning for Concurrency and Computation Designing your Database and Schema Memory Management HA Considerations Important Settings Tip SnappyData Monitoring Console is a web UI that displays information that can be used to analyse your query plan. For more details refer to SnappyData Monitoring Console .","title":"Best\u00a0Practices"},{"location":"best_practices/#best-practices","text":"The best practices section provides you guidelines for setting up your cluster and designing your database and the schema to use. The following topics are covered in this section: Tuning for Concurrency and Computation Designing your Database and Schema Memory Management HA Considerations Important Settings Tip SnappyData Monitoring Console is a web UI that displays information that can be used to analyse your query plan. For more details refer to SnappyData Monitoring Console .","title":"Best\u00a0Practices"},{"location":"best_practices/analysing_query_performance/","text":"Analysing Query Performance \u00b6 The SnappyData Monitoring Console is a web application UI that displays information that can be used to analyse your query plan: To access the SnappyData Monitoring Console, in the web browser go to the URL http://localhost:5050.","title":"Analysing Query Performance"},{"location":"best_practices/analysing_query_performance/#analysing-query-performance","text":"The SnappyData Monitoring Console is a web application UI that displays information that can be used to analyse your query plan: To access the SnappyData Monitoring Console, in the web browser go to the URL http://localhost:5050.","title":"Analysing Query Performance"},{"location":"best_practices/ha_considerations/","text":"High Availability (HA) Considerations \u00b6 High availability options are available for all the SnappyData components. Lead SnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of the secondary lead nodes takes over immediately. Setting up the secondary lead node is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries and jobs that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted. Locator SnappyData supports multiple locators in the cluster for high availability. It is recommended to set up multiple locators (ideally two). If a locator becomes unavailable, the cluster continues to be available. However, new members cannot join the cluster. With multiple locators, there are no impact on the clients and the fail over recovery is completely transparent. DataServer SnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data. So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. However, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again. Known Limitation \u00b6 In case of lead HA, the new primary lead node creates a new Snappy session for the JDBC clients. This means session specific properties (for example, spark.sql.autoBroadcastJoinThreshold , snappydata.sql.hashJoinSize ) need to be set again after a lead fail over.","title":"HA Considerations"},{"location":"best_practices/ha_considerations/#high-availability-ha-considerations","text":"High availability options are available for all the SnappyData components. Lead SnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of the secondary lead nodes takes over immediately. Setting up the secondary lead node is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries and jobs that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted. Locator SnappyData supports multiple locators in the cluster for high availability. It is recommended to set up multiple locators (ideally two). If a locator becomes unavailable, the cluster continues to be available. However, new members cannot join the cluster. With multiple locators, there are no impact on the clients and the fail over recovery is completely transparent. DataServer SnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data. So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. However, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again.","title":"High Availability (HA) Considerations"},{"location":"best_practices/ha_considerations/#known-limitation","text":"In case of lead HA, the new primary lead node creates a new Snappy session for the JDBC clients. This means session specific properties (for example, spark.sql.autoBroadcastJoinThreshold , snappydata.sql.hashJoinSize ) need to be set again after a lead fail over.","title":"Known Limitation"},{"location":"best_practices/important_settings/","text":"Important Settings \u00b6 Resource allocation is important for the execution of any job. If not configured correctly, the job can consume the entire clusters resources and cause execution failure because of memory and other related problems. This section provides guidelines for configuring the following important settings: Buckets member-timeout spark.local.dir Operating System Settings SnappyData Smart Connector mode and Local mode Settings Code Generation and Tokenization Buckets \u00b6 A bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, buckets are moved around to ensure that the data is balanced across the nodes where the table is defined. The default number of buckets in the SnappyData cluster mode is 128. In the local mode, it is cores*2, subject to a maximum of 64 buckets and a minimum of 8 buckets. The number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow. If there are more buckets in a table than required, it means there is less data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. Similarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently. Also, if the cluster is scaled at a later point of time rebalancing may not be optimal. For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data. This attribute is set when creating a table . member-timeout \u00b6 The default member-timeout in SnappyData cluster is 30 seconds. The default spark.network.timeout is 120 seconds and spark.executor.heartbeatInterval is 10 seconds as noted in the Spark documents . If applications require node failure detection to be faster, then these properties should be reduced accordingly ( spark.executor.heartbeatInterval but must always be much lower than spark.network.timeout as specified in the Spark Documents). However, note that this can cause spurious node failures to be reported due to GC pauses. For example, the applications with reduced settings need to be resistant to job failures due to GC settings. This attribute is set in the configuration files conf/locators , conf/servers and conf/leads files. spark.local.dir \u00b6 SnappyData writes table data on disk. By default, the disk location that SnappyData uses is the directory specified using -dir option, while starting the member. SnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size. To achieve better performance, it is recommended to store temporary data on a different disk (preferably using SSD storage) than the table data. This can be done by setting the spark.local.dir property to a location with enough space. For example, ~2X of the data size, in case of single thread execution. In case of concurrent thread execution, the requirement for temp space is approximately data size * number of threads. For example, if the data size in the cluster is 100 GB and three threads are executing concurrent ad hoc analytical queries in the cluster, then the temp space should be ~3X of the data size. This property is set in conf/leads as follows: localhost -spark.local.dir=/path/to/local-directory The path specified is inherited by all servers. The temporary data defaults to /tmp . In case different paths are required on each of the servers, then remove the property from conf/leads and instead set as system property in each of the conf/servers file as follows: localhost ... -J-Dspark.local.dir=/path/to/local-directory1 Operating System Settings \u00b6 For best performance, the following operating system settings are recommended on the lead and server nodes. Ulimit Spark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes. A minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K. To change the limits of these settings for a user, the /etc/security/limits.conf file needs to be updated. A typical limits.conf used for SnappyData servers and leads appears as follows: ec2-user hard nofile 32768 ec2-user soft nofile 32768 ec2-user hard nproc unlimited ec2-user soft nproc 524288 ec2-user hard sigpending unlimited ec2-user soft sigpending 524288 * ec2-user is the user running SnappyData. Recent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) need the NOFILE limit to be increased in systemd configuration too. Edit /etc/systemd/system.conf as root, search for #DefaultLimitNOFILE under the [Manager] section. Uncomment and change it to DefaultLimitNOFILE=32768 . Reboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with \"ulimit -a -S\" (soft limits) and \"ulimit -a -H\" (hard limits). OS Cache Size When there is a lot of disk activity especially during table joins and during an eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages. Add the following to /etc/sysctl.conf using the command sudo vim /etc/sysctl.conf or sudo gedit /etc/sysctl.conf or by using an editor of your choice: vm.dirty_background_ratio=2 vm.dirty_ratio=4 vm.dirty_expire_centisecs=2000 vm.dirty_writeback_centisecs=300 Then apply to current session using the command sudo sysctl -p These settings lower the OS cache buffer sizes which reduce the long GC pauses during disk flush but can decrease overall disk write throughput. This is especially true for slower magnetic disks where the bulk insert throughput can see a noticeable drop (such as 20%), while the duration of GC pauses should reduce significantly (such as 50% or more). If long GC pauses, for example in the range of 10s of seconds, during bulk inserts, updates, or deletes is not a problem then these settings can be skipped. Swap File Since modern operating systems perform lazy allocation, it has been observed that despite setting -Xmx and -Xms settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in the process going down. It is recommended to set the swap space on your system to at least 16 GB or preferably 32 GB. To set swap space use the following commands: # sets a swap space of 32 GB ## If fallocate is available, run the following command: sudo sh -c \"fallocate -l 32G /var/swapfile && chmod 0600 /var/swapfile && mkswap /var/swapfile && swapon /var/swapfile\" ## fallocate is recommended since it is much faster, although not supported by some filesystems such as ext3 and zfs. ## In case fallocate is not available, use dd: sudo dd if = /dev/zero of = /var/swapfile bs = 1M count = 32768 sudo chmod 600 /var/swapfile sudo mkswap /var/swapfile sudo swapon /var/swapfile SnappyData Smart Connector Mode and Local Mode Settings \u00b6 Managing Executor Memory \u00b6 For efficient loading of data from a Smart Connector application or a Local Mode application, all the partitions of the input data are processed in parallel by making use of all the available cores. To improve ingestion speeds, small internal columnar storage structures are created in the Spark application's cluster, which are then directly inserted into the required buckets of the column table in the SnappyData cluster. These internal structures are in encoded form, and for efficient encoding, some memory space is acquired upfront which is independent of the amount of data to be loaded into the tables. For example, if there are 32 cores for the Smart Connector application and there are 32 or more buckets on the column table, then each of the 32 executor threads will consume around 32MB of memory. This indicates that 32MB * 32MB (1 GB) of memory is required. Thus, the default of 1GB for executor memory is not sufficient, and therefore a default of at least 2 GB is recommended in this case. You can modify this setting in the spark.executor.memory property. For more information, refer to the Spark documentation . JVM settings for optimal performance \u00b6 The following JVM settings are set by default on the server nodes of SnappyData cluster. You can use these as guidelines for smart connector and local modes: -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=50 -XX:+CMSClassUnloadingEnabled -XX:-DontCompileHugeMethods -XX:CompileThreshold=2000 -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k -Djdk.nio.maxCachedBufferSize=131072 Example : -XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k CMS collector with ParNew is used by default as above and recommended. GC settings set above have been seen to work best in representative workloads and can be tuned further as per application. For enterprise users off-heap is recommended for best performance. Set in the conf/locators , conf/leads , and conf/servers file. Handling Out-of-Memory Error in SnappyData Cluster \u00b6 When the SnappyData cluster faces an Out-Of-Memory (OOM) situation, it may not function appropriately, and the JVM cannot create a new process to execute the kill command upon OOM. See JDK-8027434 . However, JVM uses the fork() system call to execute the kill command. This system call can fail for large JVMs due to memory overcommit limits in the operating system. Therefore, to solve such issues in SnappyData, jvmkill is used which has much smaller memory requirements. jvmkill is a simple JVMTI agent that forcibly terminates the JVM when it is unable to allocate memory or create a thread. It is also essential for reliability purposes because an OOM error can often leave the JVM in an inconsistent state. Whereas, terminating the JVM allows it to be restarted by an external process manager. A common alternative to this agent is to use the -XX:OnOutOfMemoryError JVM argument to execute a kill -9 command. jvmkill is applied by default to all the nodes in a SnappyData cluster, that is the server, lead, and locator nodes. The jvmkill agent is useful in a smart connector as well as in a local mode too. Optionally when using the -XX:+HeapDumpOnOutOfMemoryError option, you can specify the timeout period for scenarios when the heap dump takes an unusually long time or hangs up. This option can be specified in the configuration file for leads, locators, or servers respectively. For example: -snappydata.onCriticalHeapDumpTimeoutSeconds=10 jvmkill agent issues a SIGTERM signal initially and waits for a default period of 30 seconds. Thereby allowing for graceful shutdown before issuing a SIGKILL if the PID is still running. You can also set the environment variable JVMKILL_SLEEP_SECONDS to set the timeout period. For example: export JVMKILL_SLEEP_SECONDS=10 jvmkill is verified on centos6 and Mac OSX versions. For running SnappyData on any other versions, you can recompile the lib files by running the snappyHome/aqp/src/main/cpp/io/snappydata/build.sh script. This script replaces the lib file located at the following path: For Linux agentPath snappyHome/jars/libgemfirexd.so For Mac agentPath snappyHome/jars/libgemfirexd.dylib Handling the OOM-Killer by OS \u00b6 In the absence of any settings, the data for column tables is stored in off-heap i.e native memory. It uses Java's direct byte buffer which in turn uses glibc as native memory allocator. glibc has a known problem of fragmentation as it does not release the freed memory to the OS immediately to improve on performance. This fragmentation can grow in a long running cluster where memory requirement is extremely high and the memory is utilized rapidly thereby leading to the process being killed by the OS. This is known as OOM-Killer. This issue is faced by all products using glibc memory allocator and they overcome this by either having their own memory manager or tuning glibc according to their needs. To avoid this issuem the following parameters are set by default in the product: * MALLOC_ARENA_MAX = 4 * MALLOC_MMAP_THRESHOLD_=131072 * MALLOC_MMAP_MAX_= 2147483647 You can override the values of these default parameters by modifying the snappy-env.sh template in the conf directory and exporting these shell variables (i.e. like export MALLOC_ARENA_MAX = ... ). Code Generation and Tokenization \u00b6 SnappyData uses generated code for best performance for most of the queries and internal operations. This is done for both Spark-side whole-stage code generation for queries, for example, Technical Preview of Apache Spark 2.0 blog , and internally by SnappyData for many operations. For example, rolling over data from row buffer to column store or merging batches among others. The point key lookup queries on row tables, and JDBC inserts bypass this and perform direct operations. However, for all other operations, the product uses code generation for best performance. In many cases, the first query execution is slightly slower than subsequent query executions. This is primarily due to the overhead of compilation of generated code for the query plan and optimized machine code generation by JVM's hotspot JIT. Each distinct piece of generated code is a separate class which is loaded using its own ClassLoader. To reduce these overheads in multiple runs, this class is reused using a cache whose size is controlled by spark.sql.codegen.cacheSize property (default is 2000). Thus when the size limit of the cache is breached, the older classes that are used for a while gets removed from the cache. Further to minimize the generated plans, SnappyData performs tokenization of the values that are most constant in queries by default. Therefore the queries that differ only in constants can still create the same generated code plan. Thus if an application has a fixed number of query patterns that are used repeatedly, then the effect of the slack during the first execution, due to compilation and JIT, is minimized. Note A single query pattern constitutes of queries that differ only in constant values that are embedded in the query string. For cases where the application has many query patterns, you can increase the value of spark.sql.codegen.cacheSize property from the default size of 2000 . You can also increase the value for JVM's ReservedCodeCacheSize property and add additional RAM capacity accordingly. Note In the smart connector mode, Apache Spark has the default cache size as 100 which cannot be changed while the same property works if you are using SnappyData's Spark distribution.","title":"Important Settings"},{"location":"best_practices/important_settings/#important-settings","text":"Resource allocation is important for the execution of any job. If not configured correctly, the job can consume the entire clusters resources and cause execution failure because of memory and other related problems. This section provides guidelines for configuring the following important settings: Buckets member-timeout spark.local.dir Operating System Settings SnappyData Smart Connector mode and Local mode Settings Code Generation and Tokenization","title":"Important Settings "},{"location":"best_practices/important_settings/#buckets","text":"A bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, buckets are moved around to ensure that the data is balanced across the nodes where the table is defined. The default number of buckets in the SnappyData cluster mode is 128. In the local mode, it is cores*2, subject to a maximum of 64 buckets and a minimum of 8 buckets. The number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow. If there are more buckets in a table than required, it means there is less data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. Similarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently. Also, if the cluster is scaled at a later point of time rebalancing may not be optimal. For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data. This attribute is set when creating a table .","title":"Buckets"},{"location":"best_practices/important_settings/#member-timeout","text":"The default member-timeout in SnappyData cluster is 30 seconds. The default spark.network.timeout is 120 seconds and spark.executor.heartbeatInterval is 10 seconds as noted in the Spark documents . If applications require node failure detection to be faster, then these properties should be reduced accordingly ( spark.executor.heartbeatInterval but must always be much lower than spark.network.timeout as specified in the Spark Documents). However, note that this can cause spurious node failures to be reported due to GC pauses. For example, the applications with reduced settings need to be resistant to job failures due to GC settings. This attribute is set in the configuration files conf/locators , conf/servers and conf/leads files.","title":"member-timeout"},{"location":"best_practices/important_settings/#sparklocaldir","text":"SnappyData writes table data on disk. By default, the disk location that SnappyData uses is the directory specified using -dir option, while starting the member. SnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size. To achieve better performance, it is recommended to store temporary data on a different disk (preferably using SSD storage) than the table data. This can be done by setting the spark.local.dir property to a location with enough space. For example, ~2X of the data size, in case of single thread execution. In case of concurrent thread execution, the requirement for temp space is approximately data size * number of threads. For example, if the data size in the cluster is 100 GB and three threads are executing concurrent ad hoc analytical queries in the cluster, then the temp space should be ~3X of the data size. This property is set in conf/leads as follows: localhost -spark.local.dir=/path/to/local-directory The path specified is inherited by all servers. The temporary data defaults to /tmp . In case different paths are required on each of the servers, then remove the property from conf/leads and instead set as system property in each of the conf/servers file as follows: localhost ... -J-Dspark.local.dir=/path/to/local-directory1","title":"spark.local.dir"},{"location":"best_practices/important_settings/#operating-system-settings","text":"For best performance, the following operating system settings are recommended on the lead and server nodes. Ulimit Spark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes. A minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K. To change the limits of these settings for a user, the /etc/security/limits.conf file needs to be updated. A typical limits.conf used for SnappyData servers and leads appears as follows: ec2-user hard nofile 32768 ec2-user soft nofile 32768 ec2-user hard nproc unlimited ec2-user soft nproc 524288 ec2-user hard sigpending unlimited ec2-user soft sigpending 524288 * ec2-user is the user running SnappyData. Recent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) need the NOFILE limit to be increased in systemd configuration too. Edit /etc/systemd/system.conf as root, search for #DefaultLimitNOFILE under the [Manager] section. Uncomment and change it to DefaultLimitNOFILE=32768 . Reboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with \"ulimit -a -S\" (soft limits) and \"ulimit -a -H\" (hard limits). OS Cache Size When there is a lot of disk activity especially during table joins and during an eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages. Add the following to /etc/sysctl.conf using the command sudo vim /etc/sysctl.conf or sudo gedit /etc/sysctl.conf or by using an editor of your choice: vm.dirty_background_ratio=2 vm.dirty_ratio=4 vm.dirty_expire_centisecs=2000 vm.dirty_writeback_centisecs=300 Then apply to current session using the command sudo sysctl -p These settings lower the OS cache buffer sizes which reduce the long GC pauses during disk flush but can decrease overall disk write throughput. This is especially true for slower magnetic disks where the bulk insert throughput can see a noticeable drop (such as 20%), while the duration of GC pauses should reduce significantly (such as 50% or more). If long GC pauses, for example in the range of 10s of seconds, during bulk inserts, updates, or deletes is not a problem then these settings can be skipped. Swap File Since modern operating systems perform lazy allocation, it has been observed that despite setting -Xmx and -Xms settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in the process going down. It is recommended to set the swap space on your system to at least 16 GB or preferably 32 GB. To set swap space use the following commands: # sets a swap space of 32 GB ## If fallocate is available, run the following command: sudo sh -c \"fallocate -l 32G /var/swapfile && chmod 0600 /var/swapfile && mkswap /var/swapfile && swapon /var/swapfile\" ## fallocate is recommended since it is much faster, although not supported by some filesystems such as ext3 and zfs. ## In case fallocate is not available, use dd: sudo dd if = /dev/zero of = /var/swapfile bs = 1M count = 32768 sudo chmod 600 /var/swapfile sudo mkswap /var/swapfile sudo swapon /var/swapfile","title":"Operating System Settings"},{"location":"best_practices/important_settings/#snappydata-smart-connector-mode-and-local-mode-settings","text":"","title":"SnappyData Smart Connector Mode and Local Mode Settings"},{"location":"best_practices/important_settings/#managing-executor-memory","text":"For efficient loading of data from a Smart Connector application or a Local Mode application, all the partitions of the input data are processed in parallel by making use of all the available cores. To improve ingestion speeds, small internal columnar storage structures are created in the Spark application's cluster, which are then directly inserted into the required buckets of the column table in the SnappyData cluster. These internal structures are in encoded form, and for efficient encoding, some memory space is acquired upfront which is independent of the amount of data to be loaded into the tables. For example, if there are 32 cores for the Smart Connector application and there are 32 or more buckets on the column table, then each of the 32 executor threads will consume around 32MB of memory. This indicates that 32MB * 32MB (1 GB) of memory is required. Thus, the default of 1GB for executor memory is not sufficient, and therefore a default of at least 2 GB is recommended in this case. You can modify this setting in the spark.executor.memory property. For more information, refer to the Spark documentation .","title":"Managing Executor Memory"},{"location":"best_practices/important_settings/#jvm-settings-for-optimal-performance","text":"The following JVM settings are set by default on the server nodes of SnappyData cluster. You can use these as guidelines for smart connector and local modes: -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=50 -XX:+CMSClassUnloadingEnabled -XX:-DontCompileHugeMethods -XX:CompileThreshold=2000 -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k -Djdk.nio.maxCachedBufferSize=131072 Example : -XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k CMS collector with ParNew is used by default as above and recommended. GC settings set above have been seen to work best in representative workloads and can be tuned further as per application. For enterprise users off-heap is recommended for best performance. Set in the conf/locators , conf/leads , and conf/servers file.","title":"JVM settings for optimal performance"},{"location":"best_practices/important_settings/#handling-out-of-memory-error-in-snappydata-cluster","text":"When the SnappyData cluster faces an Out-Of-Memory (OOM) situation, it may not function appropriately, and the JVM cannot create a new process to execute the kill command upon OOM. See JDK-8027434 . However, JVM uses the fork() system call to execute the kill command. This system call can fail for large JVMs due to memory overcommit limits in the operating system. Therefore, to solve such issues in SnappyData, jvmkill is used which has much smaller memory requirements. jvmkill is a simple JVMTI agent that forcibly terminates the JVM when it is unable to allocate memory or create a thread. It is also essential for reliability purposes because an OOM error can often leave the JVM in an inconsistent state. Whereas, terminating the JVM allows it to be restarted by an external process manager. A common alternative to this agent is to use the -XX:OnOutOfMemoryError JVM argument to execute a kill -9 command. jvmkill is applied by default to all the nodes in a SnappyData cluster, that is the server, lead, and locator nodes. The jvmkill agent is useful in a smart connector as well as in a local mode too. Optionally when using the -XX:+HeapDumpOnOutOfMemoryError option, you can specify the timeout period for scenarios when the heap dump takes an unusually long time or hangs up. This option can be specified in the configuration file for leads, locators, or servers respectively. For example: -snappydata.onCriticalHeapDumpTimeoutSeconds=10 jvmkill agent issues a SIGTERM signal initially and waits for a default period of 30 seconds. Thereby allowing for graceful shutdown before issuing a SIGKILL if the PID is still running. You can also set the environment variable JVMKILL_SLEEP_SECONDS to set the timeout period. For example: export JVMKILL_SLEEP_SECONDS=10 jvmkill is verified on centos6 and Mac OSX versions. For running SnappyData on any other versions, you can recompile the lib files by running the snappyHome/aqp/src/main/cpp/io/snappydata/build.sh script. This script replaces the lib file located at the following path: For Linux agentPath snappyHome/jars/libgemfirexd.so For Mac agentPath snappyHome/jars/libgemfirexd.dylib","title":"Handling Out-of-Memory Error in SnappyData Cluster"},{"location":"best_practices/important_settings/#handling-the-oom-killer-by-os","text":"In the absence of any settings, the data for column tables is stored in off-heap i.e native memory. It uses Java's direct byte buffer which in turn uses glibc as native memory allocator. glibc has a known problem of fragmentation as it does not release the freed memory to the OS immediately to improve on performance. This fragmentation can grow in a long running cluster where memory requirement is extremely high and the memory is utilized rapidly thereby leading to the process being killed by the OS. This is known as OOM-Killer. This issue is faced by all products using glibc memory allocator and they overcome this by either having their own memory manager or tuning glibc according to their needs. To avoid this issuem the following parameters are set by default in the product: * MALLOC_ARENA_MAX = 4 * MALLOC_MMAP_THRESHOLD_=131072 * MALLOC_MMAP_MAX_= 2147483647 You can override the values of these default parameters by modifying the snappy-env.sh template in the conf directory and exporting these shell variables (i.e. like export MALLOC_ARENA_MAX = ... ).","title":"Handling the OOM-Killer by OS"},{"location":"best_practices/important_settings/#code-generation-and-tokenization","text":"SnappyData uses generated code for best performance for most of the queries and internal operations. This is done for both Spark-side whole-stage code generation for queries, for example, Technical Preview of Apache Spark 2.0 blog , and internally by SnappyData for many operations. For example, rolling over data from row buffer to column store or merging batches among others. The point key lookup queries on row tables, and JDBC inserts bypass this and perform direct operations. However, for all other operations, the product uses code generation for best performance. In many cases, the first query execution is slightly slower than subsequent query executions. This is primarily due to the overhead of compilation of generated code for the query plan and optimized machine code generation by JVM's hotspot JIT. Each distinct piece of generated code is a separate class which is loaded using its own ClassLoader. To reduce these overheads in multiple runs, this class is reused using a cache whose size is controlled by spark.sql.codegen.cacheSize property (default is 2000). Thus when the size limit of the cache is breached, the older classes that are used for a while gets removed from the cache. Further to minimize the generated plans, SnappyData performs tokenization of the values that are most constant in queries by default. Therefore the queries that differ only in constants can still create the same generated code plan. Thus if an application has a fixed number of query patterns that are used repeatedly, then the effect of the slack during the first execution, due to compilation and JIT, is minimized. Note A single query pattern constitutes of queries that differ only in constant values that are embedded in the query string. For cases where the application has many query patterns, you can increase the value of spark.sql.codegen.cacheSize property from the default size of 2000 . You can also increase the value for JVM's ReservedCodeCacheSize property and add additional RAM capacity accordingly. Note In the smart connector mode, Apache Spark has the default cache size as 100 which cannot be changed while the same property works if you are using SnappyData's Spark distribution.","title":"Code Generation and Tokenization"},{"location":"best_practices/memory_management/","text":"Memory Management \u00b6 Note The following description and best practices are ONLY applicable to the data store cluster nodes that are the nodes that manage in-memory tables in SnappyData. When running in the connector mode, your Spark job runs in isolated JVMs and you will need to estimate its memory requirements. You need to estimate and plan memory/disk for the following objects: In-memory row, column tables. Execution memory for queries, jobs. Shuffle disk space required by queries, jobs. In-memory caching of Spark dataframes, temporary tables. Spark executors and SnappyData in-memory store share the same memory space. SnappyData extends the Spark's memory manager providing a unified space for spark storage, execution and SnappyData column and row tables. This Unified MemoryManager smartly keeps track of memory allocations across Spark execution and the Store, elastically expanding into the other if the room is available. Rather than a pre-allocation strategy where Spark memory is independent of the store, SnappyData uses a unified strategy where all allocations come from a common pool. Essentially, it optimizes the memory utilization to the extent possible. SnappyData also monitors the JVM memory pools and avoids running into out-of-memory conditions in most cases. You can configure the threshold for when data evicts to disk and the critical threshold for heap utilization. When the usage exceeds this critical threshold, memory allocations within SnappyData fail, and a LowMemoryException error is reported. This, however, safeguards the server from crashing due to OutOfMemoryException. Estimating Memory Size for Column and Row Tables \u00b6 Column tables use compression by default and the amount of compression is dependent on the data itself. While we commonly see compression of 50%, it is also possible to achieve much higher compression ratios when the data has many repeated strings or text. Row tables, on the other hand, consume more space than the original data size. There is a per row overhead in SnappyData. While this overhead varies and is dependent on the options configured on the Row table, as a simple guideline we suggest you assume 100 bytes per row as overhead. Thus, it is clear that it is not straightforward to compute the memory requirements. It is recommended that you take a sample of the data set (as close as possible to your production data) and populate each of the tables. Ensure that you create the required indexes and note down the size estimates (in bytes) in the SnappyData Monitoring Console dashboard. You can then extrapolate this number given the total number of records you anticipate to load or grow into, for the memory requirements for your table. Disk and Memory Sizing \u00b6 For efficient use of the disk, the best alternative is to load some sample data and extrapolate for both memory and disk requirements. The disk usage is the sum of all the Total size of the tables. You can check the value of Total Size on the SnappyData Monitoring Console. For total disk requirement, the rule of thumb is ~4X data size which accounts for temporary space required for the compactor and the space required for spark.local.dir . In case of concurrent thread execution,the requirement will differ as mentioned in spark.local.dir . If the data and the temporary storage set with spark.local.dir are in separate locations, then the disk for data storage can be 2X of the total estimated data size while temporary storage can be 2X. The temporary storage is used to shuffle the output of large joins, and a query can potentially shuffle the entire data. Likewise, a massive import can also shuffle data before inserting into partitioned tables. Table Memory Requirements \u00b6 SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (off-heap is enabled), the entire column table is stored in off-heap memory. SnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk. TABLE IS PERSISTED? OVERFLOW IS CONFIGURED? APPROXIMATE HEAP OVERHEAD No No 64 bytes Yes No 120 bytes Yes Yes 152 bytes Note For a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.) TYPE OF INDEX ENTRY APPROXIMATE HEAP OVERHEAD New index entry 80 bytes First non-unique index entry 24 bytes Subsequent non-unique index entry 8 bytes to 24 bytes* If there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes. Estimating Memory Size for Execution \u00b6 Spark and SnappyData also need room for execution. This includes memory for sorting, joining data sets, Spark execution, application managed objects (for example, a UDF allocating memory), etc. Most of these allocations automatically overflow to disk. But it is strongly recommended to allocate minimum 6-8 GB of heap per data server/lead node for production systems that run large scale analytic queries. SnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (for example, columns stored as byte arrays). It is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The memory-size and heap-size properties control the off-heap and on-heap sizes of the SnappyData server process. SnappyData uses JVM heap memory for most of its allocations. Only column tables can use off-heap storage (if configured). We suggest going through the following options and configuring them appropriately based on the suggested sizing estimates. SnappyData Heap Memory \u00b6 Heap is provided for row tables and working/temp object memory. For large imports most external connectors still do not use off-heap for temporary buffers. SnappyData heap memory regions are divided into two parts called Heap Storage Pool and Heap Execution Pool . Sizes of each pool are determined by the configuration parameters provided at boot time to each server. These two regions are only tentative demarcation and can grow into each other based on some conditions. Heap Storage Pool \u00b6 The heap objects which belong to SnappyData storage of Spark storage are accounted here. For example, when a row is inserted into a table or deleted, this pool accounts the memory size of that row. Objects that are temporary and die young are not considered here. As it is difficult and costly to do a precise estimation, this pool is an approximation of heap memory for objects that are going to be long-lived. Since precise estimation of heap memory is difficult, there is a heap monitor thread running in the background. If the total heap as seen by JVM (and not SnappyUnifiedMemoryManager) exceeds critical-heap-percentage the database engine starts canceling jobs and queries and a LowMemoryException is reported. This is also an indication of heap pressure on the system. Heap Execution Pool: \u00b6 During query execution or while running a Spark job, all temporary object allocations are done from this pool. For instance, queries like HashJoin and aggregate queries creates expensive in-memory maps. This pool is used to allocate such memory. You can set the following configuration parameters to control the pools: Parameter Name Default Value Description heap-size 4GB in SnappyData Embedded mode cluster Max heap size which can be used by the JVM spark.memory.storageFraction 50 Fraction of workable memory allocated for storage pool and the remaining memory is allocated to the execution pool. It is recommended that you do not change this setting. critical-heap-percentage 95 The heap percent beyond which the system considers itself in a critical state. This is to safeguard the system from crashing due to an OutOfMemoryException. Beyond this point, SnappyData starts canceling all jobs and queries and a LowMemoryException is reported. This means (100 minus critical-heap-percent ) memory is not allocated to any pool and is unused. eviction-heap-percentage 85.5 Initially, the amount of memory that is available for storage pool is 50% of the total workable memory. This can however grow up to eviction-heap-percentage (default 85.5%). On reaching this threshold it starts evicting table data as per the eviction clause that was specified when creating the table. spark.memory.fraction 0.85 Total workable memory for execution and storage. This fraction is applied after removing reserved memory (100 minus critical-heap-percentage ). This gives a cushion before the system reaches a critical state. It is recommended that you do not change this setting. At the start, each of the two pools is assigned a portion of the available memory. This is driven by spark.memory.storageFraction property (default 50%). However, SnappyData allows each pool to \"balloon\" into the other if capacity is available subject to following rules: The storage pool can grow to the execution pool if the execution pool has some capacity, but not beyond the eviction-heap-percentage . If the storage pool cannot borrow from the executor pool, it evicts some of its own blocks to make space for incoming blocks. If the storage pool has already grown into the execution pool, the execution pool evicts block from the storage pool until the earlier limit (that is, 50% demarcation) is reached. Beyond that, the executor threads cannot evict blocks from the storage pool. If sufficient memory is not available, it can either fall back to disk overflow or wait until sufficient memory is available. If the storage pool has some free memory, the execution pool can borrow that memory from the storage pool during execution. The borrowed memory is returned back once execution is over. Example : Configuration for memory (typically configured in conf/leads or conf/servers ) -heap-size=20g -critical-heap-percentage=95 -eviction-heap-percentage=85.5 Example : Depicts how SnappyData derives different memory region sizes. Reserved_Heap_Memory => 20g * (1 - 0.95) = 1g ( 0.95 being derived from critical_heap_percentage) Heap_Memory_Fraction => (20g - Reserved_Memory) *(0.85) = 17.4 ( 0.85 being derived from spark.memory.fraction) Heap_Storage_Pool_Size => 17.4 * (0.5) = 8.73 ( 0.5 being derived from spark.memory.storageFraction) Heap_Execution_Pool_Size => 17.4 * (0.5) = 8.73 Heap_Max_Storage_pool_Size => 17.4 * 0.85 = 14.7 ( 0.85 derived from eviction_heap_percentage) SnappyData Off-Heap Memory \u00b6 In addition to heap memory, SnappyData can also be configured with off-heap memory. If configured, column table data, as well as many of the execution structures use off-heap memory. For a serious installation, the off-heap setting is recommended. However, several artifacts in the product need heap memory, so some minimum heap size is also required for this. Parameter Name Default Value Description memory-size The default value is either 0 or it gets auto-configured in specific scenarios . Total off-heap memory size Similar to heap pools, off-heap pools are also divided between off-heap storage pool and off-heap execution pool. The rules of borrowing memory from each other also remains same. Example : Off-heap configuration: -heap-size = 4g -memory-size=16g -critical-heap-percentage=95 -eviction-heap-percentage=85.5 Example : How SnappyData derives different memory region sizes. Reserved_Memory ( Heap Memory) => 4g * (1 - 0.95) = 200m ( 0.95 being derived from critical_heap_percentage) Memory_Fraction ( Heap Memory) => (4g - Reserved_Memory) *(0.97) = 3.5g Heap Storage_Pool_Size => 3.5 * (0.5) = 1.75 Heap Execution_Pool_Size => 3.5 * (0.5) = 1.75 Max_Heap_Storage_pool_Size => 3.5g * 0.85 = 2.9 ( 0.85 derived from eviction_heap_percentage) Off-Heap Storage_Pool_Size => 16g * (0.5) = 8g Off-Heap Execution_Pool_Size => 16g * (0.5) = 8g Max_Off_Heap_Storage_pool_Size => 16g * 0.9 = 14.4 ( 0.9 System default) Note For row tables: According to the requirements of your row table size, configure the heap size. Row tables in SnappyData do not use off-heap memory. To read write Parquet and CSV: Parquet and CSV read-write are memory consuming activities for which heap memory is used. Ensure that you provision sufficient heap memory for this. When most of your data reside in column tables: Use off-heap memory, as they are faster and puts less pressure on garbage collection threads. When configuring eviction: The tables are evicted to disk by default. This impacts performance to some degree and hence it is recommended to size your VM before you begin.","title":"Memory Management"},{"location":"best_practices/memory_management/#memory-management","text":"Note The following description and best practices are ONLY applicable to the data store cluster nodes that are the nodes that manage in-memory tables in SnappyData. When running in the connector mode, your Spark job runs in isolated JVMs and you will need to estimate its memory requirements. You need to estimate and plan memory/disk for the following objects: In-memory row, column tables. Execution memory for queries, jobs. Shuffle disk space required by queries, jobs. In-memory caching of Spark dataframes, temporary tables. Spark executors and SnappyData in-memory store share the same memory space. SnappyData extends the Spark's memory manager providing a unified space for spark storage, execution and SnappyData column and row tables. This Unified MemoryManager smartly keeps track of memory allocations across Spark execution and the Store, elastically expanding into the other if the room is available. Rather than a pre-allocation strategy where Spark memory is independent of the store, SnappyData uses a unified strategy where all allocations come from a common pool. Essentially, it optimizes the memory utilization to the extent possible. SnappyData also monitors the JVM memory pools and avoids running into out-of-memory conditions in most cases. You can configure the threshold for when data evicts to disk and the critical threshold for heap utilization. When the usage exceeds this critical threshold, memory allocations within SnappyData fail, and a LowMemoryException error is reported. This, however, safeguards the server from crashing due to OutOfMemoryException.","title":"Memory Management"},{"location":"best_practices/memory_management/#estimating-memory-size-for-column-and-row-tables","text":"Column tables use compression by default and the amount of compression is dependent on the data itself. While we commonly see compression of 50%, it is also possible to achieve much higher compression ratios when the data has many repeated strings or text. Row tables, on the other hand, consume more space than the original data size. There is a per row overhead in SnappyData. While this overhead varies and is dependent on the options configured on the Row table, as a simple guideline we suggest you assume 100 bytes per row as overhead. Thus, it is clear that it is not straightforward to compute the memory requirements. It is recommended that you take a sample of the data set (as close as possible to your production data) and populate each of the tables. Ensure that you create the required indexes and note down the size estimates (in bytes) in the SnappyData Monitoring Console dashboard. You can then extrapolate this number given the total number of records you anticipate to load or grow into, for the memory requirements for your table.","title":"Estimating Memory Size for Column and Row Tables"},{"location":"best_practices/memory_management/#disk-and-memory-sizing","text":"For efficient use of the disk, the best alternative is to load some sample data and extrapolate for both memory and disk requirements. The disk usage is the sum of all the Total size of the tables. You can check the value of Total Size on the SnappyData Monitoring Console. For total disk requirement, the rule of thumb is ~4X data size which accounts for temporary space required for the compactor and the space required for spark.local.dir . In case of concurrent thread execution,the requirement will differ as mentioned in spark.local.dir . If the data and the temporary storage set with spark.local.dir are in separate locations, then the disk for data storage can be 2X of the total estimated data size while temporary storage can be 2X. The temporary storage is used to shuffle the output of large joins, and a query can potentially shuffle the entire data. Likewise, a massive import can also shuffle data before inserting into partitioned tables.","title":"Disk and Memory Sizing"},{"location":"best_practices/memory_management/#table-memory-requirements","text":"SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (off-heap is enabled), the entire column table is stored in off-heap memory. SnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk. TABLE IS PERSISTED? OVERFLOW IS CONFIGURED? APPROXIMATE HEAP OVERHEAD No No 64 bytes Yes No 120 bytes Yes Yes 152 bytes Note For a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.) TYPE OF INDEX ENTRY APPROXIMATE HEAP OVERHEAD New index entry 80 bytes First non-unique index entry 24 bytes Subsequent non-unique index entry 8 bytes to 24 bytes* If there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes.","title":"Table Memory Requirements"},{"location":"best_practices/memory_management/#estimating-memory-size-for-execution","text":"Spark and SnappyData also need room for execution. This includes memory for sorting, joining data sets, Spark execution, application managed objects (for example, a UDF allocating memory), etc. Most of these allocations automatically overflow to disk. But it is strongly recommended to allocate minimum 6-8 GB of heap per data server/lead node for production systems that run large scale analytic queries. SnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (for example, columns stored as byte arrays). It is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The memory-size and heap-size properties control the off-heap and on-heap sizes of the SnappyData server process. SnappyData uses JVM heap memory for most of its allocations. Only column tables can use off-heap storage (if configured). We suggest going through the following options and configuring them appropriately based on the suggested sizing estimates.","title":"Estimating Memory Size for Execution"},{"location":"best_practices/memory_management/#snappydata-heap-memory","text":"Heap is provided for row tables and working/temp object memory. For large imports most external connectors still do not use off-heap for temporary buffers. SnappyData heap memory regions are divided into two parts called Heap Storage Pool and Heap Execution Pool . Sizes of each pool are determined by the configuration parameters provided at boot time to each server. These two regions are only tentative demarcation and can grow into each other based on some conditions.","title":"SnappyData Heap Memory"},{"location":"best_practices/memory_management/#heap-storage-pool","text":"The heap objects which belong to SnappyData storage of Spark storage are accounted here. For example, when a row is inserted into a table or deleted, this pool accounts the memory size of that row. Objects that are temporary and die young are not considered here. As it is difficult and costly to do a precise estimation, this pool is an approximation of heap memory for objects that are going to be long-lived. Since precise estimation of heap memory is difficult, there is a heap monitor thread running in the background. If the total heap as seen by JVM (and not SnappyUnifiedMemoryManager) exceeds critical-heap-percentage the database engine starts canceling jobs and queries and a LowMemoryException is reported. This is also an indication of heap pressure on the system.","title":"Heap Storage Pool"},{"location":"best_practices/memory_management/#heap-execution-pool","text":"During query execution or while running a Spark job, all temporary object allocations are done from this pool. For instance, queries like HashJoin and aggregate queries creates expensive in-memory maps. This pool is used to allocate such memory. You can set the following configuration parameters to control the pools: Parameter Name Default Value Description heap-size 4GB in SnappyData Embedded mode cluster Max heap size which can be used by the JVM spark.memory.storageFraction 50 Fraction of workable memory allocated for storage pool and the remaining memory is allocated to the execution pool. It is recommended that you do not change this setting. critical-heap-percentage 95 The heap percent beyond which the system considers itself in a critical state. This is to safeguard the system from crashing due to an OutOfMemoryException. Beyond this point, SnappyData starts canceling all jobs and queries and a LowMemoryException is reported. This means (100 minus critical-heap-percent ) memory is not allocated to any pool and is unused. eviction-heap-percentage 85.5 Initially, the amount of memory that is available for storage pool is 50% of the total workable memory. This can however grow up to eviction-heap-percentage (default 85.5%). On reaching this threshold it starts evicting table data as per the eviction clause that was specified when creating the table. spark.memory.fraction 0.85 Total workable memory for execution and storage. This fraction is applied after removing reserved memory (100 minus critical-heap-percentage ). This gives a cushion before the system reaches a critical state. It is recommended that you do not change this setting. At the start, each of the two pools is assigned a portion of the available memory. This is driven by spark.memory.storageFraction property (default 50%). However, SnappyData allows each pool to \"balloon\" into the other if capacity is available subject to following rules: The storage pool can grow to the execution pool if the execution pool has some capacity, but not beyond the eviction-heap-percentage . If the storage pool cannot borrow from the executor pool, it evicts some of its own blocks to make space for incoming blocks. If the storage pool has already grown into the execution pool, the execution pool evicts block from the storage pool until the earlier limit (that is, 50% demarcation) is reached. Beyond that, the executor threads cannot evict blocks from the storage pool. If sufficient memory is not available, it can either fall back to disk overflow or wait until sufficient memory is available. If the storage pool has some free memory, the execution pool can borrow that memory from the storage pool during execution. The borrowed memory is returned back once execution is over. Example : Configuration for memory (typically configured in conf/leads or conf/servers ) -heap-size=20g -critical-heap-percentage=95 -eviction-heap-percentage=85.5 Example : Depicts how SnappyData derives different memory region sizes. Reserved_Heap_Memory => 20g * (1 - 0.95) = 1g ( 0.95 being derived from critical_heap_percentage) Heap_Memory_Fraction => (20g - Reserved_Memory) *(0.85) = 17.4 ( 0.85 being derived from spark.memory.fraction) Heap_Storage_Pool_Size => 17.4 * (0.5) = 8.73 ( 0.5 being derived from spark.memory.storageFraction) Heap_Execution_Pool_Size => 17.4 * (0.5) = 8.73 Heap_Max_Storage_pool_Size => 17.4 * 0.85 = 14.7 ( 0.85 derived from eviction_heap_percentage)","title":"Heap Execution Pool:"},{"location":"best_practices/memory_management/#snappydata-off-heap-memory","text":"In addition to heap memory, SnappyData can also be configured with off-heap memory. If configured, column table data, as well as many of the execution structures use off-heap memory. For a serious installation, the off-heap setting is recommended. However, several artifacts in the product need heap memory, so some minimum heap size is also required for this. Parameter Name Default Value Description memory-size The default value is either 0 or it gets auto-configured in specific scenarios . Total off-heap memory size Similar to heap pools, off-heap pools are also divided between off-heap storage pool and off-heap execution pool. The rules of borrowing memory from each other also remains same. Example : Off-heap configuration: -heap-size = 4g -memory-size=16g -critical-heap-percentage=95 -eviction-heap-percentage=85.5 Example : How SnappyData derives different memory region sizes. Reserved_Memory ( Heap Memory) => 4g * (1 - 0.95) = 200m ( 0.95 being derived from critical_heap_percentage) Memory_Fraction ( Heap Memory) => (4g - Reserved_Memory) *(0.97) = 3.5g Heap Storage_Pool_Size => 3.5 * (0.5) = 1.75 Heap Execution_Pool_Size => 3.5 * (0.5) = 1.75 Max_Heap_Storage_pool_Size => 3.5g * 0.85 = 2.9 ( 0.85 derived from eviction_heap_percentage) Off-Heap Storage_Pool_Size => 16g * (0.5) = 8g Off-Heap Execution_Pool_Size => 16g * (0.5) = 8g Max_Off_Heap_Storage_pool_Size => 16g * 0.9 = 14.4 ( 0.9 System default) Note For row tables: According to the requirements of your row table size, configure the heap size. Row tables in SnappyData do not use off-heap memory. To read write Parquet and CSV: Parquet and CSV read-write are memory consuming activities for which heap memory is used. Ensure that you provision sufficient heap memory for this. When most of your data reside in column tables: Use off-heap memory, as they are faster and puts less pressure on garbage collection threads. When configuring eviction: The tables are evicted to disk by default. This impacts performance to some degree and hence it is recommended to size your VM before you begin.","title":"SnappyData Off-Heap Memory"},{"location":"best_practices/odbc_jdbc_clients/","text":"ODBC and JDBC Clients \u00b6 When using JDBC or ODBC clients, applications must close the ResultSet that is consumed or consume a FORWARD_ONLY ResultSet completely. These ResultSets can keep tables open and thereby block any DDL executions. If the cursor used by ResultSet remains open, then the DDL executions may time out. Such intermittent ResultSets are eventually cleaned up by the product, but that happens only in a garbage collection (GC) cycle where JVM cleans the weak references corresponding to those ResultSets. However, this process can take an indeterminate amount of time so it's recommended for users to clean up the ResultSets, Statements and other JDBC/ODBC constructs immediately after use (using try-resources in Java and equivalent in other languages).","title":"ODBC and JDBC Clients"},{"location":"best_practices/odbc_jdbc_clients/#odbc-and-jdbc-clients","text":"When using JDBC or ODBC clients, applications must close the ResultSet that is consumed or consume a FORWARD_ONLY ResultSet completely. These ResultSets can keep tables open and thereby block any DDL executions. If the cursor used by ResultSet remains open, then the DDL executions may time out. Such intermittent ResultSets are eventually cleaned up by the product, but that happens only in a garbage collection (GC) cycle where JVM cleans the weak references corresponding to those ResultSets. However, this process can take an indeterminate amount of time so it's recommended for users to clean up the ResultSets, Statements and other JDBC/ODBC constructs immediately after use (using try-resources in Java and equivalent in other languages).","title":"ODBC and JDBC Clients"},{"location":"best_practices/setup_cluster/","text":"Tuning for Concurrency and Performance \u00b6 Handling Low Latency vs Analytic Jobs \u00b6 Unlike Spark, SnappyData can distinguish requests that are cheap (low latency) vs requests that require a lot of computational resources (high latency). This is done by a resource scheduler that can balance the needs of many contending users/threads. For instance, when a SQL client executes a \u2018fetch by primary key\u2019 query, there is no need to involve any scheduler or spawn many tasks for such a simple request. The request is immediately delegated to the data node (single thread) and the response is directly sent to the requesting client (probably within a few milliseconds). In the current version of the product, all query requests that filter on a primary key, a set of keys, or can directly filter using an index are executed without routing to the Job scheduler. Only Row tables can have primary keys or indexes. When the above conditions are not met, the request is routed to the \u2018Lead\u2019 node where the Spark plan is generated and \u2018jobs\u2019 are scheduled for execution. The scheduler uses a FAIR scheduling algorithm for higher concurrency, that is, all concurrent jobs are executed in a round-robin manner. Each job is made up of one or more stages and the planning phase computes the number of parallel tasks for the stage. Tasks from scheduled jobs are then allocated to the logical cores available until all cores are allocated. A round-robin algorithm picks a task from Job1, a task from Job2 and so on. If more cores are available, the second task from Job1 is picked and the cycle continues. But, there are circumstances a single job can completely consume all cores. For instance, when all cores are available, if a large loading job is scheduled it receives all available cores of which, each of the tasks can be long running. During this time, if other concurrent jobs are assigned, none of the executing tasks is preempted. So, with multiple concurrent users, it is best to avoid running such Jobs using the default high throughput pool. Note This above scheduling logic is applicable only when queries are fully managed by SnappyData cluster. When running your application using the smart connector, each task running in the Spark cluster directly accesses the store partitions. Computing the Number of Cores for a Job \u00b6 Executing queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job first calculates the number of partitions on the underlying dataset and a task is assigned and scheduled for each partition. But, the number of concurrent tasks executing is limited by the available core count. If the scheduled task count is larger then they will be executed in a staggered manner. Each task is assigned a single core to execute. By default, SnappyData sets total available spark executor cores on any data server to be 2 multiplied by the total number of physical cores on a machine. Spark executor cores = 2 * C Where C = Total number of physical processor cores on a machine. This is done keeping in mind some threads which do the IO (Input/Output) reads while the other threads can get scheduled by the operating system. However, increasing this number does not improve query performance. If you start multiple SnappyData server instances on a machine, then spark.executor.cores should be explicitly set using the following formula: spark executor cores = N * C Where N is the number of SnappyData server instaces started on the machine and C = Total number of physical processor cores on a machine. This can impact thread scheduling and query performance. Therefore, it is recommended that you maintain only one server per machine so that the total number of threads do not exceed more than twice the number of total physical cores. If you want to run more than one server on a machine, you can use the spark.executor.cores to override the number of cores per server. The cores should be divided accordingly and specified using the spark.executor.cores property. Spark executor cores = 2 x total physical cores on a machine / No of servers per machine Note It is not recommended to reduce the heap size even if you start multiple servers per machine. For example, for a cluster with 2 servers running on two different machines with 4 CPU cores each, a maximum number of tasks that can run concurrently is 16. If a table has 16 partitions (buckets, for the row or column tables), a scan query on this table creates 16 tasks. This means, 16 tasks run concurrently and the last task runs when one of these 16 tasks has finished execution. SnappyData uses an optimization method which joins multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions. In SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get a fair distribution of resources. For example, In the following figure, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently. Pending tasks have to wait for the completion of the current tasks and are assigned to the core that is first available. When you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute. Configuring the Scheduler Pools for Concurrency \u00b6 SnappyData out of the box comes configured with two execution pools: Low-latency pool : This pool is automatically used when SnappyData determines that a request is of low latency, that is, the queries that are partition pruned to two or fewer partitions. Default pool : This is the pool that is used for the remaining requests. Two cores are statically assigned to the low latency pool. Also, the low latency pool has weight twice that of the default pool. Thus, if there are 30 cores available to an executor for a query that has 30 partitions, only 28 would be assigned to it and two cores would be reserved to not starve the low latency queries. When the system has both low latency and normal queries, 20 cores are used for the low latency queries as it has higher priority (weight=2). If a query requires all 30 partitions and no low latency queries are running at that time, all 30 cores are assigned to the first query. However, when a low latency query is assigned, the scheduler does its best to allocate cores as soon as tasks from the earlier query finish. Applications can explicitly configure to use a particular pool for the current session using a SQL configuration property, snappydata.scheduler.pool . For example, the set snappydata.scheduler.pool=lowlatency command sets the pool as low latency pool for the current session. New pools can be added and properties of the existing pools can be configured by modifying the conf/fairscheduler.xml file. We do not recommend changing the pool names ( default and lowlatency ). Controlling CPU Usage for User Jobs \u00b6 You can control the CPU usage for user jobs by configuring separate pools for different kinds of jobs. See configuration here . The product is configured with two out-of-the-box pools, that is the Default pool and the Low-latency pool . The Default pool has higher priority and also has a minShare , so that some minimum cores are reserved for those jobs if possible. The Stages tab on the SnappyData Monitoring Console shows the available pools. When you track a job for an SQL query on the SQL tab, it shows the pool that is used in the Pool Name column. In-built tasks such as ingestion can show lower priority pools by default to give priority to foreground queries. To configure such priority, do the following: Define the pools in conf/fairscheduler.xml Set a pool for a job using Spark API or use set snappydata.scheduler.pool property in a SnappySession. To configure the priority based on specific requirements, you can also either permit the users to set the priority for queries or add some pool allocation logic in the application as per client requirements. Using a Partitioning Strategy to Increase Concurrency \u00b6 The best way to increasing concurrency is to design your schema such that you minimize the need to run your queries across many partitions. The common strategy is to understand your application patterns and choose a partitioning strategy such that queries often target a specific partition. Such queries will be pruned to a single node and SnappyData automatically optimises such queries to use a single task. For more information see, How to design your schema . Using Smart Connector for Expanding Capacity of the Cluster \u00b6 One of the instances, when SnappyData Smart connector mode is useful, is when the computations is separate from the data. This allows you to increase the computational capacity without adding more servers to the SnappyData cluster. Thus, more executors can be provisioned for a Smart Connector application than the number of SnappyData servers. Also, expensive batch jobs can be run in a separate Smart Connector application and it does not impact the performance of the SnappyData cluster. See, How to Access SnappyData store from an Existing Spark Installation using Smart Connector . Disabling hashJoin/Aggregate for Cluster Stability \u00b6 SnappyData has its own operator for extremely fast joins/aggregates. One limitation of this operator is that it stores the objects in Java heap (Or Offheap , if the system is configured for it.). If heap size is small, it has the potential to cause Garbage Collection (GC) problems and eventually Out-of-memory (OOM) (If the heap (or offheap size) is small, it will no longer cause OOM (as the memory used by SHA is accounted for ), but the memory manager will cause the query to fail by throwing LowMemoryException). Sometimes, even if the heap size is large this can happen if there are many concurrent queries using these operators. If your use case is such that there will be many joins or aggregates with high cardinality then it is recommended to disable that hashJoin/hashAggregates. To disable hashJoin/hashAggregates for a particular session you can use the following: set snappydata . sql . hashAggregateSize =- 1 set snappydata . sql . hashJoinSize =- 1 To disable hashJoin/hashAggregates for every session, provide the same details in lead.conf file. For example: -snappydata.sql.hashAggregateSize=-1 -snappydata.sql.hashJoinSize=-1 You must remember that for small dataset the hashJoin operator is much faster than sort merge join. So unless you have a query pattern that can cause high heap usage, it is recommended not to disable hashJoin/hashAggregates.","title":"Tuning for Concurrency and Computation"},{"location":"best_practices/setup_cluster/#tuning-for-concurrency-and-performance","text":"","title":"Tuning for Concurrency and Performance"},{"location":"best_practices/setup_cluster/#handling-low-latency-vs-analytic-jobs","text":"Unlike Spark, SnappyData can distinguish requests that are cheap (low latency) vs requests that require a lot of computational resources (high latency). This is done by a resource scheduler that can balance the needs of many contending users/threads. For instance, when a SQL client executes a \u2018fetch by primary key\u2019 query, there is no need to involve any scheduler or spawn many tasks for such a simple request. The request is immediately delegated to the data node (single thread) and the response is directly sent to the requesting client (probably within a few milliseconds). In the current version of the product, all query requests that filter on a primary key, a set of keys, or can directly filter using an index are executed without routing to the Job scheduler. Only Row tables can have primary keys or indexes. When the above conditions are not met, the request is routed to the \u2018Lead\u2019 node where the Spark plan is generated and \u2018jobs\u2019 are scheduled for execution. The scheduler uses a FAIR scheduling algorithm for higher concurrency, that is, all concurrent jobs are executed in a round-robin manner. Each job is made up of one or more stages and the planning phase computes the number of parallel tasks for the stage. Tasks from scheduled jobs are then allocated to the logical cores available until all cores are allocated. A round-robin algorithm picks a task from Job1, a task from Job2 and so on. If more cores are available, the second task from Job1 is picked and the cycle continues. But, there are circumstances a single job can completely consume all cores. For instance, when all cores are available, if a large loading job is scheduled it receives all available cores of which, each of the tasks can be long running. During this time, if other concurrent jobs are assigned, none of the executing tasks is preempted. So, with multiple concurrent users, it is best to avoid running such Jobs using the default high throughput pool. Note This above scheduling logic is applicable only when queries are fully managed by SnappyData cluster. When running your application using the smart connector, each task running in the Spark cluster directly accesses the store partitions.","title":"Handling Low Latency vs Analytic Jobs"},{"location":"best_practices/setup_cluster/#computing-the-number-of-cores-for-a-job","text":"Executing queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job first calculates the number of partitions on the underlying dataset and a task is assigned and scheduled for each partition. But, the number of concurrent tasks executing is limited by the available core count. If the scheduled task count is larger then they will be executed in a staggered manner. Each task is assigned a single core to execute. By default, SnappyData sets total available spark executor cores on any data server to be 2 multiplied by the total number of physical cores on a machine. Spark executor cores = 2 * C Where C = Total number of physical processor cores on a machine. This is done keeping in mind some threads which do the IO (Input/Output) reads while the other threads can get scheduled by the operating system. However, increasing this number does not improve query performance. If you start multiple SnappyData server instances on a machine, then spark.executor.cores should be explicitly set using the following formula: spark executor cores = N * C Where N is the number of SnappyData server instaces started on the machine and C = Total number of physical processor cores on a machine. This can impact thread scheduling and query performance. Therefore, it is recommended that you maintain only one server per machine so that the total number of threads do not exceed more than twice the number of total physical cores. If you want to run more than one server on a machine, you can use the spark.executor.cores to override the number of cores per server. The cores should be divided accordingly and specified using the spark.executor.cores property. Spark executor cores = 2 x total physical cores on a machine / No of servers per machine Note It is not recommended to reduce the heap size even if you start multiple servers per machine. For example, for a cluster with 2 servers running on two different machines with 4 CPU cores each, a maximum number of tasks that can run concurrently is 16. If a table has 16 partitions (buckets, for the row or column tables), a scan query on this table creates 16 tasks. This means, 16 tasks run concurrently and the last task runs when one of these 16 tasks has finished execution. SnappyData uses an optimization method which joins multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions. In SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get a fair distribution of resources. For example, In the following figure, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently. Pending tasks have to wait for the completion of the current tasks and are assigned to the core that is first available. When you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute.","title":"Computing the Number of Cores for a Job"},{"location":"best_practices/setup_cluster/#configuring-the-scheduler-pools-for-concurrency","text":"SnappyData out of the box comes configured with two execution pools: Low-latency pool : This pool is automatically used when SnappyData determines that a request is of low latency, that is, the queries that are partition pruned to two or fewer partitions. Default pool : This is the pool that is used for the remaining requests. Two cores are statically assigned to the low latency pool. Also, the low latency pool has weight twice that of the default pool. Thus, if there are 30 cores available to an executor for a query that has 30 partitions, only 28 would be assigned to it and two cores would be reserved to not starve the low latency queries. When the system has both low latency and normal queries, 20 cores are used for the low latency queries as it has higher priority (weight=2). If a query requires all 30 partitions and no low latency queries are running at that time, all 30 cores are assigned to the first query. However, when a low latency query is assigned, the scheduler does its best to allocate cores as soon as tasks from the earlier query finish. Applications can explicitly configure to use a particular pool for the current session using a SQL configuration property, snappydata.scheduler.pool . For example, the set snappydata.scheduler.pool=lowlatency command sets the pool as low latency pool for the current session. New pools can be added and properties of the existing pools can be configured by modifying the conf/fairscheduler.xml file. We do not recommend changing the pool names ( default and lowlatency ).","title":"Configuring the Scheduler Pools for Concurrency"},{"location":"best_practices/setup_cluster/#controlling-cpu-usage-for-user-jobs","text":"You can control the CPU usage for user jobs by configuring separate pools for different kinds of jobs. See configuration here . The product is configured with two out-of-the-box pools, that is the Default pool and the Low-latency pool . The Default pool has higher priority and also has a minShare , so that some minimum cores are reserved for those jobs if possible. The Stages tab on the SnappyData Monitoring Console shows the available pools. When you track a job for an SQL query on the SQL tab, it shows the pool that is used in the Pool Name column. In-built tasks such as ingestion can show lower priority pools by default to give priority to foreground queries. To configure such priority, do the following: Define the pools in conf/fairscheduler.xml Set a pool for a job using Spark API or use set snappydata.scheduler.pool property in a SnappySession. To configure the priority based on specific requirements, you can also either permit the users to set the priority for queries or add some pool allocation logic in the application as per client requirements.","title":"Controlling CPU Usage for User Jobs"},{"location":"best_practices/setup_cluster/#using-a-partitioning-strategy-to-increase-concurrency","text":"The best way to increasing concurrency is to design your schema such that you minimize the need to run your queries across many partitions. The common strategy is to understand your application patterns and choose a partitioning strategy such that queries often target a specific partition. Such queries will be pruned to a single node and SnappyData automatically optimises such queries to use a single task. For more information see, How to design your schema .","title":"Using a Partitioning Strategy to Increase Concurrency"},{"location":"best_practices/setup_cluster/#using-smart-connector-for-expanding-capacity-of-the-cluster","text":"One of the instances, when SnappyData Smart connector mode is useful, is when the computations is separate from the data. This allows you to increase the computational capacity without adding more servers to the SnappyData cluster. Thus, more executors can be provisioned for a Smart Connector application than the number of SnappyData servers. Also, expensive batch jobs can be run in a separate Smart Connector application and it does not impact the performance of the SnappyData cluster. See, How to Access SnappyData store from an Existing Spark Installation using Smart Connector .","title":"Using Smart Connector for Expanding Capacity of the Cluster"},{"location":"best_practices/setup_cluster/#disabling-hashjoinaggregate-for-cluster-stability","text":"SnappyData has its own operator for extremely fast joins/aggregates. One limitation of this operator is that it stores the objects in Java heap (Or Offheap , if the system is configured for it.). If heap size is small, it has the potential to cause Garbage Collection (GC) problems and eventually Out-of-memory (OOM) (If the heap (or offheap size) is small, it will no longer cause OOM (as the memory used by SHA is accounted for ), but the memory manager will cause the query to fail by throwing LowMemoryException). Sometimes, even if the heap size is large this can happen if there are many concurrent queries using these operators. If your use case is such that there will be many joins or aggregates with high cardinality then it is recommended to disable that hashJoin/hashAggregates. To disable hashJoin/hashAggregates for a particular session you can use the following: set snappydata . sql . hashAggregateSize =- 1 set snappydata . sql . hashJoinSize =- 1 To disable hashJoin/hashAggregates for every session, provide the same details in lead.conf file. For example: -snappydata.sql.hashAggregateSize=-1 -snappydata.sql.hashJoinSize=-1 You must remember that for small dataset the hashJoin operator is much faster than sort merge join. So unless you have a query pattern that can cause high heap usage, it is recommended not to disable hashJoin/hashAggregates.","title":"Disabling hashJoin/Aggregate for Cluster Stability"},{"location":"best_practices/structured_streaming_best_practices/","text":"Structured Streaming \u00b6 The following best practices for Structured Streaming are explained in this section: Using Shared File System as Checkpoint Directory Location Limiting Batch Size Limiting Default Incoming Data Frame Size Running a Structured Streaming Query with Dedicated SnappySession Instance SnappySession used for Streaming Query should not be used by other Operations Using Shared File System as Checkpoint Directory Location \u00b6 The location used to store checkpoint directory content should be on a shared file system like HDFS, which is accessible from all the nodes. This is required because the incremental aggregation state of a streaming query is stored as part of checkpoint directory itself. So if one of the executor nodes goes down, the aggregation state stored by that node needs to be accessible from the other executor nodes for the proper functioning of the streaming query. Limiting Batch Size \u00b6 A good practice is to limit the batch size of a streaming query such that it remains below spark.sql.autoBroadcastJoinThreshold while using Snappy Sink . This gives the following advantages: Snappy Sink internally caches the incoming dataframe batch. If the batch size is too large, the cached dataframe might not fit in the memory and can start spilling over to the disk. This can lead to performance issues. By limiting the batch size to spark.sql.autoBroadcastJoinThreshold , you can ensure that the putInto operation, that is performed as part of Snappy Sink , uses broadcast join which is significantly faster than sort merge join. The batch size can be restricted using one of the following options depending upon the source: For Apache Kafka Source \u00b6 maxOffsetsPerTrigger - Rate limit on the maximum number of offsets processed for each trigger interval. The specified total number of offsets are proportionally split across topic Partitions of different volume. (default: no max) Example: val streamingDF = snappySession . readStream . format ( \"kafka\" ) . option ( \"kafka.bootstrap.servers\" , \"localhost:9091\" ) . option ( \"maxOffsetsPerTrigger\" , 100 ) . option ( \"subscribe\" , \"topic1\" ) . load For File Sources \u00b6 maxFilesPerTrigger - Maximum number of new files to be considered in every trigger (default: no max). Example: val inputPath = \"/path/to/parquet_input\" val schema = snappy . read . parquet ( inputPath ). schema val df = snappy . readStream . schema ( schema ) . option ( \"maxFilesPerTrigger\" , 1 ) . format ( \"parquet\" ) . load ( inputPath ) Limiting Default Incoming Data Frame Size \u00b6 Spark relies on the data size statistics provided by the sources to decide join type to be used for the query plan. Some sources do not provide the correct size statistics and in such a case, Spark falls down to the default value, which is Long.MaxValue which is greater than spark.sql.autoBroadcastJoinThreshold . As a result of that the putInto join query always uses the sort merge join even if the incoming batch size is lesser than spark.sql.autoBroadcastJoinThreshold . A broadcast join is more performant than a sort merge join. To overcome this, use the session level property spark.sql.defaultSizeInBytesyou and override the default size. The value set for this property should be approximately equal to the maximum batch size that you expect after complying to the suggestion mentioned in Limiting Batch Size section. It can be set using the following SQL command: set spark . sql . defaultSizeInBytes = < some long value > For example: set spark . sql . defaultSizeInBytes = 10000 Using SnappySession instance, you can be run the same as follows: snappySession . sql ( \u201c set spark . sql . defaultSizeInBytes = 10000 \u201d ) Running a Structured Streaming Query with Dedicated SnappySession Instance \u00b6 A good practice is to run each structured streaming query using it\u2019s own dedicated instance of SnappySession. A new instance of SnappySession can be created as follows: val newSession = snappySession . newSession () The newSession instance has a similar session level config as snappySession. Note For embedded snappy jobs, it is recommended to use a new snappy-job for each streaming query. SnappySession used for Streaming Query should not be used by other Operations \u00b6 When running a structured streaming query, Snappy hash aggregate is disabled for that entire session. This is done because SnappyData's hash aggregate does not work along with stateful aggregation required for streaming query aggregation. A best practice is to avoid using the same SnappySession instance for other operations since the Snappy hash aggregation is disabled for the entire session.","title":"Structured Streaming"},{"location":"best_practices/structured_streaming_best_practices/#structured-streaming","text":"The following best practices for Structured Streaming are explained in this section: Using Shared File System as Checkpoint Directory Location Limiting Batch Size Limiting Default Incoming Data Frame Size Running a Structured Streaming Query with Dedicated SnappySession Instance SnappySession used for Streaming Query should not be used by other Operations","title":"Structured Streaming"},{"location":"best_practices/structured_streaming_best_practices/#using-shared-file-system-as-checkpoint-directory-location","text":"The location used to store checkpoint directory content should be on a shared file system like HDFS, which is accessible from all the nodes. This is required because the incremental aggregation state of a streaming query is stored as part of checkpoint directory itself. So if one of the executor nodes goes down, the aggregation state stored by that node needs to be accessible from the other executor nodes for the proper functioning of the streaming query.","title":"Using Shared File System as Checkpoint Directory Location"},{"location":"best_practices/structured_streaming_best_practices/#limiting-batch-size","text":"A good practice is to limit the batch size of a streaming query such that it remains below spark.sql.autoBroadcastJoinThreshold while using Snappy Sink . This gives the following advantages: Snappy Sink internally caches the incoming dataframe batch. If the batch size is too large, the cached dataframe might not fit in the memory and can start spilling over to the disk. This can lead to performance issues. By limiting the batch size to spark.sql.autoBroadcastJoinThreshold , you can ensure that the putInto operation, that is performed as part of Snappy Sink , uses broadcast join which is significantly faster than sort merge join. The batch size can be restricted using one of the following options depending upon the source:","title":"Limiting Batch Size"},{"location":"best_practices/structured_streaming_best_practices/#for-apache-kafka-source","text":"maxOffsetsPerTrigger - Rate limit on the maximum number of offsets processed for each trigger interval. The specified total number of offsets are proportionally split across topic Partitions of different volume. (default: no max) Example: val streamingDF = snappySession . readStream . format ( \"kafka\" ) . option ( \"kafka.bootstrap.servers\" , \"localhost:9091\" ) . option ( \"maxOffsetsPerTrigger\" , 100 ) . option ( \"subscribe\" , \"topic1\" ) . load","title":"For Apache Kafka Source"},{"location":"best_practices/structured_streaming_best_practices/#for-file-sources","text":"maxFilesPerTrigger - Maximum number of new files to be considered in every trigger (default: no max). Example: val inputPath = \"/path/to/parquet_input\" val schema = snappy . read . parquet ( inputPath ). schema val df = snappy . readStream . schema ( schema ) . option ( \"maxFilesPerTrigger\" , 1 ) . format ( \"parquet\" ) . load ( inputPath )","title":"For File Sources"},{"location":"best_practices/structured_streaming_best_practices/#limiting-default-incoming-data-frame-size","text":"Spark relies on the data size statistics provided by the sources to decide join type to be used for the query plan. Some sources do not provide the correct size statistics and in such a case, Spark falls down to the default value, which is Long.MaxValue which is greater than spark.sql.autoBroadcastJoinThreshold . As a result of that the putInto join query always uses the sort merge join even if the incoming batch size is lesser than spark.sql.autoBroadcastJoinThreshold . A broadcast join is more performant than a sort merge join. To overcome this, use the session level property spark.sql.defaultSizeInBytesyou and override the default size. The value set for this property should be approximately equal to the maximum batch size that you expect after complying to the suggestion mentioned in Limiting Batch Size section. It can be set using the following SQL command: set spark . sql . defaultSizeInBytes = < some long value > For example: set spark . sql . defaultSizeInBytes = 10000 Using SnappySession instance, you can be run the same as follows: snappySession . sql ( \u201c set spark . sql . defaultSizeInBytes = 10000 \u201d )","title":"Limiting Default Incoming Data Frame Size"},{"location":"best_practices/structured_streaming_best_practices/#running-a-structured-streaming-query-with-dedicated-snappysession-instance","text":"A good practice is to run each structured streaming query using it\u2019s own dedicated instance of SnappySession. A new instance of SnappySession can be created as follows: val newSession = snappySession . newSession () The newSession instance has a similar session level config as snappySession. Note For embedded snappy jobs, it is recommended to use a new snappy-job for each streaming query.","title":"Running a Structured Streaming Query with Dedicated SnappySession Instance"},{"location":"best_practices/structured_streaming_best_practices/#snappysession-used-for-streaming-query-should-not-be-used-by-other-operations","text":"When running a structured streaming query, Snappy hash aggregate is disabled for that entire session. This is done because SnappyData's hash aggregate does not work along with stateful aggregation required for streaming query aggregation. A best practice is to avoid using the same SnappySession instance for other operations since the Snappy hash aggregation is disabled for the entire session.","title":"SnappySession used for Streaming Query should not be used by other Operations"},{"location":"best_practices/transactions_best_practices/","text":"SnappyData Distributed Transactions \u00b6 Using Transactions \u00b6 For high performance, minimize the duration of transactions to avoid conflicts with other concurrent transactions. If atomicity for only single row updates is required, then completely avoid using transactions because SnappyData provides atomicity and isolation for single rows without transactions. When using transactions, keep the number of rows involved in the transaction as low as possible. SnappyData acquires locks eagerly, and long-lasting transactions increase the probability of conflicts and transaction failures. Avoid transactions for large batch update statements or statements that effect a lot of rows. Unlike in traditional databases, SnappyData transactions can fail with a conflict exception on writes instead of on commit. This choice makes sense given that the outcome of the transaction has been determined to fail. To the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided. DDL Statements in a transaction SnappyData permits schema and data manipulation statements (DML) within a single transaction. A data definition statement (DDL) is not automatically committed when it is performed, but participates in the transaction within which it is issued. Although the table itself becomes visible in the system immediately, it acquires exclusive locks on the system tables and the affected tables on all the members in the cluster, so that any DML operations in other transactions will block and wait for the table's locks. For example, if a new index is created on a table in a transaction, then all other transactions that refer to that table wait for the transaction to commit or roll back. Because of this behavior, as a best practice you should keep transactions that involve DDL statements short (preferably in a single transaction by itself). Using Snapshot Isolation \u00b6 To the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided. In case of failure, the rollback is complete and not partial. More information Overview of SnappyData Distributed Transactions How to use Transactions Isolation Levels","title":"SnappyData Distributed Transactions"},{"location":"best_practices/transactions_best_practices/#snappydata-distributed-transactions","text":"","title":"SnappyData Distributed Transactions"},{"location":"best_practices/transactions_best_practices/#using-transactions","text":"For high performance, minimize the duration of transactions to avoid conflicts with other concurrent transactions. If atomicity for only single row updates is required, then completely avoid using transactions because SnappyData provides atomicity and isolation for single rows without transactions. When using transactions, keep the number of rows involved in the transaction as low as possible. SnappyData acquires locks eagerly, and long-lasting transactions increase the probability of conflicts and transaction failures. Avoid transactions for large batch update statements or statements that effect a lot of rows. Unlike in traditional databases, SnappyData transactions can fail with a conflict exception on writes instead of on commit. This choice makes sense given that the outcome of the transaction has been determined to fail. To the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided. DDL Statements in a transaction SnappyData permits schema and data manipulation statements (DML) within a single transaction. A data definition statement (DDL) is not automatically committed when it is performed, but participates in the transaction within which it is issued. Although the table itself becomes visible in the system immediately, it acquires exclusive locks on the system tables and the affected tables on all the members in the cluster, so that any DML operations in other transactions will block and wait for the table's locks. For example, if a new index is created on a table in a transaction, then all other transactions that refer to that table wait for the transaction to commit or roll back. Because of this behavior, as a best practice you should keep transactions that involve DDL statements short (preferably in a single transaction by itself).","title":"Using Transactions"},{"location":"best_practices/transactions_best_practices/#using-snapshot-isolation","text":"To the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided. In case of failure, the rollback is complete and not partial. More information Overview of SnappyData Distributed Transactions How to use Transactions Isolation Levels","title":"Using Snapshot Isolation"},{"location":"best_practices/design_schema/","text":"Designing your Database and Schema \u00b6 The key design goal for achieving linear scaling is to use a partitioning strategy that allows most data access (queries) to be pruned to a single partition. This avoids expensive locking operations across multiple partitions during query execution. In a highly concurrent system that has thousands of connections, multiple queries are generally spread uniformly across the entire data set (and therefore across all partitions). Therefore, increasing the number of data stores enables linear scalability. Given sufficient network performance, additional connections can be supported without degrading the response time for queries. The following topics are covered in this section: Design Principles of Scalable, Partition-Aware Databases Optimizing Query Latency: Partitioning and Replication Strategies","title":"Designing your Database and Schema"},{"location":"best_practices/design_schema/#designing-your-database-and-schema","text":"The key design goal for achieving linear scaling is to use a partitioning strategy that allows most data access (queries) to be pruned to a single partition. This avoids expensive locking operations across multiple partitions during query execution. In a highly concurrent system that has thousands of connections, multiple queries are generally spread uniformly across the entire data set (and therefore across all partitions). Therefore, increasing the number of data stores enables linear scalability. Given sufficient network performance, additional connections can be supported without degrading the response time for queries. The following topics are covered in this section: Design Principles of Scalable, Partition-Aware Databases Optimizing Query Latency: Partitioning and Replication Strategies","title":"Designing your Database and Schema"},{"location":"best_practices/design_schema/design_principles/","text":"Design Principles of Scalable, Partition-Aware Databases \u00b6 The general strategy for designing a SnappyData database is to identify the tables to partition or replicate in the SnappyData cluster, and then to determine the correct partitioning key(s) for partitioned tables. This usually requires an iterative process to produce the optimal design: Read Identify Entity Groups and Partitioning Keys and Replicate Dimension Tables to understand the basic rules for defining partitioned or replicated tables. Evaluate your data access patterns to define those entity groups that are candidates for partitioning. Focus your efforts on commonly-joined entities. Colocating commonly joined tables will improve the performance of join queries by avoiding shuffle of data when the join is on partitioning keys. Identify all of the tables in the entity groups. Identify the \u201cpartitioning key\u201d for each partitioned table. The partitioning key is the column or set of columns that are common across a set of related tables. Look for parent-child relationships in the joined tables. Identify all of the tables that are candidates for replication. You can replicate table data for high availability, or to colocate table data that is necessary to execute joins. Identify Entity Groups and Partitioning Keys \u00b6 In relational database terms, an entity group corresponds to rows that are related to one another through foreign key relationships. Members of an entity group are typically related by parent-child relationships and can be managed in a single partition. To design a SnappyData database for data partitioning, begin by identifying \u201centity groups\u201d and their associated partitioning keys. For example: In a customer order management system, most transactions operate on data related to a single customer at a time. Queries frequently join a customer\u2019s billing information with their orders and shipping information. For this type of application, you partition related tables using the customer identity. Any customer row along with their \u201corder\u201d and \u201cshipping\u201d rows forms a single entity group that has the customer ID as the entity group identity (the partitioning key). You would partition related tables using the customer identity, which would enable you to scale the system linearly by adding more members to support additional customers. In a system that manages a product catalog (product categories, product specifications, customer reviews, rebates, related products, and so forth) most data access focuses on a single product at a time. In this type of system, you would partition data on the product key and add members as needed to manage additional products. In an online auction application, you may need to stream incoming auction bids to hundreds of clients with very low latency. To do so, you would manage selected \u201chot\u201d auctions on a single partition, so that they receive sufficient processing power. As the processing demand increases, you would add more partitions and route the application logic that matches bids to clients to the data store itself. In a financial trading engine that constantly matches bid prices to asking prices for thousands of securities, you would partition data using the ID of the security. To ensure low-latency execution when a security\u2019s market data changes, you would colocate all of the related reference data with the matching algorithm. By identifying entity groups and using those groups as the basis for SnappyData partitioning and colocation, you can realize these benefits: Rebalancing : SnappyData rebalances the data automatically by making sure that related rows are migrated together and without any integrity loss. This enables you to add capacity as needed. Parallel scatter-gather : Queries that cannot be pruned to a single partition are automatically executed in parallel on data stores. Sub-queries on remote partitions : Even when a query is pruned to a single partition, the query can execute sub-queries that operate on data that is stored on remote partitions. Adapting a Database Schema \u00b6 If you have an existing database design that you want to deploy to SnappyData, translate the entity-relationship model into a physical design that is optimized for SnappyData design principles. Guidelines for Adapting a Database to SnappyData \u00b6 This example shows tables from the Microsoft Northwind Traders sample database . In order to adapt this schema for use in SnappyData, follow the basic steps outlined in Design Principles of Scalable, Partition-Aware Databases : Determine the entity groups. Entity groups are generally course-grained entities that have children, grand children, and so forth, and they are commonly used in queries. This example chooses these entity groups: Entity Group Description Customer This group uses the customer identity along with orders and order details as the children Product This group uses product details along with the associated supplier information Identify the tables in each entity group. Identify the tables that belong to each entity group. In this example, entity groups use the following tables. Entity Group Tables Customer Customers Orders Shippers Order Details Product Product Suppliers Category Define the partitioning key for each group. In this example, the partitioning keys are: Entity Group Partitioning key Customer CustomerID Product ProductID This example uses customerID as the partitioning key for the Customer group. The customer row and all associated orders will be colocated into a single partition. To explicitly colocate Orders with its parent customer row, use the colocate with clause in the create table statement: create table orders (<column definitions>) using column options ('partition_by' customerID, 'colocate_with' customers); In this way, SnappyData supports any queries that join any the Customers and Orders tables. This join query would be distributed to all partitions and executed in parallel, with the results streamed back to the client: select * from customer c , orders o where c.customerID = o.customerID; A query such as this would be pruned to the single partition that stores \u201ccustomer100\u201d and executed only on that SnappyData member: select * from customer c, orders o where c.customerID = o.customerID and c.customerID = 'customer100'; The optimization provided when queries are highly selective comes from engaging the query processor and indexing on a single member rather than on all partitions. With all customer data managed in memory, query response times are very fast. Finally, consider a case where an application needs to access customer order data for several customers: select * from customer c, orders o where c.customerID = o.customerID and c.customerID IN ('cust1', 'cust2', 'cust3'); Here, SnappyData prunes the query execution to only those partitions that host \u2018cust1\u2019, 'cust2\u2019, and 'cust3\u2019. The union of the results is then returned to the caller. Note that the selection of customerID as the partitioning key means that the OrderDetails and Shippers tables cannot be partitioned and colocated with Customers and Orders (because OrderDetails and Shippers do not contain the customerID value for partitioning). Identify replicated tables. If we assume that the number of categories and suppliers rarely changes, those tables can be replicated in the SnappyData cluster (replicated to all of the SnappyData members that host the entity group). If we assume that the Products table does change often and can be relatively large in size, then partitioning is a better strategy for that table. So for the product entity group, table Products is partitioned by ProductID, and the Suppliers and Categories tables are replicated to all of the members where Products is partitioned. Applications can now join Products, Suppliers and categories. For example: select * from Products p , Suppliers s, Categories c where c.categoryID = p.categoryID and p.supplierID = s.supplierID and p.productID IN ('someProductKey1', ' someProductKey2', ' someProductKey3'); In the above query, SnappyData prunes the query execution to only those partitions that host 'someProductKey1\u2019, \u2019 someProductKey2\u2019, and \u2019 someProductKey3.\u2019","title":"Design Principles of Scalable, Partition-Aware Databases"},{"location":"best_practices/design_schema/design_principles/#design-principles-of-scalable-partition-aware-databases","text":"The general strategy for designing a SnappyData database is to identify the tables to partition or replicate in the SnappyData cluster, and then to determine the correct partitioning key(s) for partitioned tables. This usually requires an iterative process to produce the optimal design: Read Identify Entity Groups and Partitioning Keys and Replicate Dimension Tables to understand the basic rules for defining partitioned or replicated tables. Evaluate your data access patterns to define those entity groups that are candidates for partitioning. Focus your efforts on commonly-joined entities. Colocating commonly joined tables will improve the performance of join queries by avoiding shuffle of data when the join is on partitioning keys. Identify all of the tables in the entity groups. Identify the \u201cpartitioning key\u201d for each partitioned table. The partitioning key is the column or set of columns that are common across a set of related tables. Look for parent-child relationships in the joined tables. Identify all of the tables that are candidates for replication. You can replicate table data for high availability, or to colocate table data that is necessary to execute joins.","title":"Design Principles of Scalable, Partition-Aware Databases"},{"location":"best_practices/design_schema/design_principles/#identify-entity-groups-and-partitioning-keys","text":"In relational database terms, an entity group corresponds to rows that are related to one another through foreign key relationships. Members of an entity group are typically related by parent-child relationships and can be managed in a single partition. To design a SnappyData database for data partitioning, begin by identifying \u201centity groups\u201d and their associated partitioning keys. For example: In a customer order management system, most transactions operate on data related to a single customer at a time. Queries frequently join a customer\u2019s billing information with their orders and shipping information. For this type of application, you partition related tables using the customer identity. Any customer row along with their \u201corder\u201d and \u201cshipping\u201d rows forms a single entity group that has the customer ID as the entity group identity (the partitioning key). You would partition related tables using the customer identity, which would enable you to scale the system linearly by adding more members to support additional customers. In a system that manages a product catalog (product categories, product specifications, customer reviews, rebates, related products, and so forth) most data access focuses on a single product at a time. In this type of system, you would partition data on the product key and add members as needed to manage additional products. In an online auction application, you may need to stream incoming auction bids to hundreds of clients with very low latency. To do so, you would manage selected \u201chot\u201d auctions on a single partition, so that they receive sufficient processing power. As the processing demand increases, you would add more partitions and route the application logic that matches bids to clients to the data store itself. In a financial trading engine that constantly matches bid prices to asking prices for thousands of securities, you would partition data using the ID of the security. To ensure low-latency execution when a security\u2019s market data changes, you would colocate all of the related reference data with the matching algorithm. By identifying entity groups and using those groups as the basis for SnappyData partitioning and colocation, you can realize these benefits: Rebalancing : SnappyData rebalances the data automatically by making sure that related rows are migrated together and without any integrity loss. This enables you to add capacity as needed. Parallel scatter-gather : Queries that cannot be pruned to a single partition are automatically executed in parallel on data stores. Sub-queries on remote partitions : Even when a query is pruned to a single partition, the query can execute sub-queries that operate on data that is stored on remote partitions.","title":"Identify Entity Groups and Partitioning Keys"},{"location":"best_practices/design_schema/design_principles/#adapting-a-database-schema","text":"If you have an existing database design that you want to deploy to SnappyData, translate the entity-relationship model into a physical design that is optimized for SnappyData design principles.","title":"Adapting a Database Schema"},{"location":"best_practices/design_schema/design_principles/#guidelines-for-adapting-a-database-to-snappydata","text":"This example shows tables from the Microsoft Northwind Traders sample database . In order to adapt this schema for use in SnappyData, follow the basic steps outlined in Design Principles of Scalable, Partition-Aware Databases : Determine the entity groups. Entity groups are generally course-grained entities that have children, grand children, and so forth, and they are commonly used in queries. This example chooses these entity groups: Entity Group Description Customer This group uses the customer identity along with orders and order details as the children Product This group uses product details along with the associated supplier information Identify the tables in each entity group. Identify the tables that belong to each entity group. In this example, entity groups use the following tables. Entity Group Tables Customer Customers Orders Shippers Order Details Product Product Suppliers Category Define the partitioning key for each group. In this example, the partitioning keys are: Entity Group Partitioning key Customer CustomerID Product ProductID This example uses customerID as the partitioning key for the Customer group. The customer row and all associated orders will be colocated into a single partition. To explicitly colocate Orders with its parent customer row, use the colocate with clause in the create table statement: create table orders (<column definitions>) using column options ('partition_by' customerID, 'colocate_with' customers); In this way, SnappyData supports any queries that join any the Customers and Orders tables. This join query would be distributed to all partitions and executed in parallel, with the results streamed back to the client: select * from customer c , orders o where c.customerID = o.customerID; A query such as this would be pruned to the single partition that stores \u201ccustomer100\u201d and executed only on that SnappyData member: select * from customer c, orders o where c.customerID = o.customerID and c.customerID = 'customer100'; The optimization provided when queries are highly selective comes from engaging the query processor and indexing on a single member rather than on all partitions. With all customer data managed in memory, query response times are very fast. Finally, consider a case where an application needs to access customer order data for several customers: select * from customer c, orders o where c.customerID = o.customerID and c.customerID IN ('cust1', 'cust2', 'cust3'); Here, SnappyData prunes the query execution to only those partitions that host \u2018cust1\u2019, 'cust2\u2019, and 'cust3\u2019. The union of the results is then returned to the caller. Note that the selection of customerID as the partitioning key means that the OrderDetails and Shippers tables cannot be partitioned and colocated with Customers and Orders (because OrderDetails and Shippers do not contain the customerID value for partitioning). Identify replicated tables. If we assume that the number of categories and suppliers rarely changes, those tables can be replicated in the SnappyData cluster (replicated to all of the SnappyData members that host the entity group). If we assume that the Products table does change often and can be relatively large in size, then partitioning is a better strategy for that table. So for the product entity group, table Products is partitioned by ProductID, and the Suppliers and Categories tables are replicated to all of the members where Products is partitioned. Applications can now join Products, Suppliers and categories. For example: select * from Products p , Suppliers s, Categories c where c.categoryID = p.categoryID and p.supplierID = s.supplierID and p.productID IN ('someProductKey1', ' someProductKey2', ' someProductKey3'); In the above query, SnappyData prunes the query execution to only those partitions that host 'someProductKey1\u2019, \u2019 someProductKey2\u2019, and \u2019 someProductKey3.\u2019","title":"Guidelines for Adapting a Database to SnappyData"},{"location":"best_practices/design_schema/optimizing_query_latency/","text":"Optimizing Query Latency: Partitioning and Replication Strategies \u00b6 The following topics are covered in this section: Using Column vs Row Table Using Partitioned vs Replicated Row Table Applying Partitioning Scheme Using Redundancy Overflow Configuration Using Column versus Row Table \u00b6 A columnar table data is stored in a sequence of columns, whereas, in a row table it stores table records in a sequence of rows. Using Column Tables \u00b6 Analytical Queries : A column table has distinct advantages for OLAP queries and therefore large tables involved in such queries are recommended to be created as columnar tables. These tables are rarely mutated (deleted/updated). For a given query on a column table, only the required columns are read (since only the required subset columns are to be scanned), which gives a better scan performance. Thus, aggregation queries execute faster on a column table compared to a row table. Compression of Data : Another advantage that the column table offers is that it allows highly efficient compression of data which reduces the total storage footprint for large tables. Column tables are not suitable for OLTP scenarios. In this case, row tables are recommended. Using Row Tables \u00b6 OLTP Queries : Row tables are designed to return the entire row efficiently and are suited for OLTP scenarios when the tables are required to be mutated frequently (when the table rows need to be updated/deleted based on some conditions). In these cases, row tables offer distinct advantages over the column tables. Point queries : Row tables are also suitable for point queries (for example, queries that select only a few records based on certain where clause conditions). Small Dimension Tables : Row tables are also suitable to create small dimension tables as these can be created as replicated tables (table data replicated on all data servers). Create Index : Row tables also allow the creation of an index on certain columns of the table which improves performance. Using Partitioned versus Replicated Row Table \u00b6 In SnappyData, row tables can be either partitioned across all servers or replicated on every server. For row tables, large fact tables should be partitioned whereas, dimension tables can be replicated. The SnappyData architecture encourages you to denormalize \u201cdimension\u201d tables into fact tables when possible, and then replicate remaining dimension tables to all data stores in the distributed system. Most databases follow the star schema design pattern where large \u201cfact\u201d tables store key information about the events in a system or a business process. For example, a fact table would store rows for events like product sales or bank transactions. Each fact table generally has foreign key relationships to multiple \u201cdimension\u201d tables, which describe further aspects of each row in the fact table. When designing a database schema for SnappyData, the main goal with a typical star schema database is to partition the entities in fact tables. Slow-changing dimension tables should then be replicated on each data store that hosts a partitioned fact table. In this way, a join between the fact table and any number of its dimension tables can be executed concurrently on each partition, without requiring multiple network hops to other members in the distributed system. Applying Partitioning Scheme \u00b6 Colocated Joins \u00b6 In SQL, a JOIN clause is used to combine data from two or more tables, based on a related column between them. JOINS have traditionally been expensive in distributed systems because the data for the tables involved in the JOIN may reside on different physical nodes and the operation has first to move/shuffle the relevant data to one node and perform the operation. SnappyData offers a way to declaratively \"co-locate\" tables to prevent or reduce shuffling to execute JOINS. When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData member. Therefore, in a join query, the join operation is performed on each node's local data. Eliminating data shuffling improves performance significantly. For examples refer to, How to colocate tables for doing a colocated join . Buckets \u00b6 A bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. For more information on BUCKETS, refer to BUCKETS . The default number of buckets in SnappyData cluster mode is 128. In the local mode, it is cores x 2, subject to a maximum of 64 buckets and a minimum of 8 buckets. The number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow. Before deciding the total number of buckets in a table, you must consider the size of the largest bucket required for a table. The total time taken by a query is roughly equal to the scheduling delay plus the time needed to scan the largest bucket for that query. If too many queries are simultaneously executed in the system, scheduling delay can be higher even if the time to scan the bucket is low. In case of an extremely large bucket, the time to scan the bucket will be large even if the query gets scheduled immediately. The following principles should be considered when you set the total number of buckets for a table: * Ensure that data is evenly distributed among the buckets. * If a query on the table is frequent with low latency requirement and those queries scan all the buckets, then ensure that the total number of the buckets is either greater than or equal to the total number of physical cores in a cluster. * If the query is frequent and the query gets pruned to some particular buckets, that is if the query has partitioning columns in predicate, then the total number of buckets can be low if the concurrency is high. When a new server joins or an existing server leaves the cluster, buckets are moved around to ensure that data is balanced across the nodes where the table is defined. You can set the system procedure call sys.rebalance_all_buckets() to trigger rebalance. Criteria for Column Partitioning \u00b6 It is recommended to use a relevant dimension for partitioning so that all partitions are active and the query is executed concurrently. If only a single partition is active and is used largely by queries (especially concurrent queries) it means a significant bottleneck where only a single partition is active all the time, while others are idle. This serializes execution into a single thread handling that partition. Therefore, it is not recommended to use DATE/TIMESTAMP as partitioning. Using Redundancy \u00b6 REDUNDANCY clause of CREATE TABLE specifies the number of secondary copies you want to maintain for your partitioned table. This allows the table data to be highly available even if one of the SnappyData members fails or shuts down. A REDUNDANCY value of 1 is recommended to maintain a secondary copy of the table data. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage. For an example on the REDUNDANCY clause refer to Tables in SnappyData . Overflow Configuration \u00b6 In SnappyData, row and column tables by default overflow to disk (which is equivalent to setting OVERFLOW to 'true'), based on EVICTION_BY criteria. Users cannot set OVERFLOW to 'false', except when EVICTION_BY is not set, in which case it disables the eviction. For example, setting EVICTION_BY to LRUHEAPPERCENT allows table data to be evicted to disk based on the current memory consumption of the server. Refer to CREATE TABLE link to understand how to configure OVERFLOW and EVICTION_BY clauses. Tip By default eviction is set to overflow-to-disk . Known Limitation \u00b6 Change NOT IN queries to use NOT EXISTS if possible \u00b6 Currently, all NOT IN queries use an unoptimized plan and lead to a nested-loop-join which can take long if both sides are large ( https://issues.apache.org/jira/browse/SPARK-16951 ). Change your queries to use NOT EXISTS which uses an optimized anti-join plan. For example a query like: select count ( * ) from T1 where id not in ( select id from T2 ) can be changed to: select count ( * ) from T1 where not exists ( select 1 from T2 where T1 . id = T2 . id ) Be aware of the different null value semantics of the two operators as noted here for Spark . In a nutshell, the NOT IN operator is null-aware and skips the row if the sub-query has a null value, while the NOT EXISTS operator ignores the null values in the sub-query. In other words, the following two are equivalent when dealing with null values: select count ( * ) from T1 where id not in ( select id from T2 where id is not null ) select count ( * ) from T1 where not exists ( select 1 from T2 where T1 . id = T2 . id )","title":"Optimizing Query Latency: Partitioning and Replication Strategies"},{"location":"best_practices/design_schema/optimizing_query_latency/#optimizing-query-latency-partitioning-and-replication-strategies","text":"The following topics are covered in this section: Using Column vs Row Table Using Partitioned vs Replicated Row Table Applying Partitioning Scheme Using Redundancy Overflow Configuration","title":"Optimizing Query Latency: Partitioning and Replication Strategies"},{"location":"best_practices/design_schema/optimizing_query_latency/#using-column-versus-row-table","text":"A columnar table data is stored in a sequence of columns, whereas, in a row table it stores table records in a sequence of rows.","title":"Using Column versus Row Table"},{"location":"best_practices/design_schema/optimizing_query_latency/#using-column-tables","text":"Analytical Queries : A column table has distinct advantages for OLAP queries and therefore large tables involved in such queries are recommended to be created as columnar tables. These tables are rarely mutated (deleted/updated). For a given query on a column table, only the required columns are read (since only the required subset columns are to be scanned), which gives a better scan performance. Thus, aggregation queries execute faster on a column table compared to a row table. Compression of Data : Another advantage that the column table offers is that it allows highly efficient compression of data which reduces the total storage footprint for large tables. Column tables are not suitable for OLTP scenarios. In this case, row tables are recommended.","title":"Using Column Tables"},{"location":"best_practices/design_schema/optimizing_query_latency/#using-row-tables","text":"OLTP Queries : Row tables are designed to return the entire row efficiently and are suited for OLTP scenarios when the tables are required to be mutated frequently (when the table rows need to be updated/deleted based on some conditions). In these cases, row tables offer distinct advantages over the column tables. Point queries : Row tables are also suitable for point queries (for example, queries that select only a few records based on certain where clause conditions). Small Dimension Tables : Row tables are also suitable to create small dimension tables as these can be created as replicated tables (table data replicated on all data servers). Create Index : Row tables also allow the creation of an index on certain columns of the table which improves performance.","title":"Using Row Tables"},{"location":"best_practices/design_schema/optimizing_query_latency/#using-partitioned-versus-replicated-row-table","text":"In SnappyData, row tables can be either partitioned across all servers or replicated on every server. For row tables, large fact tables should be partitioned whereas, dimension tables can be replicated. The SnappyData architecture encourages you to denormalize \u201cdimension\u201d tables into fact tables when possible, and then replicate remaining dimension tables to all data stores in the distributed system. Most databases follow the star schema design pattern where large \u201cfact\u201d tables store key information about the events in a system or a business process. For example, a fact table would store rows for events like product sales or bank transactions. Each fact table generally has foreign key relationships to multiple \u201cdimension\u201d tables, which describe further aspects of each row in the fact table. When designing a database schema for SnappyData, the main goal with a typical star schema database is to partition the entities in fact tables. Slow-changing dimension tables should then be replicated on each data store that hosts a partitioned fact table. In this way, a join between the fact table and any number of its dimension tables can be executed concurrently on each partition, without requiring multiple network hops to other members in the distributed system.","title":"Using Partitioned versus Replicated Row Table"},{"location":"best_practices/design_schema/optimizing_query_latency/#applying-partitioning-scheme","text":"","title":"Applying Partitioning Scheme"},{"location":"best_practices/design_schema/optimizing_query_latency/#colocated-joins","text":"In SQL, a JOIN clause is used to combine data from two or more tables, based on a related column between them. JOINS have traditionally been expensive in distributed systems because the data for the tables involved in the JOIN may reside on different physical nodes and the operation has first to move/shuffle the relevant data to one node and perform the operation. SnappyData offers a way to declaratively \"co-locate\" tables to prevent or reduce shuffling to execute JOINS. When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData member. Therefore, in a join query, the join operation is performed on each node's local data. Eliminating data shuffling improves performance significantly. For examples refer to, How to colocate tables for doing a colocated join .","title":"Colocated Joins"},{"location":"best_practices/design_schema/optimizing_query_latency/#buckets","text":"A bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. For more information on BUCKETS, refer to BUCKETS . The default number of buckets in SnappyData cluster mode is 128. In the local mode, it is cores x 2, subject to a maximum of 64 buckets and a minimum of 8 buckets. The number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow. Before deciding the total number of buckets in a table, you must consider the size of the largest bucket required for a table. The total time taken by a query is roughly equal to the scheduling delay plus the time needed to scan the largest bucket for that query. If too many queries are simultaneously executed in the system, scheduling delay can be higher even if the time to scan the bucket is low. In case of an extremely large bucket, the time to scan the bucket will be large even if the query gets scheduled immediately. The following principles should be considered when you set the total number of buckets for a table: * Ensure that data is evenly distributed among the buckets. * If a query on the table is frequent with low latency requirement and those queries scan all the buckets, then ensure that the total number of the buckets is either greater than or equal to the total number of physical cores in a cluster. * If the query is frequent and the query gets pruned to some particular buckets, that is if the query has partitioning columns in predicate, then the total number of buckets can be low if the concurrency is high. When a new server joins or an existing server leaves the cluster, buckets are moved around to ensure that data is balanced across the nodes where the table is defined. You can set the system procedure call sys.rebalance_all_buckets() to trigger rebalance.","title":"Buckets"},{"location":"best_practices/design_schema/optimizing_query_latency/#criteria-for-column-partitioning","text":"It is recommended to use a relevant dimension for partitioning so that all partitions are active and the query is executed concurrently. If only a single partition is active and is used largely by queries (especially concurrent queries) it means a significant bottleneck where only a single partition is active all the time, while others are idle. This serializes execution into a single thread handling that partition. Therefore, it is not recommended to use DATE/TIMESTAMP as partitioning.","title":"Criteria for Column Partitioning"},{"location":"best_practices/design_schema/optimizing_query_latency/#using-redundancy","text":"REDUNDANCY clause of CREATE TABLE specifies the number of secondary copies you want to maintain for your partitioned table. This allows the table data to be highly available even if one of the SnappyData members fails or shuts down. A REDUNDANCY value of 1 is recommended to maintain a secondary copy of the table data. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage. For an example on the REDUNDANCY clause refer to Tables in SnappyData .","title":"Using Redundancy"},{"location":"best_practices/design_schema/optimizing_query_latency/#overflow-configuration","text":"In SnappyData, row and column tables by default overflow to disk (which is equivalent to setting OVERFLOW to 'true'), based on EVICTION_BY criteria. Users cannot set OVERFLOW to 'false', except when EVICTION_BY is not set, in which case it disables the eviction. For example, setting EVICTION_BY to LRUHEAPPERCENT allows table data to be evicted to disk based on the current memory consumption of the server. Refer to CREATE TABLE link to understand how to configure OVERFLOW and EVICTION_BY clauses. Tip By default eviction is set to overflow-to-disk .","title":"Overflow Configuration"},{"location":"best_practices/design_schema/optimizing_query_latency/#known-limitation","text":"","title":"Known Limitation"},{"location":"best_practices/design_schema/optimizing_query_latency/#change-not-in-queries-to-use-not-exists-if-possible","text":"Currently, all NOT IN queries use an unoptimized plan and lead to a nested-loop-join which can take long if both sides are large ( https://issues.apache.org/jira/browse/SPARK-16951 ). Change your queries to use NOT EXISTS which uses an optimized anti-join plan. For example a query like: select count ( * ) from T1 where id not in ( select id from T2 ) can be changed to: select count ( * ) from T1 where not exists ( select 1 from T2 where T1 . id = T2 . id ) Be aware of the different null value semantics of the two operators as noted here for Spark . In a nutshell, the NOT IN operator is null-aware and skips the row if the sub-query has a null value, while the NOT EXISTS operator ignores the null values in the sub-query. In other words, the following two are equivalent when dealing with null values: select count ( * ) from T1 where id not in ( select id from T2 where id is not null ) select count ( * ) from T1 where not exists ( select 1 from T2 where T1 . id = T2 . id )","title":"Change NOT IN queries to use NOT EXISTS if possible"},{"location":"concepts/network_partition/","text":"Detecting and Handling Network Segmentation (\"Split Brain\") \u00b6 When network segmentation occurs, a distributed system that does not handle the partition condition properly allows multiple subgroups to form. This condition can lead to numerous problems, including distributed applications operating on inconsistent data. For example, because thin clients connecting to a server cluster are not tied into the membership system, a client might communicate with servers from multiple subgroups. Or, one set of clients might see one subgroup of servers while another set of clients cannot see that subgroup but can see another one. SnappyData handles this problem by allowing only one subgroup to form and survive. The distributed systems and caches of other subgroups are shut down as quickly as possible. Appropriate alerts are raised through the SnappyData logging system to alert administrators to take action. Network partition detection in SnappyData is based on the concept of a lead member and a group management coordinator. The coordinator is a member that manages entry and exit of other members of the distributed system. For network partition detection, the coordinator is always a SnappyData locator. The lead member is always the oldest member of the distributed system that does not have a locator running in the same process. Given this, two situations causes SnappyData to declare a network partition: If both a locator and the lead member leave the distributed system abnormally within a configurable period of time, a network partition is declared and the caches of members who are unable to see the locator and the lead member are immediately closed and disconnected. If a locator or lead member's distributed system is shut down normally, SnappyData automatically elects a new one and continues to operate. If no locator can be contacted by a member, it declares a network partition has occurred, closes itself, and disconnects from the distributed system. You enable network partition detection by setting the enable-network-partition-detection distributed system property to true . Enable network partition detection in all locators and in any other process that should be sensitive to network partitioning. Processes that do not have network partition detection enabled are not eligible to be the lead member, so their failure does not trigger declaration of a network partition. !!! Note The distributed system must contain locators to enable network partition detection.","title":"Detecting and Handling Network Segmentation (\"Split Brain\")"},{"location":"concepts/network_partition/#detecting-and-handling-network-segmentation-split-brain","text":"When network segmentation occurs, a distributed system that does not handle the partition condition properly allows multiple subgroups to form. This condition can lead to numerous problems, including distributed applications operating on inconsistent data. For example, because thin clients connecting to a server cluster are not tied into the membership system, a client might communicate with servers from multiple subgroups. Or, one set of clients might see one subgroup of servers while another set of clients cannot see that subgroup but can see another one. SnappyData handles this problem by allowing only one subgroup to form and survive. The distributed systems and caches of other subgroups are shut down as quickly as possible. Appropriate alerts are raised through the SnappyData logging system to alert administrators to take action. Network partition detection in SnappyData is based on the concept of a lead member and a group management coordinator. The coordinator is a member that manages entry and exit of other members of the distributed system. For network partition detection, the coordinator is always a SnappyData locator. The lead member is always the oldest member of the distributed system that does not have a locator running in the same process. Given this, two situations causes SnappyData to declare a network partition: If both a locator and the lead member leave the distributed system abnormally within a configurable period of time, a network partition is declared and the caches of members who are unable to see the locator and the lead member are immediately closed and disconnected. If a locator or lead member's distributed system is shut down normally, SnappyData automatically elects a new one and continues to operate. If no locator can be contacted by a member, it declares a network partition has occurred, closes itself, and disconnects from the distributed system. You enable network partition detection by setting the enable-network-partition-detection distributed system property to true . Enable network partition detection in all locators and in any other process that should be sensitive to network partitioning. Processes that do not have network partition detection enabled are not eligible to be the lead member, so their failure does not trigger declaration of a network partition. !!! Note The distributed system must contain locators to enable network partition detection.","title":"Detecting and Handling Network Segmentation (\"Split Brain\")"},{"location":"configuring_cluster/available_scripts/","text":"Name Description snappy-lead.sh snappy-leads.sh snappy-locator.sh snappy-locators.sh snappy-server.sh snappy-start-all.sh snappy-status-all.sh snappy-stop-all.sh collect-debug-artifacts.sh Spark start-all.sh Spark start-history-server.sh Spark start-master.sh Spark Only start-slave.sh Spark start-slaves.sh Spark start-thriftserver.sh Spark stop-all.sh Spark stop-master.sh Spark stop-slave.sh stop-slaves.sh Name Description snappy snappy-job.sh snappy-sql pyspark Spark Spark-shell Spark Spark-sql -- Spark Spark-submit Spark","title":"Available scripts"},{"location":"configuring_cluster/cluster_maintenance/","text":"Cluster Maintenance \u00b6 To perform maintenance of your cluster you may need to manage on the components of your cluster, you may need to stop or start services running on that cluster, modify allocated resources, or stop certain components while the service is still running. SnappyData provides a variety of methods for performing system maintenance. Cluster maintenance involves: Starting a cluster \u00b6 with one locator, lead and server With one locator, one lead and one server with specified heap size and memory size Start a cluster of x locators, y servers and z leads Add Servers to a Cluster \u00b6 To add an additional server to an existing cluster: Add Servers to a Running Cluster \u00b6 To add an additional server to a running cluster: Stop and Restart a Specific Server \u00b6 (same point or different points start/restart?) Rebalancing the Cluster \u00b6 Backup and Restore the Cluster \u00b6 When you back up a cluster, a back up is create of the operational disk stores for all members running in the distributed system, and each member with persistent data creates a backup of its own configuration and disk stores. Connect to a Cluster \u00b6 Connect to a cluster from external networks \u00b6","title":"Cluster Maintenance"},{"location":"configuring_cluster/cluster_maintenance/#cluster-maintenance","text":"To perform maintenance of your cluster you may need to manage on the components of your cluster, you may need to stop or start services running on that cluster, modify allocated resources, or stop certain components while the service is still running. SnappyData provides a variety of methods for performing system maintenance. Cluster maintenance involves:","title":"Cluster Maintenance"},{"location":"configuring_cluster/cluster_maintenance/#starting-a-cluster","text":"with one locator, lead and server With one locator, one lead and one server with specified heap size and memory size Start a cluster of x locators, y servers and z leads","title":"Starting a cluster"},{"location":"configuring_cluster/cluster_maintenance/#add-servers-to-a-cluster","text":"To add an additional server to an existing cluster:","title":"Add Servers to a Cluster"},{"location":"configuring_cluster/cluster_maintenance/#add-servers-to-a-running-cluster","text":"To add an additional server to a running cluster:","title":"Add Servers to a Running Cluster"},{"location":"configuring_cluster/cluster_maintenance/#stop-and-restart-a-specific-server","text":"(same point or different points start/restart?)","title":"Stop and Restart a Specific Server"},{"location":"configuring_cluster/cluster_maintenance/#rebalancing-the-cluster","text":"","title":"Rebalancing the Cluster"},{"location":"configuring_cluster/cluster_maintenance/#backup-and-restore-the-cluster","text":"When you back up a cluster, a back up is create of the operational disk stores for all members running in the distributed system, and each member with persistent data creates a backup of its own configuration and disk stores.","title":"Backup and Restore the Cluster"},{"location":"configuring_cluster/cluster_maintenance/#connect-to-a-cluster","text":"","title":"Connect to a Cluster"},{"location":"configuring_cluster/cluster_maintenance/#connect-to-a-cluster-from-external-networks","text":"","title":"Connect to a cluster from external networks"},{"location":"configuring_cluster/configure_launch_cluster/","text":"Configuring, Launching SnappyData Clusters \u00b6 Before you configure the SnappyData cluster, check the system requirements . In case you have not yet provisioned SnappyData, you can follow the instructions here . TIBCO recommends that you have at least 8 GB of memory and 4 cores available even for simple experimentation with SnappyData. Launching Single Node Cluster with Default Configuration \u00b6 If you want to launch the cluster either on Amazon EC2 or on a Kubernetes cluster, you can follow the instructions listed here (AWS) and here (Kubernetes) If you are launching on a single node, for example, on your laptop or on a linux server you have access to, you can do so using this simple command: ./sbin/snappy-start-all.sh This launches a single locator , lead and a data server . You can go to the following URL on your browser to view the cluster dashboard: http://(localhost or hostname or machineIP):5050 By default, the cluster uses the following ports: Cluster Component Port Memory Used Lead 5050 (http port used for dashboard.) 8090 (Port used to submit Spark streaming or batch jobs.) 10000 (Port for hive thrift server.) 4 GB Locator 1527 (Port used by JDBC clients.) 10334 (Ports used for all cluster members to communicate with each other.) 1 GB Server 1528 (Port used by ODBC or JDBC clients) 4 GB Note By default, the locator uses 1527 port to listen for client connections and the servers that are running on the same machine use subsequent port numbers. Therefore, 1528 port is used by the single server that is launched by the above command. But, if the server was launched on a different machine it would listen on 1527 . All the artifacts created such as the server - logs, metrics, and the database files are all stored in a folder called work in the product home directory. Click the individual member URLs on the dashboard to view the logs. Also see : Connecting with JDBC Connecting with ODBC Submitting a Spark Job Configuring and Launching a Multi-node Cluster \u00b6 Provision SnappyData and ensure that all the nodes are setup appropriately. If all the nodes are SSH enabled and can share folders using NFS or some shared file system, you can proceed to Capacity Planning . A shared file system is not a requirement but preferred. Step 1: Capacity Planning \u00b6 You must consider the capacity required for storage capacity (in-memory and disk) as well as the computational capacity that is required (memory and CPU). You can skip this step if the volume of data is low and feel you have abundant resources for the client workload. Planning Storage Capacity \u00b6 SnappyData is optimized for managing all the data in the memory. When data cannot fit in the memory, it automatically overflows the data to disk. To achieve the highest possible performance, we recommend you go through this exercise below. The capacity required is dependent on several variables such as input data format, the data types in use, use of indexes, number of redundant copies in the cluster, cardinality of the individual columns (compression ratio can vary a lot) and so on. A general rule of thumb for compressed data (say Parquet) is to configure about 1.5X the compressed size on disk. Else, you can also use the following steps: Define your external tables to access the data sources. This only creates catalog entries in SnappyData. For example, when loading data from a folder with data in CSV format, you can do the following using the Snappy Shell : create external table t1 using csv options ( inferSchema 'true' , header 'true' , path '<some folder or file containing the CSV data>' ) ; -- Spark Scala/Java/Python API example is omitted for brevity. More examples for loading data from external sources are available [here](../howto/load_data_from_external_data_stores.md) Decide if you want to manage this data in a Columnar or Row format . Load a sample of the data: create table t1_inmem using column as (select * from t1 where rand() < 0.1) ; This loads about 10% of the data from the external data source. Note that the fraction value (0.1, which is 10%) should be reduced when working with large data sets or increased when the data set is small. You must repeat this process for all the tables that you want to manage in the memory. Check the dashboard to view the actual memory consumed. Simply extrapolate to 100% (10 times for the example above). The amount of disk space is about two times the memory space requirement. Estimating Computation Capacity \u00b6 TIBCO recommends to configure off-heap memory. Even when off-heap is configured, you must also configure enough JVM heap memory for Spark temporary caching, computations, and buffering when the data is loaded. Note Only the columnar table data is managed in off-heap memory. Row tables are always in JVM heap. TIBCO recommends to allocate at least 1 GB of memory per core to JVM heap for computations up to a max of 12 GB . For example, when running on 8 core servers, configure JVM heap to be 8 GB , while on 32 core servers a heap of 12 - 16 GB will suffice. By default, 50% of the off-heap memory is available as computational memory. While, you may not need this much computational capacity when large off-heap is configured, it is still recommended for reserving enough capacity if working with large data sets. If you are going to run queries using jobs that run queries returning multiple GB of results using the Dataset/RDD.collect() then all the data will need to held in the heap of lead node so you must account for that requirement when planning the heap size of lead node. More complex the analytical processing, especially large aggregations, greater the space requirement in off-heap. For example, if your per server off-heap storage need is 100 GB then, allocate an additional 30 GB of off-heap for computations. Even if your data set is small, you must still allocate at least a few Gigabytes of off-heap storage for computations. Step 2: Configure Core Cluster Component Properties \u00b6 Configuration files for locator, lead, and server should be created in the conf folder located in the SnappyData home directory with names locators , leads , and servers . To do so, you can copy the existing template files servers.template , locators.template , leads.template , and rename them to servers , locators , leads . These files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members. Configuring Core Lead properties \u00b6 The following core properties must be set in the conf/leads file: Properties Description Default Value hostname (or IP) The hostname on which a SnappyData locator is started heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. /work classpath Location of user classes required by the SnappyData Server. This path is appended to the current classpath Appended to the current classpath spark.executor.cores The number of cores to use on each server. spark.jars Example Configurations \u00b6 In the following configuration, you set the heap size for the lead and specify the working directory location: localhost -dir=/opt/snappydata/data/lead -heap-size=6g You can add a line for each of the Lead members that you want to launch. Typically only one. In production, you may launch two. In the following configuration, you are specifying the number of cores to use on each server : localhost -spark.executor.cores=16 Tip It is a common practice to run the Lead and Locator on a single machine. The locator requires very less memory and CPU, the lead memory requirement is directly proportional to the concurrency and the potential for returning large result sets. However, data servers do most of the heavy lifting for query execution and Spark distributed processing. Configuring Core Locator Properties \u00b6 The following core properties must be set in the conf/locators file: Properties Description Default Value client-port The port that the network controller listens for client connections in the range of 1 to 65535. 1527 dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory Configuring Core Server Properties \u00b6 The following core properties must be set in the conf/servers file: Properties Description Default Value memory-size Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in specific scenarios heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory Configuration Examples \u00b6 cat <product_home>/conf/servers node1_hostname -dir=/nfs/opt/snappy-db1/server1 -heap-size=8g -memory-size=42g node1_hostname -dir=/nfs/opt/snappy-db1/server2 -heap-size=8g -memory-size=42g // You can launch more than one data server on a host node2_hostname -dir=/nfs/opt/snappy-db1/server3 -heap-size=8g -memory-size=42g node2_hostname -dir=/nfs/opt/snappy-db1/server4 -heap-size=8g -memory-size=42g Configuration Reference \u00b6 See Configuration Reference section for more details. List of Properties \u00b6 Refer SnappyData properties for complete list of properties.","title":"Configure/Launch SnappyData Cluster"},{"location":"configuring_cluster/configure_launch_cluster/#configuring-launching-snappydata-clusters","text":"Before you configure the SnappyData cluster, check the system requirements . In case you have not yet provisioned SnappyData, you can follow the instructions here . TIBCO recommends that you have at least 8 GB of memory and 4 cores available even for simple experimentation with SnappyData.","title":"Configuring, Launching SnappyData Clusters"},{"location":"configuring_cluster/configure_launch_cluster/#launching-single-node-cluster-with-default-configuration","text":"If you want to launch the cluster either on Amazon EC2 or on a Kubernetes cluster, you can follow the instructions listed here (AWS) and here (Kubernetes) If you are launching on a single node, for example, on your laptop or on a linux server you have access to, you can do so using this simple command: ./sbin/snappy-start-all.sh This launches a single locator , lead and a data server . You can go to the following URL on your browser to view the cluster dashboard: http://(localhost or hostname or machineIP):5050 By default, the cluster uses the following ports: Cluster Component Port Memory Used Lead 5050 (http port used for dashboard.) 8090 (Port used to submit Spark streaming or batch jobs.) 10000 (Port for hive thrift server.) 4 GB Locator 1527 (Port used by JDBC clients.) 10334 (Ports used for all cluster members to communicate with each other.) 1 GB Server 1528 (Port used by ODBC or JDBC clients) 4 GB Note By default, the locator uses 1527 port to listen for client connections and the servers that are running on the same machine use subsequent port numbers. Therefore, 1528 port is used by the single server that is launched by the above command. But, if the server was launched on a different machine it would listen on 1527 . All the artifacts created such as the server - logs, metrics, and the database files are all stored in a folder called work in the product home directory. Click the individual member URLs on the dashboard to view the logs. Also see : Connecting with JDBC Connecting with ODBC Submitting a Spark Job","title":"Launching Single Node Cluster with Default Configuration"},{"location":"configuring_cluster/configure_launch_cluster/#configuring-and-launching-a-multi-node-cluster","text":"Provision SnappyData and ensure that all the nodes are setup appropriately. If all the nodes are SSH enabled and can share folders using NFS or some shared file system, you can proceed to Capacity Planning . A shared file system is not a requirement but preferred.","title":"Configuring and Launching a Multi-node Cluster"},{"location":"configuring_cluster/configure_launch_cluster/#step-1-capacity-planning","text":"You must consider the capacity required for storage capacity (in-memory and disk) as well as the computational capacity that is required (memory and CPU). You can skip this step if the volume of data is low and feel you have abundant resources for the client workload.","title":"Step 1: Capacity Planning"},{"location":"configuring_cluster/configure_launch_cluster/#planning-storage-capacity","text":"SnappyData is optimized for managing all the data in the memory. When data cannot fit in the memory, it automatically overflows the data to disk. To achieve the highest possible performance, we recommend you go through this exercise below. The capacity required is dependent on several variables such as input data format, the data types in use, use of indexes, number of redundant copies in the cluster, cardinality of the individual columns (compression ratio can vary a lot) and so on. A general rule of thumb for compressed data (say Parquet) is to configure about 1.5X the compressed size on disk. Else, you can also use the following steps: Define your external tables to access the data sources. This only creates catalog entries in SnappyData. For example, when loading data from a folder with data in CSV format, you can do the following using the Snappy Shell : create external table t1 using csv options ( inferSchema 'true' , header 'true' , path '<some folder or file containing the CSV data>' ) ; -- Spark Scala/Java/Python API example is omitted for brevity. More examples for loading data from external sources are available [here](../howto/load_data_from_external_data_stores.md) Decide if you want to manage this data in a Columnar or Row format . Load a sample of the data: create table t1_inmem using column as (select * from t1 where rand() < 0.1) ; This loads about 10% of the data from the external data source. Note that the fraction value (0.1, which is 10%) should be reduced when working with large data sets or increased when the data set is small. You must repeat this process for all the tables that you want to manage in the memory. Check the dashboard to view the actual memory consumed. Simply extrapolate to 100% (10 times for the example above). The amount of disk space is about two times the memory space requirement.","title":"Planning Storage Capacity"},{"location":"configuring_cluster/configure_launch_cluster/#estimating-computation-capacity","text":"TIBCO recommends to configure off-heap memory. Even when off-heap is configured, you must also configure enough JVM heap memory for Spark temporary caching, computations, and buffering when the data is loaded. Note Only the columnar table data is managed in off-heap memory. Row tables are always in JVM heap. TIBCO recommends to allocate at least 1 GB of memory per core to JVM heap for computations up to a max of 12 GB . For example, when running on 8 core servers, configure JVM heap to be 8 GB , while on 32 core servers a heap of 12 - 16 GB will suffice. By default, 50% of the off-heap memory is available as computational memory. While, you may not need this much computational capacity when large off-heap is configured, it is still recommended for reserving enough capacity if working with large data sets. If you are going to run queries using jobs that run queries returning multiple GB of results using the Dataset/RDD.collect() then all the data will need to held in the heap of lead node so you must account for that requirement when planning the heap size of lead node. More complex the analytical processing, especially large aggregations, greater the space requirement in off-heap. For example, if your per server off-heap storage need is 100 GB then, allocate an additional 30 GB of off-heap for computations. Even if your data set is small, you must still allocate at least a few Gigabytes of off-heap storage for computations.","title":"Estimating Computation Capacity"},{"location":"configuring_cluster/configure_launch_cluster/#step-2-configure-core-cluster-component-properties","text":"Configuration files for locator, lead, and server should be created in the conf folder located in the SnappyData home directory with names locators , leads , and servers . To do so, you can copy the existing template files servers.template , locators.template , leads.template , and rename them to servers , locators , leads . These files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members.","title":"Step 2: Configure Core Cluster Component Properties"},{"location":"configuring_cluster/configure_launch_cluster/#configuring-core-lead-properties","text":"The following core properties must be set in the conf/leads file: Properties Description Default Value hostname (or IP) The hostname on which a SnappyData locator is started heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. /work classpath Location of user classes required by the SnappyData Server. This path is appended to the current classpath Appended to the current classpath spark.executor.cores The number of cores to use on each server. spark.jars","title":"Configuring Core Lead properties"},{"location":"configuring_cluster/configure_launch_cluster/#example-configurations","text":"In the following configuration, you set the heap size for the lead and specify the working directory location: localhost -dir=/opt/snappydata/data/lead -heap-size=6g You can add a line for each of the Lead members that you want to launch. Typically only one. In production, you may launch two. In the following configuration, you are specifying the number of cores to use on each server : localhost -spark.executor.cores=16 Tip It is a common practice to run the Lead and Locator on a single machine. The locator requires very less memory and CPU, the lead memory requirement is directly proportional to the concurrency and the potential for returning large result sets. However, data servers do most of the heavy lifting for query execution and Spark distributed processing.","title":"Example Configurations"},{"location":"configuring_cluster/configure_launch_cluster/#configuring-core-locator-properties","text":"The following core properties must be set in the conf/locators file: Properties Description Default Value client-port The port that the network controller listens for client connections in the range of 1 to 65535. 1527 dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory","title":"Configuring Core Locator Properties"},{"location":"configuring_cluster/configure_launch_cluster/#configuring-core-server-properties","text":"The following core properties must be set in the conf/servers file: Properties Description Default Value memory-size Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in specific scenarios heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory","title":"Configuring Core Server Properties"},{"location":"configuring_cluster/configure_launch_cluster/#configuration-examples","text":"cat <product_home>/conf/servers node1_hostname -dir=/nfs/opt/snappy-db1/server1 -heap-size=8g -memory-size=42g node1_hostname -dir=/nfs/opt/snappy-db1/server2 -heap-size=8g -memory-size=42g // You can launch more than one data server on a host node2_hostname -dir=/nfs/opt/snappy-db1/server3 -heap-size=8g -memory-size=42g node2_hostname -dir=/nfs/opt/snappy-db1/server4 -heap-size=8g -memory-size=42g","title":"Configuration Examples"},{"location":"configuring_cluster/configure_launch_cluster/#configuration-reference","text":"See Configuration Reference section for more details.","title":"Configuration Reference"},{"location":"configuring_cluster/configure_launch_cluster/#list-of-properties","text":"Refer SnappyData properties for complete list of properties.","title":"List of Properties"},{"location":"configuring_cluster/configure_launch_cluster_multinode/","text":"Configure and Launch a Multi-node Cluster \u00b6 Provision SnappyData and ensure that all the components of the cluster ( locator , lead and a data server ) are setup appropriately. If all the nodes are SSH enabled and can share folders using NFS or some shared file system, you can proceed to Capacity Planning . Step 1: Capacity Planning \u00b6 You must consider the capacity required for storage capacity (in-memory and disk) as well as the computational capacity that is required (memory and CPU). You can skip this step if the volume of data is in relation to the capacity of the nodes (CPU, memory, disk). Planning Storage Capacity \u00b6 SnappyData is optimized for managing all the data in the memory. When data cannot fit in the memory, it automatically overflows the data to disk. Therefore, figuring out the precise memory capacity is non-trivial since it is dependent on several variables such as input data format, the data types in use, use of indexes, number of redundant copies in the cluster, cardinality of the individual columns (compression ratio can vary a lot) and so on. A general rule of thumb for compressed data (say Parquet) is to configure about 1.5X the compressed size on disk. Else, you can also use the following steps: Define your external tables to access the data sources. This only creates catalog entries in SnappyData. For example, when loading data from a folder with data in CSV format, you can do the following using the Snappy Shell : create external table t1 using csv options(inferSchema 'true', header 'true', path '<some folder or file containing the CSV data>') ; // Spark Scala/Java/Python API example is omitted for brevity. Decide if you want to manage this data in a Columnar or Row format . Load a sample of the data: create table t1_inmem using column as (select * from t1 where rand() < 0.1) ; This would load about 10% of the data in the external data source. Note that the fraction value (10% above) should be inversely proportional to the input data size. You must repeat this process for all the tables that you want to manage in memory. Check the dashboard to view the actual memory consumed. Simply extrapolate to 100% (10 times for the example above). The amount of disk space is about two times the memory space requirement. Estimating Computation Capacity \u00b6 TIBCO recommends to configure off-heap memory. Even if off-heap is configured, you must also configure enough JVM heap memory for Spark temporary caching, computations, and buffering when the data is loaded. Note Only the columnar table data is managed in off-heap memory. Row tables are always in JVM heap. TIBCO recommends to allocate at least 1 GB of memory per core to JVM heap for computations. For example, when running on 8 core servers, configure JVM heap to be 8 GB . By default, 50% of the off-heap memory is available as computational memory. While, you may not need this much computational capacity when large off-heap is configured, it is still recommended for reserving a capacity that is proportional to the data being processed. More complex the analytical processing, especially large aggregations, greater the space requirement in off-heap. For example, if your per server off-heap storage need is 100 GB then, allocate an additional 30 GB of off-heap for computations. Even if your data set is small, you must still ensure the availability of some space in Gigabytes on the off-heap storage. Step 2: Configure Core Cluster Component Properties \u00b6 Configuration files for locator, lead, and server should be created in the conf folder located in the SnappyData home directory with names locators , leads , and servers . To do so, you can copy the existing template files servers.template , locators.template , leads.template , and rename them to servers , locators , leads . These files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members. Configuring Core Lead properties \u00b6 The following core properties must be set in the conf/leads file: Properties Description Default Value hostname (or IP) The hostname on which a SnappyData locator is started heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory classpath Location of user classes required by the SnappyData Server. This path is appended to the current classpath Appended to the current classpath -zeppelin.interpreter.enable=true Enable the SnappyData Zeppelin interpreter. No longer useful. Refer How to use Apache Zeppelin with SnappyData spark.executor.cores The number of cores to use on each server. spark.jars Example Configurations \u00b6 In the following configuration, you set the heap size for the lead and specify the working directory location: localhost -dir=/opt/snappydata/data/lead -heap-size=6g You can add a line for each of the Lead members that you want to launch. Typically only one. In production, you may launch two. In the following configuration, you are specifying the Spark UI port and the number of cores to use on each server as well as enabling the SnappyData Zeppelin interpreter localhost -spark.ui.port=3333 -spark.executor.cores=16 Tip It is a common practice to run the Lead and Locator on a single machine. The locator requires very less memory and CPU, the lead memory requirement is directly proportional to the concurrency and the potential for returning large result sets. However, data servers do most of the heavy lifting for query execution and Spark distributed processing. Configuring Core Locator Properties \u00b6 The following core properties must be set in the conf/locators file: Properties Description Default Value client-port The port that the network controller listens for client connections in the range of 1 to 65535. 1527 dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory Configuring Core Server Properties \u00b6 The following core properties must be set in the conf/servers file: Properties Description Default Value memory-size Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in specific scenarios heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory Configuration Examples \u00b6 cat <product_home>/conf/servers node1_hostname -dir=/nfs/opt/snappy-db1/server1 -heap-size=8g -memory-size=42g node1_hostname -dir=/nfs/opt/snappy-db1/server2 -heap-size=8g -memory-size=42g // You can launch more than one data server on a host node2_hostname -dir=/nfs/opt/snappy-db1/server3 -heap-size=8g -memory-size=42g node2_hostname -dir=/nfs/opt/snappy-db1/server4 -heap-size=8g -memory-size=42g","title":"Configure and Launch a Multi-node Cluster"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#configure-and-launch-a-multi-node-cluster","text":"Provision SnappyData and ensure that all the components of the cluster ( locator , lead and a data server ) are setup appropriately. If all the nodes are SSH enabled and can share folders using NFS or some shared file system, you can proceed to Capacity Planning .","title":"Configure and Launch a Multi-node Cluster"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#step-1-capacity-planning","text":"You must consider the capacity required for storage capacity (in-memory and disk) as well as the computational capacity that is required (memory and CPU). You can skip this step if the volume of data is in relation to the capacity of the nodes (CPU, memory, disk).","title":"Step 1: Capacity Planning"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#planning-storage-capacity","text":"SnappyData is optimized for managing all the data in the memory. When data cannot fit in the memory, it automatically overflows the data to disk. Therefore, figuring out the precise memory capacity is non-trivial since it is dependent on several variables such as input data format, the data types in use, use of indexes, number of redundant copies in the cluster, cardinality of the individual columns (compression ratio can vary a lot) and so on. A general rule of thumb for compressed data (say Parquet) is to configure about 1.5X the compressed size on disk. Else, you can also use the following steps: Define your external tables to access the data sources. This only creates catalog entries in SnappyData. For example, when loading data from a folder with data in CSV format, you can do the following using the Snappy Shell : create external table t1 using csv options(inferSchema 'true', header 'true', path '<some folder or file containing the CSV data>') ; // Spark Scala/Java/Python API example is omitted for brevity. Decide if you want to manage this data in a Columnar or Row format . Load a sample of the data: create table t1_inmem using column as (select * from t1 where rand() < 0.1) ; This would load about 10% of the data in the external data source. Note that the fraction value (10% above) should be inversely proportional to the input data size. You must repeat this process for all the tables that you want to manage in memory. Check the dashboard to view the actual memory consumed. Simply extrapolate to 100% (10 times for the example above). The amount of disk space is about two times the memory space requirement.","title":"Planning Storage Capacity"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#estimating-computation-capacity","text":"TIBCO recommends to configure off-heap memory. Even if off-heap is configured, you must also configure enough JVM heap memory for Spark temporary caching, computations, and buffering when the data is loaded. Note Only the columnar table data is managed in off-heap memory. Row tables are always in JVM heap. TIBCO recommends to allocate at least 1 GB of memory per core to JVM heap for computations. For example, when running on 8 core servers, configure JVM heap to be 8 GB . By default, 50% of the off-heap memory is available as computational memory. While, you may not need this much computational capacity when large off-heap is configured, it is still recommended for reserving a capacity that is proportional to the data being processed. More complex the analytical processing, especially large aggregations, greater the space requirement in off-heap. For example, if your per server off-heap storage need is 100 GB then, allocate an additional 30 GB of off-heap for computations. Even if your data set is small, you must still ensure the availability of some space in Gigabytes on the off-heap storage.","title":"Estimating Computation Capacity"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#step-2-configure-core-cluster-component-properties","text":"Configuration files for locator, lead, and server should be created in the conf folder located in the SnappyData home directory with names locators , leads , and servers . To do so, you can copy the existing template files servers.template , locators.template , leads.template , and rename them to servers , locators , leads . These files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members.","title":"Step 2: Configure Core Cluster Component Properties"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#configuring-core-lead-properties","text":"The following core properties must be set in the conf/leads file: Properties Description Default Value hostname (or IP) The hostname on which a SnappyData locator is started heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory classpath Location of user classes required by the SnappyData Server. This path is appended to the current classpath Appended to the current classpath -zeppelin.interpreter.enable=true Enable the SnappyData Zeppelin interpreter. No longer useful. Refer How to use Apache Zeppelin with SnappyData spark.executor.cores The number of cores to use on each server. spark.jars","title":"Configuring Core Lead properties"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#example-configurations","text":"In the following configuration, you set the heap size for the lead and specify the working directory location: localhost -dir=/opt/snappydata/data/lead -heap-size=6g You can add a line for each of the Lead members that you want to launch. Typically only one. In production, you may launch two. In the following configuration, you are specifying the Spark UI port and the number of cores to use on each server as well as enabling the SnappyData Zeppelin interpreter localhost -spark.ui.port=3333 -spark.executor.cores=16 Tip It is a common practice to run the Lead and Locator on a single machine. The locator requires very less memory and CPU, the lead memory requirement is directly proportional to the concurrency and the potential for returning large result sets. However, data servers do most of the heavy lifting for query execution and Spark distributed processing.","title":"Example Configurations"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#configuring-core-locator-properties","text":"The following core properties must be set in the conf/locators file: Properties Description Default Value client-port The port that the network controller listens for client connections in the range of 1 to 65535. 1527 dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory","title":"Configuring Core Locator Properties"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#configuring-core-server-properties","text":"The following core properties must be set in the conf/servers file: Properties Description Default Value memory-size Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in specific scenarios heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=8g It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them. dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth. Current directory","title":"Configuring Core Server Properties"},{"location":"configuring_cluster/configure_launch_cluster_multinode/#configuration-examples","text":"cat <product_home>/conf/servers node1_hostname -dir=/nfs/opt/snappy-db1/server1 -heap-size=8g -memory-size=42g node1_hostname -dir=/nfs/opt/snappy-db1/server2 -heap-size=8g -memory-size=42g // You can launch more than one data server on a host node2_hostname -dir=/nfs/opt/snappy-db1/server3 -heap-size=8g -memory-size=42g node2_hostname -dir=/nfs/opt/snappy-db1/server4 -heap-size=8g -memory-size=42g","title":"Configuration Examples"},{"location":"configuring_cluster/configuring_cluster/","text":"Configuration Reference \u00b6 The following items are inclulded in this section: Configuring Cluster Components List of Properties Specifying Configuration Properties using Environment Variables Configuring SnappyData Smart Connector Logging Auto-Configuring Off-Heap Memory Size Firewalls and Connections Configuring Cluster Components \u00b6 Configuration files for locator, lead, and server should be created in the conf folder located in the SnappyData home directory with names locators , leads , and servers . To do so, you can copy the existing template files servers.template , locators.template , leads.template , and rename them to servers , locators , leads . These files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members. Tip For system properties (set in the conf/lead, conf/servers and conf/locators file), -D and -XX: can be used. -J is NOT required for -D and -XX options. Instead of starting the SnappyData cluster, you can start and stop individual components on a system locally. Configuring Locators \u00b6 Locators provide discovery service for the cluster. Clients (for example, JDBC) connect to the locator and discover the lead and data servers in the cluster. The clients automatically connect to the data servers upon discovery (upon initial connection). Cluster members (Data servers, Lead nodes) also discover each other using the locator. Refer to the Architecture section for more information on the core components. It is recommended to configure two locators (for HA) in production using the conf/locators file located in the < SnappyData_home >/conf directory. In this file, you can specify: The hostname on which a SnappyData locator is started. The startup directory where the logs and configuration files for that locator instance are located. SnappyData specific properties that can be passed. You can refer to the conf/locators.template file for some examples. Example : To start two locators on node-a:9999 and node-b:8888, update the configuration file as follows: $ cat conf/locators node-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888 node-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999 Configuring Leads \u00b6 Lead Nodes primarily runs the SnappyData managed Spark driver. There is one primary lead node at any given instance, but there can be multiple secondary lead node instances on standby for fault tolerance. Applications can run Jobs using the REST service provided by the Lead node. Most of the SQL queries are automatically routed to the Lead to be planned and executed through a scheduler. You can refer to the conf/leads.template file for some examples. Create the configuration file ( leads ) for leads in the < SnappyData_home >/conf directory. Note In the conf/spark-env.sh file set the SPARK_PUBLIC_DNS property to the public DNS name of the lead node. This enables the Member Logs to be displayed correctly to users accessing SnappyData Monitoring Console from outside the network. Example : To start a lead (node-l), set spark.executor.cores as 10 on all servers, and change the Spark UI port from 5050 to 9090, update the configuration file as follows: $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 Configuring Secondary Lead \u00b6 To configure secondary leads, you must add the required number of entries in the conf/leads file. For example: $ cat conf/leads node-l1 -heap-size=4096m -locators=node-b:8888,node-a:9999 node-l2 -heap-size=4096m -locators=node-b:8888,node-a:9999 In this example, two leads (one on node-l1 and another on node-l2) are configured. Using sbin/snappy-start-all.sh , when you launch the cluster, one of them becomes the primary lead and the other becomes the secondary lead. Configuring Data Servers \u00b6 Data Servers hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than the Spark engine. Data servers use intelligent query routing to either execute the query directly on the node or to pass it to the lead node for execution by Spark SQL. You can refer to the conf/servers.template file for some examples. Create the configuration file ( servers ) for data servers in the < SnappyData_home >/conf directory. Example : To start a two servers (node-c and node-c), update the configuration file as follows: $ cat conf/servers node-c -dir=/node-c/server1 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999 node-c -dir=/node-c/server2 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999 List of Properties \u00b6 Refer SnappyData properties . Specifying Configuration Properties using Environment Variables \u00b6 SnappyData configuration properties can be specified using environment variables LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, and LEAD_STARTUP_OPTIONS respectively for locators, leads and servers. These environment variables are useful to specify common properties for locators, servers, and leads. These startup environment variables can be specified in conf/spark-env.sh file. This file is sourced when SnappyData system is started. A template file conf/spark-env.sh.template is provided in conf directory for reference. You can copy this file and use it to configure properties. For example: # create a spark-env.sh from the template file $cp conf/spark-env.sh.template conf/spark-env.sh # Following example configuration can be added to spark-env.sh, # it shows how to add security configuration using the environment variables SECURITY_ARGS = \"-auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=password123 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=password123\" #applies the configuration specified by SECURITY_ARGS to all locators LOCATOR_STARTUP_OPTIONS = \u201d $SECURITY_ARGS \u201d #applies the configuration specified by SECURITY_ARGS to all servers SERVER_STARTUP_OPTIONS = \u201d $SECURITY_ARGS \u201d #applies the configuration specified by SECURITY_ARGS to all leads LEAD_STARTUP_OPTIONS = \u201d $SECURITY_ARGS \u201d Configuring SnappyData Smart Connector \u00b6 Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). In Smart connector mode, a Spark application connects to SnappyData cluster to store and process data. SnappyData currently works with Spark version 2.1.1 to 2.1.3. To work with SnappyData cluster, a Spark application must set the snappydata.connection property while starting. Property Description snappydata.connection SnappyData cluster's locator host and JDBC client port on which locator listens for connections. Has to be specified while starting a Spark application. Example : $ ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass --master spark://localhost:7077 --conf spark.snappydata.connection = localhost:1527 --packages 'io.snappydata:snappydata-spark-connector_2.11:1.3.1' Environment Settings \u00b6 Any Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in SNAPPY_HOME/conf . Logging \u00b6 Currently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property -log-file as the path of the directory. The logging levels can be modified by adding a conf/log4j2.properties file in the product directory. $ cat conf/log4j2.properties logger.dag.name = org.apache.spark.scheduler.DAGScheduler logger.dag.level = debug logger.taskset.name = org.apache.spark.scheduler.TaskSetManager logger.taskset.level = debug Note For a set of applicable class names and default values see the file conf/log4j2.properties.template , which can be used as a starting point. Consult the log4j 2 documentation for more details on the configuration file. Auto-Configuring Off-Heap Memory Size \u00b6 Off-Heap memory size is auto-configured by default in the following scenarios: When the lead, locator, and server are setup on different host machines: In this case, off-heap memory size is configured by default for the host machines with the server setup. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. For example, if the RAM is greater than 8GB, the heap memory is between 4-8 GB and the remaining becomes the off-heap memory. When leads and one of the server node are on the same host: In this case, off-heap memory size is configured by default and is adjusted based on the number of leads that are present. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. However, here the heap memory is the total heap size of the server as well as that of the lead. Note The off-heap memory size is not auto-configured when the heap memory and the off-heap memory are explicitly configured through properties or when multiple servers are on the same host machine. Firewalls and Connections \u00b6 You may face possible connection problems that can result from running a firewall on your machine. SnappyData is a network-centric distributed system, so if you have a firewall running on your machine it could cause connection problems. For example, your connections may fail if your firewall places restrictions on inbound or outbound permissions for Java-based sockets. You may need to modify your firewall configuration to permit traffic to Java applications running on your machine. The specific configuration depends on the firewall you are using. As one example, firewalls may close connections to SnappyData due to timeout settings. If a firewall senses no activity in a certain time period, it may close a connection and open a new connection when activity resumes, which can cause some confusion about which connections you have. Firewall and Port Considerations \u00b6 You can configure and limit port usage for situations that involve firewalls, for example, between client-server or server-server connections. Make sure your port settings are configured correctly for firewalls. For each SnappyData member, there are two different port settings you may need to be concerned with regarding firewalls: The port that the server or locator listens on for client connections. This is configurable using the -client-port option to the snappy server or snappy locator command. The peer discovery port. SnappyData members connect to the locator for peer-to-peer messaging. The locator port is configurable using the -peer-discovery-port option to the snappy server or snappy locator command. By default, SnappyData servers and locators discover each other on a pre-defined port (10334) on the localhost. Limiting Ephemeral Ports for Peer-to-Peer Membership \u00b6 By default, SnappyData utilizes ephemeral ports for UDP messaging and TCP failure detection. Ephemeral ports are temporary ports assigned from a designated range, which can encompass a large number of possible ports. When a firewall is present, the ephemeral port range usually must be limited to a much smaller number, for example six. If you are configuring P2P communications through a firewall, you must also set each the tcp port for each process and ensure that UDP traffic is allowed through the firewall. Properties for Firewall and Port Configuration \u00b6 Store Layer \u00b6 This following tables contain properties potentially involved in firewall behavior, with a brief description of each property. The Configuration Properties section contains detailed information for each property. Configuration Area Property or Setting Definition peer-to-peer config locators The list of locators used by system members. The list must be configured consistently for every member of the distributed system. peer-to-peer config membership-port-range The range of ephemeral ports available for unicast UDP messaging and for TCP failure detection in the peer-to-peer distributed system. member config -J-Dgemfirexd.hostname-for-clients The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. member config client-port option to the snappy server and snappy locator commands Port that the member listens on for client communication. Locator locator command 10334 Spark Layer \u00b6 The following table lists the Spark properties you can set to configure the ports required for Spark infrastructure. Refer to Spark Configuration in the official documentation for detailed information. Property Default Description spark.blockManager.port random Port for all block managers to listen on. These exist on both the driver and the executors. spark.driver.blockManager.port (value of spark.blockManager.port) Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors. spark.driver.port random Port for the driver to listen on. This is used for communicating with the executors and the standalone Master. spark.port.maxRetries 16 Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries. spark.shuffle.service.port 7337 Port on which the external shuffle service will run. spark.ui.port 4040 Port for your application's dashboard, which shows memory and workload data. spark.ssl.[namespace].port None The port where the SSL service will listen on. The port must be defined within a namespace configuration; see SSL Configuration for the available namespaces. When not set, the SSL port will be derived from the non-SSL port for the same service. A value of \"0\" will make the service bind to an ephemeral port. spark.history.ui.port The port to which the web interface of the history server binds. 18080 SPARK_MASTER_PORT Start the master on a different port. Default: 7077 SPARK_WORKER_PORT Start the Spark worker on a specific port. (Default: random Locators and Ports \u00b6 The ephemeral port range and TCP port range for locators must be accessible to members through the firewall. Locators are used in the peer-to-peer cache to discover other processes. They can be used by clients to locate servers as an alternative to configuring clients with a collection of server addresses and ports. Locators have a TCP/IP port that all members must be able to connect to. They also start a distributed system and so need to have their ephemeral port range and TCP port accessible to other members through the firewall. Clients need only be able to connect to the locator's locator port. They don't interact with the locator's distributed system; clients get server names and ports from the locator and use these to connect to the servers. For more information, see Using Locators .","title":"Configuration Reference"},{"location":"configuring_cluster/configuring_cluster/#configuration-reference","text":"The following items are inclulded in this section: Configuring Cluster Components List of Properties Specifying Configuration Properties using Environment Variables Configuring SnappyData Smart Connector Logging Auto-Configuring Off-Heap Memory Size Firewalls and Connections","title":"Configuration Reference"},{"location":"configuring_cluster/configuring_cluster/#configuring-cluster-components","text":"Configuration files for locator, lead, and server should be created in the conf folder located in the SnappyData home directory with names locators , leads , and servers . To do so, you can copy the existing template files servers.template , locators.template , leads.template , and rename them to servers , locators , leads . These files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members. Tip For system properties (set in the conf/lead, conf/servers and conf/locators file), -D and -XX: can be used. -J is NOT required for -D and -XX options. Instead of starting the SnappyData cluster, you can start and stop individual components on a system locally.","title":"Configuring Cluster Components"},{"location":"configuring_cluster/configuring_cluster/#configuring-locators","text":"Locators provide discovery service for the cluster. Clients (for example, JDBC) connect to the locator and discover the lead and data servers in the cluster. The clients automatically connect to the data servers upon discovery (upon initial connection). Cluster members (Data servers, Lead nodes) also discover each other using the locator. Refer to the Architecture section for more information on the core components. It is recommended to configure two locators (for HA) in production using the conf/locators file located in the < SnappyData_home >/conf directory. In this file, you can specify: The hostname on which a SnappyData locator is started. The startup directory where the logs and configuration files for that locator instance are located. SnappyData specific properties that can be passed. You can refer to the conf/locators.template file for some examples. Example : To start two locators on node-a:9999 and node-b:8888, update the configuration file as follows: $ cat conf/locators node-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888 node-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999","title":"Configuring Locators"},{"location":"configuring_cluster/configuring_cluster/#configuring-leads","text":"Lead Nodes primarily runs the SnappyData managed Spark driver. There is one primary lead node at any given instance, but there can be multiple secondary lead node instances on standby for fault tolerance. Applications can run Jobs using the REST service provided by the Lead node. Most of the SQL queries are automatically routed to the Lead to be planned and executed through a scheduler. You can refer to the conf/leads.template file for some examples. Create the configuration file ( leads ) for leads in the < SnappyData_home >/conf directory. Note In the conf/spark-env.sh file set the SPARK_PUBLIC_DNS property to the public DNS name of the lead node. This enables the Member Logs to be displayed correctly to users accessing SnappyData Monitoring Console from outside the network. Example : To start a lead (node-l), set spark.executor.cores as 10 on all servers, and change the Spark UI port from 5050 to 9090, update the configuration file as follows: $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10","title":"Configuring Leads"},{"location":"configuring_cluster/configuring_cluster/#configuring-secondary-lead","text":"To configure secondary leads, you must add the required number of entries in the conf/leads file. For example: $ cat conf/leads node-l1 -heap-size=4096m -locators=node-b:8888,node-a:9999 node-l2 -heap-size=4096m -locators=node-b:8888,node-a:9999 In this example, two leads (one on node-l1 and another on node-l2) are configured. Using sbin/snappy-start-all.sh , when you launch the cluster, one of them becomes the primary lead and the other becomes the secondary lead.","title":"Configuring Secondary Lead"},{"location":"configuring_cluster/configuring_cluster/#configuring-data-servers","text":"Data Servers hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than the Spark engine. Data servers use intelligent query routing to either execute the query directly on the node or to pass it to the lead node for execution by Spark SQL. You can refer to the conf/servers.template file for some examples. Create the configuration file ( servers ) for data servers in the < SnappyData_home >/conf directory. Example : To start a two servers (node-c and node-c), update the configuration file as follows: $ cat conf/servers node-c -dir=/node-c/server1 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999 node-c -dir=/node-c/server2 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999","title":"Configuring Data Servers"},{"location":"configuring_cluster/configuring_cluster/#list-of-properties","text":"Refer SnappyData properties .","title":"List of Properties"},{"location":"configuring_cluster/configuring_cluster/#specifying-configuration-properties-using-environment-variables","text":"SnappyData configuration properties can be specified using environment variables LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, and LEAD_STARTUP_OPTIONS respectively for locators, leads and servers. These environment variables are useful to specify common properties for locators, servers, and leads. These startup environment variables can be specified in conf/spark-env.sh file. This file is sourced when SnappyData system is started. A template file conf/spark-env.sh.template is provided in conf directory for reference. You can copy this file and use it to configure properties. For example: # create a spark-env.sh from the template file $cp conf/spark-env.sh.template conf/spark-env.sh # Following example configuration can be added to spark-env.sh, # it shows how to add security configuration using the environment variables SECURITY_ARGS = \"-auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=password123 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=password123\" #applies the configuration specified by SECURITY_ARGS to all locators LOCATOR_STARTUP_OPTIONS = \u201d $SECURITY_ARGS \u201d #applies the configuration specified by SECURITY_ARGS to all servers SERVER_STARTUP_OPTIONS = \u201d $SECURITY_ARGS \u201d #applies the configuration specified by SECURITY_ARGS to all leads LEAD_STARTUP_OPTIONS = \u201d $SECURITY_ARGS \u201d","title":"Specifying Configuration Properties using Environment Variables"},{"location":"configuring_cluster/configuring_cluster/#configuring-snappydata-smart-connector","text":"Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). In Smart connector mode, a Spark application connects to SnappyData cluster to store and process data. SnappyData currently works with Spark version 2.1.1 to 2.1.3. To work with SnappyData cluster, a Spark application must set the snappydata.connection property while starting. Property Description snappydata.connection SnappyData cluster's locator host and JDBC client port on which locator listens for connections. Has to be specified while starting a Spark application. Example : $ ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass --master spark://localhost:7077 --conf spark.snappydata.connection = localhost:1527 --packages 'io.snappydata:snappydata-spark-connector_2.11:1.3.1'","title":"Configuring SnappyData Smart Connector"},{"location":"configuring_cluster/configuring_cluster/#environment-settings","text":"Any Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in SNAPPY_HOME/conf .","title":"Environment Settings"},{"location":"configuring_cluster/configuring_cluster/#logging","text":"Currently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property -log-file as the path of the directory. The logging levels can be modified by adding a conf/log4j2.properties file in the product directory. $ cat conf/log4j2.properties logger.dag.name = org.apache.spark.scheduler.DAGScheduler logger.dag.level = debug logger.taskset.name = org.apache.spark.scheduler.TaskSetManager logger.taskset.level = debug Note For a set of applicable class names and default values see the file conf/log4j2.properties.template , which can be used as a starting point. Consult the log4j 2 documentation for more details on the configuration file.","title":"Logging"},{"location":"configuring_cluster/configuring_cluster/#auto-configuring-off-heap-memory-size","text":"Off-Heap memory size is auto-configured by default in the following scenarios: When the lead, locator, and server are setup on different host machines: In this case, off-heap memory size is configured by default for the host machines with the server setup. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. For example, if the RAM is greater than 8GB, the heap memory is between 4-8 GB and the remaining becomes the off-heap memory. When leads and one of the server node are on the same host: In this case, off-heap memory size is configured by default and is adjusted based on the number of leads that are present. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. However, here the heap memory is the total heap size of the server as well as that of the lead. Note The off-heap memory size is not auto-configured when the heap memory and the off-heap memory are explicitly configured through properties or when multiple servers are on the same host machine.","title":"Auto-Configuring Off-Heap Memory Size"},{"location":"configuring_cluster/configuring_cluster/#firewalls-and-connections","text":"You may face possible connection problems that can result from running a firewall on your machine. SnappyData is a network-centric distributed system, so if you have a firewall running on your machine it could cause connection problems. For example, your connections may fail if your firewall places restrictions on inbound or outbound permissions for Java-based sockets. You may need to modify your firewall configuration to permit traffic to Java applications running on your machine. The specific configuration depends on the firewall you are using. As one example, firewalls may close connections to SnappyData due to timeout settings. If a firewall senses no activity in a certain time period, it may close a connection and open a new connection when activity resumes, which can cause some confusion about which connections you have.","title":"Firewalls and Connections"},{"location":"configuring_cluster/configuring_cluster/#firewall-and-port-considerations","text":"You can configure and limit port usage for situations that involve firewalls, for example, between client-server or server-server connections. Make sure your port settings are configured correctly for firewalls. For each SnappyData member, there are two different port settings you may need to be concerned with regarding firewalls: The port that the server or locator listens on for client connections. This is configurable using the -client-port option to the snappy server or snappy locator command. The peer discovery port. SnappyData members connect to the locator for peer-to-peer messaging. The locator port is configurable using the -peer-discovery-port option to the snappy server or snappy locator command. By default, SnappyData servers and locators discover each other on a pre-defined port (10334) on the localhost.","title":"Firewall and Port Considerations"},{"location":"configuring_cluster/configuring_cluster/#limiting-ephemeral-ports-for-peer-to-peer-membership","text":"By default, SnappyData utilizes ephemeral ports for UDP messaging and TCP failure detection. Ephemeral ports are temporary ports assigned from a designated range, which can encompass a large number of possible ports. When a firewall is present, the ephemeral port range usually must be limited to a much smaller number, for example six. If you are configuring P2P communications through a firewall, you must also set each the tcp port for each process and ensure that UDP traffic is allowed through the firewall.","title":"Limiting Ephemeral Ports for Peer-to-Peer Membership"},{"location":"configuring_cluster/configuring_cluster/#properties-for-firewall-and-port-configuration","text":"","title":"Properties for Firewall and Port Configuration"},{"location":"configuring_cluster/configuring_cluster/#store-layer","text":"This following tables contain properties potentially involved in firewall behavior, with a brief description of each property. The Configuration Properties section contains detailed information for each property. Configuration Area Property or Setting Definition peer-to-peer config locators The list of locators used by system members. The list must be configured consistently for every member of the distributed system. peer-to-peer config membership-port-range The range of ephemeral ports available for unicast UDP messaging and for TCP failure detection in the peer-to-peer distributed system. member config -J-Dgemfirexd.hostname-for-clients The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. member config client-port option to the snappy server and snappy locator commands Port that the member listens on for client communication. Locator locator command 10334","title":"Store Layer"},{"location":"configuring_cluster/configuring_cluster/#spark-layer","text":"The following table lists the Spark properties you can set to configure the ports required for Spark infrastructure. Refer to Spark Configuration in the official documentation for detailed information. Property Default Description spark.blockManager.port random Port for all block managers to listen on. These exist on both the driver and the executors. spark.driver.blockManager.port (value of spark.blockManager.port) Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors. spark.driver.port random Port for the driver to listen on. This is used for communicating with the executors and the standalone Master. spark.port.maxRetries 16 Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries. spark.shuffle.service.port 7337 Port on which the external shuffle service will run. spark.ui.port 4040 Port for your application's dashboard, which shows memory and workload data. spark.ssl.[namespace].port None The port where the SSL service will listen on. The port must be defined within a namespace configuration; see SSL Configuration for the available namespaces. When not set, the SSL port will be derived from the non-SSL port for the same service. A value of \"0\" will make the service bind to an ephemeral port. spark.history.ui.port The port to which the web interface of the history server binds. 18080 SPARK_MASTER_PORT Start the master on a different port. Default: 7077 SPARK_WORKER_PORT Start the Spark worker on a specific port. (Default: random","title":"Spark Layer"},{"location":"configuring_cluster/configuring_cluster/#locators-and-ports","text":"The ephemeral port range and TCP port range for locators must be accessible to members through the firewall. Locators are used in the peer-to-peer cache to discover other processes. They can be used by clients to locate servers as an alternative to configuring clients with a collection of server addresses and ports. Locators have a TCP/IP port that all members must be able to connect to. They also start a distributed system and so need to have their ephemeral port range and TCP port accessible to other members through the firewall. Clients need only be able to connect to the locator's locator port. They don't interact with the locator's distributed system; clients get server names and ports from the locator and use these to connect to the servers. For more information, see Using Locators .","title":"Locators and Ports"},{"location":"configuring_cluster/firewalls_connections/","text":"Firewalls and Connections \u00b6 You may face possible connection problems that can result from running a firewall on your machine. SnappyData is a network-centric distributed system, so if you have a firewall running on your machine it could cause connection problems. For example, your connections may fail if your firewall places restrictions on inbound or outbound permissions for Java-based sockets. You may need to modify your firewall configuration to permit traffic to Java applications running on your machine. The specific configuration depends on the firewall you are using. As one example, firewalls may close connections to SnappyData due to timeout settings. If a firewall senses no activity in a certain time period, it may close a connection and open a new connection when activity resumes, which can cause some confusion about which connections you have. Firewall and Port Considerations \u00b6 You can configure and limit port usage for situations that involve firewalls, for example, between client-server or server-server connections. Make sure your port settings are configured correctly for firewalls. For each SnappyData member, there are two different port settings you may need to be concerned with regarding firewalls: The port that the server or locator listens on for client connections. This is configurable using the -client-port option to the snappy server or snappy locator command. The peer discovery port. SnappyData members connect to the locator for peer-to-peer messaging. The locator port is configurable using the -peer-discovery-port option to the snappy server or snappy locator command. By default, SnappyData servers and locators discover each other on a pre-defined port (10334) on the localhost. Limiting Ephemeral Ports for Peer-to-Peer Membership \u00b6 By default, SnappyData utilizes ephemeral ports for UDP messaging and TCP failure detection. Ephemeral ports are temporary ports assigned from a designated range, which can encompass a large number of possible ports. When a firewall is present, the ephemeral port range usually must be limited to a much smaller number, for example six. If you are configuring P2P communications through a firewall, you must also set each the tcp port for each process and ensure that UDP traffic is allowed through the firewall. Properties for Firewall and Port Configuration \u00b6 Store Layer \u00b6 This following tables contain properties potentially involved in firewall behavior, with a brief description of each property. The Configuration Properties section contains detailed information for each property. Configuration Area Property or Setting Definition peer-to-peer config locators The list of locators used by system members. The list must be configured consistently for every member of the distributed system. peer-to-peer config membership-port-range The range of ephemeral ports available for unicast UDP messaging and for TCP failure detection in the peer-to-peer distributed system. member config -J-Dgemfirexd.hostname-for-clients The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. member config client-port option to the snappy server and snappy locator commands Port that the member listens on for client communication. Locator locator command 10334 Spark Layer \u00b6 The following table lists the Spark properties you can set to configure the ports required for Spark infrastructure. Refer to Spark Configuration in the official documentation for detailed information. Property Default Description spark.blockManager.port random Port for all block managers to listen on. These exist on both the driver and the executors. spark.driver.blockManager.port (value of spark.blockManager.port) Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors. spark.driver.port random Port for the driver to listen on. This is used for communicating with the executors and the standalone Master. spark.port.maxRetries 16 Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries. spark.shuffle.service.port 7337 Port on which the external shuffle service will run. spark.ui.port 4040 Port for your application's dashboard, which shows memory and workload data. spark.ssl.[namespace].port None The port where the SSL service will listen on. The port must be defined within a namespace configuration; see SSL Configuration for the available namespaces. When not set, the SSL port will be derived from the non-SSL port for the same service. A value of \"0\" will make the service bind to an ephemeral port. spark.history.ui.port The port to which the web interface of the history server binds. 18080 SPARK_MASTER_PORT Start the master on a different port. Default: 7077 SPARK_WORKER_PORT Start the Spark worker on a specific port. (Default: random Locators and Ports \u00b6 The ephemeral port range and TCP port range for locators must be accessible to members through the firewall. Locators are used in the peer-to-peer cache to discover other processes. They can be used by clients to locate servers as an alternative to configuring clients with a collection of server addresses and ports. Locators have a TCP/IP port that all members must be able to connect to. They also start a distributed system and so need to have their ephemeral port range and TCP port accessible to other members through the firewall. Clients need only be able to connect to the locator's locator port. They don't interact with the locator's distributed system; clients get server names and ports from the locator and use these to connect to the servers. For more information, see Using Locators .","title":"Firewalls and Connections"},{"location":"configuring_cluster/firewalls_connections/#firewalls-and-connections","text":"You may face possible connection problems that can result from running a firewall on your machine. SnappyData is a network-centric distributed system, so if you have a firewall running on your machine it could cause connection problems. For example, your connections may fail if your firewall places restrictions on inbound or outbound permissions for Java-based sockets. You may need to modify your firewall configuration to permit traffic to Java applications running on your machine. The specific configuration depends on the firewall you are using. As one example, firewalls may close connections to SnappyData due to timeout settings. If a firewall senses no activity in a certain time period, it may close a connection and open a new connection when activity resumes, which can cause some confusion about which connections you have.","title":"Firewalls and Connections"},{"location":"configuring_cluster/firewalls_connections/#firewall-and-port-considerations","text":"You can configure and limit port usage for situations that involve firewalls, for example, between client-server or server-server connections. Make sure your port settings are configured correctly for firewalls. For each SnappyData member, there are two different port settings you may need to be concerned with regarding firewalls: The port that the server or locator listens on for client connections. This is configurable using the -client-port option to the snappy server or snappy locator command. The peer discovery port. SnappyData members connect to the locator for peer-to-peer messaging. The locator port is configurable using the -peer-discovery-port option to the snappy server or snappy locator command. By default, SnappyData servers and locators discover each other on a pre-defined port (10334) on the localhost.","title":"Firewall and Port Considerations"},{"location":"configuring_cluster/firewalls_connections/#limiting-ephemeral-ports-for-peer-to-peer-membership","text":"By default, SnappyData utilizes ephemeral ports for UDP messaging and TCP failure detection. Ephemeral ports are temporary ports assigned from a designated range, which can encompass a large number of possible ports. When a firewall is present, the ephemeral port range usually must be limited to a much smaller number, for example six. If you are configuring P2P communications through a firewall, you must also set each the tcp port for each process and ensure that UDP traffic is allowed through the firewall.","title":"Limiting Ephemeral Ports for Peer-to-Peer Membership"},{"location":"configuring_cluster/firewalls_connections/#properties-for-firewall-and-port-configuration","text":"","title":"Properties for Firewall and Port Configuration"},{"location":"configuring_cluster/firewalls_connections/#store-layer","text":"This following tables contain properties potentially involved in firewall behavior, with a brief description of each property. The Configuration Properties section contains detailed information for each property. Configuration Area Property or Setting Definition peer-to-peer config locators The list of locators used by system members. The list must be configured consistently for every member of the distributed system. peer-to-peer config membership-port-range The range of ephemeral ports available for unicast UDP messaging and for TCP failure detection in the peer-to-peer distributed system. member config -J-Dgemfirexd.hostname-for-clients The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. member config client-port option to the snappy server and snappy locator commands Port that the member listens on for client communication. Locator locator command 10334","title":"Store Layer"},{"location":"configuring_cluster/firewalls_connections/#spark-layer","text":"The following table lists the Spark properties you can set to configure the ports required for Spark infrastructure. Refer to Spark Configuration in the official documentation for detailed information. Property Default Description spark.blockManager.port random Port for all block managers to listen on. These exist on both the driver and the executors. spark.driver.blockManager.port (value of spark.blockManager.port) Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors. spark.driver.port random Port for the driver to listen on. This is used for communicating with the executors and the standalone Master. spark.port.maxRetries 16 Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries. spark.shuffle.service.port 7337 Port on which the external shuffle service will run. spark.ui.port 4040 Port for your application's dashboard, which shows memory and workload data. spark.ssl.[namespace].port None The port where the SSL service will listen on. The port must be defined within a namespace configuration; see SSL Configuration for the available namespaces. When not set, the SSL port will be derived from the non-SSL port for the same service. A value of \"0\" will make the service bind to an ephemeral port. spark.history.ui.port The port to which the web interface of the history server binds. 18080 SPARK_MASTER_PORT Start the master on a different port. Default: 7077 SPARK_WORKER_PORT Start the Spark worker on a specific port. (Default: random","title":"Spark Layer"},{"location":"configuring_cluster/firewalls_connections/#locators-and-ports","text":"The ephemeral port range and TCP port range for locators must be accessible to members through the firewall. Locators are used in the peer-to-peer cache to discover other processes. They can be used by clients to locate servers as an alternative to configuring clients with a collection of server addresses and ports. Locators have a TCP/IP port that all members must be able to connect to. They also start a distributed system and so need to have their ephemeral port range and TCP port accessible to other members through the firewall. Clients need only be able to connect to the locator's locator port. They don't interact with the locator's distributed system; clients get server names and ports from the locator and use these to connect to the servers. For more information, see Using Locators .","title":"Locators and Ports"},{"location":"configuring_cluster/properties/","text":"Using a Snappy session, you can read an existing Hive table that is defined in an external hive catalog, use hive tables as external tables from SnappySession for queries, including joins with tables defined in SnappyData catalog, and also define new Hive table/view to be stored in external hive catalog. When working with Hive, one must instantiate Snappy session with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. If the underlying storage for Hive is HDFS, you can configure Hive with Snappy session. For this, you must place hive-site.xml , core-site.xml (for security configuration) and hdfs-site.xml (for HDFS configuration) files in the conf/ folder of Snappy. In addition to this, you must configure spark-env.sh file into the conf/ folder. The content in the hadoop_spark-env.sh file should be as follows: export SPARK_DIST_CLASSPATH = $( /home/user/hadoop-2.7.3/bin/hadoop classpath ) Snappy has been tested with default hive database i.e. embedded derby database. User can also use and configure the remote metastore as well like SQL. In hive-site xml, user needs to configure the parameters as per the requirement. With derby as database, the following are the hive-site.xml configuration: <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:derby:;databaseName=metastore_db;create=true </value> <description> JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. </description> </property> If you want to setup remote meta store instead of using default database derby, you can use the following configuration: <property> <name> hive.metastore.uris </name> <value> thrift://chbhatt-dell:9083 </value> <description> Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore. </description> </property> Run the following steps to test Snappy with Apache Hadoop: Start the Hadoop daemons. Start the Hive thrift server. Start the Snappy-shell. After starting the Snappy Shell, you can do the following: # To point to external hive catalog from snappy session, set the below property. set spark.sql.catalogImplementation=hive. snappy-sql> set spark.sql.catalogImplementation=hive; This property can be set at the session level and global level. # To point to Snappy internal catalog from snappy session. set spark.sql.catalogImplementation=in-memory. snappy-sql> set spark.sql.catalogImplementation=in-memory; # To access hive tables use below command. snappy-sql> show tables in default; Please note that it is mandatory to specify the schema \u2018default\u2019. If any other schema is created then it is mandatory to use the created schema name. For example, if schema / database hiveDB created then use, snappy-sql> show tables in hiveDB; # To read the hive tables from snappy. snappy-sql> SELECT FirstName, LastName FROM default.hiveemployees ORDER BY LastName; # To join Snappy tables and Hive tables. snappy-sql> SELECT emp.EmployeeID, emp.FirstName, emp.LastName, o.OrderID, o.OrderDate FROM default.hive_employees emp JOIN snappy_orders o ON (emp.EmployeeID = o.EmployeeID) ORDER BY o.OrderDate; # To create the hive table and insert the data into it from Snappy. snappy-sql> create table if not exists default.t1(id int) row format delimited fields terminated by ','; snappy-sql> insert into default.t1 select id, concat(id) from range(100); Note If you have not configure any of the configuration files mentioned above( hive-site.xml, core-site.xml, hdfs-site.xml) and started the Hadoop and Hive daemons, you will see the following error: No Datastore found in the Distributed System for 'execution on remote node null'. If you have connected to Hive and Hadoop and in case the configuration files get removed or deleted, errors or exceptions will not be shown. However, you cannot perform any DDL and DML statements in Hive. For more details, refer the following links: https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started","title":"Properties"},{"location":"configuring_cluster/property_description/","text":"List of Properties \u00b6 The following list of commonly used configuration properties can be set to configure the cluster. These properties can be set in the conf/servers , conf/leads or conf/locators configuration files. Network Configuration Memory Configuration Disk Configuration Security Configuration Spark Configuration Logging, Metrics Configuration JVM Properties SQL Properties AQP Properties Connection Properties Tip For system properties (set in the conf/lead, conf/servers and conf/locators file), -D and -XX: can be used. -J is NOT required for -D and -XX options. Network Configuration \u00b6 Property Description Components -ack-severe-alert-threshold See ack-severe-alert-threshold -ack-wait-threshold See ack-wait-threshold -bind-address IP address on which the member is bound. The default behavior is to bind to all local addresses. Also see bind-address Server Lead Locator -client-port The port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527. Locator Server -hostname-for-clients Set the IP address or host name that this server/locator sends to JDBC/ODBC/thrift clients to use for connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where locators, servers are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases this is handled by hostname translation itself, i.e. hostname used in client-bind-address resolves to internal IP address from inside but to public IP address from outside, but for other cases this property will be required. Server -enable-network-partition-detection See enable-network-partition-detection -enforce-unique-host See enforce-unique-host -locators List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. The list must include all locators in use and must be configured consistently for every member of the distributed system. This property should be configured for all the nodes in the respective configuration files, if there are multiple locators. Server Lead Locator -member-timeout Uses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways: 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case. 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. Valid values are in the range 1000-600000 milliseconds. For more information, refer to Best Practices Server Lead Locator -membership-port-range See membership-port-range -peer-discovery-address Use this as value for the port in the \"host:port\" value of \"-locators\" property Locator -peer-discovery-port Port on which the locator listens for peer discovery (includes servers as well as other locators). Valid values are in the range 1-65535, with a default of 10334. Locator read-timeout See read-timeout -spark.ui.port Port for your SnappyData Monitoring Console, which shows tables, memory and workload data. The default is 5050 Lead -redundancy-zone See redundancy-zone -secondary-locators See secondary-locators -socket-buffer-size See socket-buffer-size -socket-lease-time See socket-lease-time -gemfirexd.max-lock-wait See gemfirexd.max-lock-wait Memory Configuration \u00b6 Property Description Components -critical-heap-percentage Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. If you set -heap-size , the default value for critical-heap-percentage is set to 95% of the heap size. Use this switch to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory. Server Lead -critical-off-heap-percentage Sets the critical threshold for off-heap memory usage in percentage, 0-100. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory. Server -eviction-heap-percentage Sets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 85.5% of whatever is set for -critical-heap-percentage . Use this switch to override the default. Server Lead -eviction-off-heap-percentage Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. By default, the eviction threshold is 85.5% of whatever is set for -critical-off-heap-percentage . Use this switch to override the default. Server -heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=1GB. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM. Server Lead Locator -memory-size Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in specific scenarios . Server Lead -spark.driver.maxResultSize Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1MB or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB. Lead Disk Configuration \u00b6 Property Description Components -archive-disk-space-limit See archive-disk-space-limit -archive-file-size-limit See archive-file-size-limit -dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory). Server Lead Locator gemfirexd.default-startup-recovery-delay See gemfirexd.default-startup-recovery-delay -spark.local.dir Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. For more information, refer to Best Practices . Lead Server -sys-disk-dir See sys-disk-dir Security Configuration \u00b6 Property Description Components -DCHECK_EXTERNAL_TABLE_AUTHZ Enable authorization of external tables by setting this system property to true when the cluster's security is enabled. System admin or the schema owner can grant or revoke the permissions on external tables to other users. Lead -Dsnappydata.enable-rls Enables the system for row level security when set to true. By default this is off. If this property is set to true, then the Smart Connector access to SnappyData fails. Server Lead Locator -Dsnappydata.RESTRICT_TABLE_CREATION Applicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false. Server Lead Locator -spark.ssl.enabled Enables or disables Spark layer encryption. The default is false. Lead -spark.ssl.keyPassword The password to the private key in the key store. Lead -spark.ssl.keyStore Path to the key store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.keyStorePassword The password used to access the keystore. Lead -spark.ssl.trustStore Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.trustStorePassword The password used to access the truststore. Lead -spark.ssl.protocol The protocol that must be supported by JVM. For example, TLS. Lead SSL Configuration for P2P ssl-enabled , ssl-ciphers , ssl-protocols , ssl-require-authentication . These properties need not be added to the Lead members in case of a client-server connection. Server Lead Locator -thrift-ssl Specifies if you want to enable or disable SSL. Values are true or false. Server Lead Locator -thrift-ssl-properties Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated keystore : Path to key store file keystore-type : The type of key-store (default \"JKS\") keystore-password : Password for the key store file keymanager-type : The type of key manager factory truststore : Path to trust store file truststore-type : The type of trust-store (default \"JKS\") truststore-password : Password for the trust store file trustmanager-type : The type of trust manager factory Server Spark Configuration \u00b6 Property Description Components -spark.executor.cores The number of cores to use on each server. Lead -spark.context-settings.num-cpu-cores The number of cores that can be allocated. The default is 4. Lead -spark.context-settings.memory-per-node The executor memory per node (-Xmx style. For example: 512m, 1G). The default is 512m. Lead -spark.context-settings.streaming.batch_interval The batch interval for Spark Streaming contexts in milliseconds. The default is 1000. Lead -spark.context-settings.streaming.stopGracefully If set to true, the streaming stops gracefully by waiting for the completion of processing of all the received data. The default is true. Lead -spark.context-settings.streaming.stopSparkContext if set to true, the SparkContext is stopped along with the StreamingContext. The default is true. Lead -spark.driver.maxResultSize Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1MB or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB. Lead -spark.eventLog.enabled Set to True to enable event logging for Spark jobs. Lead -spark.eventLog.dir Specify the directory path where the events can be logged for Spark jobs. Lead -spark.jobserver.bind-address The address on which the jobserver listens. Default address is 0.0.0. Lead -spark.jobserver.job-result-cache-size The number of job results to keep per JobResultActor/context. The default is 5000. Lead -spark.jobserver.max-jobs-per-context The number of jobs that can be run simultaneously in the context. The default is 8. Lead -spark.jobserver.port The port on which to run the jobserver. Default port is 8090. Lead -spark.local.dir Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. For more information, refer to Best Practices . Lead Server -spark.network.timeout The default timeout for all network interactions while running queries. Lead -spark.sql.autoBroadcastJoinThreshold Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Lead -spark.sql.codegen.cacheSize Size of the generated code cache. This effectively controls the maximum number of query plans whose generated code (Classes) is cached. The default is 2000. Lead -spark.sql.codegen.wholeStage Turn ON/OFF whole stage code generation in Spark/Snappy SQL. The default is True. Lead -spark.sql.aqp.numBootStrapTrials Number of bootstrap trials to do for calculating error bounds. The default value is100. This property must be set in the conf/leads file. Lead -spark.sql.aqp.error Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. This property can be set as connection property in the Snappy SQL shell. Lead -spark.sql.aqp.confidence Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. The default value is0.95. This property can be set as connection property in the Snappy SQL shell. Lead -spark.sql.aqp.behavior The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING . This property can be set as connection property in the Snappy SQL shell. Lead -spark.ssl.enabled Enables or disables Spark layer encryption. The default is false. Lead -spark.ssl.keyPassword The password to the private key in the key store. Lead -spark.ssl.keyStore Path to the key store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.keyStorePassword The password used to access the keystore. Lead -spark.ssl.trustStore Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.trustStorePassword The password used to access the truststore. Lead -spark.ssl.protocol The protocol that must be supported by JVM. For example, TLS. Lead -spark.ui.port Port for your SnappyData Monitoring Console, which shows tables, memory and workload data. The default is 5050 Lead Logging, Metrics Configuration \u00b6 Property Description Components -enable-stats See enable-stats -enable-time-statistics See enable-time-statisticss -enable-timestats See enable-timestats -gemfirexd.debug.true Use this property to set the required trace flag which enables the logging of specific features of SnappyData. Server Lead Locator -log-file Path of the file to which this member writes log messages (default is snappy[member].log in the working directory. For example, snappylocator.log , snappyleader.log , snappyserver.log . In case logging is set via log4j, the default log file is snappydata.log for each of the SnappyData member.) Server Lead Locator -log-level See log-level -snappy.history See snappy.history -statistic-archive-file See statistic-archive-file -statistic-sample-rate See statistic-sample-rate -statistic-sampling-enabled See statistic-sampling-enabled JVM Properties \u00b6 Property Description Components -classpath Location of user classes required by the SnappyData Server. This path is appended to the current classpath. Server Lead Locator -J JVM option passed to the spawned SnappyData server JVM. For example, use -J-Xmx1GB to set the JVM heap to 1GB. Server Lead Locator Other than the above properties, you can also refer the Configuration Parameters section for properties that are used in special cases. SQL Properties \u00b6 These properties can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy > connect client 'localhost:1527' ; snappy > set snappydata . column . batchSize = 100 k ; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Property Description -allow-explicit-commit See allow-explicit-commit -init-scripts See init-scripts -skip-constraint-checks See skip-constraint-checks -skip-locks See skip-locks -gemfirexd.datadictionary.allow-startup-errors See gemfirexd.datadictionary.allow-startup-errors -gemfirexd.query-cancellation-interval See gemfirexd.query-cancellation-interval -gemfirexd.query-timeout See gemfirexd.query-timeout -snappydata.column.batchSize The default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes or k/m/g suffixes for unit) that is used to split the data into chunks for efficient storage and retrieval. This property can also be set for each table in the create table DDL. Maximum allowed size is 2GB. The default is 24m. -snappydata.column.maxDeltaRows The maximum number of rows that can be in the delta buffer of a column table. The size of the delta buffer is already limited by ColumnBatchSize property, but this allows a lower limit on the number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of ColumnBatchSize and this property is hit first. It can also be set for each table in the create table DDL, else this setting is used for the create table snappydata.column.compactionRatio The ratio of deleted rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive) with the default as 0.1. The compaction is triggered in one of the foreground threads performing delete or update. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. snappydata.column.updateCompactionRatio The ratio of updated rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive) with the default as 0.2. The compaction of the column batch is triggered in one of the foreground threads performing delete or update. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. spark.sql.maxMemoryResultSize Maximum size of results from a JDBC/ODBC/SQL query in a partition that will be held in memory beyond which the results will be written to disk. The disk file will continue to grow till 8 times this initial value after which a new disk file will be written. The default value is 4MB. spark.sql.resultPersistenceTimeout Maximum duration in seconds for which results overflowed to disk are held on disk after which they are cleaned up. The default value is 21600 i.e. 6 hours. -snappydata.hiveServer.enabled Enables the Hive Thrift server for SnappyData.This is enabled by default when you start the cluster. Thus it adds an additional 10 seconds to the cluster startup time. To avoid this additional time, you can set the property to false. -snappydata.maxRetryAttemptsForWrite Default retry of Spark tasks on failure can cause duplicates in the case of insert operations. This property can be set to 0 to avoid this scenario. Other operations, as usual, retries without causing any consistency issues. -snappydata.sql.hashJoinSize The join would be converted into a hash join if the table is of size less than the hashJoinSize . The limit specifies an estimate on the input data size (in bytes or k/m/g/t suffixes for unit). The default value is 100MB. -snappydata.sql.hashAggregateSize Aggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (in bytes or k/m/g/t suffixes for unit) and not the output size. Set this only if there are queries that can return a large number of rows in aggregation results. The default value is set to 0 which means, no limit is set on the size, so the optimized hash aggregation is always used. -snappydata.sql.useDriverCollectForGroupBy Allow driver do the direct collect of partial results for top-level GROUP BY avoiding the last EXCHANGE for partial results of a GROUP BY query, improving the performance substantially for sub-second queries. This is a SQL session property. Default value is false. It should be enabled only when the final size of results of the query is known to be small for all queries in the session else can cause heap memory issues on the driver. -snappydata.sql.planCacheSize Number of query plans that will be cached. -spark.sql.autoBroadcastJoinThreshold Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join. By setting this value to -1 broadcasting can be disabled. -snappydata.linkPartitionsToBuckets When this property is set to true, each bucket is always treated as a separate partition in column/row table scans. When this is set to false, SnappyData creates only as many partitions as executor cores by clubbing multiple buckets into each partition when possible. The default is false. -snappydata.preferPrimaries Use this property to configure your preference to use primary buckets in queries. This reduces the scalability of queries in the interest of reduced memory usage for secondary buckets. The default is false. -snappydata.sql.partitionPruning Use this property to set/unset the partition pruning of queries. -snappydata.sql.tokenize Use this property to enable/disable tokenization. -snappydata.cache.putIntoInnerJoinResultSize The putInto inner join would be cached if the result of join with incoming Dataset is of size less than this limit. Value is in bytes or k[b]/m[b]/g[b]/t[b] suffixes for unit. Large cache will overflow to disk using Spark's BlockManager. Default value is -1 which indicates no limit. -snappydata.cache.putIntoInnerJoinLocalCache Cache the putInto inner join locally at the driver node. Use only if the size of the putInto data and the resulting updates is small. This is a SQL session property. Default is do local caching only if the data being put is a local relation created from a list of Rows its size is within the limit specified by snappydata.sql.hashJoinSize . This behavior is different from setting it to false explicitly which will always turn it off. Leave at default of unset unless you are completely certain of all puts that can be done in the system. -snappydata.scheduler.pool Use this property to define scheduler pool to either default or low latency. You can also assign queries to different pools. -snappydata.enable-experimental-features Use this property to enable and disable experimental features. You can call out in case some features are completely broken and need to be removed from the product. -snappydata.sql.planCaching Use this property to enable/disable plan caching. By default it is disabled. snappydata.recovery.enableTableCountInUI In the recovery mode, by default, the table counts and sizes do not appear on the UI. To view the table counts, you should set this property to true in the lead's conf file. By default, the property is set to false , and the table count is shown as -1 . sync-commits See sync-commits AQP Properties \u00b6 The AQP properties can be set using a Snappy SQL shell (snappy-sql) command or using the configuration properties in the conf/leads file. The command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the Snappy SQL shell (snappy-sql) snappy > connect client 'localhost:1527' ; snappy > set snappydata . flushReservoirThreshold = 20000 ; Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k -spark.sql.aqp.error=0.5 This sets the property for the snappy SQL shell's session. Properties Description -snappydata.flushReservoirThreshold Reservoirs of sample table will be flushed and stored in columnar format if sampling is done on the base table of size more than flushReservoirThreshold. The default value is10,000. This property must be set in the conf/servers and conf/leads file. -spark.sql.aqp.numBootStrapTrials Number of bootstrap trials to do for calculating error bounds. The default value is100. This property must be set in the conf/leads file. -spark.sql.aqp.error Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. This property can be set as connection property in the Snappy SQL shell. -spark.sql.aqp.confidence Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. The default value is0.95. This property can be set as connection property in the Snappy SQL shell. -spark.sql.aqp.behavior The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING . This property can be set as connection property in the Snappy SQL shell. Connection Properties \u00b6 You can define connection properties directly in the JDBC connection URL, or in the Properties object while using JDBC API DriverManager.getConnection(String url, java.util.Properties info) . You can also define connection properties in the connect command in an interactive SnappyData session using snappy shell. An example URL that defines a connection property is shown below. In the URL, replace the property1=value1 string with appropriate property that you want to use. Multiple properties can be be specified by separating them with a semicolon. Example URL: jdbc:snappydata://locatorHostName:1527/property1=value1;property2=value2 Example connect command that sets connection properties while using snappy shell snappy> connect client 'localhost:1527/property1=value1;property2=value2'; Property Description Components -allow-explicit-commit See allow-explicit-commit -enable-stats See enable-stats -enable-timestats See enable-timestats -load-balance See load-balance -log-file See log-file -password See password -read-timeout See read-timeout -skip-constraint-checks See skip-constraint-checks -skip-locks See skip-locks -user See user","title":"List of Properties"},{"location":"configuring_cluster/property_description/#list-of-properties","text":"The following list of commonly used configuration properties can be set to configure the cluster. These properties can be set in the conf/servers , conf/leads or conf/locators configuration files. Network Configuration Memory Configuration Disk Configuration Security Configuration Spark Configuration Logging, Metrics Configuration JVM Properties SQL Properties AQP Properties Connection Properties Tip For system properties (set in the conf/lead, conf/servers and conf/locators file), -D and -XX: can be used. -J is NOT required for -D and -XX options.","title":"List of Properties"},{"location":"configuring_cluster/property_description/#network-configuration","text":"Property Description Components -ack-severe-alert-threshold See ack-severe-alert-threshold -ack-wait-threshold See ack-wait-threshold -bind-address IP address on which the member is bound. The default behavior is to bind to all local addresses. Also see bind-address Server Lead Locator -client-port The port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527. Locator Server -hostname-for-clients Set the IP address or host name that this server/locator sends to JDBC/ODBC/thrift clients to use for connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where locators, servers are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases this is handled by hostname translation itself, i.e. hostname used in client-bind-address resolves to internal IP address from inside but to public IP address from outside, but for other cases this property will be required. Server -enable-network-partition-detection See enable-network-partition-detection -enforce-unique-host See enforce-unique-host -locators List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. The list must include all locators in use and must be configured consistently for every member of the distributed system. This property should be configured for all the nodes in the respective configuration files, if there are multiple locators. Server Lead Locator -member-timeout Uses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways: 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case. 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. Valid values are in the range 1000-600000 milliseconds. For more information, refer to Best Practices Server Lead Locator -membership-port-range See membership-port-range -peer-discovery-address Use this as value for the port in the \"host:port\" value of \"-locators\" property Locator -peer-discovery-port Port on which the locator listens for peer discovery (includes servers as well as other locators). Valid values are in the range 1-65535, with a default of 10334. Locator read-timeout See read-timeout -spark.ui.port Port for your SnappyData Monitoring Console, which shows tables, memory and workload data. The default is 5050 Lead -redundancy-zone See redundancy-zone -secondary-locators See secondary-locators -socket-buffer-size See socket-buffer-size -socket-lease-time See socket-lease-time -gemfirexd.max-lock-wait See gemfirexd.max-lock-wait","title":"Network Configuration"},{"location":"configuring_cluster/property_description/#memory-configuration","text":"Property Description Components -critical-heap-percentage Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. If you set -heap-size , the default value for critical-heap-percentage is set to 95% of the heap size. Use this switch to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory. Server Lead -critical-off-heap-percentage Sets the critical threshold for off-heap memory usage in percentage, 0-100. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory. Server -eviction-heap-percentage Sets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 85.5% of whatever is set for -critical-heap-percentage . Use this switch to override the default. Server Lead -eviction-off-heap-percentage Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. By default, the eviction threshold is 85.5% of whatever is set for -critical-off-heap-percentage . Use this switch to override the default. Server -heap-size Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=1GB. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the eviction-heap-percentage to 85.5% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM. Server Lead Locator -memory-size Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in specific scenarios . Server Lead -spark.driver.maxResultSize Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1MB or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB. Lead","title":"Memory Configuration"},{"location":"configuring_cluster/property_description/#disk-configuration","text":"Property Description Components -archive-disk-space-limit See archive-disk-space-limit -archive-file-size-limit See archive-file-size-limit -dir Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory). Server Lead Locator gemfirexd.default-startup-recovery-delay See gemfirexd.default-startup-recovery-delay -spark.local.dir Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. For more information, refer to Best Practices . Lead Server -sys-disk-dir See sys-disk-dir","title":"Disk Configuration"},{"location":"configuring_cluster/property_description/#security-configuration","text":"Property Description Components -DCHECK_EXTERNAL_TABLE_AUTHZ Enable authorization of external tables by setting this system property to true when the cluster's security is enabled. System admin or the schema owner can grant or revoke the permissions on external tables to other users. Lead -Dsnappydata.enable-rls Enables the system for row level security when set to true. By default this is off. If this property is set to true, then the Smart Connector access to SnappyData fails. Server Lead Locator -Dsnappydata.RESTRICT_TABLE_CREATION Applicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false. Server Lead Locator -spark.ssl.enabled Enables or disables Spark layer encryption. The default is false. Lead -spark.ssl.keyPassword The password to the private key in the key store. Lead -spark.ssl.keyStore Path to the key store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.keyStorePassword The password used to access the keystore. Lead -spark.ssl.trustStore Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.trustStorePassword The password used to access the truststore. Lead -spark.ssl.protocol The protocol that must be supported by JVM. For example, TLS. Lead SSL Configuration for P2P ssl-enabled , ssl-ciphers , ssl-protocols , ssl-require-authentication . These properties need not be added to the Lead members in case of a client-server connection. Server Lead Locator -thrift-ssl Specifies if you want to enable or disable SSL. Values are true or false. Server Lead Locator -thrift-ssl-properties Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated keystore : Path to key store file keystore-type : The type of key-store (default \"JKS\") keystore-password : Password for the key store file keymanager-type : The type of key manager factory truststore : Path to trust store file truststore-type : The type of trust-store (default \"JKS\") truststore-password : Password for the trust store file trustmanager-type : The type of trust manager factory Server","title":"Security Configuration"},{"location":"configuring_cluster/property_description/#spark-configuration","text":"Property Description Components -spark.executor.cores The number of cores to use on each server. Lead -spark.context-settings.num-cpu-cores The number of cores that can be allocated. The default is 4. Lead -spark.context-settings.memory-per-node The executor memory per node (-Xmx style. For example: 512m, 1G). The default is 512m. Lead -spark.context-settings.streaming.batch_interval The batch interval for Spark Streaming contexts in milliseconds. The default is 1000. Lead -spark.context-settings.streaming.stopGracefully If set to true, the streaming stops gracefully by waiting for the completion of processing of all the received data. The default is true. Lead -spark.context-settings.streaming.stopSparkContext if set to true, the SparkContext is stopped along with the StreamingContext. The default is true. Lead -spark.driver.maxResultSize Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1MB or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB. Lead -spark.eventLog.enabled Set to True to enable event logging for Spark jobs. Lead -spark.eventLog.dir Specify the directory path where the events can be logged for Spark jobs. Lead -spark.jobserver.bind-address The address on which the jobserver listens. Default address is 0.0.0. Lead -spark.jobserver.job-result-cache-size The number of job results to keep per JobResultActor/context. The default is 5000. Lead -spark.jobserver.max-jobs-per-context The number of jobs that can be run simultaneously in the context. The default is 8. Lead -spark.jobserver.port The port on which to run the jobserver. Default port is 8090. Lead -spark.local.dir Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. For more information, refer to Best Practices . Lead Server -spark.network.timeout The default timeout for all network interactions while running queries. Lead -spark.sql.autoBroadcastJoinThreshold Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Lead -spark.sql.codegen.cacheSize Size of the generated code cache. This effectively controls the maximum number of query plans whose generated code (Classes) is cached. The default is 2000. Lead -spark.sql.codegen.wholeStage Turn ON/OFF whole stage code generation in Spark/Snappy SQL. The default is True. Lead -spark.sql.aqp.numBootStrapTrials Number of bootstrap trials to do for calculating error bounds. The default value is100. This property must be set in the conf/leads file. Lead -spark.sql.aqp.error Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. This property can be set as connection property in the Snappy SQL shell. Lead -spark.sql.aqp.confidence Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. The default value is0.95. This property can be set as connection property in the Snappy SQL shell. Lead -spark.sql.aqp.behavior The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING . This property can be set as connection property in the Snappy SQL shell. Lead -spark.ssl.enabled Enables or disables Spark layer encryption. The default is false. Lead -spark.ssl.keyPassword The password to the private key in the key store. Lead -spark.ssl.keyStore Path to the key store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.keyStorePassword The password used to access the keystore. Lead -spark.ssl.trustStore Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. Lead -spark.ssl.trustStorePassword The password used to access the truststore. Lead -spark.ssl.protocol The protocol that must be supported by JVM. For example, TLS. Lead -spark.ui.port Port for your SnappyData Monitoring Console, which shows tables, memory and workload data. The default is 5050 Lead","title":"Spark Configuration"},{"location":"configuring_cluster/property_description/#logging-metrics-configuration","text":"Property Description Components -enable-stats See enable-stats -enable-time-statistics See enable-time-statisticss -enable-timestats See enable-timestats -gemfirexd.debug.true Use this property to set the required trace flag which enables the logging of specific features of SnappyData. Server Lead Locator -log-file Path of the file to which this member writes log messages (default is snappy[member].log in the working directory. For example, snappylocator.log , snappyleader.log , snappyserver.log . In case logging is set via log4j, the default log file is snappydata.log for each of the SnappyData member.) Server Lead Locator -log-level See log-level -snappy.history See snappy.history -statistic-archive-file See statistic-archive-file -statistic-sample-rate See statistic-sample-rate -statistic-sampling-enabled See statistic-sampling-enabled","title":"Logging, Metrics Configuration"},{"location":"configuring_cluster/property_description/#jvm-properties","text":"Property Description Components -classpath Location of user classes required by the SnappyData Server. This path is appended to the current classpath. Server Lead Locator -J JVM option passed to the spawned SnappyData server JVM. For example, use -J-Xmx1GB to set the JVM heap to 1GB. Server Lead Locator Other than the above properties, you can also refer the Configuration Parameters section for properties that are used in special cases.","title":"JVM Properties"},{"location":"configuring_cluster/property_description/#sql-properties","text":"These properties can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy > connect client 'localhost:1527' ; snappy > set snappydata . column . batchSize = 100 k ; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Property Description -allow-explicit-commit See allow-explicit-commit -init-scripts See init-scripts -skip-constraint-checks See skip-constraint-checks -skip-locks See skip-locks -gemfirexd.datadictionary.allow-startup-errors See gemfirexd.datadictionary.allow-startup-errors -gemfirexd.query-cancellation-interval See gemfirexd.query-cancellation-interval -gemfirexd.query-timeout See gemfirexd.query-timeout -snappydata.column.batchSize The default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes or k/m/g suffixes for unit) that is used to split the data into chunks for efficient storage and retrieval. This property can also be set for each table in the create table DDL. Maximum allowed size is 2GB. The default is 24m. -snappydata.column.maxDeltaRows The maximum number of rows that can be in the delta buffer of a column table. The size of the delta buffer is already limited by ColumnBatchSize property, but this allows a lower limit on the number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of ColumnBatchSize and this property is hit first. It can also be set for each table in the create table DDL, else this setting is used for the create table snappydata.column.compactionRatio The ratio of deleted rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive) with the default as 0.1. The compaction is triggered in one of the foreground threads performing delete or update. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. snappydata.column.updateCompactionRatio The ratio of updated rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive) with the default as 0.2. The compaction of the column batch is triggered in one of the foreground threads performing delete or update. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. spark.sql.maxMemoryResultSize Maximum size of results from a JDBC/ODBC/SQL query in a partition that will be held in memory beyond which the results will be written to disk. The disk file will continue to grow till 8 times this initial value after which a new disk file will be written. The default value is 4MB. spark.sql.resultPersistenceTimeout Maximum duration in seconds for which results overflowed to disk are held on disk after which they are cleaned up. The default value is 21600 i.e. 6 hours. -snappydata.hiveServer.enabled Enables the Hive Thrift server for SnappyData.This is enabled by default when you start the cluster. Thus it adds an additional 10 seconds to the cluster startup time. To avoid this additional time, you can set the property to false. -snappydata.maxRetryAttemptsForWrite Default retry of Spark tasks on failure can cause duplicates in the case of insert operations. This property can be set to 0 to avoid this scenario. Other operations, as usual, retries without causing any consistency issues. -snappydata.sql.hashJoinSize The join would be converted into a hash join if the table is of size less than the hashJoinSize . The limit specifies an estimate on the input data size (in bytes or k/m/g/t suffixes for unit). The default value is 100MB. -snappydata.sql.hashAggregateSize Aggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (in bytes or k/m/g/t suffixes for unit) and not the output size. Set this only if there are queries that can return a large number of rows in aggregation results. The default value is set to 0 which means, no limit is set on the size, so the optimized hash aggregation is always used. -snappydata.sql.useDriverCollectForGroupBy Allow driver do the direct collect of partial results for top-level GROUP BY avoiding the last EXCHANGE for partial results of a GROUP BY query, improving the performance substantially for sub-second queries. This is a SQL session property. Default value is false. It should be enabled only when the final size of results of the query is known to be small for all queries in the session else can cause heap memory issues on the driver. -snappydata.sql.planCacheSize Number of query plans that will be cached. -spark.sql.autoBroadcastJoinThreshold Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join. By setting this value to -1 broadcasting can be disabled. -snappydata.linkPartitionsToBuckets When this property is set to true, each bucket is always treated as a separate partition in column/row table scans. When this is set to false, SnappyData creates only as many partitions as executor cores by clubbing multiple buckets into each partition when possible. The default is false. -snappydata.preferPrimaries Use this property to configure your preference to use primary buckets in queries. This reduces the scalability of queries in the interest of reduced memory usage for secondary buckets. The default is false. -snappydata.sql.partitionPruning Use this property to set/unset the partition pruning of queries. -snappydata.sql.tokenize Use this property to enable/disable tokenization. -snappydata.cache.putIntoInnerJoinResultSize The putInto inner join would be cached if the result of join with incoming Dataset is of size less than this limit. Value is in bytes or k[b]/m[b]/g[b]/t[b] suffixes for unit. Large cache will overflow to disk using Spark's BlockManager. Default value is -1 which indicates no limit. -snappydata.cache.putIntoInnerJoinLocalCache Cache the putInto inner join locally at the driver node. Use only if the size of the putInto data and the resulting updates is small. This is a SQL session property. Default is do local caching only if the data being put is a local relation created from a list of Rows its size is within the limit specified by snappydata.sql.hashJoinSize . This behavior is different from setting it to false explicitly which will always turn it off. Leave at default of unset unless you are completely certain of all puts that can be done in the system. -snappydata.scheduler.pool Use this property to define scheduler pool to either default or low latency. You can also assign queries to different pools. -snappydata.enable-experimental-features Use this property to enable and disable experimental features. You can call out in case some features are completely broken and need to be removed from the product. -snappydata.sql.planCaching Use this property to enable/disable plan caching. By default it is disabled. snappydata.recovery.enableTableCountInUI In the recovery mode, by default, the table counts and sizes do not appear on the UI. To view the table counts, you should set this property to true in the lead's conf file. By default, the property is set to false , and the table count is shown as -1 . sync-commits See sync-commits","title":"SQL Properties"},{"location":"configuring_cluster/property_description/#aqp-properties","text":"The AQP properties can be set using a Snappy SQL shell (snappy-sql) command or using the configuration properties in the conf/leads file. The command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the Snappy SQL shell (snappy-sql) snappy > connect client 'localhost:1527' ; snappy > set snappydata . flushReservoirThreshold = 20000 ; Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k -spark.sql.aqp.error=0.5 This sets the property for the snappy SQL shell's session. Properties Description -snappydata.flushReservoirThreshold Reservoirs of sample table will be flushed and stored in columnar format if sampling is done on the base table of size more than flushReservoirThreshold. The default value is10,000. This property must be set in the conf/servers and conf/leads file. -spark.sql.aqp.numBootStrapTrials Number of bootstrap trials to do for calculating error bounds. The default value is100. This property must be set in the conf/leads file. -spark.sql.aqp.error Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. This property can be set as connection property in the Snappy SQL shell. -spark.sql.aqp.confidence Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. The default value is0.95. This property can be set as connection property in the Snappy SQL shell. -spark.sql.aqp.behavior The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING . This property can be set as connection property in the Snappy SQL shell.","title":"AQP Properties"},{"location":"configuring_cluster/property_description/#connection-properties","text":"You can define connection properties directly in the JDBC connection URL, or in the Properties object while using JDBC API DriverManager.getConnection(String url, java.util.Properties info) . You can also define connection properties in the connect command in an interactive SnappyData session using snappy shell. An example URL that defines a connection property is shown below. In the URL, replace the property1=value1 string with appropriate property that you want to use. Multiple properties can be be specified by separating them with a semicolon. Example URL: jdbc:snappydata://locatorHostName:1527/property1=value1;property2=value2 Example connect command that sets connection properties while using snappy shell snappy> connect client 'localhost:1527/property1=value1;property2=value2'; Property Description Components -allow-explicit-commit See allow-explicit-commit -enable-stats See enable-stats -enable-timestats See enable-timestats -load-balance See load-balance -log-file See log-file -password See password -read-timeout See read-timeout -skip-constraint-checks See skip-constraint-checks -skip-locks See skip-locks -user See user","title":"Connection Properties"},{"location":"configuring_cluster/property_details/","text":"Properties Details \u00b6 WORK IN PROGRESS - OUTDATED CONTENT autoBroadcastJoinThreshold bind-address classpath critical-heap-percentage critical-off-heap-percentage dir eviction-heap-percentage eviction-off-heap-percentage heap-size J J-Dgemfirexd.hostname-for-clients locators log-file member-timeout memory-size peer-discovery-address peer-discovery-port rebalance snappydata.column.batchSize snappydata.column.compactionRatio snappydata.column.updateCompactionRatio spark.driver.maxResultSize spark.sql.maxMemoryResultSize spark.sql.resultPersistenceTimeout spark.executor.cores spark.local.dir spark.network.timeout thrift-ssl-properties Behavior ColumnBatchSize ColumnMaxDeltaRows Confidence EnableExperimentalFeatures Error FlushReservoirThreshold ForceLinkPartitionsToBuckets HashAggregateSize HashJoinSize JobServerEnabled JobServerWaitForInit NumBootStrapTrials ParserTraceError PartitionPruning PlanCaching PlanCachingAll PlanCacheSize PreferPrimariesInQuery SchedulerPool SnappyConnection Tokenize autoBroadcastJoinThreshold \u00b6 Description Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join. By setting this value to -1 broadcasting can be disabled. This is an SQL property. Default Values 10L * 1024 * 1024 Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example // To set auto broadcast snc . sql ( s\"set spark.sql.autoBroadcastJoinThreshold=<_SizeInBytes_>\" ) // To disable auto broadcast snc . conf . set ( \"spark.sql.autoBroadcastJoinThreshold\" , \"-1\" ) bind-address \u00b6 Description IP address on which the locator is bound. The default behavior is to bind to all local addresses. Default Values localhost Components Server Lead Locator Example -client-bind-address=localhost classpath \u00b6 Description Location of user classes required by the SnappyData Server/Lead nodes. This path is appended to the current classpath. Default Values Components Server Lead Example -classpath=<path to user jar> critical-heap-percentage \u00b6 Description Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. Use this to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory. Default Values If you set heap-size , the default value for critical-heap-percentage is set to 90% of the heap size. Components Server Lead Example conf . set ( \"snappydata.store.critical-heap-percentage\" , \"95\" ) critical-off-heap-percentage \u00b6 Description Sets the critical threshold for off-heap memory usage in percentage, 0-100. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory. Default Values Components Server Example conf . set ( \"snappydata.store.critical-off-heap-percentage\" , \"95\" ) dir \u00b6 Description Working directory of the server that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, etc. Default Values Defaults to the product's work directory. Components Server Lead Locator Example This is the launcher property and needs to be specified in the respective conf files before starting the cluster. -dir = <path to log directory> eviction-heap-percentage \u00b6 Description Sets the memory usage percentage threshold (0-100) that the Resource Manager uses to start evicting data from the heap. Use this to override the default. Default Values By default, the eviction threshold is 81% of that value that is set for critical-heap-percentage . Components Server Lead Example This can be set as a launcher property or a boot property as below: -eviction-heap-percentage = \u201c20\u201d props . setProperty ( \"eviction-heap-percentage\" , \"20\" ) eviction-off-heap-percentage \u00b6 Description Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. Use this to override the default. Default Values By default, the eviction threshold is 81% of whatever is set for critical-off-heap-percentage . Components Server Example This can be set as a launcher property or a boot property as below: -eviction-heap-percentage = \u201c20\u201d props . setProperty ( \"eviction-off-heap-percentage\" , \"15\" ) heap-size \u00b6 Description Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=1024m. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 90% of the heap size, and the eviction-heap-percentage to 81% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM. Default Values Components Server Lead Locator Example J \u00b6 Description JVM option passed to the spawned SnappyData server JVM. For example, use -J-Xmx1024m to set the JVM heap to 1GB. Default Values Components Server Lead Locator Example J-Dgemfirexd.hostname-for-clients \u00b6 Description The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where the servers/locators are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in client-bind-address resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required Default Values Components Server Lead Locator Example locators \u00b6 Description List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. The list must include all locators in use and must be configured consistently for every member of the distributed system. Default Values Components Server Lead Locator Example locator1 -peer-discovery-port=9988 -locators=locator2:8899 log-file \u00b6 Description Path of the file to which this member writes log messages (default is snappyserver.log in the working directory) Default Values Components Server Lead Locator Example member-timeout \u00b6 Description Uses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways: 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case. 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. Valid values are in the range 1000..600000. Default Values Components Server Lead Locator Example memory-size \u00b6 Description Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is 0 (OFF_HEAP is not used by default) Default Values Components Server Lead Example peer-discovery-address \u00b6 Description Use this as value for the port in the \"host:port\" value of locators property. Default Values Components Locator Example peer-discovery-port \u00b6 Description Port on which the locator listens for peer discovery (includes servers as well as other locators). Valid values are in the range 1-65535, with a default of 10334. Default Values Components Locator Example locator1 -peer-discovery-port=9988 -locators=locator2:8899 rebalance \u00b6 Description Triggers a rebalancing operation for all partitioned tables in the system. The system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option. Default Values Components Server Example [-rebalance] [-init-scripts=<sql-files>] snappydata.column.batchSize \u00b6 Description The default size of blocks to use for storage in the SnappyData column store (in bytes or k/m/g suffixes for the unit). The default value is 24M. This is an SQL property Default Values 24MB Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k snappydata.column.compactionRatio \u00b6 Description The ratio of deleted rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive). The compaction is triggered in one of the foreground threads performing delete or update. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 0.1 Components Server Example -snappydata.column.compactionRatio=0.15 snappydata.column.updateCompactionRatio \u00b6 Description The ratio of updated rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive). The compaction is triggered in one of the foreground threads performing update or delete. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 0.2 Components Server Example -snappydata.column.updateCompactionRatio=0.15 spark.driver.maxResultSize \u00b6 Description Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1M or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. Default Values 1GB Components Lead Example -spark.driver.maxResultSize=2g spark.sql.maxMemoryResultSize \u00b6 Description The maximum size of results from a JDBC/ODBC/SQL query in a partition that will be held in memory beyond which the results will be written to disk. The disk file will continue to grow till 8 times this initial value after which a new disk file will be written. The server/lead to which the client is connected will decompress one disk file at a time and return the results, so the disk file size should not be too large (e.g. with the default 4MB for this property, maximum disk file size is 32MB). Note that this only works with JDBC/ODBC drivers or when using SnappySession.sql().toLocalIterator() API. If there is any intermediate Dataset API operation before the toLocalIterator(), then the regular Spark DataFrame will get created which will hold the results only in memory and whose behaviour is governed by spark.driver.maxResultSize instead. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 4MB Components Server Example -spark.sql.maxMemoryResultSize=8m spark.sql.resultPersistenceTimeout \u00b6 Description Maximum duration in seconds for which results overflowed to disk are held on disk after which they are cleaned up. This works in conjunction with spark.sql.maxMemoryResultSize to delete the intermediate disk files generated. This will take effect only for the operations mentioned in that property description. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 21600 i.e. 6 hours Components Server Example -spark.sql.resultPersistenceTimeout=14400 spark.executor.cores \u00b6 Description The number of cores to use on each server. Default Values Components Lead Example -spark.executor.cores=10 spark.local.dir \u00b6 Description Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. Default Values Components Lead Example conf . set ( \"spark.local.dir\" , localDir . getAbsolutePath ) spark.network.timeout \u00b6 Description The default timeout for all network interactions while running queries. Default Values Components Lead Example conf . get ( \"spark.network.timeout\" , \"120s\" )); thrift-ssl-properties \u00b6 Description Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated keystore : Path to key store file keystore-type : The type of key-store (default \"JKS\") keystore-password : Password for the key store file keymanager-type : The type of key manager factory truststore : Path to trust store file truststore-type : The type of trust-store (default \"JKS\") truststore-password : Password for the trust store file trustmanager-type : The type of trust manager factory Default Values Components Server Example -thrift-ssl-properties=keystore=keystore Behavior \u00b6 Description The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING . This property can be set as connection property in the Snappy SQL shell. Synopsis Data Engine has HAC support using the following behavior clauses: - do_nothing : The SDE engine returns the estimate as is. - local_omit : For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\". - strict : If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception. - run_on_full_table : If any of the single output row exceeds the specified error, then the full query is re-executed on the base table. - partial_run_on_base_table : If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups. This result is then merged (without any duplicates) with the result derived from the sample table. This is an SDE Property Default Values Components Lead Example SELECT ... FROM .. WHERE .. GROUP BY ... < br > WITH [ BEHAVIOR `< string > ] ` ColumnBatchSize \u00b6 Description The default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes or k/m/g suffixes for unit) that is used to split the data into chunks for efficient storage and retrieval. This property can also be set for each table in the create table DDL. Default Values Components Can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example ColumnMaxDeltaRows \u00b6 Description The maximum number of rows that can be in the delta buffer of a column table. The size of delta buffer is already limited by ColumnBatchSize property, but this allows a lower limit on the number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of ColumnBatchSize and this property is hit first. It can also be set for each table in the create table DDL, else this setting is used for the `create table. This is an SQL property. Default Values Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example Confidence \u00b6 Description Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. Default Values The default value is 0.95. Components Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. The default value is0.95. This property can be set as connection property in the Snappy SQL shell. Example SELECT ... FROM .. WHERE .. GROUP BY ... < br > WITH [ CONFIDENCE ` < fraction >` ] EnableExperimentalFeatures \u00b6 Description SQLConf property that enables experimental features like distributed index optimizer choice during query planning. Default Values Default is False. Components Example snc . setConf ( io . snappydata . Property . EnableExperimentalFeatures . name , \"false\" ) Error \u00b6 Description Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. The following four methods are available to be used in query projection when running approximate queries: absolute_error(column alias) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) relative_error(column alias) : Indicates ratio of absolute error to estimate. lower_bound(column alias) : Lower value of an estimate interval for a given confidence. upper_bound(column alias) : Upper value of an estimate interval for a given confidence. This is an SDE Property Default Values Components This property can be set as connection property in the Snappy SQL shell Example SELECT ... FROM .. WHERE .. GROUP BY ... < br > WITH ERROR `< fraction > ` FlushReservoirThreshold \u00b6 Description Reservoirs of sample table will be flushed and stored in columnar format if sampling is done on the base table of size more than flushReservoirThreshold. This is an SDE Property Default Values The default value is 10,000. Components This property must be set in the conf/servers and conf/leads file Example ForceLinkPartitionsToBuckets \u00b6 Description This property enables you to treat each bucket as separate partition in column/row table scans. When set to false, SnappyData tries to create only as many partitions as executor cores combining multiple buckets into each partition when possible. Default Values False Components Example HashAggregateSize \u00b6 Description Aggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (in bytes or k/m/g/t suffixes for unit) and not the output size. Set this only if there are queries that can return large number of rows in aggregation results. This is an SQL property. Default Values The default value is set to 0 which means, no limit is set on the size, so the optimized hash aggregation is always used. Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example HashJoinSize \u00b6 Description The join would be converted into a hash join if the table size is less than the hashJoinSize . The limit specifies an estimate on the input data size (in bytes or k/m/g/t suffixes for unit). This is an SQL property. Default Values The default value is 100MB. Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example JobServerEnabled \u00b6 Allows you to enable REST API access via Spark jobserver in the SnappyData cluster. Description Default Values False Components Lead Example setProperty ( Property . JobServerEnabled . name , \"false\" ) JobServerWaitForInit \u00b6 Description When enabled, the cluster startup waits for Spark jobserver to be fully initialized before marking the lead as 'RUNNING'. Default Values False Components Example NumBootStrapTrials \u00b6 Description Number of bootstrap trials to do for calculating error bounds. This is an SDE Property Default Values The default value is 100. Components conf/leads Example ParserTraceError \u00b6 Description Enables detailed rule tracing for parse errors. Default Values Components Example showTraces = Property . ParserTraceError . get ( session . sessionState . conf )))) PartitionPruning \u00b6 Description Allows you to enable partition pruning of queries. Default Values Components Example PlanCaching \u00b6 Description Allows you to enable plan caching. Default Values Components Example PlanCachingAll \u00b6 Description Allows you to enable plan caching on all sessions. Default Values Components Can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example PlanCacheSize \u00b6 Description Sets the number of query plans to be cached. This is an SQL property. Default Values Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example PreferPrimariesInQuery \u00b6 Description Allows you to use primary buckets in queries. This reduces scalability of queries in the interest of reduced memory usage for secondary buckets. Default Values Default is false Components Example SchedulerPool \u00b6 Description Set the scheduler pool for the current session. This property can be used to assign queries to different pools for improving throughput of specific queries. Default Values Components Example SnappyConnection \u00b6 Description Used in the Smart Connector mode to connect to the SnappyData cluster using the JDBC driver. Provide the host name and client port in the form host:clientPort . This is used to form a JDBC URL of the form \"\\\"jdbc:snappydata://host:clientPort/\\\" (or use the form \\\"host[clientPort]\\\") . It is recommended that the hostname and the client port of the locator is specified for this property. Default Values Components Example Tokenize \u00b6 Description Property to enable/disable tokenization Default Values Components Example","title":"Properties Details"},{"location":"configuring_cluster/property_details/#properties-details","text":"WORK IN PROGRESS - OUTDATED CONTENT autoBroadcastJoinThreshold bind-address classpath critical-heap-percentage critical-off-heap-percentage dir eviction-heap-percentage eviction-off-heap-percentage heap-size J J-Dgemfirexd.hostname-for-clients locators log-file member-timeout memory-size peer-discovery-address peer-discovery-port rebalance snappydata.column.batchSize snappydata.column.compactionRatio snappydata.column.updateCompactionRatio spark.driver.maxResultSize spark.sql.maxMemoryResultSize spark.sql.resultPersistenceTimeout spark.executor.cores spark.local.dir spark.network.timeout thrift-ssl-properties Behavior ColumnBatchSize ColumnMaxDeltaRows Confidence EnableExperimentalFeatures Error FlushReservoirThreshold ForceLinkPartitionsToBuckets HashAggregateSize HashJoinSize JobServerEnabled JobServerWaitForInit NumBootStrapTrials ParserTraceError PartitionPruning PlanCaching PlanCachingAll PlanCacheSize PreferPrimariesInQuery SchedulerPool SnappyConnection Tokenize","title":"Properties Details"},{"location":"configuring_cluster/property_details/#autobroadcastjointhreshold","text":"Description Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join. By setting this value to -1 broadcasting can be disabled. This is an SQL property. Default Values 10L * 1024 * 1024 Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example // To set auto broadcast snc . sql ( s\"set spark.sql.autoBroadcastJoinThreshold=<_SizeInBytes_>\" ) // To disable auto broadcast snc . conf . set ( \"spark.sql.autoBroadcastJoinThreshold\" , \"-1\" )","title":"autoBroadcastJoinThreshold"},{"location":"configuring_cluster/property_details/#bind-address","text":"Description IP address on which the locator is bound. The default behavior is to bind to all local addresses. Default Values localhost Components Server Lead Locator Example -client-bind-address=localhost","title":"bind-address"},{"location":"configuring_cluster/property_details/#classpath","text":"Description Location of user classes required by the SnappyData Server/Lead nodes. This path is appended to the current classpath. Default Values Components Server Lead Example -classpath=<path to user jar>","title":"classpath"},{"location":"configuring_cluster/property_details/#critical-heap-percentage","text":"Description Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. Use this to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory. Default Values If you set heap-size , the default value for critical-heap-percentage is set to 90% of the heap size. Components Server Lead Example conf . set ( \"snappydata.store.critical-heap-percentage\" , \"95\" )","title":"critical-heap-percentage"},{"location":"configuring_cluster/property_details/#critical-off-heap-percentage","text":"Description Sets the critical threshold for off-heap memory usage in percentage, 0-100. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory. Default Values Components Server Example conf . set ( \"snappydata.store.critical-off-heap-percentage\" , \"95\" )","title":"critical-off-heap-percentage"},{"location":"configuring_cluster/property_details/#dir","text":"Description Working directory of the server that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, etc. Default Values Defaults to the product's work directory. Components Server Lead Locator Example This is the launcher property and needs to be specified in the respective conf files before starting the cluster. -dir = <path to log directory>","title":"dir"},{"location":"configuring_cluster/property_details/#eviction-heap-percentage","text":"Description Sets the memory usage percentage threshold (0-100) that the Resource Manager uses to start evicting data from the heap. Use this to override the default. Default Values By default, the eviction threshold is 81% of that value that is set for critical-heap-percentage . Components Server Lead Example This can be set as a launcher property or a boot property as below: -eviction-heap-percentage = \u201c20\u201d props . setProperty ( \"eviction-heap-percentage\" , \"20\" )","title":"eviction-heap-percentage"},{"location":"configuring_cluster/property_details/#eviction-off-heap-percentage","text":"Description Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. Use this to override the default. Default Values By default, the eviction threshold is 81% of whatever is set for critical-off-heap-percentage . Components Server Example This can be set as a launcher property or a boot property as below: -eviction-heap-percentage = \u201c20\u201d props . setProperty ( \"eviction-off-heap-percentage\" , \"15\" )","title":"eviction-off-heap-percentage"},{"location":"configuring_cluster/property_details/#heap-size","text":"Description Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. For example, -heap-size=1024m. If you use the -heap-size option, by default SnappyData sets the critical-heap-percentage to 90% of the heap size, and the eviction-heap-percentage to 81% of the critical-heap-percentage . SnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM. Default Values Components Server Lead Locator Example","title":"heap-size"},{"location":"configuring_cluster/property_details/#j","text":"Description JVM option passed to the spawned SnappyData server JVM. For example, use -J-Xmx1024m to set the JVM heap to 1GB. Default Values Components Server Lead Locator Example","title":"J"},{"location":"configuring_cluster/property_details/#j-dgemfirexdhostname-for-clients","text":"Description The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where the servers/locators are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in client-bind-address resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required Default Values Components Server Lead Locator Example","title":"J-Dgemfirexd.hostname-for-clients"},{"location":"configuring_cluster/property_details/#locators","text":"Description List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. The list must include all locators in use and must be configured consistently for every member of the distributed system. Default Values Components Server Lead Locator Example locator1 -peer-discovery-port=9988 -locators=locator2:8899","title":"locators"},{"location":"configuring_cluster/property_details/#log-file","text":"Description Path of the file to which this member writes log messages (default is snappyserver.log in the working directory) Default Values Components Server Lead Locator Example","title":"log-file"},{"location":"configuring_cluster/property_details/#member-timeout","text":"Description Uses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways: 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case. 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. Valid values are in the range 1000..600000. Default Values Components Server Lead Locator Example","title":"member-timeout"},{"location":"configuring_cluster/property_details/#memory-size","text":"Description Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is 0 (OFF_HEAP is not used by default) Default Values Components Server Lead Example","title":"memory-size"},{"location":"configuring_cluster/property_details/#peer-discovery-address","text":"Description Use this as value for the port in the \"host:port\" value of locators property. Default Values Components Locator Example","title":"peer-discovery-address"},{"location":"configuring_cluster/property_details/#peer-discovery-port","text":"Description Port on which the locator listens for peer discovery (includes servers as well as other locators). Valid values are in the range 1-65535, with a default of 10334. Default Values Components Locator Example locator1 -peer-discovery-port=9988 -locators=locator2:8899","title":"peer-discovery-port"},{"location":"configuring_cluster/property_details/#rebalance","text":"Description Triggers a rebalancing operation for all partitioned tables in the system. The system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option. Default Values Components Server Example [-rebalance] [-init-scripts=<sql-files>]","title":"rebalance"},{"location":"configuring_cluster/property_details/#snappydatacolumnbatchsize","text":"Description The default size of blocks to use for storage in the SnappyData column store (in bytes or k/m/g suffixes for the unit). The default value is 24M. This is an SQL property Default Values 24MB Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k","title":"snappydata.column.batchSize"},{"location":"configuring_cluster/property_details/#snappydatacolumncompactionratio","text":"Description The ratio of deleted rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive). The compaction is triggered in one of the foreground threads performing delete or update. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 0.1 Components Server Example -snappydata.column.compactionRatio=0.15","title":"snappydata.column.compactionRatio"},{"location":"configuring_cluster/property_details/#snappydatacolumnupdatecompactionratio","text":"Description The ratio of updated rows in a column batch that will trigger its compaction. The value should be between 0 and 1 (both exclusive). The compaction is triggered in one of the foreground threads performing update or delete. The compacted batch will be put into the delta buffer if it has become too small else it will be put into the column store. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 0.2 Components Server Example -snappydata.column.updateCompactionRatio=0.15","title":"snappydata.column.updateCompactionRatio"},{"location":"configuring_cluster/property_details/#sparkdrivermaxresultsize","text":"Description Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1M or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. Default Values 1GB Components Lead Example -spark.driver.maxResultSize=2g","title":"spark.driver.maxResultSize"},{"location":"configuring_cluster/property_details/#sparksqlmaxmemoryresultsize","text":"Description The maximum size of results from a JDBC/ODBC/SQL query in a partition that will be held in memory beyond which the results will be written to disk. The disk file will continue to grow till 8 times this initial value after which a new disk file will be written. The server/lead to which the client is connected will decompress one disk file at a time and return the results, so the disk file size should not be too large (e.g. with the default 4MB for this property, maximum disk file size is 32MB). Note that this only works with JDBC/ODBC drivers or when using SnappySession.sql().toLocalIterator() API. If there is any intermediate Dataset API operation before the toLocalIterator(), then the regular Spark DataFrame will get created which will hold the results only in memory and whose behaviour is governed by spark.driver.maxResultSize instead. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 4MB Components Server Example -spark.sql.maxMemoryResultSize=8m","title":"spark.sql.maxMemoryResultSize"},{"location":"configuring_cluster/property_details/#sparksqlresultpersistencetimeout","text":"Description Maximum duration in seconds for which results overflowed to disk are held on disk after which they are cleaned up. This works in conjunction with spark.sql.maxMemoryResultSize to delete the intermediate disk files generated. This will take effect only for the operations mentioned in that property description. This is a Spark property that needs to be set in the node configuration as an argument or as system property. Default Value 21600 i.e. 6 hours Components Server Example -spark.sql.resultPersistenceTimeout=14400","title":"spark.sql.resultPersistenceTimeout"},{"location":"configuring_cluster/property_details/#sparkexecutorcores","text":"Description The number of cores to use on each server. Default Values Components Lead Example -spark.executor.cores=10","title":"spark.executor.cores"},{"location":"configuring_cluster/property_details/#sparklocaldir","text":"Description Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. Default Values Components Lead Example conf . set ( \"spark.local.dir\" , localDir . getAbsolutePath )","title":"spark.local.dir"},{"location":"configuring_cluster/property_details/#sparknetworktimeout","text":"Description The default timeout for all network interactions while running queries. Default Values Components Lead Example conf . get ( \"spark.network.timeout\" , \"120s\" ));","title":"spark.network.timeout"},{"location":"configuring_cluster/property_details/#thrift-ssl-properties","text":"Description Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated keystore : Path to key store file keystore-type : The type of key-store (default \"JKS\") keystore-password : Password for the key store file keymanager-type : The type of key manager factory truststore : Path to trust store file truststore-type : The type of trust-store (default \"JKS\") truststore-password : Password for the trust store file trustmanager-type : The type of trust manager factory Default Values Components Server Example -thrift-ssl-properties=keystore=keystore","title":"thrift-ssl-properties"},{"location":"configuring_cluster/property_details/#behavior","text":"Description The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING . This property can be set as connection property in the Snappy SQL shell. Synopsis Data Engine has HAC support using the following behavior clauses: - do_nothing : The SDE engine returns the estimate as is. - local_omit : For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\". - strict : If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception. - run_on_full_table : If any of the single output row exceeds the specified error, then the full query is re-executed on the base table. - partial_run_on_base_table : If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups. This result is then merged (without any duplicates) with the result derived from the sample table. This is an SDE Property Default Values Components Lead Example SELECT ... FROM .. WHERE .. GROUP BY ... < br > WITH [ BEHAVIOR `< string > ] `","title":"Behavior"},{"location":"configuring_cluster/property_details/#columnbatchsize","text":"Description The default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes or k/m/g suffixes for unit) that is used to split the data into chunks for efficient storage and retrieval. This property can also be set for each table in the create table DDL. Default Values Components Can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example","title":"ColumnBatchSize"},{"location":"configuring_cluster/property_details/#columnmaxdeltarows","text":"Description The maximum number of rows that can be in the delta buffer of a column table. The size of delta buffer is already limited by ColumnBatchSize property, but this allows a lower limit on the number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of ColumnBatchSize and this property is hit first. It can also be set for each table in the create table DDL, else this setting is used for the `create table. This is an SQL property. Default Values Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example","title":"ColumnMaxDeltaRows"},{"location":"configuring_cluster/property_details/#confidence","text":"Description Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. Default Values The default value is 0.95. Components Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. The default value is0.95. This property can be set as connection property in the Snappy SQL shell. Example SELECT ... FROM .. WHERE .. GROUP BY ... < br > WITH [ CONFIDENCE ` < fraction >` ]","title":"Confidence"},{"location":"configuring_cluster/property_details/#enableexperimentalfeatures","text":"Description SQLConf property that enables experimental features like distributed index optimizer choice during query planning. Default Values Default is False. Components Example snc . setConf ( io . snappydata . Property . EnableExperimentalFeatures . name , \"false\" )","title":"EnableExperimentalFeatures"},{"location":"configuring_cluster/property_details/#error","text":"Description Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. The following four methods are available to be used in query projection when running approximate queries: absolute_error(column alias) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) relative_error(column alias) : Indicates ratio of absolute error to estimate. lower_bound(column alias) : Lower value of an estimate interval for a given confidence. upper_bound(column alias) : Upper value of an estimate interval for a given confidence. This is an SDE Property Default Values Components This property can be set as connection property in the Snappy SQL shell Example SELECT ... FROM .. WHERE .. GROUP BY ... < br > WITH ERROR `< fraction > `","title":"Error"},{"location":"configuring_cluster/property_details/#flushreservoirthreshold","text":"Description Reservoirs of sample table will be flushed and stored in columnar format if sampling is done on the base table of size more than flushReservoirThreshold. This is an SDE Property Default Values The default value is 10,000. Components This property must be set in the conf/servers and conf/leads file Example","title":"FlushReservoirThreshold"},{"location":"configuring_cluster/property_details/#forcelinkpartitionstobuckets","text":"Description This property enables you to treat each bucket as separate partition in column/row table scans. When set to false, SnappyData tries to create only as many partitions as executor cores combining multiple buckets into each partition when possible. Default Values False Components Example","title":"ForceLinkPartitionsToBuckets"},{"location":"configuring_cluster/property_details/#hashaggregatesize","text":"Description Aggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (in bytes or k/m/g/t suffixes for unit) and not the output size. Set this only if there are queries that can return large number of rows in aggregation results. This is an SQL property. Default Values The default value is set to 0 which means, no limit is set on the size, so the optimized hash aggregation is always used. Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example","title":"HashAggregateSize"},{"location":"configuring_cluster/property_details/#hashjoinsize","text":"Description The join would be converted into a hash join if the table size is less than the hashJoinSize . The limit specifies an estimate on the input data size (in bytes or k/m/g/t suffixes for unit). This is an SQL property. Default Values The default value is 100MB. Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example","title":"HashJoinSize"},{"location":"configuring_cluster/property_details/#jobserverenabled","text":"Allows you to enable REST API access via Spark jobserver in the SnappyData cluster. Description Default Values False Components Lead Example setProperty ( Property . JobServerEnabled . name , \"false\" )","title":"JobServerEnabled"},{"location":"configuring_cluster/property_details/#jobserverwaitforinit","text":"Description When enabled, the cluster startup waits for Spark jobserver to be fully initialized before marking the lead as 'RUNNING'. Default Values False Components Example","title":"JobServerWaitForInit"},{"location":"configuring_cluster/property_details/#numbootstraptrials","text":"Description Number of bootstrap trials to do for calculating error bounds. This is an SDE Property Default Values The default value is 100. Components conf/leads Example","title":"NumBootStrapTrials"},{"location":"configuring_cluster/property_details/#parsertraceerror","text":"Description Enables detailed rule tracing for parse errors. Default Values Components Example showTraces = Property . ParserTraceError . get ( session . sessionState . conf ))))","title":"ParserTraceError"},{"location":"configuring_cluster/property_details/#partitionpruning","text":"Description Allows you to enable partition pruning of queries. Default Values Components Example","title":"PartitionPruning"},{"location":"configuring_cluster/property_details/#plancaching","text":"Description Allows you to enable plan caching. Default Values Components Example","title":"PlanCaching"},{"location":"configuring_cluster/property_details/#plancachingall","text":"Description Allows you to enable plan caching on all sessions. Default Values Components Can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. Example","title":"PlanCachingAll"},{"location":"configuring_cluster/property_details/#plancachesize","text":"Description Sets the number of query plans to be cached. This is an SQL property. Default Values Components This can be set using a SET SQL command or using the configuration properties in the conf/leads file. The SET SQL command sets the property for the current SnappySession while setting it in conf/leads file sets the property for all SnappySession. For example: Set in the snappy SQL shell snappy> connect client 'localhost:1527'; snappy> set snappydata.column.batchSize=100k; This sets the property for the snappy SQL shell's session. Set in the conf/leads file $ cat conf/leads node-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k Example","title":"PlanCacheSize"},{"location":"configuring_cluster/property_details/#preferprimariesinquery","text":"Description Allows you to use primary buckets in queries. This reduces scalability of queries in the interest of reduced memory usage for secondary buckets. Default Values Default is false Components Example","title":"PreferPrimariesInQuery"},{"location":"configuring_cluster/property_details/#schedulerpool","text":"Description Set the scheduler pool for the current session. This property can be used to assign queries to different pools for improving throughput of specific queries. Default Values Components Example","title":"SchedulerPool"},{"location":"configuring_cluster/property_details/#snappyconnection","text":"Description Used in the Smart Connector mode to connect to the SnappyData cluster using the JDBC driver. Provide the host name and client port in the form host:clientPort . This is used to form a JDBC URL of the form \"\\\"jdbc:snappydata://host:clientPort/\\\" (or use the form \\\"host[clientPort]\\\") . It is recommended that the hostname and the client port of the locator is specified for this property. Default Values Components Example","title":"SnappyConnection"},{"location":"configuring_cluster/property_details/#tokenize","text":"Description Property to enable/disable tokenization Default Values Components Example","title":"Tokenize"},{"location":"configuring_cluster/securinguiconnection/","text":"Securing SnappyData Monitoring Console Connection \u00b6 You can secure the SnappyData Monitoring Console with SSL authentication, so that the UI can be accessed only over HTTPS. The following configurations are needed to set up SSL enabled connections for SnappyData Monitoring Console: To set up SSL enabled connections for SnappyData Monitoring Console: Make sure that you have valid SSL certificate imported into truststore. Provide the following spark configuration in the conf/lead files: localhost -spark.ssl.enabled=true -spark.ssl.protocol=<ssl-protocol> -spark.ssl.enabledAlgorithms=<comma-separated-list-of-ciphers> -spark.ssl.keyPassword=<key-password> -spark.ssl.keyStore=<path-to-key-store> -spark.ssl.keyStorePassword=<key-store-password> -spark.ssl.keyStoreType=<key-store-type> -spark.ssl.trustStore=<path-to-trust-store> -spark.ssl.trustStorePassword=<trust-store-password> -spark.ssl.trustStoreType=<trust-store-type> Note If using TLS SSL protocol, the enabledAlgorithms can be TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA Store types could be JKS or PKCS12. Launch the Snappy cluster. ./sbin/snappy-start-all.sh Launch the SnappyData Monitoring Console in your web browser. You are directed to the HTTPS site. Note You are automatically redirected to HTTPS (on port 5450) even if the SnappyData Monitoring Console is accessed with HTTP protocol.","title":"Securing SnappyData UI Connection"},{"location":"configuring_cluster/securinguiconnection/#securing-snappydata-monitoring-console-connection","text":"You can secure the SnappyData Monitoring Console with SSL authentication, so that the UI can be accessed only over HTTPS. The following configurations are needed to set up SSL enabled connections for SnappyData Monitoring Console: To set up SSL enabled connections for SnappyData Monitoring Console: Make sure that you have valid SSL certificate imported into truststore. Provide the following spark configuration in the conf/lead files: localhost -spark.ssl.enabled=true -spark.ssl.protocol=<ssl-protocol> -spark.ssl.enabledAlgorithms=<comma-separated-list-of-ciphers> -spark.ssl.keyPassword=<key-password> -spark.ssl.keyStore=<path-to-key-store> -spark.ssl.keyStorePassword=<key-store-password> -spark.ssl.keyStoreType=<key-store-type> -spark.ssl.trustStore=<path-to-trust-store> -spark.ssl.trustStorePassword=<trust-store-password> -spark.ssl.trustStoreType=<trust-store-type> Note If using TLS SSL protocol, the enabledAlgorithms can be TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA Store types could be JKS or PKCS12. Launch the Snappy cluster. ./sbin/snappy-start-all.sh Launch the SnappyData Monitoring Console in your web browser. You are directed to the HTTPS site. Note You are automatically redirected to HTTPS (on port 5450) even if the SnappyData Monitoring Console is accessed with HTTP protocol.","title":"Securing SnappyData Monitoring Console Connection"},{"location":"configuring_cluster/ssl_setup/","text":"Enabling SSL Encryption in Different Socket Endpoints of SnappyData \u00b6 SnappyData supports SSL encryption for the following: Client-server Peer-to-peer (P2P) Spark layer SSL encryption for the three socket endpoints (P2P, client-server, and Spark) must be configured individually , and the corresponding configuration must be added in configuration files of the respective SnappyData cluster members. Note Properties that begin with thrift is for client-server, javax.net.ssl is for P2P, and spark is for Spark layer SSL settings. Enabling SSL Encryption Simultaneously for all the Socket Endpoints in SnappyData Cluster \u00b6 Using the following configuration files, you can simultaneously enable SSL encryption for all the three socket endpoints (P2P, client-server, and Spark layer SSL encryption) in a SnappyData cluster. Note Currently, you must configure the SSL encryption for all the socket endpoints separately. SnappyData intends to integrate Spark and P2P endpoints in a future release. In the following configuration files, a two-node SnappyData cluster setup is done using the physical hosts. All examples given here includes one locator, one server, and one lead member in a SnappyData cluster configuration. If you want to configure multiple locators, servers and lead members in a cluster, then ensure to copy all the SSL properties for each member configuration into the respective configuration files. For more information about each of these properties mentioned in the respective conf files, as well as for details on configuring multiple locators, servers, and lead members in a cluster, refer to SnappyData Configuration . Locator Configuration File (conf/locators) \u00b6 # hostname locators <all ssl properties> dev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-locatorKeyStore.keyfile> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-locatorKeyStore.keyfile> -J-Djavax.net.ssl.trustStorePassword=<password> -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256 Server Configuration File (conf/servers) \u00b6 # hostname locators <all ssl properties> dev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-serverKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-serverKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS -client-port=1528 -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256 Lead Configuration File (conf/leads) \u00b6 # hostname locators <all ssl properties> dev14 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-leadKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-leadKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS -client-port=1529 Configuring SSL Encryption for Different Socket Endpoints in SnappyData Cluster \u00b6 You can individually configure the SnappyData socket endpoints as per your requirements. This section provides the instructions to configure each of the socket endpoints. Configuring SSL encryption for client-server Configuring SSL encryption for p2p Configuring SSL encryption for Spark Configuring SSL Encryption for Client-server \u00b6 SnappyData store supports the Thrift protocol that provides the functionality that is equivalent to JDBC/ODBC protocols and can be used to access the store from other languages that are not yet supported directly by SnappyData. In the command-line, SnappyData locators and servers accept the -thrift-server-address and -thrift-server-port arguments to start a Thrift server. The Thrift servers use the Thrift Compact Protocol by default which is not SSL enabled. When using the snappy-start-all.sh script, these properties can be specified in the conf/locators and conf/servers files in the product directory like any other locator/server properties. For more information, refer to SnappyData Configuration . In the conf/locators and conf/servers files, you need to add -thrift-ssl and the required SSL setup in -thrift-ssl-properties . Refer to the SnappyData thrift properties section for more information. In the following example, the SSL configuration for client-server is demonstrated along with the startup of SnappyData members with SSL encryption. Requirements \u00b6 Configure SSL keypairs and certificates as needed for client and server. Ensure that all the locator and server members, in the cluster, use the same SSL boot parameters at startup. Provider-Specific Configuration Files \u00b6 This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. To create the keystore and certificate for the client and server, run the following: keytool -genkey -alias myserverkey -keystore serverKeyStoreRSA.jks -keyalg RSA keytool -export -alias myserverkey -keystore serverKeyStoreRSA.jks -rfc -file myServerRSA.cert keytool -import -alias myserverkey -file myServerRSA.cert -keystore trustStore.key The same keystore is used for SnappyData locator and server members as well as for client connection. You can enable SSL encryption for client-server connections by specifying the properties as the startup options for locator and server members. In the following example, the SSL encryption is enabled for communication between client-server. Locator Configuration File (conf/locators) \u00b6 # hostname locators <ssl properties for configuring SSL for SnappyData client-server connections> localhost -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256 Server Configuration File (conf/servers) \u00b6 # hostname locators <ssl properties for configuring SSL for SnappyData client-server connections> localhost -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256 Start the SnappyData cluster using snappy-start-all.sh script and perform operations either by using SnappyData shell or through JDBC connection. You can run the SnappyData quickstart example scripts for this. Refer to SnappyData Cluster SQL Tutorial for the same. Use the protocol/ciphers as per requirement. The corresponding setup on client-side can appear as follows: snappy> connect client 'localhost:1527;ssl=true;ssl-properties=truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256'; Configuring SSL Encryption for P2P \u00b6 In addition to using SSL for client-server connections, you can optionally configure SnappyData members to use SSL encryption and authorization for peer-to-peer(P2P) connections in the distributed system. Peer SSL configuration is managed using javax.net.ssl system properties and the following SnappyData boot properties: ssl-enabled ssl-protocols ssl-ciphers ssl-require-authentication The following sections provide an example of P2P connections encryption that demonstrates the configuration and startup of SnappyData members with SSL encryption. Requirements \u00b6 To configure SSL for SnappyData peer connections: Configure SSL keypairs and certificates as needed for each SnappyData member. Refer to the following example. Ensure that all SnappyData members use the same SSL boot parameters at startup. Provider-Specific Configuration Files \u00b6 This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. To create the keystore and certificate for the locator, run the following: keytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key keytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert You can use similar commands for a server member and a lead member respectively: keytool -genkey -alias mySnappyServer -keystore serverKeyStore.key keytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert keytool -genkey -alias mySnappyLead -keystore leadKeyStore.key keytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert Each of the member's certificate is then imported into the other member's trust store. For the server member: keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key For the locator member: keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key For the lead member: keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key You can enable SSL encryption for peer connections by specifying the properties as the startup options for each member. In the following example, SSL encryption is enabled for communication between the members. Locator Configuration File (conf/locators) \u00b6 # hostname locators <ssl properties for configuring SSL for SnappyData P2P connections> localhost -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> Server Configuration File (conf/servers) \u00b6 # hostname locators <ssl properties for configuring SSL for #SnappyData P2P connections> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-serverKeyStore.keyfile> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -client-port=1528 Lead Configuration File (conf/leads) \u00b6 # hostname locators <ssl properties for configuring SSL for #SnappyData P2P connections> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -client-port=1529 Start the SnappyData cluster using snappy-start-all.sh script and perform operations using the SnappyData shell, SnappyData job, and Smart Connector mode. You can run the SnappyData quickstart example scripts for this. Refer to SnappyData Cluster SQL Tutorial for more information. Configuring SSL Encryption for Spark layer \u00b6 Spark layer SSL configuration is managed using the following SnappyData boot properties: spark.ssl.enabled spark.ssl.keyPassword spark.ssl.keyStore spark.ssl.keyStorePassword spark.ssl.trustStore spark.ssl.trustStorePassword spark.ssl.protocol The following sections provide a simple example that demonstrates the configuration and startup of SnappyData members for enabling Spark layer for Wire Encryption. Requirements \u00b6 To configure SSL for Spark layer: Configure SSL keypairs and certificates as needed for each SnappyData member. Ensure that SnappyData locator, server, and lead members use the same SSL boot parameters at startup. Note For enabling Spark layer SSL encryption, you must first enable the P2P encryption for SSL. Provider-Specific Configuration Files \u00b6 This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. On each node, create keystore files, certificates, and truststore files. Here, in this example, to create the keystore and certificate for the locator, the following was run: keytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key keytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert Similar commands were used for a server member and lead member respectively: keytool -genkey -alias mySnappyServer -keystore serverKeyStore.key keytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert keytool -genkey -alias mySnappyLead -keystore leadKeyStore.key keytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert After this, each of the member's certificate is imported into the other member's trust store. For the locator member: keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key For the server member: keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key For the lead member: keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key You can enable SSL encryption for Spark layer by specifying the properties as the startup options for server and lead members. This example demonstrates how SSL encryption is enabled for a Spark layer connection. Locator Configuration File (conf/locators) \u00b6 # hostname locators <ssl properties for configuring SSL for SnappyData Spark layer> localhost -ssl-enabled=true -spark.ssl.enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-locatorKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-locatorKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS Server Configuration File (conf/servers) \u00b6 # hostname locators <ssl properties for configuring SSL for #SnappyData spark layer> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-serverKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-serverKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS Lead Configuration File (conf/leads) \u00b6 # hostname locators <ssl properties for configuring SSL for #SnappyData spark layer> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-leadKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=/<path-to-leadKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS Start the SnappyData cluster using snappy-start-all.sh script and perform operations using SnappyData shell, SnappyData job, and Smart Connector mode. You can run the SnappyData quickstart example scripts for this. Refer to SnappyData Cluster SQL Tutorial for more information.","title":"Enabling SSL Encryption in Different Socket Endpoints of SnappyData"},{"location":"configuring_cluster/ssl_setup/#enabling-ssl-encryption-in-different-socket-endpoints-of-snappydata","text":"SnappyData supports SSL encryption for the following: Client-server Peer-to-peer (P2P) Spark layer SSL encryption for the three socket endpoints (P2P, client-server, and Spark) must be configured individually , and the corresponding configuration must be added in configuration files of the respective SnappyData cluster members. Note Properties that begin with thrift is for client-server, javax.net.ssl is for P2P, and spark is for Spark layer SSL settings.","title":"Enabling SSL Encryption in Different Socket Endpoints of SnappyData"},{"location":"configuring_cluster/ssl_setup/#enabling-ssl-encryption-simultaneously-for-all-the-socket-endpoints-in-snappydata-cluster","text":"Using the following configuration files, you can simultaneously enable SSL encryption for all the three socket endpoints (P2P, client-server, and Spark layer SSL encryption) in a SnappyData cluster. Note Currently, you must configure the SSL encryption for all the socket endpoints separately. SnappyData intends to integrate Spark and P2P endpoints in a future release. In the following configuration files, a two-node SnappyData cluster setup is done using the physical hosts. All examples given here includes one locator, one server, and one lead member in a SnappyData cluster configuration. If you want to configure multiple locators, servers and lead members in a cluster, then ensure to copy all the SSL properties for each member configuration into the respective configuration files. For more information about each of these properties mentioned in the respective conf files, as well as for details on configuring multiple locators, servers, and lead members in a cluster, refer to SnappyData Configuration .","title":"Enabling SSL Encryption Simultaneously for all the Socket Endpoints in SnappyData Cluster"},{"location":"configuring_cluster/ssl_setup/#locator-configuration-file-conflocators","text":"# hostname locators <all ssl properties> dev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-locatorKeyStore.keyfile> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-locatorKeyStore.keyfile> -J-Djavax.net.ssl.trustStorePassword=<password> -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256","title":"Locator Configuration File (conf/locators)"},{"location":"configuring_cluster/ssl_setup/#server-configuration-file-confservers","text":"# hostname locators <all ssl properties> dev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-serverKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-serverKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS -client-port=1528 -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256","title":"Server Configuration File (conf/servers)"},{"location":"configuring_cluster/ssl_setup/#lead-configuration-file-confleads","text":"# hostname locators <all ssl properties> dev14 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-leadKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-leadKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS -client-port=1529","title":"Lead Configuration File (conf/leads)"},{"location":"configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-different-socket-endpoints-in-snappydata-cluster","text":"You can individually configure the SnappyData socket endpoints as per your requirements. This section provides the instructions to configure each of the socket endpoints. Configuring SSL encryption for client-server Configuring SSL encryption for p2p Configuring SSL encryption for Spark","title":"Configuring SSL Encryption for Different Socket Endpoints in SnappyData Cluster"},{"location":"configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-client-server","text":"SnappyData store supports the Thrift protocol that provides the functionality that is equivalent to JDBC/ODBC protocols and can be used to access the store from other languages that are not yet supported directly by SnappyData. In the command-line, SnappyData locators and servers accept the -thrift-server-address and -thrift-server-port arguments to start a Thrift server. The Thrift servers use the Thrift Compact Protocol by default which is not SSL enabled. When using the snappy-start-all.sh script, these properties can be specified in the conf/locators and conf/servers files in the product directory like any other locator/server properties. For more information, refer to SnappyData Configuration . In the conf/locators and conf/servers files, you need to add -thrift-ssl and the required SSL setup in -thrift-ssl-properties . Refer to the SnappyData thrift properties section for more information. In the following example, the SSL configuration for client-server is demonstrated along with the startup of SnappyData members with SSL encryption.","title":"Configuring SSL Encryption for Client-server"},{"location":"configuring_cluster/ssl_setup/#requirements","text":"Configure SSL keypairs and certificates as needed for client and server. Ensure that all the locator and server members, in the cluster, use the same SSL boot parameters at startup.","title":"Requirements"},{"location":"configuring_cluster/ssl_setup/#provider-specific-configuration-files","text":"This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. To create the keystore and certificate for the client and server, run the following: keytool -genkey -alias myserverkey -keystore serverKeyStoreRSA.jks -keyalg RSA keytool -export -alias myserverkey -keystore serverKeyStoreRSA.jks -rfc -file myServerRSA.cert keytool -import -alias myserverkey -file myServerRSA.cert -keystore trustStore.key The same keystore is used for SnappyData locator and server members as well as for client connection. You can enable SSL encryption for client-server connections by specifying the properties as the startup options for locator and server members. In the following example, the SSL encryption is enabled for communication between client-server.","title":"Provider-Specific Configuration Files"},{"location":"configuring_cluster/ssl_setup/#locator-configuration-file-conflocators_1","text":"# hostname locators <ssl properties for configuring SSL for SnappyData client-server connections> localhost -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256","title":"Locator Configuration File (conf/locators)"},{"location":"configuring_cluster/ssl_setup/#server-configuration-file-confservers_1","text":"# hostname locators <ssl properties for configuring SSL for SnappyData client-server connections> localhost -thrift-ssl=true -thrift-ssl-properties=keystore=<path-to-serverKeyStoreRSA.jks file>,keystore-password=<password>,truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256 Start the SnappyData cluster using snappy-start-all.sh script and perform operations either by using SnappyData shell or through JDBC connection. You can run the SnappyData quickstart example scripts for this. Refer to SnappyData Cluster SQL Tutorial for the same. Use the protocol/ciphers as per requirement. The corresponding setup on client-side can appear as follows: snappy> connect client 'localhost:1527;ssl=true;ssl-properties=truststore=<path-to-trustStore.key file>,truststore-password=<password>,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256';","title":"Server Configuration File (conf/servers)"},{"location":"configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-p2p","text":"In addition to using SSL for client-server connections, you can optionally configure SnappyData members to use SSL encryption and authorization for peer-to-peer(P2P) connections in the distributed system. Peer SSL configuration is managed using javax.net.ssl system properties and the following SnappyData boot properties: ssl-enabled ssl-protocols ssl-ciphers ssl-require-authentication The following sections provide an example of P2P connections encryption that demonstrates the configuration and startup of SnappyData members with SSL encryption.","title":"Configuring SSL Encryption for P2P"},{"location":"configuring_cluster/ssl_setup/#requirements_1","text":"To configure SSL for SnappyData peer connections: Configure SSL keypairs and certificates as needed for each SnappyData member. Refer to the following example. Ensure that all SnappyData members use the same SSL boot parameters at startup.","title":"Requirements"},{"location":"configuring_cluster/ssl_setup/#provider-specific-configuration-files_1","text":"This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. To create the keystore and certificate for the locator, run the following: keytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key keytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert You can use similar commands for a server member and a lead member respectively: keytool -genkey -alias mySnappyServer -keystore serverKeyStore.key keytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert keytool -genkey -alias mySnappyLead -keystore leadKeyStore.key keytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert Each of the member's certificate is then imported into the other member's trust store. For the server member: keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key For the locator member: keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key For the lead member: keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key You can enable SSL encryption for peer connections by specifying the properties as the startup options for each member. In the following example, SSL encryption is enabled for communication between the members.","title":"Provider-Specific Configuration Files"},{"location":"configuring_cluster/ssl_setup/#locator-configuration-file-conflocators_2","text":"# hostname locators <ssl properties for configuring SSL for SnappyData P2P connections> localhost -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password>","title":"Locator Configuration File (conf/locators)"},{"location":"configuring_cluster/ssl_setup/#server-configuration-file-confservers_2","text":"# hostname locators <ssl properties for configuring SSL for #SnappyData P2P connections> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-serverKeyStore.keyfile> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -client-port=1528","title":"Server Configuration File (conf/servers)"},{"location":"configuring_cluster/ssl_setup/#lead-configuration-file-confleads_1","text":"# hostname locators <ssl properties for configuring SSL for #SnappyData P2P connections> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -client-port=1529 Start the SnappyData cluster using snappy-start-all.sh script and perform operations using the SnappyData shell, SnappyData job, and Smart Connector mode. You can run the SnappyData quickstart example scripts for this. Refer to SnappyData Cluster SQL Tutorial for more information.","title":"Lead Configuration File (conf/leads)"},{"location":"configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-spark-layer","text":"Spark layer SSL configuration is managed using the following SnappyData boot properties: spark.ssl.enabled spark.ssl.keyPassword spark.ssl.keyStore spark.ssl.keyStorePassword spark.ssl.trustStore spark.ssl.trustStorePassword spark.ssl.protocol The following sections provide a simple example that demonstrates the configuration and startup of SnappyData members for enabling Spark layer for Wire Encryption.","title":"Configuring SSL Encryption for Spark layer"},{"location":"configuring_cluster/ssl_setup/#requirements_2","text":"To configure SSL for Spark layer: Configure SSL keypairs and certificates as needed for each SnappyData member. Ensure that SnappyData locator, server, and lead members use the same SSL boot parameters at startup. Note For enabling Spark layer SSL encryption, you must first enable the P2P encryption for SSL.","title":"Requirements"},{"location":"configuring_cluster/ssl_setup/#provider-specific-configuration-files_2","text":"This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. On each node, create keystore files, certificates, and truststore files. Here, in this example, to create the keystore and certificate for the locator, the following was run: keytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key keytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert Similar commands were used for a server member and lead member respectively: keytool -genkey -alias mySnappyServer -keystore serverKeyStore.key keytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert keytool -genkey -alias mySnappyLead -keystore leadKeyStore.key keytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert After this, each of the member's certificate is imported into the other member's trust store. For the locator member: keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key For the server member: keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key For the lead member: keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key You can enable SSL encryption for Spark layer by specifying the properties as the startup options for server and lead members. This example demonstrates how SSL encryption is enabled for a Spark layer connection.","title":"Provider-Specific Configuration Files"},{"location":"configuring_cluster/ssl_setup/#locator-configuration-file-conflocators_3","text":"# hostname locators <ssl properties for configuring SSL for SnappyData Spark layer> localhost -ssl-enabled=true -spark.ssl.enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-locatorKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-locatorKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-locatorKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS","title":"Locator Configuration File (conf/locators)"},{"location":"configuring_cluster/ssl_setup/#server-configuration-file-confservers_3","text":"# hostname locators <ssl properties for configuring SSL for #SnappyData spark layer> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-serverKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-serverKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=<path-to-serverKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS","title":"Server Configuration File (conf/servers)"},{"location":"configuring_cluster/ssl_setup/#lead-configuration-file-confleads_2","text":"# hostname locators <ssl properties for configuring SSL for #SnappyData spark layer> localhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.keyStorePassword=<password> -J-Djavax.net.ssl.trustStore=<path-to-leadKeyStore.key file> -J-Djavax.net.ssl.trustStorePassword=<password> -spark.ssl.enabled=true -spark.ssl.keyPassword=<password> -spark.ssl.keyStore=<path-to-leadKeyStore.key file> -spark.ssl.keyStorePassword=<password> -spark.ssl.trustStore=/<path-to-leadKeyStore.key file> -spark.ssl.trustStorePassword=<password> -spark.ssl.protocol=TLS Start the SnappyData cluster using snappy-start-all.sh script and perform operations using SnappyData shell, SnappyData job, and Smart Connector mode. You can run the SnappyData quickstart example scripts for this. Refer to SnappyData Cluster SQL Tutorial for more information.","title":"Lead Configuration File (conf/leads)"},{"location":"connectors/","text":"Working with Data Sources \u00b6 SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. Any data source or database that supports Spark to load or save state can be accessed from within SnappyData. There is built-in support for many data sources as well as data formats. Built-in data sources include - Amazon S3, GCS (Google Cloud storage), Azure Blob store, file systems, HDFS, Hive metastore, RDB access using JDBC, TIBCO Data Virtualization and Pivotal GemFire. SnappyData supports the following data formats: CSV, Parquet, ORC, Avro, JSON, XML and Text. You can also deploy other third party connectors using the SQL Deploy command. Refer Deployment of Third Party Connectors . You will likely find a Spark connector for your data source via the Spark packages portal or doing a web search. START HERE - for a quick overview of the concepts and some examples for loading data Data Loading examples using Spark SQL/Data Sources API Supported Data Formats Accessing Cloud Stores Connecting to External Hive Metastores Using the SnappyData Change Data Capture (CDC) Connector Using the SnappyData JDBC Streaming Connector Connecting to TIBCO Data Virtualization (TDV)","title":"Working with Data Sources"},{"location":"connectors/#working-with-data-sources","text":"SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. Any data source or database that supports Spark to load or save state can be accessed from within SnappyData. There is built-in support for many data sources as well as data formats. Built-in data sources include - Amazon S3, GCS (Google Cloud storage), Azure Blob store, file systems, HDFS, Hive metastore, RDB access using JDBC, TIBCO Data Virtualization and Pivotal GemFire. SnappyData supports the following data formats: CSV, Parquet, ORC, Avro, JSON, XML and Text. You can also deploy other third party connectors using the SQL Deploy command. Refer Deployment of Third Party Connectors . You will likely find a Spark connector for your data source via the Spark packages portal or doing a web search. START HERE - for a quick overview of the concepts and some examples for loading data Data Loading examples using Spark SQL/Data Sources API Supported Data Formats Accessing Cloud Stores Connecting to External Hive Metastores Using the SnappyData Change Data Capture (CDC) Connector Using the SnappyData JDBC Streaming Connector Connecting to TIBCO Data Virtualization (TDV)","title":"Working with Data Sources"},{"location":"connectors/access_cloud_data/","text":"Accessing Cloud Data \u00b6 AWS \u00b6 To access the data on AWS cloud storage, you must set the credentials using any of the following methods: Using hive-site.xml File Setting the Access or Secret Key Properties Setting the Access or Secret Key Properties through Environment Variable Using hive-site.xml File \u00b6 Add the following key properties into conf > hive-site.xml file: <property> <name> fs.s3a.access.key </name> <value> Amazon S3 Access Key </value> </property> <property> <name> fs.s3a.secret.key </name> <value> Amazon S3 Secret Key </value> </property> Setting the Access or Secret Key Properties \u00b6 Add the following properties in conf / leads file: -spark.hadoop.fs.s3a.access.key=<Amazon S3 Access Key> -spark.hadoop.fs.s3a.secret.key=<Amazon S3 Secret Key> Create an external table using the following command: s3a:// / command. Setting the Access or Secret Key Properties through Environment Variable \u00b6 Set credentials as environment variables: export AWS_ACCESS_KEY_ID = <Amazon S3 Access Key> export AWS_SECRET_ACCESS_KEY = <Amazon S3 Secret Key> Accessing Data from AWS Cloud Storage \u00b6 Create an external table using the following command: create external table staging_parquet using parquet options ( path 's3a://<bucketName>/<folderName>' ); create table parquet_test using column as select * from staging_parquet ; Unsetting the Access or Secret Key Properties \u00b6 You can run the org.apache.hadoop.fs.FileSystem.closeAll() command on the snappy-scala shell or in the job. This clears the cache. Ensure that there are no queries running on the cluster when you are executing this property. After this you can set the new credentials. Azure Blob \u00b6 To access the data on Azure Blob storage, you must set the credentials using any of the following methods: Setting Credentials through hive-site.xml Setting Credentials through Spark Property Setting Credentials through hive-site.xml \u00b6 Set the following property in hive-site.xml <property> <name> fs.azure.account.key.youraccount.blob.core.windows.net </name> <value> YOUR ACCESS KEY </value> </property> Setting Credentials through Spark Property \u00b6 sc . hadoopConfiguration . set ( \"fs.azure.account.key.<your-storage_account_name>.dfs.core.windows.net\" , \"<YOUR ACCESS KEY>\" ) Accessing Data from Azure Unsecured BLOB Storage \u00b6 CREATE EXTERNAL TABLE testADLS1 USING PARQUET Options ( path 'wasb://container_name@storage_account_name.blob.core.windows.net/dir/file' ) Accessing Data from Azure Secured BLOB Storage \u00b6 CREATE EXTERNAL TABLE testADLS1 USING PARQUET Options ( path 'wasbs://container_name@storage_account_name.blob.core.windows.net/dir/file' ) GCS \u00b6 To access the data on GCS cloud storage, you must set the credentials using any of the following methods: Setting Credentials through hive-site.xml Setting Credentials through Spark Property Setting Credentials in hive-site.xml \u00b6 <property> <name> google.cloud.auth.service.account.json.keyfile </name> <value> /path/to/keyfile </value> </property> Setting Credentials through Spark property on Shell \u00b6 sc . hadoopConfiguration . set ( \"google.cloud.auth.service.account.json.keyfile\" , \"`<json file path>`\" ) Accessing Data from GCS Cloud Storage \u00b6 CREATE EXTERNAL TABLE airline_ext USING parquet OPTIONS ( path 'gs://bucket_name/object_name' ) EMR HDFS \u00b6 You can use the following command to access data from EMR HDFS location for a cluster that is running on ec2: create external table categories using csv options ( path 'hdfs://<masternode IP address>/<file_path>' ); Azure Data Lake Storage (ADLS) \u00b6 SnappyData supports ADLS gen 2 out of the box. Refer to Hadoop Azure Documentation for more details. The abfs: URLs mentioned in the documentation can be used directly in SnappyData as file paths to store or load data.","title":"Accessing Cloud Storages"},{"location":"connectors/access_cloud_data/#accessing-cloud-data","text":"","title":"Accessing Cloud Data"},{"location":"connectors/access_cloud_data/#aws","text":"To access the data on AWS cloud storage, you must set the credentials using any of the following methods: Using hive-site.xml File Setting the Access or Secret Key Properties Setting the Access or Secret Key Properties through Environment Variable","title":"AWS"},{"location":"connectors/access_cloud_data/#using-hive-sitexml-file","text":"Add the following key properties into conf > hive-site.xml file: <property> <name> fs.s3a.access.key </name> <value> Amazon S3 Access Key </value> </property> <property> <name> fs.s3a.secret.key </name> <value> Amazon S3 Secret Key </value> </property>","title":"Using hive-site.xml File"},{"location":"connectors/access_cloud_data/#setting-the-access-or-secret-key-properties","text":"Add the following properties in conf / leads file: -spark.hadoop.fs.s3a.access.key=<Amazon S3 Access Key> -spark.hadoop.fs.s3a.secret.key=<Amazon S3 Secret Key> Create an external table using the following command: s3a:// / command.","title":"Setting the Access or Secret Key Properties"},{"location":"connectors/access_cloud_data/#setting-the-access-or-secret-key-properties-through-environment-variable","text":"Set credentials as environment variables: export AWS_ACCESS_KEY_ID = <Amazon S3 Access Key> export AWS_SECRET_ACCESS_KEY = <Amazon S3 Secret Key>","title":"Setting the Access or Secret Key Properties through Environment Variable"},{"location":"connectors/access_cloud_data/#accessing-data-from-aws-cloud-storage","text":"Create an external table using the following command: create external table staging_parquet using parquet options ( path 's3a://<bucketName>/<folderName>' ); create table parquet_test using column as select * from staging_parquet ;","title":"Accessing Data from AWS Cloud Storage"},{"location":"connectors/access_cloud_data/#unsetting-the-access-or-secret-key-properties","text":"You can run the org.apache.hadoop.fs.FileSystem.closeAll() command on the snappy-scala shell or in the job. This clears the cache. Ensure that there are no queries running on the cluster when you are executing this property. After this you can set the new credentials.","title":"Unsetting the Access or Secret Key Properties"},{"location":"connectors/access_cloud_data/#azure-blob","text":"To access the data on Azure Blob storage, you must set the credentials using any of the following methods: Setting Credentials through hive-site.xml Setting Credentials through Spark Property","title":"Azure Blob"},{"location":"connectors/access_cloud_data/#setting-credentials-through-hive-sitexml","text":"Set the following property in hive-site.xml <property> <name> fs.azure.account.key.youraccount.blob.core.windows.net </name> <value> YOUR ACCESS KEY </value> </property>","title":"Setting Credentials through hive-site.xml"},{"location":"connectors/access_cloud_data/#setting-credentials-through-spark-property","text":"sc . hadoopConfiguration . set ( \"fs.azure.account.key.<your-storage_account_name>.dfs.core.windows.net\" , \"<YOUR ACCESS KEY>\" )","title":"Setting Credentials through Spark Property"},{"location":"connectors/access_cloud_data/#accessing-data-from-azure-unsecured-blob-storage","text":"CREATE EXTERNAL TABLE testADLS1 USING PARQUET Options ( path 'wasb://container_name@storage_account_name.blob.core.windows.net/dir/file' )","title":"Accessing Data from Azure Unsecured BLOB Storage"},{"location":"connectors/access_cloud_data/#accessing-data-from-azure-secured-blob-storage","text":"CREATE EXTERNAL TABLE testADLS1 USING PARQUET Options ( path 'wasbs://container_name@storage_account_name.blob.core.windows.net/dir/file' )","title":"Accessing Data from Azure Secured BLOB Storage"},{"location":"connectors/access_cloud_data/#gcs","text":"To access the data on GCS cloud storage, you must set the credentials using any of the following methods: Setting Credentials through hive-site.xml Setting Credentials through Spark Property","title":"GCS"},{"location":"connectors/access_cloud_data/#setting-credentials-in-hive-sitexml","text":"<property> <name> google.cloud.auth.service.account.json.keyfile </name> <value> /path/to/keyfile </value> </property>","title":"Setting Credentials in hive-site.xml"},{"location":"connectors/access_cloud_data/#setting-credentials-through-spark-property-on-shell","text":"sc . hadoopConfiguration . set ( \"google.cloud.auth.service.account.json.keyfile\" , \"`<json file path>`\" )","title":"Setting Credentials through Spark property on Shell"},{"location":"connectors/access_cloud_data/#accessing-data-from-gcs-cloud-storage","text":"CREATE EXTERNAL TABLE airline_ext USING parquet OPTIONS ( path 'gs://bucket_name/object_name' )","title":"Accessing Data from GCS Cloud Storage"},{"location":"connectors/access_cloud_data/#emr-hdfs","text":"You can use the following command to access data from EMR HDFS location for a cluster that is running on ec2: create external table categories using csv options ( path 'hdfs://<masternode IP address>/<file_path>' );","title":"EMR HDFS"},{"location":"connectors/access_cloud_data/#azure-data-lake-storage-adls","text":"SnappyData supports ADLS gen 2 out of the box. Refer to Hadoop Azure Documentation for more details. The abfs: URLs mentioned in the documentation can be used directly in SnappyData as file paths to store or load data.","title":"Azure Data Lake Storage (ADLS)"},{"location":"connectors/cdc_connector/","text":"Using the SnappyData Change Data Capture (CDC) Connector \u00b6 As data keeps growing rapidly techniques like Change Data Capture (CDC) is crucial for handling and processing the data inflow. The CDC technology is used in a database to track changed data so that the identified changes can be used to keep target systems in sync with the changes made to the source systems. A CDC enabled system (SQL database) automatically captures changes from the source table, these changes are then updated on the target system (SnappyData tables). It provides an efficient framework which allows users to capture individual data changes like insert, update, and delete in the source tables (instead of dealing with the entire data), and apply them to the SnappyData tables to keep both the source and target tables in sync. Info Spark structured streaming and SnappyData mutable APIs are used to keep the source and target tables in sync. For writing a Spark structured streaming application, refer to the Spark documentation. CDC is supported on both the Smart Connector Mode and the Embedded mode. For more information on the modes, refer to the documentation on Smart Connector Mode and Embedded SnappyData Store Mode . In this topic, we explain how SnappyData uses the JDBC streaming connector to pull changed data from the SQL database and ingest it into SnappyData tables. The following image illustrates the data flow for change data capture: Prerequisites \u00b6 Ensure that change data capture is enabled for the database and table. Refer to your database documentation for more information on enabling CDC. A user account with the required roles and privileges to the database. Ensure that a JDBC source to which SnappyData CDC Connector can connect is running and available from the node where CDC connector is running. The snappydata-jdbc-stream-connector_ .jar , which is available in the $SNAPPY_HOME/jars directory. If you are using Maven or Gradle project to develop the streaming application, you need to publish the above jar into a local maven repository. Understanding the Program Structure \u00b6 The RDB CDC connector ingests data into SnappyData from any CDC enabled JDBC source. We have a custom source with alias \u201cjdbcStream\u201d and a custom Sink with alias \u201csnappystore\u201d. Source has the capability to read from a JDBC source and Sink can perform inserts, updates or deletes based on CDC operations. Configuring the Stream Reader \u00b6 Structured streaming and Spark\u2019s JDBC source is used to read from the source database system. To enable this, set a stream referring to the source table. For example: DataStreamReader reader = snappySession.readStream() .format(\"jdbcStream\") .option(\"spec\", \"org.apache.spark.sql.streaming.jdbc.SqlServerSpec\") .option(\"sourceTableName\", sourceTable) .option(\"maxEvents\", \"50000\") .options(connectionOptions); The JDBC Stream Reader options are: jdbcStream : The format in which the source data is received from the JDBC source. This parameter is mandatory. Currently, only JDBC stream is supported. spec : A CDC spec class name which is used to query offsets from different databases. We have a default specification for CDC enabled SQL server. You can extend the org.apache.spark.sql.streaming.jdbc.SourceSpec trait to provide any other implementation. trait SourceSpec { /** A fixed offset column name in the source table. */ def offsetColumn: String /** An optional function to convert offset * to string format, if offset is not in string format. */ def offsetToStrFunc: Option[String] /** An optional function to convert string offset to native offset format. */ def strToOffsetFunc: Option[String] /** Query to find the next offset */ def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String } The default implementation of SQLServerSpec can be: class SqlServerSpec extends SourceSpec { override def offsetColumn: String = \"__$start_lsn\" override def offsetToStrFunc: Option[String] = Some(\"master.dbo.fn_varbintohexstr\") override def strToOffsetFunc: Option[String] = Some(\"master.dbo.fn_cdc_hexstrtobin\") /** make sure to convert the LSN to string and give a column alias for the * user defined query one is supplying. Alternatively, a procedure can be invoked * with $table, $currentOffset Snappy provided variables returning one string column * representing the next LSN string. */ override def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String = s\"\"\"select master.dbo.fn_varbintohexstr(max(${offsetColumn})) nextLSN from (select ${offsetColumn}, sum(count(1)) over (order by ${offsetColumn}) runningCount from $tableName where ${offsetColumn} > master.dbo.fn_cdc_hexstrtobin('$currentOffset') group by ${offsetColumn}) x where runningCount <= $maxEvents\"\"\" } sourceTableName : The source table from which the data is read. For example, testdatabase.cdc.dbo_customer_CT . maxEvents : Number of rows to be read in one batch. connectionOptions : A map of key values containing different parameters to access the source database. Key Value driver The JDBC driver to be used to access source database. E.g. com.microsoft.sqlserver.jdbc.SQLServerDriver url The JDBC connection url user User name to access the database password Password to access the database databaseName The name of the database poolImpl Connection pool implementation(default Tomcat). Currently, only Tomcat and Hikari are supported poolProperties Comma separated pool specific properties. For more details see Tomcat or Hikari JDBC connection pool documents. Example: connectionTestQuery=select 1,minimumIdle=5 Optionally, you can use any Spark API to transform your data obtained from the stream reader. The sample usage can be as follows: Dataset<Row> ds = reader.load(); ds.filter(<filter_condition>) Writing into SnappyData tables \u00b6 To write into SnappyData tables you need to have a StreamWriter as follows: ds.writeStream() .trigger(ProcessingTime.create(10, TimeUnit.SECONDS)) .format(\"snappystore\") .option(\"sink\", ProcessEvents.class.getName()) .option(\"tableName\", tableName) .start(); Here, the value of the .format parameter is always snappystore and is a mandatory. SnappyData Stream Writer options \u00b6 The sink option is mandatory for SnappyStore sink. This option is required to give the user control of the obtained data frame. When writing streaming data to the tables, you can also provide any custom option. Key Value sink A user-defined callback class which gets a data frame in each batch. The class must implement org.apache.spark.sql.streaming.jdbc.SnappyStreamSink interface. org.apache.spark.sql.streaming.jdbc.SnappyStreamSink The above trait contains a single method, which user needs to implement. A user can use SnappyData mutable APIs (INSERT, UPDATE, DELETE, PUT INTO) to maintain tables. def process(snappySession: SnappySession, sinkProps: Map[String, String], batchId: Long, df: Dataset[Row]): Unit The following examples illustrates how you can write into a SnappyData table : package io.snappydata.app; import java.util.List; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SnappySession; import org.apache.spark.sql.streaming.jdbc.SnappyStreamSink; import static java.util.Arrays.asList; import static org.apache.spark.SnappyJavaUtils.snappyJavaUtil; public class ProcessEvents implements SnappyStreamSink { private static Logger log = LoggerFactory.getLogger(ProcessEvents.class.getName()); private static List<String> metaColumns = asList(\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$operation\", \"__$update_mask\", \"__$command_id\"); @Override public void process(SnappySession snappySession, scala.collection.immutable.Map<String, String> sinkProps, long batchId, Dataset<Row> df) { String snappyTable = sinkProps.apply(\"tablename\").toUpperCase(); log.info(\"SB: Processing for \" + snappyTable + \" batchId \" + batchId); df.cache(); Dataset<Row> snappyCustomerDelete = df // pick only delete ops .filter(\"\\\"__$operation\\\" = 1\") // exclude the first 5 columns and pick the columns that needs to control // the WHERE clause of the delete operation. .drop(metaColumns.toArray(new String[metaColumns.size()])); if (snappyCustomerDelete.count() > 0) { snappyJavaUtil(snappyCustomerDelete.write()).deleteFrom(\"APP.\" + snappyTable); } Dataset<Row> snappyCustomerUpsert = df // pick only insert/update ops .filter(\"\\\"__$operation\\\" = 4 OR \\\"__$operation\\\" = 2\") .drop(metaColumns.toArray(new String[metaColumns.size()])); snappyJavaUtil(snappyCustomerUpsert.write()).putInto(\"APP.\" + snappyTable); } } Additional Information \u00b6 Offset Management : SnappyData keeps a persistent table for offset management (Offset table name for application = SNAPPY_JDBC_STREAM_OFFSET_TABLE). The schema of the table is: APP_NAME TABLE_NAME LAST_OFFSET Initially, the table contains no rows. The connector inserts a row for the table with minimum offset queried from the database. On subsequent intervals, it consults this table to get the offset number. If connector application crashes, it refers to this table on a restart to query further. Idempotency for Streaming application : If DELETE and PUT INTO APIs are used, SnappyData ensures idempotent behavior. This is useful when the application restarts after a crash and some of the CDC events are replayed. The PUT INTO API either inserts a record (if not present) or updates the record (if already exists). The existence of the record is checked based on the key columns defined when a table is created. As primary keys are not supported for column tables, you can use key_columns instead, to uniquely identify each row/record in a database table. For more information, see CREATE TABLE . For example: CREATE TABLE <table_name> USING column OPTIONS(partition_by '<column_name>', buckets '<num_partitions>',key_columns '<primary_key>') AS (SELECT * FROM from external table); Note Writing data from different tables to a single table is currently not supported as the schema for incoming data frame cannot be changed. For every source table that needs to be tracked for changes, ensure that there is a corresponding destination table in SnappyData.","title":"Using the SnappyData CDC (Change Data Capture) Connector"},{"location":"connectors/cdc_connector/#using-the-snappydata-change-data-capture-cdc-connector","text":"As data keeps growing rapidly techniques like Change Data Capture (CDC) is crucial for handling and processing the data inflow. The CDC technology is used in a database to track changed data so that the identified changes can be used to keep target systems in sync with the changes made to the source systems. A CDC enabled system (SQL database) automatically captures changes from the source table, these changes are then updated on the target system (SnappyData tables). It provides an efficient framework which allows users to capture individual data changes like insert, update, and delete in the source tables (instead of dealing with the entire data), and apply them to the SnappyData tables to keep both the source and target tables in sync. Info Spark structured streaming and SnappyData mutable APIs are used to keep the source and target tables in sync. For writing a Spark structured streaming application, refer to the Spark documentation. CDC is supported on both the Smart Connector Mode and the Embedded mode. For more information on the modes, refer to the documentation on Smart Connector Mode and Embedded SnappyData Store Mode . In this topic, we explain how SnappyData uses the JDBC streaming connector to pull changed data from the SQL database and ingest it into SnappyData tables. The following image illustrates the data flow for change data capture:","title":"Using the SnappyData Change Data Capture (CDC) Connector"},{"location":"connectors/cdc_connector/#prerequisites","text":"Ensure that change data capture is enabled for the database and table. Refer to your database documentation for more information on enabling CDC. A user account with the required roles and privileges to the database. Ensure that a JDBC source to which SnappyData CDC Connector can connect is running and available from the node where CDC connector is running. The snappydata-jdbc-stream-connector_ .jar , which is available in the $SNAPPY_HOME/jars directory. If you are using Maven or Gradle project to develop the streaming application, you need to publish the above jar into a local maven repository.","title":"Prerequisites"},{"location":"connectors/cdc_connector/#understanding-the-program-structure","text":"The RDB CDC connector ingests data into SnappyData from any CDC enabled JDBC source. We have a custom source with alias \u201cjdbcStream\u201d and a custom Sink with alias \u201csnappystore\u201d. Source has the capability to read from a JDBC source and Sink can perform inserts, updates or deletes based on CDC operations.","title":"Understanding the Program Structure"},{"location":"connectors/cdc_connector/#configuring-the-stream-reader","text":"Structured streaming and Spark\u2019s JDBC source is used to read from the source database system. To enable this, set a stream referring to the source table. For example: DataStreamReader reader = snappySession.readStream() .format(\"jdbcStream\") .option(\"spec\", \"org.apache.spark.sql.streaming.jdbc.SqlServerSpec\") .option(\"sourceTableName\", sourceTable) .option(\"maxEvents\", \"50000\") .options(connectionOptions); The JDBC Stream Reader options are: jdbcStream : The format in which the source data is received from the JDBC source. This parameter is mandatory. Currently, only JDBC stream is supported. spec : A CDC spec class name which is used to query offsets from different databases. We have a default specification for CDC enabled SQL server. You can extend the org.apache.spark.sql.streaming.jdbc.SourceSpec trait to provide any other implementation. trait SourceSpec { /** A fixed offset column name in the source table. */ def offsetColumn: String /** An optional function to convert offset * to string format, if offset is not in string format. */ def offsetToStrFunc: Option[String] /** An optional function to convert string offset to native offset format. */ def strToOffsetFunc: Option[String] /** Query to find the next offset */ def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String } The default implementation of SQLServerSpec can be: class SqlServerSpec extends SourceSpec { override def offsetColumn: String = \"__$start_lsn\" override def offsetToStrFunc: Option[String] = Some(\"master.dbo.fn_varbintohexstr\") override def strToOffsetFunc: Option[String] = Some(\"master.dbo.fn_cdc_hexstrtobin\") /** make sure to convert the LSN to string and give a column alias for the * user defined query one is supplying. Alternatively, a procedure can be invoked * with $table, $currentOffset Snappy provided variables returning one string column * representing the next LSN string. */ override def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String = s\"\"\"select master.dbo.fn_varbintohexstr(max(${offsetColumn})) nextLSN from (select ${offsetColumn}, sum(count(1)) over (order by ${offsetColumn}) runningCount from $tableName where ${offsetColumn} > master.dbo.fn_cdc_hexstrtobin('$currentOffset') group by ${offsetColumn}) x where runningCount <= $maxEvents\"\"\" } sourceTableName : The source table from which the data is read. For example, testdatabase.cdc.dbo_customer_CT . maxEvents : Number of rows to be read in one batch. connectionOptions : A map of key values containing different parameters to access the source database. Key Value driver The JDBC driver to be used to access source database. E.g. com.microsoft.sqlserver.jdbc.SQLServerDriver url The JDBC connection url user User name to access the database password Password to access the database databaseName The name of the database poolImpl Connection pool implementation(default Tomcat). Currently, only Tomcat and Hikari are supported poolProperties Comma separated pool specific properties. For more details see Tomcat or Hikari JDBC connection pool documents. Example: connectionTestQuery=select 1,minimumIdle=5 Optionally, you can use any Spark API to transform your data obtained from the stream reader. The sample usage can be as follows: Dataset<Row> ds = reader.load(); ds.filter(<filter_condition>)","title":"Configuring the Stream Reader"},{"location":"connectors/cdc_connector/#writing-into-snappydata-tables","text":"To write into SnappyData tables you need to have a StreamWriter as follows: ds.writeStream() .trigger(ProcessingTime.create(10, TimeUnit.SECONDS)) .format(\"snappystore\") .option(\"sink\", ProcessEvents.class.getName()) .option(\"tableName\", tableName) .start(); Here, the value of the .format parameter is always snappystore and is a mandatory.","title":"Writing into SnappyData tables"},{"location":"connectors/cdc_connector/#snappydata-stream-writer-options","text":"The sink option is mandatory for SnappyStore sink. This option is required to give the user control of the obtained data frame. When writing streaming data to the tables, you can also provide any custom option. Key Value sink A user-defined callback class which gets a data frame in each batch. The class must implement org.apache.spark.sql.streaming.jdbc.SnappyStreamSink interface. org.apache.spark.sql.streaming.jdbc.SnappyStreamSink The above trait contains a single method, which user needs to implement. A user can use SnappyData mutable APIs (INSERT, UPDATE, DELETE, PUT INTO) to maintain tables. def process(snappySession: SnappySession, sinkProps: Map[String, String], batchId: Long, df: Dataset[Row]): Unit The following examples illustrates how you can write into a SnappyData table : package io.snappydata.app; import java.util.List; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SnappySession; import org.apache.spark.sql.streaming.jdbc.SnappyStreamSink; import static java.util.Arrays.asList; import static org.apache.spark.SnappyJavaUtils.snappyJavaUtil; public class ProcessEvents implements SnappyStreamSink { private static Logger log = LoggerFactory.getLogger(ProcessEvents.class.getName()); private static List<String> metaColumns = asList(\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$operation\", \"__$update_mask\", \"__$command_id\"); @Override public void process(SnappySession snappySession, scala.collection.immutable.Map<String, String> sinkProps, long batchId, Dataset<Row> df) { String snappyTable = sinkProps.apply(\"tablename\").toUpperCase(); log.info(\"SB: Processing for \" + snappyTable + \" batchId \" + batchId); df.cache(); Dataset<Row> snappyCustomerDelete = df // pick only delete ops .filter(\"\\\"__$operation\\\" = 1\") // exclude the first 5 columns and pick the columns that needs to control // the WHERE clause of the delete operation. .drop(metaColumns.toArray(new String[metaColumns.size()])); if (snappyCustomerDelete.count() > 0) { snappyJavaUtil(snappyCustomerDelete.write()).deleteFrom(\"APP.\" + snappyTable); } Dataset<Row> snappyCustomerUpsert = df // pick only insert/update ops .filter(\"\\\"__$operation\\\" = 4 OR \\\"__$operation\\\" = 2\") .drop(metaColumns.toArray(new String[metaColumns.size()])); snappyJavaUtil(snappyCustomerUpsert.write()).putInto(\"APP.\" + snappyTable); } }","title":"SnappyData Stream Writer options"},{"location":"connectors/cdc_connector/#additional-information","text":"Offset Management : SnappyData keeps a persistent table for offset management (Offset table name for application = SNAPPY_JDBC_STREAM_OFFSET_TABLE). The schema of the table is: APP_NAME TABLE_NAME LAST_OFFSET Initially, the table contains no rows. The connector inserts a row for the table with minimum offset queried from the database. On subsequent intervals, it consults this table to get the offset number. If connector application crashes, it refers to this table on a restart to query further. Idempotency for Streaming application : If DELETE and PUT INTO APIs are used, SnappyData ensures idempotent behavior. This is useful when the application restarts after a crash and some of the CDC events are replayed. The PUT INTO API either inserts a record (if not present) or updates the record (if already exists). The existence of the record is checked based on the key columns defined when a table is created. As primary keys are not supported for column tables, you can use key_columns instead, to uniquely identify each row/record in a database table. For more information, see CREATE TABLE . For example: CREATE TABLE <table_name> USING column OPTIONS(partition_by '<column_name>', buckets '<num_partitions>',key_columns '<primary_key>') AS (SELECT * FROM from external table); Note Writing data from different tables to a single table is currently not supported as the schema for incoming data frame cannot be changed. For every source table that needs to be tracked for changes, ensure that there is a corresponding destination table in SnappyData.","title":"Additional Information"},{"location":"connectors/deployment_dependency_jar/","text":"Deploying Third Party Connectors \u00b6 A job submitted to SnappyData, the creation of external tables as SQL or through API, the creation of a user-defined function etc. may have an external jar and package dependencies. For example, you may want to create an external table in SnappyData which points to a Cassandra table. For that, you would need the Spark Cassandra connector classes which are available in maven central repository. Today, Spark connectors are available to virtually all modern data stores - RDBs, NoSQL, cloud databases like Redshift, Snowflake, S3, etc. Most of these connectors are available as mvn or spark packages or as published jars by the respective vendors. SnappyData\u2019s compatibility with Spark allows SnappyData to work with the same connectors of all the popular data sources. SnappyData\u2019s deploy command allows you to deploy these packages by using its maven coordinates. When it is not available, you can simply upload jars into any provisioned SnappyData cluster. SnappyData offers the following SQL commands: deploy package - to deploy maven packages deploy jar - to deploy your application or library Jars Besides these SQL extensions, support is provided in SnappyData 1.1.1 version to deploy packages as part of SnappyData Job submission. This is similar to Spark\u2019s support for --packages when submitting Spark jobs. The following sections are included in this topic: Deploying Packages in SnappyData Deploying Jars in SnappyData Viewing the Deployed Jars and Packages Removing Deployed Jars Submitting SnappyData Job with Packages Deploying Packages in SnappyData \u00b6 Packages can be deployed in SnappyData using the DEPLOY PACKAGE SQL. Execute the deploy package command to deploy packages. You can pass the following through this SQL: Name of the package. Repository where the package is located. Path to a local cache of jars. Note SnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise, the resolution of the package fails. For resolving the package, Maven Central and Spark packages, located at http://dl.bintray.com/spark-packages , are searched by default. Hence, you must specify the repository only if the package is not there at Maven Central or in the spark-package repository. Tip Use spark-packages.org to search for Spark packages. Most of the popular Spark packages are listed here. SQL Syntax to Deploy Package Dependencies in SnappyData \u00b6 deploy package <unique-alias-name> \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ] * unique-alias-name - A name to identify a package. This name can be used to remove the package from the cluster. You can use alphabets, numbers, and underscores to create the name. packages - Comma-delimited string of maven packages. repos - Comma-delimited string of remote repositories other than the Maven Central and spark-package repositories. These two repositories are searched by default. The format specified by Maven is: groupId:artifactId:version . path - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the ivy.home property. if even that is not specified, the .ivy2 in the user\u2019s home directory is used. Example \u00b6 Deploy packages from a default repository: deploy package deeplearning 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work' ; deploy package Sparkredshift 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work' ; Deploying Jars in SnappyData \u00b6 SnappyData provides a method to deploy a jar in a running system through DEPLOY JAR SQL. You can execute the deploy jar command to deploy dependency jars. Syntax for Deploying Jars in SnappyData \u00b6 deploy jar <unique-alias-name> 'jars' * unique-alias-name - A name to identify the jar. This name can be used to remove the jar from the cluster. You can use alphabets, numbers and underscores to create the name. jars - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData. Example \u00b6 Deploying jars: deploy jar SparkDaria 'spark-daria_2.11.8-2.2.0_0.10.0.jar' All the deployed commands are stored in the SnappyData cluster. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system. Viewing the Deployed Jars and Packages \u00b6 You can view all the packages and jars deployed in the system by using the list packages command. Syntax for Listing Deployed Jars and Packages \u00b6 snappy> list packages; Or snappy> list jars; Both of the above commands will list all the packages as well the jars that are installed in the system. Hence, you can use either one of those commands. Sample Output of Listing Jars/Packages \u00b6 snappy> list jars; alias |coordinate |isPackage ------------------------------------------------------------------------- CASSCONN |datastax:spark-cassandra-connector:2.0.1-s_2.11|true MYJAR |b.jar |false Removing Deployed Jars \u00b6 You can remove the deployed jars with the undeploy command. This command removes the jars that are directly installed and the jars that are associated with a package, from the system. Syntax for Removing Deployed Jars \u00b6 undeploy <unique-alias-name>; Note The removal is only captured when you use the undeploy command, the jars are removed only when you restart the cluster. Example \u00b6 undeploy spark_deep_learning_0_3_0; Attention If during restart, for any reason the deployed jars and packages are not reinstalled automatically by the system, a warning is shown in the log file. If you want to fail the restart, then you need to set a system property in the conf file to stop restarts completely. The name of the system property is FAIL_ON_JAR_UNAVAILABILITY . If you want to use external repositories, ensure to maintain internet connectivity at least on the lead nodes. It is highly recommended to use a local cache path to store the downloaded jars of a package, because the next time the same deploy is executed, it can be picked from the local path. Ensure that this path is available on the lead nodes. Similarly keep the standalone jars also in a path where it is available to all the lead nodes. Submitting SnappyData Job with Packages \u00b6 You can specify the name of the packages which can be used by a job that is submitted to SnappyData. This package is visible only to the job submitted with this argument. If another job tries to access a class belonging to the jar of this package then it will get ClassNotFoundException. A --packages option is added to the snappy-job.sh script where you can specify the packages. Use the following syntax: $./snappy-job.sh submit --app-name <app-name> --class <job-class> [--lead <hostname:port>] [--app-jar <jar-path>] [other existing options] [--packages <comma separated package-coordinates> ] [--repos <comma separated mvn repositories] [--jarcache <path where resolved jars will be kept>] Example of SnappyData Job Submission with Packages \u00b6 ./bin/snappy-job.sh submit --app-name cassconn --class <SnappyJobClassName> --app-jar <app.jar> --lead localhost:8090 --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.7 If you want global deployment, you can instead use the deploy command SQL and then the packages become visible everywhere. snappy> deploy package cassconn 'datastax:spark-cassandra-connector:2.0.1-s_2.11' path '/home/alex/mycache';","title":"Deploying Third Party Connectors"},{"location":"connectors/deployment_dependency_jar/#deploying-third-party-connectors","text":"A job submitted to SnappyData, the creation of external tables as SQL or through API, the creation of a user-defined function etc. may have an external jar and package dependencies. For example, you may want to create an external table in SnappyData which points to a Cassandra table. For that, you would need the Spark Cassandra connector classes which are available in maven central repository. Today, Spark connectors are available to virtually all modern data stores - RDBs, NoSQL, cloud databases like Redshift, Snowflake, S3, etc. Most of these connectors are available as mvn or spark packages or as published jars by the respective vendors. SnappyData\u2019s compatibility with Spark allows SnappyData to work with the same connectors of all the popular data sources. SnappyData\u2019s deploy command allows you to deploy these packages by using its maven coordinates. When it is not available, you can simply upload jars into any provisioned SnappyData cluster. SnappyData offers the following SQL commands: deploy package - to deploy maven packages deploy jar - to deploy your application or library Jars Besides these SQL extensions, support is provided in SnappyData 1.1.1 version to deploy packages as part of SnappyData Job submission. This is similar to Spark\u2019s support for --packages when submitting Spark jobs. The following sections are included in this topic: Deploying Packages in SnappyData Deploying Jars in SnappyData Viewing the Deployed Jars and Packages Removing Deployed Jars Submitting SnappyData Job with Packages","title":"Deploying Third Party Connectors"},{"location":"connectors/deployment_dependency_jar/#deploying-packages-in-snappydata","text":"Packages can be deployed in SnappyData using the DEPLOY PACKAGE SQL. Execute the deploy package command to deploy packages. You can pass the following through this SQL: Name of the package. Repository where the package is located. Path to a local cache of jars. Note SnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise, the resolution of the package fails. For resolving the package, Maven Central and Spark packages, located at http://dl.bintray.com/spark-packages , are searched by default. Hence, you must specify the repository only if the package is not there at Maven Central or in the spark-package repository. Tip Use spark-packages.org to search for Spark packages. Most of the popular Spark packages are listed here.","title":"Deploying Packages in SnappyData"},{"location":"connectors/deployment_dependency_jar/#sql-syntax-to-deploy-package-dependencies-in-snappydata","text":"deploy package <unique-alias-name> \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ] * unique-alias-name - A name to identify a package. This name can be used to remove the package from the cluster. You can use alphabets, numbers, and underscores to create the name. packages - Comma-delimited string of maven packages. repos - Comma-delimited string of remote repositories other than the Maven Central and spark-package repositories. These two repositories are searched by default. The format specified by Maven is: groupId:artifactId:version . path - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the ivy.home property. if even that is not specified, the .ivy2 in the user\u2019s home directory is used.","title":"SQL Syntax to Deploy Package Dependencies in SnappyData"},{"location":"connectors/deployment_dependency_jar/#example","text":"Deploy packages from a default repository: deploy package deeplearning 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work' ; deploy package Sparkredshift 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work' ;","title":"Example"},{"location":"connectors/deployment_dependency_jar/#deploying-jars-in-snappydata","text":"SnappyData provides a method to deploy a jar in a running system through DEPLOY JAR SQL. You can execute the deploy jar command to deploy dependency jars.","title":"Deploying Jars in SnappyData"},{"location":"connectors/deployment_dependency_jar/#syntax-for-deploying-jars-in-snappydata","text":"deploy jar <unique-alias-name> 'jars' * unique-alias-name - A name to identify the jar. This name can be used to remove the jar from the cluster. You can use alphabets, numbers and underscores to create the name. jars - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData.","title":"Syntax for Deploying Jars in SnappyData"},{"location":"connectors/deployment_dependency_jar/#example_1","text":"Deploying jars: deploy jar SparkDaria 'spark-daria_2.11.8-2.2.0_0.10.0.jar' All the deployed commands are stored in the SnappyData cluster. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system.","title":"Example"},{"location":"connectors/deployment_dependency_jar/#viewing-the-deployed-jars-and-packages","text":"You can view all the packages and jars deployed in the system by using the list packages command.","title":"Viewing the Deployed Jars and Packages"},{"location":"connectors/deployment_dependency_jar/#syntax-for-listing-deployed-jars-and-packages","text":"snappy> list packages; Or snappy> list jars; Both of the above commands will list all the packages as well the jars that are installed in the system. Hence, you can use either one of those commands.","title":"Syntax for Listing Deployed Jars and Packages"},{"location":"connectors/deployment_dependency_jar/#sample-output-of-listing-jarspackages","text":"snappy> list jars; alias |coordinate |isPackage ------------------------------------------------------------------------- CASSCONN |datastax:spark-cassandra-connector:2.0.1-s_2.11|true MYJAR |b.jar |false","title":"Sample Output of Listing Jars/Packages"},{"location":"connectors/deployment_dependency_jar/#removing-deployed-jars","text":"You can remove the deployed jars with the undeploy command. This command removes the jars that are directly installed and the jars that are associated with a package, from the system.","title":"Removing Deployed Jars"},{"location":"connectors/deployment_dependency_jar/#syntax-for-removing-deployed-jars","text":"undeploy <unique-alias-name>; Note The removal is only captured when you use the undeploy command, the jars are removed only when you restart the cluster.","title":"Syntax for Removing Deployed Jars"},{"location":"connectors/deployment_dependency_jar/#example_2","text":"undeploy spark_deep_learning_0_3_0; Attention If during restart, for any reason the deployed jars and packages are not reinstalled automatically by the system, a warning is shown in the log file. If you want to fail the restart, then you need to set a system property in the conf file to stop restarts completely. The name of the system property is FAIL_ON_JAR_UNAVAILABILITY . If you want to use external repositories, ensure to maintain internet connectivity at least on the lead nodes. It is highly recommended to use a local cache path to store the downloaded jars of a package, because the next time the same deploy is executed, it can be picked from the local path. Ensure that this path is available on the lead nodes. Similarly keep the standalone jars also in a path where it is available to all the lead nodes.","title":"Example"},{"location":"connectors/deployment_dependency_jar/#submitting-snappydata-job-with-packages","text":"You can specify the name of the packages which can be used by a job that is submitted to SnappyData. This package is visible only to the job submitted with this argument. If another job tries to access a class belonging to the jar of this package then it will get ClassNotFoundException. A --packages option is added to the snappy-job.sh script where you can specify the packages. Use the following syntax: $./snappy-job.sh submit --app-name <app-name> --class <job-class> [--lead <hostname:port>] [--app-jar <jar-path>] [other existing options] [--packages <comma separated package-coordinates> ] [--repos <comma separated mvn repositories] [--jarcache <path where resolved jars will be kept>]","title":"Submitting SnappyData Job with Packages"},{"location":"connectors/deployment_dependency_jar/#example-of-snappydata-job-submission-with-packages","text":"./bin/snappy-job.sh submit --app-name cassconn --class <SnappyJobClassName> --app-jar <app.jar> --lead localhost:8090 --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.7 If you want global deployment, you can instead use the deploy command SQL and then the packages become visible everywhere. snappy> deploy package cassconn 'datastax:spark-cassandra-connector:2.0.1-s_2.11' path '/home/alex/mycache';","title":"Example of SnappyData Job Submission with Packages"},{"location":"connectors/gemfire_connector/","text":"SnappyData GemFire Connector \u00b6 This feature is not available in SnappyData Community edition. Overview \u00b6 The SnappyData GemFire Connector allows SnappyData/Spark programs to read from data regions as well as write into data regions within GemFire clusters. You can connect the applications to one or more GemFire clusters, expose GemFire regions as SnappyData tables/Spark DataFrames, run complex SQL queries on data stored in GemFire and save SnappyData tables onto GemFire regions. The connector is designed to execute in a highly parallelized manner targeting GemFire partitioned datasets (buckets) for the highest possible performance. By exposing GemFire regions as Spark DataFrames, applications can benefit from the analytic features in SnappyData such as, flexible data transformations, analytics and moving data from/into almost all modern data stores. Features \u00b6 Expose GemFire regions as SnappyData external tables Run SQL queries on GemFire regions from SnappyData Support joins on GemFire regions from SnappyData Save SnappyData tables/DataFrames to GemFire Support for POJOs as well as native support for GemFire PDX Push query predicates and projection down to GemFire and use OQL for query execution Query federation across multiple GemFire clusters Version and Compatibility \u00b6 SnappyData GemFire Connector supports Spark 2.1 and has been tested with GemFire 8.2 or later. Quick Start Guide \u00b6 This Quick Start guide explains, how to start a GemFire cluster, load data onto a partitioned region, access this region as an SQL table, replicate to a SnappyData column table, and then run queries on both GemFire and SnappyData tables. Set SnappyData GemFire Connector Configure the SnappyData Cluster for GemFire Connector Access GemFire as an SQL Table to Run Queries Replicate to SnappyData Table and Running Join Queries Setting SnappyData GemFire Connector \u00b6 The following prerequisites are required for setting up GemFire connector: Prerequisites Basic knowledge of GemFire GemFire version 8.2 or later installed and running. The following section provides instructions to get a two-node GemFire cluster running and to deploy the functions required by SnappyData to access GemFire. To start GemFire cluster and deploy SnappyData Connector functions Start the GemFire shell. $ <GemFire_home>/bin/gfsh gfsh> start locator --name=locator1 --port=55221 You need to use a non-default port, as SnappyData uses the same defaults as GemFire. Start two data servers. gfsh>start server --name=server1 --locators=localhost[55221] gfsh>start server --name=server2 --locators=localhost[55221] --server-port=40405 Create Region. gfsh>create region --name=GemRegion --type=PARTITION --key-constraint=java.lang.String --value-constraint=java.lang.String Add at least 10 entries in this region using the PUT command. gfsh> put --key=1 --value='James' --region=/GemRegion gfsh> put --key=2 --value='Jim' --region=/GemRegion gfsh> put --key=3 --value='James Bond' --region=/GemRegion gfsh> put --key=4 --value='007' --region=/GemRegion Deploy these functions to the GemFire cluster. gfsh>deploy --jar=<SnappyData-home>/connectors/gfeFunctions-0.9.jar A two node GemFire cluster is up and running with a region **GemRegion** and the added entries. Configuring the SnappyData Cluster for GemFire Connector \u00b6 The SnappyData cluster must be configured with details of the GemFire cluster with which the connector interfaces. The configuration details should be provided to both the SnappyData lead and server nodes. Modify the server and lead configuration files that are located at: < SnappyData-home >/conf/leads < SnappyData-home >/conf/servers Add the connector jar (connector-0.9.jar) to the classpath and configure the remote GemFire cluster (locators, the servers and lead files) as follows: localhost -locators=localhost:10334 -client-bind-address=localhost -classpath= <SnappyData-home>/connectors/connector-0.9.jar -spark.gemfire-grid.\\<UniqueID\\>=localhost[55221] Here, the UniqueID is a name assigned for the Grid. For example, SnappyData GemFire connector can connect to multiple Grids for federated data access. -spark.gemfire-grid.gridOne=localhost[55221] -spark.gemfire-grid.gridTwo=localhost[65221] Start the SnappyData cluster using the following command: $ <SnappyData-home>/sbin/snappy-start-all.sh Accessing GemFire as an SQL Table to Run Queries \u00b6 The following section provides instructions to access GemFire as an SQL table to run queries. To access GemFire as an SQL table. Start the Snappy Shell. $<SnappyData-home>/bin/snappy snappy> connect client 'localhost:1527'; Register an external table in SnappyData pointing to the GemFire region. snappy> create external table GemTable using gemfire options(regionPath 'GemRegion', keyClass 'java.lang.String', valueClass 'java.lang.String') ; The schema is automatically inferred from the object data in GemFire: snappy > describe gemTable ; snappy > select * from gemTable ; Replicating to SnappyData Table and Running Join Queries \u00b6 You can replicate the data in GemFire SQL table into a SnappyData table and then run join queries. Create a SnappyData table based on the external table that was created using GemFire. snappy > create table SnappyDataTable using column as ( select * from gemTable ); snappy > select * from SnappyDataTable ; Run join queries. snappy > select t1 . key_Column , t1 . value_Column , t2 . value_Column from GemTable t1 , SnappyDataTable t2 where t1 . key_Column = t2 . key_Column ; Initializing the GemFire Connector \u00b6 SnappyData uses a set of functions that are deployed in the GemFire cluster, to interact with the cluster, for accessing metadata, runnning queries, and accessing data in GemFire. You must deploy the SnappyData GemFire Connector's jar that is gemfire-function jar into the GemFire cluster to enable the connector functionality. Enabling Connector Functionality \u00b6 To enable the connector functionality, you must deploy the SnappyData GemFire Connector's gemfire-function jar. Execute the following to deploy the gemfire-function jar: Deploy SnappyData GemFire Connector's gemfire-function jar (`gfeFunct ions-0.9.3.jar`): gfsh>deploy --jar=<SnappyData Product Home>//connectors/gfeFunctions-0.9.3.jar Executing Queries with GemFire Connector \u00b6 During the query execution, snappydata passes the names of attributes and filter conditions to the GemFire cluster, to prune the data that is fetched from GemFire. For example, if you query for only attribute A from a GemFire Region value and that too of only those Region values which meet the filter condition, then instead of fetching the complete value only the pruned data needs to be sent to SnappyData. For this purpose by default SnappyData relies on OQL of GemFire to prune the data. However, you can write custom QueryExecutor to retrieve the pruned data from GemFire. This is done by implementing QueryExecutor interface. The QueryExecutor implementation should be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. As a part of the contract, the jar should include the following file with the path described: META-INF/services/io.snappydata.spark.gemfire.connector.query.QueryExecutor This file should contain the fully qualified class name of the custom QueryExecutor . Note The name of the file should be io.snappydata.spark.gemfire.connector.query.QueryExecutor . This jar needs to be deployed on the GemFire cluster using gfsh . Attention It is important that as part of deployment, gfeFunctions.jar must be deployed first and then the jars containing custom QueryExecutors . Following is an example of the implementation of QueryExecutor interface: package io.snappydata.spark.gemfire.connector.query ; import java.util.Iterator ; import com.gemstone.gemfire.cache.Region ; import com.gemstone.gemfire.cache.execute.RegionFunctionContext ; import com.gemstone.gemfire.pdx.PdxInstance ; import io.snappydata.spark.gemfire.connector.internal.gemfirefunctions.shared.filter.Filter ; public interface QueryExecutor < T , S > { /** * * @param region * @param requiredAttributes * @param filter * @param isPdxData * @return A stateful object which will be passed to the querying apis on every invocation. * The idea is that if the user can prepare an object which could be used for every bucket, it will be * efficient. So if a compiled Query object is prepared at the start of evaluation, then every bucket can * use this Query object instead of creating a compiled query object from string every time. */ public S initStatefulObject ( Region region , String [] requiredAttributes , T filter , boolean isPdxData ); /** * * @return any String which uniquely identifies the Executor */ public String getUniqueID (); /** * This method will be invoked if the query needs a single field projection * User can return null in which case default implementation using OQL will be used * @param region Region on which query is to be performed * @param requiredAttribute projection which need to be returned * @param filter The desired format of the Filter condition to filter out relevant rows * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion * @param statefulObject which can be used across all buckets eg a compiled Query object. * @return An Iterator over the projected columns */ public Iterator < Object > executeSingleProjectionQuery ( Region region , String requiredAttribute , T filter , int forBucket , RegionFunctionContext context , S statefulObject ); /** * This method will be invoked if the query needs multiple fields projection * User can return null in which case default implementation using OQL will be used * @param region Region on which query is to be performed * @param requiredAttributes A String array containing field names which need to be projected * @param filter The desired format of the Filter condition to filter out relevant rows * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion * @param holder An instance of MultiProjectionHolder which should be used to store the projections during * iteration. It is a wrapper over object[] intended to avoid creation of Object[] during ietaration * @param statefulObject which can be used across all buckets eg a compiled Query object. * @return An Iterator of MultiProjectionHolder which contains the projected columns */ public Iterator < MultiProjectionHolder > executeMultiProjectionQuery ( Region region , String [] requiredAttributes , T filter , int forBucket , RegionFunctionContext context , MultiProjectionHolder holder , S statefulObject ); /** * This method is invoked if the region contains PdxInstances. User needs to return the filtered PdxInstances * without applying projection, which will be applied internally . This will ensure that projected columns * do not get unnecessarily deserialized * User can return null in which case default implementation using OQL will be used * @param region Region on which query is to be performed * @param filter The desired format of the Filter condition to filter out relevant rows * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion * @param statefulObject which can be used across all buckets eg a compiled Query object. * @return Iterator over PdxInstance */ public Iterator < PdxInstance > filteredPdxData ( Region region , T filter , int forBucket , RegionFunctionContext context , S statefulObject ); /** * * @param filters Filter objects which need to be transformed by the user in the way it can be used in querying * @return transformed filter */ public T transformFilter ( Filter [] filters ); } The working of QueryExecutor can be broken as follows: Converting the filter condition into a desired form. Initializing a stateful context object which avoids recreating a reusable unchanging object during data fetch on the individual buckets. Retrieval of data for Region as a whole, in case of replicated region, or for individual buckets, in case of partitioned region. Convert Filter Condition \u00b6 During execution, SnappyData checks with the available QueryExecutor by invoking the transformFilter method, if the filter is present. In case the QueryExecutor is capable of handling it, a transformed filter is returned which can be used for data retrieval or else an UnsupportedOperationException is shown. Initialize Stateful Context Object \u00b6 If a transformed value is returned, then initStatefulObject method is invoked. You can utilize this to instantiate a reusable object such as, a parsed QueryPlan or a compiled structure or can simply return null. At this stage too, if the query cannot be handled, UnsupportedOperationException is shown. Retrieve Data for Region \u00b6 Next step is invocation of the appropriate method to get Iterator on the pruned data. This method can get invoked multiple times based on the number of partitioned region buckets that is operated upon. If the region contains pdx instances, then the method filteredPdxData is invoked. Note This method requires the user to return the iterator on valid Pdx instances. The relevant attributes are extracted by the framework. If the region contains POJOs, then depending upon single or multiple attributes, one of the following method is invoked: executeSingleProjectionQuery executeMultiProjectionQuery * Configuring the SnappyData Cluster for GemFire Connector \u00b6 The following configurations can be set in SnappyData Cluster for GemFire Connector: Customize DistributedSystem in the cluster with additional attributes Specify Static Locators Dynamic Discovery of Grid for a Region Customizing the DistributedSystem of SnappyData Cluster at Startup \u00b6 You can configure the DistributedSystem with additional attributes by implementing the following interface: package io.snappydata.spark.gemfire.connector.dsinit ; /** * @param @link{DSConfig} instance which can be used to configure the DistributedSystem properties at the * time of snappydata startup */ public interface DistributedSystemInitializer { public void configure ( DSConfig config ); } Any required properties of the DistributedSystem can be configured via the DSConfig instance passed. The DistributedSystemInitializer implementation needs to be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. The jar should include the following file along with the path as described: META-INF/services/io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer This file should contain the fully qualified class name of the custom DistributedSystemInitializer Note The name of the file should be io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer The connector interacts with the GemFire cluster. Therefore, you should configure the SnappyData cluster with the details of GemFire cluster. The configuration details must be provided in both the SnappyData lead and server nodes at the following locations: - SnappyData-Home directory/conf/leads - SnappyData-Home directory/conf/servers Modify the servers and leads configuration file to add the connector jar ( connector-1.0.0.jar ) and the person.jar in the classpath as well as to configure the remote GemFire cluster locator. Specifying Static Locators \u00b6 To statically specify the running locators of the GemFire cluster, set the property as follows, where uniqueIDForGrid is any unique identifier key: -snappydata.connector.gemfire-grid.\\<uniqueIDForGrid\\>=localhost[55221] Following is a sample from the servers and leads file: localhost -locators=localhost:10334 -client-bind-address=localhost -client-port=1528 -heap-size=20g -classpath=<SnappyData-Product-Home>//connectors/connector_2.11-0.9.3.jar:<path-to-jar>/persons.jar -snappydata.connector.gemfire-grid.<uniqueIDForGrid>=localhost[55221] Dynamically Discovering Grid for a Region \u00b6 Instead of specifying the locators via the property, it is possible to provide custom logic for discovery of the grid for the region by implementing GridResolver trait as follows: trait GridResolver { /** * Optional method to identify the locators and delegate to SnappyData for creating connection * pool using [[GridHelper]]. Invoked once in the lifecycle. * @param gfeGridProps * @param gridHelper */ def initConnections ( gfeGridProps : java . util . Map [ String , String ], gridHelper : GridHelper ): Unit /** * Resolve the grid for the region or return null if unable to do so * @param regionName * @param gfeGridProps * @param gridHelper * @param userOptions * @return the grid name ( the name of the connection pool) to use */ def getGridForRegion ( regionName : String , gfeGridProps : java . util . Map [ String , String ], gridHelper : GridHelper , userOptions : java . util . Map [ String , String ]): String /** * * @return a String identifying the resolver uniquely */ def getUniqueID : String } SnappyData attempts to resolve the grid for the region using existing ConnectionPools . If it is not successful, it checks with the available resolvers by invoking getGridForRegion . The resolver returns null, if it cannot resolve the grid. The GridResolver implementation should be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. The jar should also include the following file along with the described path: META-INF/services/io.snappydata.spark.gemfire.connector.grids.GridResolver This file should contain the fully qualified class name of the custom GridResolver . Note The name of the file should be io.snappydata.spark.gemfire.connector.grids.GridResolver . To initialize the GemFire connector and enable its functions in SnappyData, you must include the following import statement before creating the external table: import io . snappydata . spark . gemfire . connector Accessing Data from GemFire \u00b6 In SnappyData applications, you can create external tables that represent GemFire regions and run SQL queries against GemFire. For accesing data from GemFire, you must first expose the GemFire regions: You can use any the following options to expose GemFire regions: Expose GemFire PDX Regions as External Tables Expose Regions Containing POJOs as External Tables Expose GemFire Region Using Dataframe based External Tables Expose GemFire Regions as RDDs Exposing GemFire PDX Regions as External Tables \u00b6 You can create an external table that represents a GemFire region which stores PDX instances. The SnappyData schema for this external table is derived from the PDXType. Here, the GemFire region is already populated with data and SnappyData infers the schema based on the inspection of the PDX types. The following syntax creates an external table that represents a GemFire region which stores PDX instances. val externalBsegTable = snc . createExternalTable ( \"bsegInGem\" , \"gemfire\" , Map [ String , String ]( \"region.path\" -> \"bseg1\" , \"data.as.pdx\" -> \"true\" )) The SnappyData external table schema for the GemFire region can optionally include the GemFire region key as a column in the table. To enable this, the key.class attribute should be set when you create the table as shown in the following example: val externalBsegTable = snc . createExternalTable ( \"bsegInGem\" , \"gemfire\" , Map [ String , String ]( \"region.path\" -> \"bseg1\" , \"data.as.pdx\" -> \"true\" , \"key.class\" -> \"java.lang.Long\" )) Exposing Regions Containing POJOs as External Tables \u00b6 In the following example, an external table is created using the getter methods on POJOs as SnappyData column names: snc . createExternalTable ( externalPersonsTable1 , \"gemfire\" , Map [ String , String ]( \"region.path\" -> personsRegionName , \"value.class\" -> \"load.Person\" )) As in the previous case, if the GemFire key field has to be included as a column, then the Key.class attribute has to be passed in as an option. snc . createExternalTable ( externalPersonsTable1 , \"gemfire\" , Map [ String , String ]( \"region.path\" -> personsRegionName , \"value.class\" -> \"load.Person\" ), \"key.class\" -> \"java.lang.Long\" )) Expose GemFire Region Using Dataframe Based External Tables \u00b6 In the following example, a DataFrame is used to create an external table bsegTable , with the schema which is same as that of DataFrame bsegDF . The primary key column name should be specified for this to work correctly. In the following example, it is assumed that the dataframe contains a column named \"id1\". bsegDF . write . format ( \"gemfire\" ). option ( \"region.path\" , \"bseg1\" ). option ( \"primary.key.column.name\" , \"id1\" ). option ( \"pdx.identity.fields\" , \"id1\" ). saveAsTable ( \"bsegTable\" ) Note The pdx.identity.fields specification has a huge impact on performance within GemFire since this specification informs GemFire to use only the specified field for computing hashCode and equality for PdxInstance. Expose GemFire Regions as RDDs \u00b6 Invoking the gemfireRegion method on the SparkContext in SnappyData exposes the full data set of a GemFire region as a Spark RDD. The same API exposes both replicated and partitioned region as RDDs. scala > val rdd = sc . gemfireRegion [ String , String ]( \"gemTable1\" ) scala > rdd . foreach ( println ) ( 1 , one ) ( 3 , three ) ( 2 , two ) Note when the RDD is used, it is important to specify the correct type for both the region key and value, otherwise a ClassCastException is encountered. Controlling the Schema of the External Table \u00b6 The pdx fields (of pdx instances) or the getter methods (of the POJOs) define the schema of the external table by default. However, you can also control the schema by excluding the columns as per requirement. This is done by implementing the following trait: package io . snappydata . spark . gemfire . connector . lifecycle import org . apache . spark . sql . types . StructType trait ColumnSelector { def selectColumns ( gridName : Option [ String ], regionName : String , allColumns : StructType ): StructType } Based on the requirement, a new StructType which contains fewer columns can be returned. Note The new StructType returned can only have the subset of the columns that are passed. No new columns should be added. The ColumnSelector implemetation needs to be added to the startup classpath of the SnappyDataCluster. At the time of table creation, an option with key = column.selector and value as the fully qualified class name of the ColumnSelector implementation class should be passed. Other Optional Configuration Parameters \u00b6 column column max.rows.restrict This parameter is used to restrict the number of rows that are fetched from an external table, when a query of type select * from external_table is executed. This restriction, if specified, is applied to queries without any filter, limit, aggregate function, and projection such that it tends to bring all the rows from the external region. Note that this restriction is not applied in case of DDL statement such as create table X using row column as select * from external_table . The default value for is 10,000. column.selector This parameter is used to control the columns that must be included in the schema as described here . max.buckets.per.partition This parameter is used to control the concurrency and number of tasks created to fetch data from an external region. This property is useful only for Partitioned Region. For more details, refer to Controlling Task Concurrency in SnappyData When Accessing GemFire . The default value is 3. security.username and security.password By default the external table is created and queried using the credential of the current user. For example user X creates external table, and if user Y is querying, then credentials of user Y is fetched from data. But if the parameters security.username and security.password are passed then those credentials are used to create external table and for querying the data irrespective of the current user. pdxtype.maxscan This parameter determines the maximum number of entries from the region which is scanned to completely determine the schema of the external table from the Pdx instances that is stored in the region. The default value is 100. Controlling Task Concurrency in SnappyData When Accessing GemFire \u00b6 There are two types of regions in GemFire; replicated and partitioned . All the data for a replicated region is present on every server where the region is defined. A GemFire partitioned region splits the data across the servers that define the partitioned region. When operating with replicated regions, there is only a single RDD partition representing the replicated region in SnappyData. Since there is only one data bucket for the replicated region. As compared to this, a GemFire partitioned region can be represented by a configurable number of RDD partitions in SnappyData. Choosing the number of RDD partitions directly controls the task concurrency in SnappyData when you run queries on GemFire regions. By default, the GemFire connector works out the number of buckets per GemFire server, assigns partitions to each server, and uses a default value of maximum three buckets per partition. You can configure the max.buckets.per.partition attribute to change this value. When queries are executed on an external table, the degree of parallelism in query execution is directly proportional to the number of RDD partitions that represents the table. The following example shows how to configure the RDD partitions count for an external table representing a GemFire region: import io . snappydata . spark . gemfire . connector . _ val externalBsegTable = snc . createExternalTable ( \"bsegInGem\" , \"gemfire\" , Map [ String , String ]( \"region.path\" -> \"bseg1\" , \"data.as.pdx\" -> \"true\" , \"key.class\" -> \"java.lang.Long\" , \"max.buckets.per.partition\" -> \"5\" )) Saving Data to GemFire Region \u00b6 You can save data to GemFire Regions using any of the following : Saving Pair RDD to GemFire Region Saving Non-Pair RDD to GemFire Saving DataFrame to GemFire Saving Pair RDD to GemFire Region \u00b6 A pair RDD can be saved from SnappyData into a GemFire region as follows: Import the implicits as shown: import io . snappydata . spark . gemfire . connector In the Spark shell, create a simple pair RDD and save it to GemFire Region: scala > import io . snappydata . spark . gemfire . connector . _ scala > val data = Array (( \"1\" , \"one\" ), ( \"2\" , \"two\" ), ( \"3\" , \"three\" )) data : Array [( String , String )] = Array (( 1 , one ), ( 2 , two ), ( 3 , three )) scala > val distData = sc . parallelize ( data ) distData : org . apache . spark . rdd . RDD [( String , String )] = ParallelCollectionRDD [ 0 ] at parallelize at < console >: 14 scala > distData . saveToGemFire ( \"gemTable1\" ) 15 / 02 / 17 07 : 11 : 54 INFO DAGScheduler : Job 0 finished : runJob at GemFireRDDFunctions . scala : 29 , took 0.341288 s Verify the data is saved in GemFire using gfsh : gfsh>query --query=\"select key,value from /gemTable1.entries\" Result : true startCount : 0 endCount : 20 Rows : 3 key | value --- | ----- 1 | one 3 | three 2 | two Saving Non-Pair RDD to GemFire \u00b6 Saving a non-pair RDD to GemFire requires an extra function that converts each element of RDD to a key-value pair. Here's a sample session in Spark shell: scala > import io . snappydata . spark . gemfire . connector . _ scala > val data2 = Array ( \"a\" , \"ab\" , \"abc\" ) data2 : Array [ String ] = Array ( a , ab , abc ) scala > val distData2 = sc . parallelize ( data2 ) distData2 : org . apache . spark . rdd . RDD [ String ] = ParallelCollectionRDD [ 0 ] at parallelize at < console >: 17 scala > distData2 . saveToGemFire ( \"gemTable1\" , e => ( e . length , e )) [ info 2015 / 02 / 17 12 : 43 : 21.174 PST < main > tid = 0x1 ] ... 15 / 02 / 17 12 : 43 : 21 INFO DAGScheduler : Job 0 finished : runJob at GemFireRDDFunctions . scala : 52 , took 0.251194 s Verify the result with gfsh : gfsh>query --query=\"select key,value from /gemTable1.entrySet\" Result : true startCount : 0 endCount : 20 Rows : 3 key | value --- | ----- 2 | ab 3 | abc 1 | a Saving a DataFrame to GemFire \u00b6 To save a DataFrame, that is dataSet of row objects, into GemFire, use the following API which is available as an implicit definition. The rows of the dataframes are converted into PDX instances for storage in the GemFire's region. In the following example, it is assumed that there is a column \"id1\" present in the dataframe's schema. To specify the PDX Identity Fields for the PDX Type, use the option as (\"pdx.identity.fields\", \"Col1, Col2, Col3\") to specify one or more columns to be used as PDX Identity fields. SnappyData recommends to define the identity fields for performance during comparison of PDX Instances. import io . snappydata . spark . gemfire . connector . _ df . write . format ( \"gemfire\" ). option ( \"region.path\" , \"/region1\" ). option ( \"primary.key.column.name\" , \"id1\" ). option ( \"pdx.identity.fields\" , \"id1\" ) . save () To dynamically generate the GemFire Region's key, import the implicits and use the following API: saveToGemFire [ K ]( regionPath : String , keyExtractor : Row => K , opConf : Map [ String , String ] = Map . empty ) import io . snappydata . spark . gemfire . connector . _ df . saveToGemFire [ String ]( \"/region1\" , ( row : Row ) => ( row . getString ( 1 ) + \"_\" + row . getString ( 10 )), Map [ String , String ]( \"pdx.identity.fields\" -> \"id1, id10\" ) ) Running OQL queries Directly on GemFire from SnappyData \u00b6 Most applications using SnappyData will choose to run regular SQL queries on GemFire regions. Refer to Accessing Data From GemFire Additionally, you can directly execute OQL queries on GemFire regions using the GemFire connector. In scenarios where the data stored in GemFire regions is neither PDX nor Java bean compliant POJO, you can execute OQL queries and retrieve the data from the server and make it available as a data frame. An instance of SQLContext is required to run OQL query. val snc = new org . apache . spark . sql . SnappyContext ( sc ) Create a DataFrame using OQL: val dataFrame = snc . gemfireOQL ( \"SELECT iter.name,itername.address.city, iter.id FROM /personRegion iter\" ) You can repartition the DataFrame using DataFrame.repartition() if required. After you have the DataFrame , you can register it as a table and use Spark SQL to query: dataFrame . registerTempTable ( \"person\" ) val SQLResult = sqlContext . sql ( \"SELECT * FROM person WHERE id > 100\" )","title":"SnappyData GemFire Connector"},{"location":"connectors/gemfire_connector/#snappydata-gemfire-connector","text":"This feature is not available in SnappyData Community edition.","title":"SnappyData GemFire Connector"},{"location":"connectors/gemfire_connector/#overview","text":"The SnappyData GemFire Connector allows SnappyData/Spark programs to read from data regions as well as write into data regions within GemFire clusters. You can connect the applications to one or more GemFire clusters, expose GemFire regions as SnappyData tables/Spark DataFrames, run complex SQL queries on data stored in GemFire and save SnappyData tables onto GemFire regions. The connector is designed to execute in a highly parallelized manner targeting GemFire partitioned datasets (buckets) for the highest possible performance. By exposing GemFire regions as Spark DataFrames, applications can benefit from the analytic features in SnappyData such as, flexible data transformations, analytics and moving data from/into almost all modern data stores.","title":"Overview"},{"location":"connectors/gemfire_connector/#features","text":"Expose GemFire regions as SnappyData external tables Run SQL queries on GemFire regions from SnappyData Support joins on GemFire regions from SnappyData Save SnappyData tables/DataFrames to GemFire Support for POJOs as well as native support for GemFire PDX Push query predicates and projection down to GemFire and use OQL for query execution Query federation across multiple GemFire clusters","title":"Features"},{"location":"connectors/gemfire_connector/#version-and-compatibility","text":"SnappyData GemFire Connector supports Spark 2.1 and has been tested with GemFire 8.2 or later.","title":"Version and Compatibility"},{"location":"connectors/gemfire_connector/#quick-start-guide","text":"This Quick Start guide explains, how to start a GemFire cluster, load data onto a partitioned region, access this region as an SQL table, replicate to a SnappyData column table, and then run queries on both GemFire and SnappyData tables. Set SnappyData GemFire Connector Configure the SnappyData Cluster for GemFire Connector Access GemFire as an SQL Table to Run Queries Replicate to SnappyData Table and Running Join Queries","title":"Quick Start Guide"},{"location":"connectors/gemfire_connector/#setting-snappydata-gemfire-connector","text":"The following prerequisites are required for setting up GemFire connector: Prerequisites Basic knowledge of GemFire GemFire version 8.2 or later installed and running. The following section provides instructions to get a two-node GemFire cluster running and to deploy the functions required by SnappyData to access GemFire. To start GemFire cluster and deploy SnappyData Connector functions Start the GemFire shell. $ <GemFire_home>/bin/gfsh gfsh> start locator --name=locator1 --port=55221 You need to use a non-default port, as SnappyData uses the same defaults as GemFire. Start two data servers. gfsh>start server --name=server1 --locators=localhost[55221] gfsh>start server --name=server2 --locators=localhost[55221] --server-port=40405 Create Region. gfsh>create region --name=GemRegion --type=PARTITION --key-constraint=java.lang.String --value-constraint=java.lang.String Add at least 10 entries in this region using the PUT command. gfsh> put --key=1 --value='James' --region=/GemRegion gfsh> put --key=2 --value='Jim' --region=/GemRegion gfsh> put --key=3 --value='James Bond' --region=/GemRegion gfsh> put --key=4 --value='007' --region=/GemRegion Deploy these functions to the GemFire cluster. gfsh>deploy --jar=<SnappyData-home>/connectors/gfeFunctions-0.9.jar A two node GemFire cluster is up and running with a region **GemRegion** and the added entries.","title":"Setting SnappyData GemFire Connector"},{"location":"connectors/gemfire_connector/#configuring-the-snappydata-cluster-for-gemfire-connector","text":"The SnappyData cluster must be configured with details of the GemFire cluster with which the connector interfaces. The configuration details should be provided to both the SnappyData lead and server nodes. Modify the server and lead configuration files that are located at: < SnappyData-home >/conf/leads < SnappyData-home >/conf/servers Add the connector jar (connector-0.9.jar) to the classpath and configure the remote GemFire cluster (locators, the servers and lead files) as follows: localhost -locators=localhost:10334 -client-bind-address=localhost -classpath= <SnappyData-home>/connectors/connector-0.9.jar -spark.gemfire-grid.\\<UniqueID\\>=localhost[55221] Here, the UniqueID is a name assigned for the Grid. For example, SnappyData GemFire connector can connect to multiple Grids for federated data access. -spark.gemfire-grid.gridOne=localhost[55221] -spark.gemfire-grid.gridTwo=localhost[65221] Start the SnappyData cluster using the following command: $ <SnappyData-home>/sbin/snappy-start-all.sh","title":"Configuring the SnappyData Cluster for GemFire Connector"},{"location":"connectors/gemfire_connector/#accessing-gemfire-as-an-sql-table-to-run-queries","text":"The following section provides instructions to access GemFire as an SQL table to run queries. To access GemFire as an SQL table. Start the Snappy Shell. $<SnappyData-home>/bin/snappy snappy> connect client 'localhost:1527'; Register an external table in SnappyData pointing to the GemFire region. snappy> create external table GemTable using gemfire options(regionPath 'GemRegion', keyClass 'java.lang.String', valueClass 'java.lang.String') ; The schema is automatically inferred from the object data in GemFire: snappy > describe gemTable ; snappy > select * from gemTable ;","title":"Accessing GemFire as an SQL Table to Run Queries"},{"location":"connectors/gemfire_connector/#replicating-to-snappydata-table-and-running-join-queries","text":"You can replicate the data in GemFire SQL table into a SnappyData table and then run join queries. Create a SnappyData table based on the external table that was created using GemFire. snappy > create table SnappyDataTable using column as ( select * from gemTable ); snappy > select * from SnappyDataTable ; Run join queries. snappy > select t1 . key_Column , t1 . value_Column , t2 . value_Column from GemTable t1 , SnappyDataTable t2 where t1 . key_Column = t2 . key_Column ;","title":"Replicating to SnappyData Table and Running Join Queries"},{"location":"connectors/gemfire_connector/#initializing-the-gemfire-connector","text":"SnappyData uses a set of functions that are deployed in the GemFire cluster, to interact with the cluster, for accessing metadata, runnning queries, and accessing data in GemFire. You must deploy the SnappyData GemFire Connector's jar that is gemfire-function jar into the GemFire cluster to enable the connector functionality.","title":"Initializing the GemFire Connector"},{"location":"connectors/gemfire_connector/#enabling-connector-functionality","text":"To enable the connector functionality, you must deploy the SnappyData GemFire Connector's gemfire-function jar. Execute the following to deploy the gemfire-function jar: Deploy SnappyData GemFire Connector's gemfire-function jar (`gfeFunct ions-0.9.3.jar`): gfsh>deploy --jar=<SnappyData Product Home>//connectors/gfeFunctions-0.9.3.jar","title":"Enabling Connector Functionality"},{"location":"connectors/gemfire_connector/#executing-queries-with-gemfire-connector","text":"During the query execution, snappydata passes the names of attributes and filter conditions to the GemFire cluster, to prune the data that is fetched from GemFire. For example, if you query for only attribute A from a GemFire Region value and that too of only those Region values which meet the filter condition, then instead of fetching the complete value only the pruned data needs to be sent to SnappyData. For this purpose by default SnappyData relies on OQL of GemFire to prune the data. However, you can write custom QueryExecutor to retrieve the pruned data from GemFire. This is done by implementing QueryExecutor interface. The QueryExecutor implementation should be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. As a part of the contract, the jar should include the following file with the path described: META-INF/services/io.snappydata.spark.gemfire.connector.query.QueryExecutor This file should contain the fully qualified class name of the custom QueryExecutor . Note The name of the file should be io.snappydata.spark.gemfire.connector.query.QueryExecutor . This jar needs to be deployed on the GemFire cluster using gfsh . Attention It is important that as part of deployment, gfeFunctions.jar must be deployed first and then the jars containing custom QueryExecutors . Following is an example of the implementation of QueryExecutor interface: package io.snappydata.spark.gemfire.connector.query ; import java.util.Iterator ; import com.gemstone.gemfire.cache.Region ; import com.gemstone.gemfire.cache.execute.RegionFunctionContext ; import com.gemstone.gemfire.pdx.PdxInstance ; import io.snappydata.spark.gemfire.connector.internal.gemfirefunctions.shared.filter.Filter ; public interface QueryExecutor < T , S > { /** * * @param region * @param requiredAttributes * @param filter * @param isPdxData * @return A stateful object which will be passed to the querying apis on every invocation. * The idea is that if the user can prepare an object which could be used for every bucket, it will be * efficient. So if a compiled Query object is prepared at the start of evaluation, then every bucket can * use this Query object instead of creating a compiled query object from string every time. */ public S initStatefulObject ( Region region , String [] requiredAttributes , T filter , boolean isPdxData ); /** * * @return any String which uniquely identifies the Executor */ public String getUniqueID (); /** * This method will be invoked if the query needs a single field projection * User can return null in which case default implementation using OQL will be used * @param region Region on which query is to be performed * @param requiredAttribute projection which need to be returned * @param filter The desired format of the Filter condition to filter out relevant rows * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion * @param statefulObject which can be used across all buckets eg a compiled Query object. * @return An Iterator over the projected columns */ public Iterator < Object > executeSingleProjectionQuery ( Region region , String requiredAttribute , T filter , int forBucket , RegionFunctionContext context , S statefulObject ); /** * This method will be invoked if the query needs multiple fields projection * User can return null in which case default implementation using OQL will be used * @param region Region on which query is to be performed * @param requiredAttributes A String array containing field names which need to be projected * @param filter The desired format of the Filter condition to filter out relevant rows * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion * @param holder An instance of MultiProjectionHolder which should be used to store the projections during * iteration. It is a wrapper over object[] intended to avoid creation of Object[] during ietaration * @param statefulObject which can be used across all buckets eg a compiled Query object. * @return An Iterator of MultiProjectionHolder which contains the projected columns */ public Iterator < MultiProjectionHolder > executeMultiProjectionQuery ( Region region , String [] requiredAttributes , T filter , int forBucket , RegionFunctionContext context , MultiProjectionHolder holder , S statefulObject ); /** * This method is invoked if the region contains PdxInstances. User needs to return the filtered PdxInstances * without applying projection, which will be applied internally . This will ensure that projected columns * do not get unnecessarily deserialized * User can return null in which case default implementation using OQL will be used * @param region Region on which query is to be performed * @param filter The desired format of the Filter condition to filter out relevant rows * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion * @param statefulObject which can be used across all buckets eg a compiled Query object. * @return Iterator over PdxInstance */ public Iterator < PdxInstance > filteredPdxData ( Region region , T filter , int forBucket , RegionFunctionContext context , S statefulObject ); /** * * @param filters Filter objects which need to be transformed by the user in the way it can be used in querying * @return transformed filter */ public T transformFilter ( Filter [] filters ); } The working of QueryExecutor can be broken as follows: Converting the filter condition into a desired form. Initializing a stateful context object which avoids recreating a reusable unchanging object during data fetch on the individual buckets. Retrieval of data for Region as a whole, in case of replicated region, or for individual buckets, in case of partitioned region.","title":"Executing Queries with GemFire Connector"},{"location":"connectors/gemfire_connector/#convert-filter-condition","text":"During execution, SnappyData checks with the available QueryExecutor by invoking the transformFilter method, if the filter is present. In case the QueryExecutor is capable of handling it, a transformed filter is returned which can be used for data retrieval or else an UnsupportedOperationException is shown.","title":"Convert Filter Condition"},{"location":"connectors/gemfire_connector/#initialize-stateful-context-object","text":"If a transformed value is returned, then initStatefulObject method is invoked. You can utilize this to instantiate a reusable object such as, a parsed QueryPlan or a compiled structure or can simply return null. At this stage too, if the query cannot be handled, UnsupportedOperationException is shown.","title":"Initialize Stateful Context Object"},{"location":"connectors/gemfire_connector/#retrieve-data-for-region","text":"Next step is invocation of the appropriate method to get Iterator on the pruned data. This method can get invoked multiple times based on the number of partitioned region buckets that is operated upon. If the region contains pdx instances, then the method filteredPdxData is invoked. Note This method requires the user to return the iterator on valid Pdx instances. The relevant attributes are extracted by the framework. If the region contains POJOs, then depending upon single or multiple attributes, one of the following method is invoked: executeSingleProjectionQuery executeMultiProjectionQuery *","title":"Retrieve Data for Region"},{"location":"connectors/gemfire_connector/#configuring-the-snappydata-cluster-for-gemfire-connector_1","text":"The following configurations can be set in SnappyData Cluster for GemFire Connector: Customize DistributedSystem in the cluster with additional attributes Specify Static Locators Dynamic Discovery of Grid for a Region","title":"Configuring the SnappyData Cluster for GemFire Connector"},{"location":"connectors/gemfire_connector/#customizing-the-distributedsystem-of-snappydata-cluster-at-startup","text":"You can configure the DistributedSystem with additional attributes by implementing the following interface: package io.snappydata.spark.gemfire.connector.dsinit ; /** * @param @link{DSConfig} instance which can be used to configure the DistributedSystem properties at the * time of snappydata startup */ public interface DistributedSystemInitializer { public void configure ( DSConfig config ); } Any required properties of the DistributedSystem can be configured via the DSConfig instance passed. The DistributedSystemInitializer implementation needs to be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. The jar should include the following file along with the path as described: META-INF/services/io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer This file should contain the fully qualified class name of the custom DistributedSystemInitializer Note The name of the file should be io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer The connector interacts with the GemFire cluster. Therefore, you should configure the SnappyData cluster with the details of GemFire cluster. The configuration details must be provided in both the SnappyData lead and server nodes at the following locations: - SnappyData-Home directory/conf/leads - SnappyData-Home directory/conf/servers Modify the servers and leads configuration file to add the connector jar ( connector-1.0.0.jar ) and the person.jar in the classpath as well as to configure the remote GemFire cluster locator.","title":"Customizing the DistributedSystem of SnappyData Cluster at Startup"},{"location":"connectors/gemfire_connector/#specifying-static-locators","text":"To statically specify the running locators of the GemFire cluster, set the property as follows, where uniqueIDForGrid is any unique identifier key: -snappydata.connector.gemfire-grid.\\<uniqueIDForGrid\\>=localhost[55221] Following is a sample from the servers and leads file: localhost -locators=localhost:10334 -client-bind-address=localhost -client-port=1528 -heap-size=20g -classpath=<SnappyData-Product-Home>//connectors/connector_2.11-0.9.3.jar:<path-to-jar>/persons.jar -snappydata.connector.gemfire-grid.<uniqueIDForGrid>=localhost[55221]","title":"Specifying Static Locators"},{"location":"connectors/gemfire_connector/#dynamically-discovering-grid-for-a-region","text":"Instead of specifying the locators via the property, it is possible to provide custom logic for discovery of the grid for the region by implementing GridResolver trait as follows: trait GridResolver { /** * Optional method to identify the locators and delegate to SnappyData for creating connection * pool using [[GridHelper]]. Invoked once in the lifecycle. * @param gfeGridProps * @param gridHelper */ def initConnections ( gfeGridProps : java . util . Map [ String , String ], gridHelper : GridHelper ): Unit /** * Resolve the grid for the region or return null if unable to do so * @param regionName * @param gfeGridProps * @param gridHelper * @param userOptions * @return the grid name ( the name of the connection pool) to use */ def getGridForRegion ( regionName : String , gfeGridProps : java . util . Map [ String , String ], gridHelper : GridHelper , userOptions : java . util . Map [ String , String ]): String /** * * @return a String identifying the resolver uniquely */ def getUniqueID : String } SnappyData attempts to resolve the grid for the region using existing ConnectionPools . If it is not successful, it checks with the available resolvers by invoking getGridForRegion . The resolver returns null, if it cannot resolve the grid. The GridResolver implementation should be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. The jar should also include the following file along with the described path: META-INF/services/io.snappydata.spark.gemfire.connector.grids.GridResolver This file should contain the fully qualified class name of the custom GridResolver . Note The name of the file should be io.snappydata.spark.gemfire.connector.grids.GridResolver . To initialize the GemFire connector and enable its functions in SnappyData, you must include the following import statement before creating the external table: import io . snappydata . spark . gemfire . connector","title":"Dynamically Discovering Grid for a Region"},{"location":"connectors/gemfire_connector/#accessing-data-from-gemfire","text":"In SnappyData applications, you can create external tables that represent GemFire regions and run SQL queries against GemFire. For accesing data from GemFire, you must first expose the GemFire regions: You can use any the following options to expose GemFire regions: Expose GemFire PDX Regions as External Tables Expose Regions Containing POJOs as External Tables Expose GemFire Region Using Dataframe based External Tables Expose GemFire Regions as RDDs","title":"Accessing Data from GemFire"},{"location":"connectors/gemfire_connector/#exposing-gemfire-pdx-regions-as-external-tables","text":"You can create an external table that represents a GemFire region which stores PDX instances. The SnappyData schema for this external table is derived from the PDXType. Here, the GemFire region is already populated with data and SnappyData infers the schema based on the inspection of the PDX types. The following syntax creates an external table that represents a GemFire region which stores PDX instances. val externalBsegTable = snc . createExternalTable ( \"bsegInGem\" , \"gemfire\" , Map [ String , String ]( \"region.path\" -> \"bseg1\" , \"data.as.pdx\" -> \"true\" )) The SnappyData external table schema for the GemFire region can optionally include the GemFire region key as a column in the table. To enable this, the key.class attribute should be set when you create the table as shown in the following example: val externalBsegTable = snc . createExternalTable ( \"bsegInGem\" , \"gemfire\" , Map [ String , String ]( \"region.path\" -> \"bseg1\" , \"data.as.pdx\" -> \"true\" , \"key.class\" -> \"java.lang.Long\" ))","title":"Exposing GemFire PDX Regions as External Tables"},{"location":"connectors/gemfire_connector/#exposing-regions-containing-pojos-as-external-tables","text":"In the following example, an external table is created using the getter methods on POJOs as SnappyData column names: snc . createExternalTable ( externalPersonsTable1 , \"gemfire\" , Map [ String , String ]( \"region.path\" -> personsRegionName , \"value.class\" -> \"load.Person\" )) As in the previous case, if the GemFire key field has to be included as a column, then the Key.class attribute has to be passed in as an option. snc . createExternalTable ( externalPersonsTable1 , \"gemfire\" , Map [ String , String ]( \"region.path\" -> personsRegionName , \"value.class\" -> \"load.Person\" ), \"key.class\" -> \"java.lang.Long\" ))","title":"Exposing Regions Containing POJOs as External Tables"},{"location":"connectors/gemfire_connector/#expose-gemfire-region-using-dataframe-based-external-tables","text":"In the following example, a DataFrame is used to create an external table bsegTable , with the schema which is same as that of DataFrame bsegDF . The primary key column name should be specified for this to work correctly. In the following example, it is assumed that the dataframe contains a column named \"id1\". bsegDF . write . format ( \"gemfire\" ). option ( \"region.path\" , \"bseg1\" ). option ( \"primary.key.column.name\" , \"id1\" ). option ( \"pdx.identity.fields\" , \"id1\" ). saveAsTable ( \"bsegTable\" ) Note The pdx.identity.fields specification has a huge impact on performance within GemFire since this specification informs GemFire to use only the specified field for computing hashCode and equality for PdxInstance.","title":"Expose GemFire Region Using Dataframe Based External Tables"},{"location":"connectors/gemfire_connector/#expose-gemfire-regions-as-rdds","text":"Invoking the gemfireRegion method on the SparkContext in SnappyData exposes the full data set of a GemFire region as a Spark RDD. The same API exposes both replicated and partitioned region as RDDs. scala > val rdd = sc . gemfireRegion [ String , String ]( \"gemTable1\" ) scala > rdd . foreach ( println ) ( 1 , one ) ( 3 , three ) ( 2 , two ) Note when the RDD is used, it is important to specify the correct type for both the region key and value, otherwise a ClassCastException is encountered.","title":"Expose GemFire Regions as RDDs"},{"location":"connectors/gemfire_connector/#controlling-the-schema-of-the-external-table","text":"The pdx fields (of pdx instances) or the getter methods (of the POJOs) define the schema of the external table by default. However, you can also control the schema by excluding the columns as per requirement. This is done by implementing the following trait: package io . snappydata . spark . gemfire . connector . lifecycle import org . apache . spark . sql . types . StructType trait ColumnSelector { def selectColumns ( gridName : Option [ String ], regionName : String , allColumns : StructType ): StructType } Based on the requirement, a new StructType which contains fewer columns can be returned. Note The new StructType returned can only have the subset of the columns that are passed. No new columns should be added. The ColumnSelector implemetation needs to be added to the startup classpath of the SnappyDataCluster. At the time of table creation, an option with key = column.selector and value as the fully qualified class name of the ColumnSelector implementation class should be passed.","title":"Controlling the Schema of the External Table"},{"location":"connectors/gemfire_connector/#other-optional-configuration-parameters","text":"column column max.rows.restrict This parameter is used to restrict the number of rows that are fetched from an external table, when a query of type select * from external_table is executed. This restriction, if specified, is applied to queries without any filter, limit, aggregate function, and projection such that it tends to bring all the rows from the external region. Note that this restriction is not applied in case of DDL statement such as create table X using row column as select * from external_table . The default value for is 10,000. column.selector This parameter is used to control the columns that must be included in the schema as described here . max.buckets.per.partition This parameter is used to control the concurrency and number of tasks created to fetch data from an external region. This property is useful only for Partitioned Region. For more details, refer to Controlling Task Concurrency in SnappyData When Accessing GemFire . The default value is 3. security.username and security.password By default the external table is created and queried using the credential of the current user. For example user X creates external table, and if user Y is querying, then credentials of user Y is fetched from data. But if the parameters security.username and security.password are passed then those credentials are used to create external table and for querying the data irrespective of the current user. pdxtype.maxscan This parameter determines the maximum number of entries from the region which is scanned to completely determine the schema of the external table from the Pdx instances that is stored in the region. The default value is 100.","title":"Other Optional Configuration Parameters"},{"location":"connectors/gemfire_connector/#controlling-task-concurrency-in-snappydata-when-accessing-gemfire","text":"There are two types of regions in GemFire; replicated and partitioned . All the data for a replicated region is present on every server where the region is defined. A GemFire partitioned region splits the data across the servers that define the partitioned region. When operating with replicated regions, there is only a single RDD partition representing the replicated region in SnappyData. Since there is only one data bucket for the replicated region. As compared to this, a GemFire partitioned region can be represented by a configurable number of RDD partitions in SnappyData. Choosing the number of RDD partitions directly controls the task concurrency in SnappyData when you run queries on GemFire regions. By default, the GemFire connector works out the number of buckets per GemFire server, assigns partitions to each server, and uses a default value of maximum three buckets per partition. You can configure the max.buckets.per.partition attribute to change this value. When queries are executed on an external table, the degree of parallelism in query execution is directly proportional to the number of RDD partitions that represents the table. The following example shows how to configure the RDD partitions count for an external table representing a GemFire region: import io . snappydata . spark . gemfire . connector . _ val externalBsegTable = snc . createExternalTable ( \"bsegInGem\" , \"gemfire\" , Map [ String , String ]( \"region.path\" -> \"bseg1\" , \"data.as.pdx\" -> \"true\" , \"key.class\" -> \"java.lang.Long\" , \"max.buckets.per.partition\" -> \"5\" ))","title":"Controlling Task Concurrency in SnappyData When Accessing GemFire"},{"location":"connectors/gemfire_connector/#saving-data-to-gemfire-region","text":"You can save data to GemFire Regions using any of the following : Saving Pair RDD to GemFire Region Saving Non-Pair RDD to GemFire Saving DataFrame to GemFire","title":"Saving Data to GemFire Region"},{"location":"connectors/gemfire_connector/#saving-pair-rdd-to-gemfire-region","text":"A pair RDD can be saved from SnappyData into a GemFire region as follows: Import the implicits as shown: import io . snappydata . spark . gemfire . connector In the Spark shell, create a simple pair RDD and save it to GemFire Region: scala > import io . snappydata . spark . gemfire . connector . _ scala > val data = Array (( \"1\" , \"one\" ), ( \"2\" , \"two\" ), ( \"3\" , \"three\" )) data : Array [( String , String )] = Array (( 1 , one ), ( 2 , two ), ( 3 , three )) scala > val distData = sc . parallelize ( data ) distData : org . apache . spark . rdd . RDD [( String , String )] = ParallelCollectionRDD [ 0 ] at parallelize at < console >: 14 scala > distData . saveToGemFire ( \"gemTable1\" ) 15 / 02 / 17 07 : 11 : 54 INFO DAGScheduler : Job 0 finished : runJob at GemFireRDDFunctions . scala : 29 , took 0.341288 s Verify the data is saved in GemFire using gfsh : gfsh>query --query=\"select key,value from /gemTable1.entries\" Result : true startCount : 0 endCount : 20 Rows : 3 key | value --- | ----- 1 | one 3 | three 2 | two","title":"Saving Pair RDD to GemFire Region"},{"location":"connectors/gemfire_connector/#saving-non-pair-rdd-to-gemfire","text":"Saving a non-pair RDD to GemFire requires an extra function that converts each element of RDD to a key-value pair. Here's a sample session in Spark shell: scala > import io . snappydata . spark . gemfire . connector . _ scala > val data2 = Array ( \"a\" , \"ab\" , \"abc\" ) data2 : Array [ String ] = Array ( a , ab , abc ) scala > val distData2 = sc . parallelize ( data2 ) distData2 : org . apache . spark . rdd . RDD [ String ] = ParallelCollectionRDD [ 0 ] at parallelize at < console >: 17 scala > distData2 . saveToGemFire ( \"gemTable1\" , e => ( e . length , e )) [ info 2015 / 02 / 17 12 : 43 : 21.174 PST < main > tid = 0x1 ] ... 15 / 02 / 17 12 : 43 : 21 INFO DAGScheduler : Job 0 finished : runJob at GemFireRDDFunctions . scala : 52 , took 0.251194 s Verify the result with gfsh : gfsh>query --query=\"select key,value from /gemTable1.entrySet\" Result : true startCount : 0 endCount : 20 Rows : 3 key | value --- | ----- 2 | ab 3 | abc 1 | a","title":"Saving Non-Pair RDD to GemFire"},{"location":"connectors/gemfire_connector/#saving-a-dataframe-to-gemfire","text":"To save a DataFrame, that is dataSet of row objects, into GemFire, use the following API which is available as an implicit definition. The rows of the dataframes are converted into PDX instances for storage in the GemFire's region. In the following example, it is assumed that there is a column \"id1\" present in the dataframe's schema. To specify the PDX Identity Fields for the PDX Type, use the option as (\"pdx.identity.fields\", \"Col1, Col2, Col3\") to specify one or more columns to be used as PDX Identity fields. SnappyData recommends to define the identity fields for performance during comparison of PDX Instances. import io . snappydata . spark . gemfire . connector . _ df . write . format ( \"gemfire\" ). option ( \"region.path\" , \"/region1\" ). option ( \"primary.key.column.name\" , \"id1\" ). option ( \"pdx.identity.fields\" , \"id1\" ) . save () To dynamically generate the GemFire Region's key, import the implicits and use the following API: saveToGemFire [ K ]( regionPath : String , keyExtractor : Row => K , opConf : Map [ String , String ] = Map . empty ) import io . snappydata . spark . gemfire . connector . _ df . saveToGemFire [ String ]( \"/region1\" , ( row : Row ) => ( row . getString ( 1 ) + \"_\" + row . getString ( 10 )), Map [ String , String ]( \"pdx.identity.fields\" -> \"id1, id10\" ) )","title":"Saving a DataFrame to GemFire"},{"location":"connectors/gemfire_connector/#running-oql-queries-directly-on-gemfire-from-snappydata","text":"Most applications using SnappyData will choose to run regular SQL queries on GemFire regions. Refer to Accessing Data From GemFire Additionally, you can directly execute OQL queries on GemFire regions using the GemFire connector. In scenarios where the data stored in GemFire regions is neither PDX nor Java bean compliant POJO, you can execute OQL queries and retrieve the data from the server and make it available as a data frame. An instance of SQLContext is required to run OQL query. val snc = new org . apache . spark . sql . SnappyContext ( sc ) Create a DataFrame using OQL: val dataFrame = snc . gemfireOQL ( \"SELECT iter.name,itername.address.city, iter.id FROM /personRegion iter\" ) You can repartition the DataFrame using DataFrame.repartition() if required. After you have the DataFrame , you can register it as a table and use Spark SQL to query: dataFrame . registerTempTable ( \"person\" ) val SQLResult = sqlContext . sql ( \"SELECT * FROM person WHERE id > 100\" )","title":"Running OQL queries Directly on GemFire from SnappyData"},{"location":"connectors/jdbc_streaming_connector/","text":"Getting Started with Snappy JDBC Streaming Connector \u00b6 Databases like Microsoft SQL Server provide a mechanism to capture changed data. The Change Data Capture (CDC) functionality can be used to source event streams from JDBC sources to SnappyData streaming. Once the data is ingested in SnappyData, it can be used for both real-time analysis and batch analysis. In this quickstart, we talk about how SnappyData's smart connector application can use the JDBC streaming connector to pull changed data from Microsoft SQL Server and ingest it into SnappyData tables. Start the SnappyData Cluster \u00b6 Run the sbin/snappy-start-all.sh script to start the SnappyData cluster $ <SnappyData_home>/sbin/snappy-start-all.sh Dependencies \u00b6 SnappyData core and SnappyData jdbc streaming connector maven dependencies would be needed for your application. <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-jdbc-stream-connector_2.11 </artifactId> <version> 0.9.3 </version> <scope> compile </scope> </dependency> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-core_2.11 </artifactId> <version> 1.3.1 </version> <scope> compile </scope> </dependency> Configuring the JDBC Source \u00b6 Spark(SnappyData) Streaming polls the JDBC source and reads the data that is newer than the offset last read. The offset management is done by streaming but the offset has to be defined by the JDBC source. An offset has to be a monotonically increasing column inside the source table. Following few properties of the JDBC source have to be configured. table: Table from which the data would be sourced. offsetColumn: the designated column for offset. For e.g. in SQLServer CDC tables __$start_lsn should be used. getNextOffset: function to get the next offset. This can be a JDBC function or query. offsetToStrFunc and strToOffsetFunc - functions to convert offset to and from a string. This is needed to internally manipulate an offset. offsetIncFunc - If getNextOffset is not available, a function to increment the offset In this quickstart since we are using spark-shell, use the following command which loads the required dependencies of Snappy and SQL Server. $ <SnappyData-product-home>/bin/spark-shell --master local [ * ] --conf snappydata.connection = localhost:1527 --packages com.microsoft.sqlserver:mssql-jdbc:6.1.0.jre8 --jars <SnappyData-product-home>/connectors/snappydata-jdbc-stream-connector_2.11-0.9.jar Run the following commands to create a Stream reader for JDBC source import org . apache . spark . sql . _ val snc = new SnappySession ( spark . sparkContext ); // query to read the next offset from the table // it finds the maxEvents from the last offset val getNextOffset = \"select master.dbo.fn_varbintohexstr(max(__$start_lsn)) nextLSN from \" + \" ( select __$start_lsn, sum(count(1)) over (order by __$start_lsn) runningCount from $table \" + \" where __$start_lsn > master.dbo.fn_cdc_hexstrtobin('$currentOffset') \" + \" group by __$start_lsn) x where runningCount <= $maxEvents\" // Properties needed to configure the JDBC Source val props = Map ( \"driver\" -> \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" , \"url\" -> \"jdbc:sqlserver://somehost.com:1433\" , \"user\" -> \"xxx\" , \"password\" -> \"xxx\" , \"dbtable\" -> \"tengb.cdc.dbo_customer_CT\" , // CDC table for Customer \"maxEvents\" -> \"100\" , \"jdbc.offsetColumn\" -> \"__$start_lsn\" , \"jdbc.getNextOffset\" -> getNextOffset , \"jdbc.offsetToStrFunc\" -> \"master.dbo.fn_varbintohexstr\" , \"jdbc.strToOffsetFunc\" -> \"master.dbo.fn_cdc_hexstrtobin\" , \"jdbc.offsetIncFunc\" -> \"master.dbo.fn_cdc_increment_lsn\" ) // Creates a stream reader val reader = snc . readStream . format ( \"jdbcStream\" ). options ( props ). load Configuring the Sink \u00b6 The events generated by the reader i.e. JDBC source can be sent to console sink. reader.writeStream.outputMode(\"append\").format(\"console\").start For ingesting the events in a SnappyData table, one needs to implement a SnappyStoreSink to ingest the events inside SnappyData. class Mysink extends org . apache . spark . sql . streaming . jdbc . SnappyStreamSink { def process ( snappySession : org . apache . spark . sql . SnappySession , sinkProps : java . util . Properties , batchId : Long , df : org . apache . spark . sql . Dataset [ org . apache . spark . sql . Row ]): Unit = { df . write (). format ( \"row\" ). insertInto ( \"snappytable\" ) } } return reader . writeStream () . format ( \"snappystore\" ) . option ( \"sink\" , Mysink . class . getName ()) . option ( \"checkpointLocation\" , Utils . createTempDir ( \"/data/wrk/w/snappydata/abc-temp\" , \"abc-spark\" ) Utils . createTempDir ( \"/data/wrk/w/snappydata/temp\" , \"snappy-sink\" ) . getCanonicalPath ()) . option ( \"tableName\" , tableName ) . start (); The above code needs to be in a jar though. You can find the working code for a SnappyData Smart Connector Streaming application that sources data from a JDBC source here .","title":"Using the SnappyData JDBC Streaming Connector"},{"location":"connectors/jdbc_streaming_connector/#getting-started-with-snappy-jdbc-streaming-connector","text":"Databases like Microsoft SQL Server provide a mechanism to capture changed data. The Change Data Capture (CDC) functionality can be used to source event streams from JDBC sources to SnappyData streaming. Once the data is ingested in SnappyData, it can be used for both real-time analysis and batch analysis. In this quickstart, we talk about how SnappyData's smart connector application can use the JDBC streaming connector to pull changed data from Microsoft SQL Server and ingest it into SnappyData tables.","title":"Getting Started with Snappy JDBC Streaming Connector"},{"location":"connectors/jdbc_streaming_connector/#start-the-snappydata-cluster","text":"Run the sbin/snappy-start-all.sh script to start the SnappyData cluster $ <SnappyData_home>/sbin/snappy-start-all.sh","title":"Start the SnappyData Cluster"},{"location":"connectors/jdbc_streaming_connector/#dependencies","text":"SnappyData core and SnappyData jdbc streaming connector maven dependencies would be needed for your application. <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-jdbc-stream-connector_2.11 </artifactId> <version> 0.9.3 </version> <scope> compile </scope> </dependency> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-core_2.11 </artifactId> <version> 1.3.1 </version> <scope> compile </scope> </dependency>","title":"Dependencies"},{"location":"connectors/jdbc_streaming_connector/#configuring-the-jdbc-source","text":"Spark(SnappyData) Streaming polls the JDBC source and reads the data that is newer than the offset last read. The offset management is done by streaming but the offset has to be defined by the JDBC source. An offset has to be a monotonically increasing column inside the source table. Following few properties of the JDBC source have to be configured. table: Table from which the data would be sourced. offsetColumn: the designated column for offset. For e.g. in SQLServer CDC tables __$start_lsn should be used. getNextOffset: function to get the next offset. This can be a JDBC function or query. offsetToStrFunc and strToOffsetFunc - functions to convert offset to and from a string. This is needed to internally manipulate an offset. offsetIncFunc - If getNextOffset is not available, a function to increment the offset In this quickstart since we are using spark-shell, use the following command which loads the required dependencies of Snappy and SQL Server. $ <SnappyData-product-home>/bin/spark-shell --master local [ * ] --conf snappydata.connection = localhost:1527 --packages com.microsoft.sqlserver:mssql-jdbc:6.1.0.jre8 --jars <SnappyData-product-home>/connectors/snappydata-jdbc-stream-connector_2.11-0.9.jar Run the following commands to create a Stream reader for JDBC source import org . apache . spark . sql . _ val snc = new SnappySession ( spark . sparkContext ); // query to read the next offset from the table // it finds the maxEvents from the last offset val getNextOffset = \"select master.dbo.fn_varbintohexstr(max(__$start_lsn)) nextLSN from \" + \" ( select __$start_lsn, sum(count(1)) over (order by __$start_lsn) runningCount from $table \" + \" where __$start_lsn > master.dbo.fn_cdc_hexstrtobin('$currentOffset') \" + \" group by __$start_lsn) x where runningCount <= $maxEvents\" // Properties needed to configure the JDBC Source val props = Map ( \"driver\" -> \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" , \"url\" -> \"jdbc:sqlserver://somehost.com:1433\" , \"user\" -> \"xxx\" , \"password\" -> \"xxx\" , \"dbtable\" -> \"tengb.cdc.dbo_customer_CT\" , // CDC table for Customer \"maxEvents\" -> \"100\" , \"jdbc.offsetColumn\" -> \"__$start_lsn\" , \"jdbc.getNextOffset\" -> getNextOffset , \"jdbc.offsetToStrFunc\" -> \"master.dbo.fn_varbintohexstr\" , \"jdbc.strToOffsetFunc\" -> \"master.dbo.fn_cdc_hexstrtobin\" , \"jdbc.offsetIncFunc\" -> \"master.dbo.fn_cdc_increment_lsn\" ) // Creates a stream reader val reader = snc . readStream . format ( \"jdbcStream\" ). options ( props ). load","title":"Configuring the JDBC Source"},{"location":"connectors/jdbc_streaming_connector/#configuring-the-sink","text":"The events generated by the reader i.e. JDBC source can be sent to console sink. reader.writeStream.outputMode(\"append\").format(\"console\").start For ingesting the events in a SnappyData table, one needs to implement a SnappyStoreSink to ingest the events inside SnappyData. class Mysink extends org . apache . spark . sql . streaming . jdbc . SnappyStreamSink { def process ( snappySession : org . apache . spark . sql . SnappySession , sinkProps : java . util . Properties , batchId : Long , df : org . apache . spark . sql . Dataset [ org . apache . spark . sql . Row ]): Unit = { df . write (). format ( \"row\" ). insertInto ( \"snappytable\" ) } } return reader . writeStream () . format ( \"snappystore\" ) . option ( \"sink\" , Mysink . class . getName ()) . option ( \"checkpointLocation\" , Utils . createTempDir ( \"/data/wrk/w/snappydata/abc-temp\" , \"abc-spark\" ) Utils . createTempDir ( \"/data/wrk/w/snappydata/temp\" , \"snappy-sink\" ) . getCanonicalPath ()) . option ( \"tableName\" , tableName ) . start (); The above code needs to be in a jar though. You can find the working code for a SnappyData Smart Connector Streaming application that sources data from a JDBC source here .","title":"Configuring the Sink"},{"location":"connectors/tdv/","text":"Connecting to TDV as Data Source from SnappyData \u00b6 Overview \u00b6 TIBCO Data Virtualization (TDV) is an enterprise data virtualization solution that orchestrates access to multiple and varied data sources and delivers the datasets and IT-curated data services foundation for nearly any solution. SnappyData can connect to TDV as a data source to import, process, and store data. Version and Compatibility \u00b6 SnappyData edition 1.3.1 is tested and works with TIBCO Data Virtualization 8.2.0 - 8.5.x versions. Connecting to TDV from SnappyData \u00b6 Publish the view from TDV Studio that needs to be accessed as a data source from SnappyData. Note the name of the Data Source that is associated with the view that should be published. For more information, refer to the TDV documentation. Log on to the machine where SnappyData is installed and go to the SnappyData installation directory. Make sure SnappyData is running by executing the following command: ./sbin/snappy-status-all.sh Launch the SnappyData SQL shell: ./bin/snappy Connect to the running SnappyData cluster: snappy>connect client '<host-name>:<locator port>'; For example: snappy>connect client '<localhost>:<1527>'; Deploy the JDBC jar of the TDV module: snappy>deploy jar <jar alias> '<SnappyData installation directory>/connectors/csjdbc8.jar'; For example: snappy>deploy jar dv-jar '/snappydata/snappy-connectors/tdv-connector/lib/csjdbc8.jar'; !!!Note The above jar may not be available in the SnappyData Community edition. In that case find and copy it from your TDV installation. !!!Note You should execute this command only once when you connect to the TDV cluster for the first time. Create an external table with JDBC options: CREATE external TABLE <table-name> USING jdbc OPTIONS (dbtable '<View name in TDV studio>', driver 'cs.jdbc.driver.CompositeDriver', user '<TDV user>', password '<TDV password>', url 'jdbc:compositesw:dbapi@<TDV hostname>:<Port>?domain=composite&dataSource=<TDV Data source>'); For example: CREATE external TABLE ext_tdv USING jdbc OPTIONS (dbtable 'CompositeView', driver 'cs.jdbc.driver.CompositeDriver', user 'admin', password '<TDV password>', url 'jdbc:compositesw:dbapi@tdv-host:9401?domain=composite&dataSource=DV-DS'); Run the required SQL queries on the created external table. If you install both TDV Studio and SnappyData on different machines, ensure the machine on which TDV Studio is hosted, is accessible from the machine on which SnappyData is installed. If TDV is installed on a Windows machine, ensure to create a rule in the Firewall settings. This rule should allow the inbound traffic for all the ports from the machine on which SnappyData is installed. Use the following steps to do this on Windows Server 2012 edition: Go to Windows Firewall with Advanced Security -> New rule. Provide the following details: Item Option to Select Type of rule Port Does this rule apply to TCP or UDP TCP Does this rule apply to all local ports or specific local ports Select All local ports. What action should be taken\u2026. Allow the connection When does this rule apply? Select all the options Enter the appropriate name for the rule and click on the Finish button.","title":"Connecting to TIBCO Data Virtualization (TDV)"},{"location":"connectors/tdv/#connecting-to-tdv-as-data-source-from-snappydata","text":"","title":"Connecting to TDV as Data Source from SnappyData"},{"location":"connectors/tdv/#overview","text":"TIBCO Data Virtualization (TDV) is an enterprise data virtualization solution that orchestrates access to multiple and varied data sources and delivers the datasets and IT-curated data services foundation for nearly any solution. SnappyData can connect to TDV as a data source to import, process, and store data.","title":"Overview"},{"location":"connectors/tdv/#version-and-compatibility","text":"SnappyData edition 1.3.1 is tested and works with TIBCO Data Virtualization 8.2.0 - 8.5.x versions.","title":"Version and Compatibility"},{"location":"connectors/tdv/#connecting-to-tdv-from-snappydata","text":"Publish the view from TDV Studio that needs to be accessed as a data source from SnappyData. Note the name of the Data Source that is associated with the view that should be published. For more information, refer to the TDV documentation. Log on to the machine where SnappyData is installed and go to the SnappyData installation directory. Make sure SnappyData is running by executing the following command: ./sbin/snappy-status-all.sh Launch the SnappyData SQL shell: ./bin/snappy Connect to the running SnappyData cluster: snappy>connect client '<host-name>:<locator port>'; For example: snappy>connect client '<localhost>:<1527>'; Deploy the JDBC jar of the TDV module: snappy>deploy jar <jar alias> '<SnappyData installation directory>/connectors/csjdbc8.jar'; For example: snappy>deploy jar dv-jar '/snappydata/snappy-connectors/tdv-connector/lib/csjdbc8.jar'; !!!Note The above jar may not be available in the SnappyData Community edition. In that case find and copy it from your TDV installation. !!!Note You should execute this command only once when you connect to the TDV cluster for the first time. Create an external table with JDBC options: CREATE external TABLE <table-name> USING jdbc OPTIONS (dbtable '<View name in TDV studio>', driver 'cs.jdbc.driver.CompositeDriver', user '<TDV user>', password '<TDV password>', url 'jdbc:compositesw:dbapi@<TDV hostname>:<Port>?domain=composite&dataSource=<TDV Data source>'); For example: CREATE external TABLE ext_tdv USING jdbc OPTIONS (dbtable 'CompositeView', driver 'cs.jdbc.driver.CompositeDriver', user 'admin', password '<TDV password>', url 'jdbc:compositesw:dbapi@tdv-host:9401?domain=composite&dataSource=DV-DS'); Run the required SQL queries on the created external table. If you install both TDV Studio and SnappyData on different machines, ensure the machine on which TDV Studio is hosted, is accessible from the machine on which SnappyData is installed. If TDV is installed on a Windows machine, ensure to create a rule in the Firewall settings. This rule should allow the inbound traffic for all the ports from the machine on which SnappyData is installed. Use the following steps to do this on Windows Server 2012 edition: Go to Windows Firewall with Advanced Security -> New rule. Provide the following details: Item Option to Select Type of rule Port Does this rule apply to TCP or UDP TCP Does this rule apply to all local ports or specific local ports Select All local ports. What action should be taken\u2026. Allow the connection When does this rule apply? Select all the options Enter the appropriate name for the rule and click on the Finish button.","title":"Connecting to TDV from SnappyData"},{"location":"consistency/","text":"Overview of SnappyData Distributed Transactions \u00b6 SnappyData supports transaction characteristics of isolation and atomicity. Transactions are supported using JDBC/ODBC through statements such as SET autocommit , SET Isolation , COMMIT , and ROLLBACK . Note Full distributed transactions (that is, multiple update SQL statements in one logical transaction) is currently supported only for row tables. Column tables only support single statement implicit transactions. That is, every DML (insert/update/delete) statement is executed in a implicit transaction. The DML statement can in-fact be a multi-row statement and is executed with \"all or nothing\" semantics. Transactions execution do not depend on a central locking facility and is highly scalable. SnappyData supports high concurrency for transactions. Readers (queries) do not acquire locks and isolated from concurrent transactions using an MVCC implementation. Currently, demarcated transactions (Commit, rollback) is only supported through the JDBC and ODBC API. Support for commit/rollback will be added to the Spark API will be added in a later release. Additional Information How to use Transactions Isolation Levels Best Practices for using Distributed Transactions and Snapshot Isolation","title":"Overview of SnappyData Distributed Transactions"},{"location":"consistency/#overview-of-snappydata-distributed-transactions","text":"SnappyData supports transaction characteristics of isolation and atomicity. Transactions are supported using JDBC/ODBC through statements such as SET autocommit , SET Isolation , COMMIT , and ROLLBACK . Note Full distributed transactions (that is, multiple update SQL statements in one logical transaction) is currently supported only for row tables. Column tables only support single statement implicit transactions. That is, every DML (insert/update/delete) statement is executed in a implicit transaction. The DML statement can in-fact be a multi-row statement and is executed with \"all or nothing\" semantics. Transactions execution do not depend on a central locking facility and is highly scalable. SnappyData supports high concurrency for transactions. Readers (queries) do not acquire locks and isolated from concurrent transactions using an MVCC implementation. Currently, demarcated transactions (Commit, rollback) is only supported through the JDBC and ODBC API. Support for commit/rollback will be added to the Spark API will be added in a later release. Additional Information How to use Transactions Isolation Levels Best Practices for using Distributed Transactions and Snapshot Isolation","title":"Overview of SnappyData Distributed Transactions"},{"location":"consistency/using_snapshot_isolation_column/","text":"Lock-free Queries using MVCC (multi-version concurrency control) and Snapshot Isolation for Column Tables \u00b6 Note Snapshot isolation is supported only for column tables. As the term suggests, all queries in the system operate on a snapshot view of the database. This is, even if concurrent updates are in progress, the querying system gets a non-changing view of the state of the database at the moment in time when the query is executed. The snapshot is partition wise. The snapshot of the partition is taken the moment the query accesses the partition. This behavior is set by default for column tables and cannot be modified. How the Snapshot Model Works \u00b6 SnappyData maintains a version vector for each of the table on every node. The version information for each row of the table is also maintained. When a user query is executed, a snapshot is taken of the version vector of all the tables on the node on which the query is executed. The write operation modifies the row, increments its version while still maintaining a reference to the older version. At the time of commit, the version information is published under a lock so that all the changes of an operation is published atomically. Older rows are cleaned periodically once it is made sure that there are no operations that require these older rows. The read operations compare the version of each row to the ones in its snapshot and return the row whose version is same as the snapshot. In case of failure, the versions are not published, which makes the rows invisible to any future operations. A new node joining the cluster copies all the committed rows from the existing node making sure that any snapshot will see only committed data. The following image represents the functioning of read and write operations in the Snapshot isolation model: By default, all individual operations (read/write) on column table have snapshot isolation with autocommit set to ON . This means, in case of a failure the user operation fails and rollback is triggered. You cannot set autocommit to Off . Snapshot isolation ensures that changes made, after the ongoing operation has taken a snapshot is not visible partially or totally. If there are concurrent updates in a row, then the last commit is used. Note To get per statement transactional behavior, all the write operations can span only one partition. However, if you have operations that span multiple partitions, then, ensure that: In case of failure on one partition, the operation is retried on another copy of the same partition. Set redundancy to more than 0, if transactional behavior with operations spanning more than one partition is required. If the operation fails on all the redundant copies of a partition and the same operation succeeds on some of the other partitions, then, partial rollback is initiated. In this case, you can retry the operation at the application level. Rollback Behavior and Member Failures \u00b6 In column tables, roll back is performed in case of low memory. If the operation fails due to low memory, automatic roll back is initiated. Snapshot Limitations \u00b6 The following limitations have been reported: For column tables, snapshot isolation is enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a partition is always available (redundant members are available) in the event of member failures. Write-write conflict is not detected. The last write option is applied. Multi-statement is not supported. Snapshot Isolation with SELECT FOR UPDATE \u00b6 The SELECT FOR UPDATE statement and other statements that implicitly place locks are not supported for column tables, and snapshot isolation is applied by default for updates. In case of multiple concurrent updates, the last update is applied.","title":"Lock-free Queries using MVCC and Snapshot Isolation for Column Tables"},{"location":"consistency/using_snapshot_isolation_column/#lock-free-queries-using-mvcc-multi-version-concurrency-control-and-snapshot-isolation-for-column-tables","text":"Note Snapshot isolation is supported only for column tables. As the term suggests, all queries in the system operate on a snapshot view of the database. This is, even if concurrent updates are in progress, the querying system gets a non-changing view of the state of the database at the moment in time when the query is executed. The snapshot is partition wise. The snapshot of the partition is taken the moment the query accesses the partition. This behavior is set by default for column tables and cannot be modified.","title":"Lock-free Queries using MVCC (multi-version concurrency control) and Snapshot Isolation for Column Tables"},{"location":"consistency/using_snapshot_isolation_column/#how-the-snapshot-model-works","text":"SnappyData maintains a version vector for each of the table on every node. The version information for each row of the table is also maintained. When a user query is executed, a snapshot is taken of the version vector of all the tables on the node on which the query is executed. The write operation modifies the row, increments its version while still maintaining a reference to the older version. At the time of commit, the version information is published under a lock so that all the changes of an operation is published atomically. Older rows are cleaned periodically once it is made sure that there are no operations that require these older rows. The read operations compare the version of each row to the ones in its snapshot and return the row whose version is same as the snapshot. In case of failure, the versions are not published, which makes the rows invisible to any future operations. A new node joining the cluster copies all the committed rows from the existing node making sure that any snapshot will see only committed data. The following image represents the functioning of read and write operations in the Snapshot isolation model: By default, all individual operations (read/write) on column table have snapshot isolation with autocommit set to ON . This means, in case of a failure the user operation fails and rollback is triggered. You cannot set autocommit to Off . Snapshot isolation ensures that changes made, after the ongoing operation has taken a snapshot is not visible partially or totally. If there are concurrent updates in a row, then the last commit is used. Note To get per statement transactional behavior, all the write operations can span only one partition. However, if you have operations that span multiple partitions, then, ensure that: In case of failure on one partition, the operation is retried on another copy of the same partition. Set redundancy to more than 0, if transactional behavior with operations spanning more than one partition is required. If the operation fails on all the redundant copies of a partition and the same operation succeeds on some of the other partitions, then, partial rollback is initiated. In this case, you can retry the operation at the application level.","title":"How the Snapshot Model Works"},{"location":"consistency/using_snapshot_isolation_column/#rollback-behavior-and-member-failures","text":"In column tables, roll back is performed in case of low memory. If the operation fails due to low memory, automatic roll back is initiated.","title":"Rollback Behavior and Member Failures"},{"location":"consistency/using_snapshot_isolation_column/#snapshot-limitations","text":"The following limitations have been reported: For column tables, snapshot isolation is enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a partition is always available (redundant members are available) in the event of member failures. Write-write conflict is not detected. The last write option is applied. Multi-statement is not supported.","title":"Snapshot Limitations"},{"location":"consistency/using_snapshot_isolation_column/#snapshot-isolation-with-select-for-update","text":"The SELECT FOR UPDATE statement and other statements that implicitly place locks are not supported for column tables, and snapshot isolation is applied by default for updates. In case of multiple concurrent updates, the last update is applied.","title":"Snapshot Isolation with SELECT FOR UPDATE"},{"location":"consistency/using_transactions_row/","text":"How Transactions Work for Row Tables \u00b6 Note Distributed transaction is supported only for row tables. There is no centralized transaction coordinator in SnappyData. Instead, the member on which a transaction was started acts as the coordinator for the duration of the transaction. If the application updates one or more rows, the transaction coordinator determines which owning members are involved, and acquires local \"write\" locks on all of the copies of the rows. At commit time, all changes are applied to the local store and any redundant copies. If another concurrent transaction attempts to change one of the rows, the local \"write\" acquisition fails for the row, and that transaction is automatically rolled back. Unlike traditional distributed databases, SnappyData does not use write-ahead logging for transaction recovery in case the commit fails during replication or redundant updates to one or more members. The most likely failure scenario is one where the member is unhealthy and gets forced out of the distributed system, guaranteeing the consistency of the data. When the failed member comes back online, it automatically recovers the replicated/redundant data set and establishes coherency with the other members. If all copies of some data go down before the commit is issued, then this condition is detected using the group membership system, and the transaction is rolled back automatically on all members. Note SnappyData does not support transactions is new data store members are added while in progress. If you add a new member to the cluster in the middle of a transaction and the new member is involved in the transaction (e.g. owns a partition of the data or is a replica), SnappyData implicitly rolls back the transaction and throws an SQLException (SQLState: \"X0Z05\"). The following images represent the functioning of read and write operations in the transaction model: Using Transactions for Row Tables \u00b6 Transactions specify an isolation level that defines the degree to which one transaction must be isolated from resource or data modifications made by other transactions. The transaction isolation levels define the type of locks acquired on read operations. Only one of the isolation level options can be set at a time, and it remains set for that connection until it is explicitly changed. Note If you set the isolation level to READ_COMMITTED or REPEATABLE_READ , queries on column table report an error if autocommit is set to off (false). Queries on column tables are supported when isolation level is set to READ_COMMITTED or REPEATABLE_READ and autocommit is set to true . DDL execution (for example CREATE TABLE / DROP TABLE ) is not allowed when autocommit is set to false and transaction isolation level is READ_COMMITTED or REPEATABLE_READ . DDL commands reports syntax error in such cases. DDL execution is allowed if autocommit is true for READ_COMMITTED or REPEATABLE_READ isolation levels. The following isolation levels are supported for row tables: Isolation level Description NONE Default isolation level. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads. READ_COMMITTED SnappyData ensures that ongoing transactional as well as non-transactional (isolation-level NONE) operations never read uncommitted (dirty) data. SnappyData accomplishes this by maintaining transactional changes in a separate transaction state that are applied to the actual data-store for the table only at commit time. SnappyData detects only Write-Write conflicts while in READ_COMMITTED isolation level. In READ COMMITTED, a read view is created at the start of each statement and lasts only as long as each statement execution. REPEATABLE_READ In this isolation level, a lock-based concurrency control DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. In REPEATABLE READ every lock acquired during a transaction is held for the duration of the transaction. For more information, see, SET ISOLATION Rollback Behavior and Member Failures \u00b6 Within the scope of a transaction, SnappyData automatically initiates a rollback if it encounters a constraint violation. Any errors that occur while parsing queries or while binding parameters in a SQL statement do not cause a rollback. For example, a syntax error that occurs while executing a SQL statement does not cause previous statements in the transaction to rollback. However, a column constraint violation would cause all previous SQL operations in the transaction to roll back. Handling Member Failures \u00b6 The following steps describe specific events that can occur depending on which member fails and when the failure occurs during a transaction: If the transaction coordinator member fails before a commit is fired, then each of the cohort members aborts the ongoing transaction. If a participating member fails before a commit is fired, then it is simply ignored. If the copies/replicas go to zero for certain keys, then any subsequent update operations on those keys throw an exception as in the case of non-transactional updates. If a commit is fired in this state, then the whole transaction is aborted. If the transaction coordinator fails before completing the commit process (with or without sending the commit message to all cohorts), the surviving cohorts determine the outcome of the transaction. If all of the cohorts are in the PREPARED state and successfully apply changes to the cache without any unique constraint violations, the transaction is committed on all cohorts. Otherwise, if any member reports failure or the last copy the associated rows go down during the PREPARED state, the transaction is rolled back on all cohorts. If a participating member fails before acknowledging to the client, then the transaction continues on other members without any interruption. However, if that member contains the last copy of a table or bucket, then the transaction is rolled back. The transaction coordinator might also fail while executing a rollback operation. In this case, the client would see such a failure as an SQLState error. If the client was performing a SELECT statement in a transaction, the member failure would result in SQLState error X0Z01:: ERROR X0Z01: Node 'node-name' went down or data no longer available while iterating the results (method 'rollback()'). Please retry the operation. Clients that were performing a DML statement in the context of a transaction would fail with one of the SQLState errors: X0Z05, X0Z16, 40XD2, or 40XD0. Note Outside the scope of a transaction, a DML statement would not see an exception due to a member failure. Instead, the statement would be automatically retried on another SnappyData member. However, SELECT statements would receive the X0Z01 statement even outside of a transaction. If this type of failure occurs, the remaining members of the SnappyData distributed system clean-up the open transactions for the failed node, and no additional steps are needed to recover from the failure. Note In this release of SnappyData, a transaction fails if any of the cohorts depart abnormally. Other Rollback Scenarios \u00b6 SnappyData may cancel an executing statement due to low memory, a timeout, or a manual request to cancel the statement. If a statement that is being executed within the context of a transaction is canceled due to low memory or a manual cancellation request, then SnappyData rolls back the associated transaction. Note SnappyData does not roll back a transaction if a statement is canceled due to a timeout. Transaction Functionality and Limitations \u00b6 In this release of SnappyData, the scope for transactional functionality is: The result set that is obtained from executing a query should either be completely consumed, or the result set is explicitly closed. Otherwise, DDL operations wait until the ResultSet is garbage-collected. Transactions for persistent tables are enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a row is always available (redundant members are available) in the event of member failures. SQL statements that implicitly place locks, such as select for update , are not supported outside of transactions (default isolation level). The supported isolation levels are 'READ COMMITTED' and 'READ UNCOMMITTED' where both behave as 'READ COMMITTED.' Autocommit is OFF by default in SnappyData, unlike in other JDBC drivers. Transactions always do \"write-write\" conflict detection at operation or commit time. Applications do not need to use select for update or explicit locking to get this behavior, as compared to other databases. ( select for update is not supported outside of a transaction.) Nested transactions and savepoints are not supported. SnappyData does not support transactions on partitioned tables that are configured with the DESTROY evict action. This restriction exists because the requirements of ACID transactions can conflict with the semantics of destroying evicted entries. For example, a transaction may need to update a number of entries that is greater than the amount allowed by the eviction setting. Transactions are supported with the OVERFLOW evict action because the required entries can be loaded into memory as necessary to support transaction semantics. SnappyData does not restrict concurrent non-transactional clients from updating tables that may be involved in transactions. This is by design, to maintain very high performance when no transactions are in use. If an application uses transactions on a table, make sure the application consistently uses transactions when updating that table. All DML on a single row is atomic in nature inside or outside of transactions. There is a small window during a commit when the committed set is being applied to the underlying table and concurrent readers, which do not consult any transactional state, have visibility to the partially-committed state. The larger the transaction, the larger the window. Also, transaction state is maintained in a memory-based buffer. The shorter and smaller the transaction, the less likely the transaction manager will run short on memory. Transactions with SELECT FOR UPDATE \u00b6 The SELECT FOR UPDATE statement and other statements that implicitly place locks are not supported outside of a transaction (default isolation level). A SELECT FOR UPDATE begins by obtaining a read lock, which allows other transactions to possibly obtain read locks on the same data. A transaction's read lock is immediately upgraded to an exclusive write lock after a row is qualified for the SELECT FOR UPDATE statement. At this point, any other transactions that obtained a read lock on the data receive a conflict exception and can roll back and release their locks. The transaction that has the exclusive lock can successfully commit only after all other read locks on the table have been released. In some cases, it is possible for one transaction to obtain an exclusive lock for data on one SnappyData member, while another transaction obtains an exclusive lock on a different member. In this case, both transactions will fail during the commit.","title":"How Transactions Work for Row Tables"},{"location":"consistency/using_transactions_row/#how-transactions-work-for-row-tables","text":"Note Distributed transaction is supported only for row tables. There is no centralized transaction coordinator in SnappyData. Instead, the member on which a transaction was started acts as the coordinator for the duration of the transaction. If the application updates one or more rows, the transaction coordinator determines which owning members are involved, and acquires local \"write\" locks on all of the copies of the rows. At commit time, all changes are applied to the local store and any redundant copies. If another concurrent transaction attempts to change one of the rows, the local \"write\" acquisition fails for the row, and that transaction is automatically rolled back. Unlike traditional distributed databases, SnappyData does not use write-ahead logging for transaction recovery in case the commit fails during replication or redundant updates to one or more members. The most likely failure scenario is one where the member is unhealthy and gets forced out of the distributed system, guaranteeing the consistency of the data. When the failed member comes back online, it automatically recovers the replicated/redundant data set and establishes coherency with the other members. If all copies of some data go down before the commit is issued, then this condition is detected using the group membership system, and the transaction is rolled back automatically on all members. Note SnappyData does not support transactions is new data store members are added while in progress. If you add a new member to the cluster in the middle of a transaction and the new member is involved in the transaction (e.g. owns a partition of the data or is a replica), SnappyData implicitly rolls back the transaction and throws an SQLException (SQLState: \"X0Z05\"). The following images represent the functioning of read and write operations in the transaction model:","title":"How Transactions Work for Row Tables"},{"location":"consistency/using_transactions_row/#using-transactions-for-row-tables","text":"Transactions specify an isolation level that defines the degree to which one transaction must be isolated from resource or data modifications made by other transactions. The transaction isolation levels define the type of locks acquired on read operations. Only one of the isolation level options can be set at a time, and it remains set for that connection until it is explicitly changed. Note If you set the isolation level to READ_COMMITTED or REPEATABLE_READ , queries on column table report an error if autocommit is set to off (false). Queries on column tables are supported when isolation level is set to READ_COMMITTED or REPEATABLE_READ and autocommit is set to true . DDL execution (for example CREATE TABLE / DROP TABLE ) is not allowed when autocommit is set to false and transaction isolation level is READ_COMMITTED or REPEATABLE_READ . DDL commands reports syntax error in such cases. DDL execution is allowed if autocommit is true for READ_COMMITTED or REPEATABLE_READ isolation levels. The following isolation levels are supported for row tables: Isolation level Description NONE Default isolation level. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads. READ_COMMITTED SnappyData ensures that ongoing transactional as well as non-transactional (isolation-level NONE) operations never read uncommitted (dirty) data. SnappyData accomplishes this by maintaining transactional changes in a separate transaction state that are applied to the actual data-store for the table only at commit time. SnappyData detects only Write-Write conflicts while in READ_COMMITTED isolation level. In READ COMMITTED, a read view is created at the start of each statement and lasts only as long as each statement execution. REPEATABLE_READ In this isolation level, a lock-based concurrency control DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. In REPEATABLE READ every lock acquired during a transaction is held for the duration of the transaction. For more information, see, SET ISOLATION","title":"Using Transactions for Row Tables"},{"location":"consistency/using_transactions_row/#rollback-behavior-and-member-failures","text":"Within the scope of a transaction, SnappyData automatically initiates a rollback if it encounters a constraint violation. Any errors that occur while parsing queries or while binding parameters in a SQL statement do not cause a rollback. For example, a syntax error that occurs while executing a SQL statement does not cause previous statements in the transaction to rollback. However, a column constraint violation would cause all previous SQL operations in the transaction to roll back.","title":"Rollback Behavior and Member Failures"},{"location":"consistency/using_transactions_row/#handling-member-failures","text":"The following steps describe specific events that can occur depending on which member fails and when the failure occurs during a transaction: If the transaction coordinator member fails before a commit is fired, then each of the cohort members aborts the ongoing transaction. If a participating member fails before a commit is fired, then it is simply ignored. If the copies/replicas go to zero for certain keys, then any subsequent update operations on those keys throw an exception as in the case of non-transactional updates. If a commit is fired in this state, then the whole transaction is aborted. If the transaction coordinator fails before completing the commit process (with or without sending the commit message to all cohorts), the surviving cohorts determine the outcome of the transaction. If all of the cohorts are in the PREPARED state and successfully apply changes to the cache without any unique constraint violations, the transaction is committed on all cohorts. Otherwise, if any member reports failure or the last copy the associated rows go down during the PREPARED state, the transaction is rolled back on all cohorts. If a participating member fails before acknowledging to the client, then the transaction continues on other members without any interruption. However, if that member contains the last copy of a table or bucket, then the transaction is rolled back. The transaction coordinator might also fail while executing a rollback operation. In this case, the client would see such a failure as an SQLState error. If the client was performing a SELECT statement in a transaction, the member failure would result in SQLState error X0Z01:: ERROR X0Z01: Node 'node-name' went down or data no longer available while iterating the results (method 'rollback()'). Please retry the operation. Clients that were performing a DML statement in the context of a transaction would fail with one of the SQLState errors: X0Z05, X0Z16, 40XD2, or 40XD0. Note Outside the scope of a transaction, a DML statement would not see an exception due to a member failure. Instead, the statement would be automatically retried on another SnappyData member. However, SELECT statements would receive the X0Z01 statement even outside of a transaction. If this type of failure occurs, the remaining members of the SnappyData distributed system clean-up the open transactions for the failed node, and no additional steps are needed to recover from the failure. Note In this release of SnappyData, a transaction fails if any of the cohorts depart abnormally.","title":"Handling Member Failures"},{"location":"consistency/using_transactions_row/#other-rollback-scenarios","text":"SnappyData may cancel an executing statement due to low memory, a timeout, or a manual request to cancel the statement. If a statement that is being executed within the context of a transaction is canceled due to low memory or a manual cancellation request, then SnappyData rolls back the associated transaction. Note SnappyData does not roll back a transaction if a statement is canceled due to a timeout.","title":"Other Rollback Scenarios"},{"location":"consistency/using_transactions_row/#transaction-functionality-and-limitations","text":"In this release of SnappyData, the scope for transactional functionality is: The result set that is obtained from executing a query should either be completely consumed, or the result set is explicitly closed. Otherwise, DDL operations wait until the ResultSet is garbage-collected. Transactions for persistent tables are enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a row is always available (redundant members are available) in the event of member failures. SQL statements that implicitly place locks, such as select for update , are not supported outside of transactions (default isolation level). The supported isolation levels are 'READ COMMITTED' and 'READ UNCOMMITTED' where both behave as 'READ COMMITTED.' Autocommit is OFF by default in SnappyData, unlike in other JDBC drivers. Transactions always do \"write-write\" conflict detection at operation or commit time. Applications do not need to use select for update or explicit locking to get this behavior, as compared to other databases. ( select for update is not supported outside of a transaction.) Nested transactions and savepoints are not supported. SnappyData does not support transactions on partitioned tables that are configured with the DESTROY evict action. This restriction exists because the requirements of ACID transactions can conflict with the semantics of destroying evicted entries. For example, a transaction may need to update a number of entries that is greater than the amount allowed by the eviction setting. Transactions are supported with the OVERFLOW evict action because the required entries can be loaded into memory as necessary to support transaction semantics. SnappyData does not restrict concurrent non-transactional clients from updating tables that may be involved in transactions. This is by design, to maintain very high performance when no transactions are in use. If an application uses transactions on a table, make sure the application consistently uses transactions when updating that table. All DML on a single row is atomic in nature inside or outside of transactions. There is a small window during a commit when the committed set is being applied to the underlying table and concurrent readers, which do not consult any transactional state, have visibility to the partially-committed state. The larger the transaction, the larger the window. Also, transaction state is maintained in a memory-based buffer. The shorter and smaller the transaction, the less likely the transaction manager will run short on memory.","title":"Transaction Functionality and Limitations"},{"location":"consistency/using_transactions_row/#transactions-with-select-for-update","text":"The SELECT FOR UPDATE statement and other statements that implicitly place locks are not supported outside of a transaction (default isolation level). A SELECT FOR UPDATE begins by obtaining a read lock, which allows other transactions to possibly obtain read locks on the same data. A transaction's read lock is immediately upgraded to an exclusive write lock after a row is qualified for the SELECT FOR UPDATE statement. At this point, any other transactions that obtained a read lock on the data receive a conflict exception and can roll back and release their locks. The transaction that has the exclusive lock can successfully commit only after all other read locks on the table have been released. In some cases, it is possible for one transaction to obtain an exclusive lock for data on one SnappyData member, while another transaction obtains an exclusive lock on a different member. In this case, both transactions will fail during the commit.","title":"Transactions with SELECT FOR UPDATE"},{"location":"data/data_formats/","text":"Supported Data Formats \u00b6 SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. By integrating the loading mechanism with the Query engine (Catalyst optimizer) it is often possible to push down filters and projections all the way to the data source minimizing data transfer. Here is the list of important features: Support for many Sources There is built-in support for many data sources as well as data formats. Data can be accessed from S3, file system, HDFS, Hive, RDB, etc. And the loaders have built-in support to handle CSV, Parquet, ORC, Avro, JSON, XML, text etc as the data formats. Access virtually any modern data store Virtually all major data providers have a native Spark connector that complies with the Data Sources API. For e.g. you can load data from any RDB like Amazon Redshift, Cassandra, Redis, Elastic Search, Neo4J, etc. While these connectors are not built-in, you can easily deploy these connectors as dependencies into a SnappyData cluster. All the connectors are typically registered in spark-packages.org Avoid Schema wrangling Spark supports schema inference. Which means, all you need to do is point to the external source in your 'create table' DDL (or Spark SQL API) and schema definition is learned by reading in the data. There is no need to explicitly define each column and type. This is extremely useful when dealing with disparate, complex and wide data sets. Read nested, sparse data sets When data is accessed from a source, the schema inference occurs by not just reading a header but often by reading the entire data set. For instance, when reading JSON files the structure could change from document to document. The inference engine builds up the schema as it reads each record and keeps unioning them to create a unified schema. This approach allows developers to become very productive with disparate data sets. Load using Spark API or SQL You can use SQL to point to any data source or use the native Spark Scala/Java API to load. For instance, you can first create an external table . CREATE EXTERNAL TABLE < tablename > USING < any - data - source - supported > OPTIONS < options > Next, use it in any SQL query or DDL. For example, CREATE EXTERNAL TABLE STAGING_CUSTOMER USING parquet OPTIONS ( path 'quickstart/src/main/resources/customerparquet' ) CREATE TABLE CUSTOMER USING column OPTIONS ( buckets '8' ) AS ( SELECT * FROM STAGING_CUSTOMER ) The following data formats are supported in SnappyData: CSV Parquet ORC AVRO JSON Multiline JSON XML TEXT The following table provides information about the supported data formats along with the methods to create an external table using SQL as well as API: Format SQL API CSV create external table staging_csv using csv options (path '<csv_file_path>' , delimiter ',', header 'true'); create table csv_test using column as select * from staging_csv; val extCSVDF = snappy.createExternalTable(\"csvTable_ext\",\"csv\", Map(\"path\"-> \" <csv_file_path> \" ,\"header\" -> \"true\", \"inferSchema\"->\"true\"),false) snappy.createTable(\"csvTable\", \"column\", extCSVDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extCSVDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"csvTable\") Parquet create external table staging_parquet using parquet options (path ' <parquet_path> '); create table parquet_test using column as select * from staging_parquet; val extParquetDF = snappy.createExternalTable(\"parquetTable_ext\",\"Parquet\", Map(\"path\"->\" <parquet_file_path> \"),false) snappy.createTable(\"parquetTable\", \"column\",extParquetDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extParquetDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"parquetTable\"); ORC create external table staging_orc using orc options (path '<orc_path>' ); create table orc_test using column as select * from staging_orc; AVRO create external table staging_avro using com.databricks.spark.avro options (path '<avro_file_path>' ); create table avro_test using column as select * from staging_avro; val extAvroDF = snappy.createExternalTable(\"avroTable_ext\",\"com.databricks.spark.avro\", Map(\"path\"->\" <avro_file_path> \"),false) snappy.createTable(\"avroTable\", \"column\", extAvroDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extAvroDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"avroTable\"); JSON create external table staging_json using json options (path '<json_file_path>' ); create table json_test using column as select * from staging_json; val extJsonDF = snappy.createExternalTable(\"jsonTable_ext\",\"json\", Map(\"path\"-> \" <json_file_path> \"),false) snappy.createTable(\"jsonTable\", \"column\", extJsonDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extJsonDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"jsonTable\"); Multiline JSON create external table staging_json_multiline using json options (path ' <json_file_path> ', wholeFile 'true'); create table json_test using column as select * from staging_json_multiline; val extJsonMultiLineDF = snappy.createExternalTable(\"jsonTableMultiLine_ext\",\"json\", Map(\"path\"-> \" <json_file_path> \",\"wholeFile\" -> \"true\"),false) snappy.createTable(\"jsonTableMultiLine\", \"column\", extJsonMultiLineDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extJsonMultiLineDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"jsonTableMultiLine\"); XML create external table staging_xml using xml options (rowTag ' <rowTag> ',path ' <xml-file-path> '); create table xml_test using column as select * from staging_xml; val extXmlDF = snappy.createExternalTable(\"xmlTable_ext\",\"xml\", Map(\"path\"-> \" <xml-file-path> \",\"rowTag\" -> \" <rowTag> \"),false) snappy.createTable(\"xmlTable\", \"column\", extXmlDF.schema, Map(\"buckets\"->\"9\"), false) ;import org.apache.spark.sql.SaveMode; extXmlDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"xmlTable\"); TEXT create external table staging_text using text options (path ' <text_file_path> '); create table text_test using column as select * from staging_text;","title":"Supported Data Formats"},{"location":"data/data_formats/#supported-data-formats","text":"SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. By integrating the loading mechanism with the Query engine (Catalyst optimizer) it is often possible to push down filters and projections all the way to the data source minimizing data transfer. Here is the list of important features: Support for many Sources There is built-in support for many data sources as well as data formats. Data can be accessed from S3, file system, HDFS, Hive, RDB, etc. And the loaders have built-in support to handle CSV, Parquet, ORC, Avro, JSON, XML, text etc as the data formats. Access virtually any modern data store Virtually all major data providers have a native Spark connector that complies with the Data Sources API. For e.g. you can load data from any RDB like Amazon Redshift, Cassandra, Redis, Elastic Search, Neo4J, etc. While these connectors are not built-in, you can easily deploy these connectors as dependencies into a SnappyData cluster. All the connectors are typically registered in spark-packages.org Avoid Schema wrangling Spark supports schema inference. Which means, all you need to do is point to the external source in your 'create table' DDL (or Spark SQL API) and schema definition is learned by reading in the data. There is no need to explicitly define each column and type. This is extremely useful when dealing with disparate, complex and wide data sets. Read nested, sparse data sets When data is accessed from a source, the schema inference occurs by not just reading a header but often by reading the entire data set. For instance, when reading JSON files the structure could change from document to document. The inference engine builds up the schema as it reads each record and keeps unioning them to create a unified schema. This approach allows developers to become very productive with disparate data sets. Load using Spark API or SQL You can use SQL to point to any data source or use the native Spark Scala/Java API to load. For instance, you can first create an external table . CREATE EXTERNAL TABLE < tablename > USING < any - data - source - supported > OPTIONS < options > Next, use it in any SQL query or DDL. For example, CREATE EXTERNAL TABLE STAGING_CUSTOMER USING parquet OPTIONS ( path 'quickstart/src/main/resources/customerparquet' ) CREATE TABLE CUSTOMER USING column OPTIONS ( buckets '8' ) AS ( SELECT * FROM STAGING_CUSTOMER ) The following data formats are supported in SnappyData: CSV Parquet ORC AVRO JSON Multiline JSON XML TEXT The following table provides information about the supported data formats along with the methods to create an external table using SQL as well as API: Format SQL API CSV create external table staging_csv using csv options (path '<csv_file_path>' , delimiter ',', header 'true'); create table csv_test using column as select * from staging_csv; val extCSVDF = snappy.createExternalTable(\"csvTable_ext\",\"csv\", Map(\"path\"-> \" <csv_file_path> \" ,\"header\" -> \"true\", \"inferSchema\"->\"true\"),false) snappy.createTable(\"csvTable\", \"column\", extCSVDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extCSVDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"csvTable\") Parquet create external table staging_parquet using parquet options (path ' <parquet_path> '); create table parquet_test using column as select * from staging_parquet; val extParquetDF = snappy.createExternalTable(\"parquetTable_ext\",\"Parquet\", Map(\"path\"->\" <parquet_file_path> \"),false) snappy.createTable(\"parquetTable\", \"column\",extParquetDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extParquetDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"parquetTable\"); ORC create external table staging_orc using orc options (path '<orc_path>' ); create table orc_test using column as select * from staging_orc; AVRO create external table staging_avro using com.databricks.spark.avro options (path '<avro_file_path>' ); create table avro_test using column as select * from staging_avro; val extAvroDF = snappy.createExternalTable(\"avroTable_ext\",\"com.databricks.spark.avro\", Map(\"path\"->\" <avro_file_path> \"),false) snappy.createTable(\"avroTable\", \"column\", extAvroDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extAvroDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"avroTable\"); JSON create external table staging_json using json options (path '<json_file_path>' ); create table json_test using column as select * from staging_json; val extJsonDF = snappy.createExternalTable(\"jsonTable_ext\",\"json\", Map(\"path\"-> \" <json_file_path> \"),false) snappy.createTable(\"jsonTable\", \"column\", extJsonDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extJsonDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"jsonTable\"); Multiline JSON create external table staging_json_multiline using json options (path ' <json_file_path> ', wholeFile 'true'); create table json_test using column as select * from staging_json_multiline; val extJsonMultiLineDF = snappy.createExternalTable(\"jsonTableMultiLine_ext\",\"json\", Map(\"path\"-> \" <json_file_path> \",\"wholeFile\" -> \"true\"),false) snappy.createTable(\"jsonTableMultiLine\", \"column\", extJsonMultiLineDF.schema, Map(\"buckets\"->\"9\"), false); import org.apache.spark.sql.SaveMode; extJsonMultiLineDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"jsonTableMultiLine\"); XML create external table staging_xml using xml options (rowTag ' <rowTag> ',path ' <xml-file-path> '); create table xml_test using column as select * from staging_xml; val extXmlDF = snappy.createExternalTable(\"xmlTable_ext\",\"xml\", Map(\"path\"-> \" <xml-file-path> \",\"rowTag\" -> \" <rowTag> \"),false) snappy.createTable(\"xmlTable\", \"column\", extXmlDF.schema, Map(\"buckets\"->\"9\"), false) ;import org.apache.spark.sql.SaveMode; extXmlDF.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"xmlTable\"); TEXT create external table staging_text using text options (path ' <text_file_path> '); create table text_test using column as select * from staging_text;","title":"Supported Data Formats"},{"location":"data/external_hive_support/","text":"External Hive Metastore Support \u00b6 Overview \u00b6 Hive Metastore is the central repository of Apache Hive metadata. It contains metadata (column names, data types, partitions, comments, etc.) of the objects that you create in Apache Hive. All Apache Hive implementations require a Hive service. Hive Service includes three processes; HiveServer2 (includes compiler and execution engine), MetaStore, and WebHCat. The Thrift interfaces include Drivers , which are processes that interpret the query. You can implement the Hive Metastore using the tables in a relational database such as MySQL. By default, Apache Hive uses a built-in Derby SQL server for this purpose. For running Apache Hive on a production cluster, you require MySQL or any other similar relational database. Apache Hive Metastore service supports the following databases. You can change the relational database to use any of the following supported databases: Derby MySQL MS SQL Server Oracle Postgres SnappyData can connect to the Hive Metastore in any of the following modes: Local Mode \u00b6 In this mode, the Hive Metastore service runs in the same process as that of the main HiveServer process, but the Metastore database runs in a separate process. It can be either on the same machine or a different machine. The Metastore service communicates with the Metastore database over JDBC. External processes or applications cannot connect to the Metastore service directly. Only Hive clients have this privilege as both the Hive and the Metastore service are running in the same JVM process. Remote Mode \u00b6 In this mode, the Hive Metastore service and the related Metastore database run in their own processes. External processes and applications can communicate with the Metastore service using the Thrift network API. The Metastore service communicates with the Metastore database over JDBC. The database, the HiveServer process, and the Metastore service can all be on the same machine, but running the HiveServer process on a separate machine provides better availability and scalability. How Spark Connects to External Hive Metastore \u00b6 Spark SQL uses a Hive Metastore to manage the metadata of persistent relational entities (for example, databases, tables, columns, partitions) in a relational database for faster access. By default, Spark SQL uses the embedded deployment mode of a Hive Metastore with an Apache Derby database. When you create a SparkSession with Hive support, the external catalog (aka Metastore) is HiveExternalCatalog . In Spark, the HiveExternalCatalog uses the spark.sql.warehouse.dir directory to locate the databases. Moreover, you can use the following javax.jdo.option properties to connect to the Hive Metastore database: Item Description javax.jdo.option.ConnectionURL The JDBC connection URL of a Hive Metastore database to use: jdbc:derby:;databaseName=metastore_db;create=true jdbc:derby:memory:;databaseName=${metastoreLocation.getAbsolutePath};create=true jdbc:mysql://192.168.175.160:3306/metastore?useSSL=false javax.jdo.option.ConnectionDriverName The JDBC driver of a Hive Metastore database to use: org.apache.derby.jdbc.EmbeddedDriver javax.jdo.option.ConnectionUserName The user name to use to connect to a Hive Metastore database. javax.jdo.option.ConnectionPassword The password to use to connect to a Hive Metastore database. From a SnappyData cluster, you can access and query the Hive tables using a Snappy Session. After establishing a connection with the Metastore, you can do the following: Import Hive tables defined in an external Hive catalog into SnappyData catalog tables. Use Hive tables as external tables for queries, including joins with tables defined in the SnappyData catalog. Query the Hive tables. Define new Hive tables in an external Hive catalog. Spark optimizations for Hive tables are also available through Snappy Session. You can access Hive tables created in the following ecosystems: Apache Hive AWS Azure Cloudera How SnappyData Connects to Hive Metastore \u00b6 The hive-site.xml configuration file provides information about the Hive configurations to SnappyData. You must copy this file into the SnappyData conf folder, configure the hive-site.xml with the required settings, and copy the appropriate JDBC driver into the SnappyData classpath. This establishes the connection between the SnappyData cluster and the Hive Metastore. After you have established the connection, you can access hive tables and run queries on it from the SnappyData cluster. For using Hive Metastore in SnappyData, do the following: Copy the hive-site.xml from <Hive installation folder> / conf directory to <TCDB installation folder> /conf folder or use the trimmed version of hive-site.xml . If you are using MySQL as the Metastore database, copy the mysql-connector jar file (for example, mysql-connector-java-5.1.48.jar ) from <Hive installation folder> /lib to the SnappyData classpath, that is <SnappyData installation folder> /jars/ . For any other Metastore database, copy the relevant connector jars to the SnappyData jars folder. Example - Trimmed Version of hive-site.xml \u00b6 <configuration> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:mysql://dev11:3306/metastore_db?createDatabaseIfNotExist=true </value> <description> JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide a database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. </description> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> hive </value> <description> Username to use against metastore database </description> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> Snappy!23 </value> <description> password to use against metastore database </description> </property> </configuration> List of Ecosystem Connectors \u00b6 The following connectors are tested with the corresponding ecosytems for SnappyData: Ecosystem Connector Example Apache Hive MySQL mysql-connector-java-5.1.48.jar Azure HDInsight MSSQL mssql-jdbc-7.0.0.jre8.jar AWS EMR MySQL mysql-connector-java-5.1.12.jar Cloudera MySQL mysql-connector-java-5.1.48.jar Accessing Hive Tables from SnappyData Cluster \u00b6 After configuring the settings to connect the SnappyData cluster to an external Metastore, set the property spark.sql.catalogImplementation = hive . You can set this property only at the session-level. After setting this property, you can run queries on the Hive table, perform the join operations between Hive and SnappyData native tables, perform the join operations between Hive tables. If you set the property spark.sql.catalogImplementation = in-memory , you cannot access the Hive tables. SnappyData acts as the execution engine for Hive table query execution. You can refer the following examples to access and query the Hive tables: To point to the external Hive catalog from the snappy session, set the following property. This can be set only at session level and not at global level. snappy-sql> set spark.sql.catalogImplementation=hive; To point to SnappyData internal catalog from the snappy session. snappy-sql> set spark.sql.catalogImplementation=in-memory; To access Hive tables use the following command: snappy-sql> show tables in <database name>; It is mandatory to specify the Hive database to access the Hive tables from Snappy Session. If you have not created any other database, then you can use the default database. For example, in Hive, if you create database hiveDB , use the following command: snappy - sql > show tables in hiveDB ; To read the Hive tables from snappy: snappy - sql > SELECT FirstName , LastName FROM default . hive employees ORDER BY LastName ; To join SnappyData tables and Hive tables: snappy - sql > SELECT emp . EmployeeID , emp . FirstName , emp . LastName , o . OrderID , o . OrderDate FROM default . hive_employees emp JOIN snappy_orders o ON ( emp . EmployeeID = o . EmployeeID ) ORDER BY o . OrderDate ; Creating and Querying Hive Tables \u00b6 To create or access, the hive tables, you must first create a database or schema in the external hive catalog from SnappyData. Else, the default database default is used. Go to the Snappy Shell prompt and enter the following command. Note that a DDL will only be executed against the external hive if it uses hive-specific extensions in the CREATE TABLE like \"row format\" shown below. Or else when \"using hive\" is specifically mentioned after the table schema. In other words, it is not possible to create Spark format EXTERNAL tables (e.g. a parquet table) in the external hive metastore. snappy > create database hivedb ; After creating the database or schema, you can create hive tables, load data into the hive tables, and run DDL queries on hive tables from SnappyData using one of the following methods: Method 1 \u00b6 In this method, you must use the schema or database name before the table name as shown in the following example: snappy > create table hivedb . hive_regions ( RegionID int , RegionDescription string ) row format delimited fields terminated by ',' tblproperties ( \"skip.header.line.count\" = \"1\" ); snappy > load data local inpath '/export/shared/QA_DATA/NW_1GB/regions.csv' overwrite into table hivedb . hive_regions ; snappy > select * from hivedb . hive_regions where RegionDescription <> 'RegionDescription' ; RegionID | RegionDescription -------------------------------------------------------------------------------------------------------------------------------------------- 1 | Eastern 2 | Western 3 | Northern 4 | Southern 4 rows selected Method 2 \u00b6 In this method, you must first run the command use <database-name> as shown in the following example. If you use this method you need not mention the schema or database name. snappy > use hivedb ; snappy > create table hive_employee_territories ( EmployeeID int , TerritoryID string ) row format delimited fields terminated by ',' ; snappy > load data local inpath '/export/shared/QA_DATA/NW_1GB/employee_territories.csv' overwrite into table hive_employee_territories ; snappy > select * from hive_employee_territories limit 10 ; EmployeeID | TerritoryID -------------------------------------------------------------------------------------------------------------------------------------------- NULL | TerritoryID 1 | 06897 1 | 19713 2 | 01581 2 | 01730 2 | 01833 2 | 02116 2 | 02139 2 | 02184 2 | 40222 10 rows selected snappy > drop table hive_employee_territories ; Creating and Querying SnappyData Tables \u00b6 After the hive catalog is visible, you can create and access the SnappyData tables as well. For accessing the tables, the following two methods are available. Method 1 \u00b6 Run the command set spark.sql.catalogImplementation=in-memory . After this run the command use <database-name> or use schema / <database name used while creating the table> . After above two steps, the external hive catalog is not visible from SnappyData and hence all the tables that are created and accessed are SnappyData tables only. The following examples demonstrates how to create and access SnappyData tables from an external hive catalog. snappy > set spark . sql . catalogImplementation = in - memory ; snappy > use app ; snappy > create table t1 ( id int ) using column ; snappy > insert into t1 select id from range ( 1000 ); 1000 rows inserted / updated / deleted snappy > select * from t1 order by id limit 10 ; id ----------- 0 1 2 3 4 5 6 7 8 9 10 rows selected snappy > show tables in app ; schemaName | tableName | isTemporary -----------------------------------------------------------------------------------------------------------------------------------------------------------------app |t1 |false 1 row selected snappy > drop table t1 ; snappy> set spark.sql.catalogImplementation=in-memory; snappy> create table app.t1(id int) using column; snappy> insert into app.t1 select id from range(10); 10 rows inserted/updated/deleted snappy> select * from app.t1; id ----------- 4 2 6 1 3 0 7 5 9 8 10 rows selected snappy> drop table app.t1; snappy> show tables in app; schemaName |tableName |isTemporary ----------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 rows selected Method 2 \u00b6 If you want to create and access the SnappyData tables, while keeping the hive catalog visible, then use the <schemaname>.<tablename> in the DDL / DML statements as shown in following examples: In case you have run the command spark.sql.catalogImplementation=in-memory , then you can run the command that points to any hive catalog database. For Example: use hivedb; snappy > set spark . sql . catalogImplementation = hive ; snappy > create table app . t1 ( id int , name String ) using column ; snappy > show tables in app ; schemaName | tableName | isTemporary ------------------------------------------------------------------------------------------------------------------------ app | t1 | false 1 row selected snappy > insert into app . t1 select id , concat ( 'TIBCO_' , id ) from range ( 1000 ); 1000 rows inserted / updated / deleted snappy > select * from app . t1 where id > 995 order by id ; id | name 996 | TIBCO_996 997 | TIBCO_997 998 | TIBCO_998 999 | TIBCO_999 4 rows selected snappy > drop table app . t1 ; If you want to join the hive table and SnappyData table, then use the <schemaname>.<tablename> in the join query for both the hive table as well as for the SnappyData table. Example snappy > SELECT emp . EmployeeID , emp . FirstName , emp . LastName , o . OrderID , o . OrderDate FROM default . hive_employees emp JOIN app . snappy_orders o ON ( emp . EmployeeID = o . EmployeeID ) ORDER BY o . OrderID ; Troubleshooting \u00b6 Following are some issues that you may come across and their corresponding solutions / workarounds: If the property hive.execution.engine is set as Tez then the following exception is shown: java.lang.RuntimeException: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveExternalCatalog' This exception can occur when you have copied the hive-site.xml file to Conf folder and if you have provided the value of the xml property hive.execution.engine as Tez. This exception is caused by java.lang.ClassNotFoundException: org.apache.tez.dag.api.SessionNotRunning Workaround: In case you have copied all the contents from the hive-site.xml file to the SnappyData conf file, ensure that the value of the XML property hive.execution.engine is left empty. Unable to open a test connection to the given database. JDBC url = jdbc:mysql://dev11:3306/metastore_db?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app This error occurs when the MySQL process (Metadata server process) does not get started. Workaround : Ensure to start the Metastore service before connecting to Hive Beeline CLI. When the TIBCO Compute DB cluster, Hive, MySQL (Metadata Server) are running, and MySQL (Metadata Server) process stops abruptly, then Snappy shell becomes unresponsive. While accessing the Hive table, if you do not set the parameter javax.jdo.option.ConnectionPassword , then the following exception is shown: ERROR XJ001: (SQLState=XJ001 Severity=0) (Server=hostname:port Thread=ThriftProcessor-0) Java exception: 'Unable to open a test connection to the given database. JDBC url = jdbc:mysql:// : / ?createDatabaseIfNotExist=true, username = user. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------ java.sql.SQLException: Access denied for user 'user'@' ' (using password: YES) While accessing the hive table, if you do not set the parameter javax.jdo.option.ConnectionUserName , then the following exception is shown: ERROR XJ001: (SQLState=XJ001 Severity=0) (Server=hostname:port Thread=ThriftProcessor-0) Java exception: 'Unable to open a test connection to the given database. JDBC url = jdbc:mysql:// : / ?createDatabaseIfNotExist=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------ java.sql.SQLException: Access denied for user 'APP'@' ' (using password: YES) While accessing the hive table, if you do not set the parameter javax.jdo.option.ConnectionURL , then an exception is not shown. However, you cannot access the Hive tables. For example: snappy > show tables in default ; schemaName | tableName | isTemporary ------------------------------------------------------------------------------- 0 rows selected snappy > set spark . sql . catalogImplementation = hive ; snappy > show tables in default ; schemaName | tableName | isTemporary ------------------------------------------------------------------------------- 0 rows selected While using Azure HDInsight, if you do not set the parameter fs.azure.account.key.hadoopmetastohdistorage.blob.core.windows.net , the following exception is shown: ERROR 38000: (SQLState=38000 Severity=20000) (Server=hostname:port Thread=ThriftProcessor-0) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(22841) :51028, caused by org.apache.hadoop.fs.azure.AzureException: org.apache.hadoop.fs.azure.AzureException: Container hadoopmetastore-2019-12-13t09-18-59-359z in account hadoopmetastohdistorage.blob.core.windows.net not found, and we can't create it using anoynomous credentials.' was thrown while evaluating an expression.","title":"Connecting to External Hive Metastores"},{"location":"data/external_hive_support/#external-hive-metastore-support","text":"","title":"External Hive Metastore Support"},{"location":"data/external_hive_support/#overview","text":"Hive Metastore is the central repository of Apache Hive metadata. It contains metadata (column names, data types, partitions, comments, etc.) of the objects that you create in Apache Hive. All Apache Hive implementations require a Hive service. Hive Service includes three processes; HiveServer2 (includes compiler and execution engine), MetaStore, and WebHCat. The Thrift interfaces include Drivers , which are processes that interpret the query. You can implement the Hive Metastore using the tables in a relational database such as MySQL. By default, Apache Hive uses a built-in Derby SQL server for this purpose. For running Apache Hive on a production cluster, you require MySQL or any other similar relational database. Apache Hive Metastore service supports the following databases. You can change the relational database to use any of the following supported databases: Derby MySQL MS SQL Server Oracle Postgres SnappyData can connect to the Hive Metastore in any of the following modes:","title":"Overview"},{"location":"data/external_hive_support/#local-mode","text":"In this mode, the Hive Metastore service runs in the same process as that of the main HiveServer process, but the Metastore database runs in a separate process. It can be either on the same machine or a different machine. The Metastore service communicates with the Metastore database over JDBC. External processes or applications cannot connect to the Metastore service directly. Only Hive clients have this privilege as both the Hive and the Metastore service are running in the same JVM process.","title":"Local Mode"},{"location":"data/external_hive_support/#remote-mode","text":"In this mode, the Hive Metastore service and the related Metastore database run in their own processes. External processes and applications can communicate with the Metastore service using the Thrift network API. The Metastore service communicates with the Metastore database over JDBC. The database, the HiveServer process, and the Metastore service can all be on the same machine, but running the HiveServer process on a separate machine provides better availability and scalability.","title":"Remote Mode"},{"location":"data/external_hive_support/#how-spark-connects-to-external-hive-metastore","text":"Spark SQL uses a Hive Metastore to manage the metadata of persistent relational entities (for example, databases, tables, columns, partitions) in a relational database for faster access. By default, Spark SQL uses the embedded deployment mode of a Hive Metastore with an Apache Derby database. When you create a SparkSession with Hive support, the external catalog (aka Metastore) is HiveExternalCatalog . In Spark, the HiveExternalCatalog uses the spark.sql.warehouse.dir directory to locate the databases. Moreover, you can use the following javax.jdo.option properties to connect to the Hive Metastore database: Item Description javax.jdo.option.ConnectionURL The JDBC connection URL of a Hive Metastore database to use: jdbc:derby:;databaseName=metastore_db;create=true jdbc:derby:memory:;databaseName=${metastoreLocation.getAbsolutePath};create=true jdbc:mysql://192.168.175.160:3306/metastore?useSSL=false javax.jdo.option.ConnectionDriverName The JDBC driver of a Hive Metastore database to use: org.apache.derby.jdbc.EmbeddedDriver javax.jdo.option.ConnectionUserName The user name to use to connect to a Hive Metastore database. javax.jdo.option.ConnectionPassword The password to use to connect to a Hive Metastore database. From a SnappyData cluster, you can access and query the Hive tables using a Snappy Session. After establishing a connection with the Metastore, you can do the following: Import Hive tables defined in an external Hive catalog into SnappyData catalog tables. Use Hive tables as external tables for queries, including joins with tables defined in the SnappyData catalog. Query the Hive tables. Define new Hive tables in an external Hive catalog. Spark optimizations for Hive tables are also available through Snappy Session. You can access Hive tables created in the following ecosystems: Apache Hive AWS Azure Cloudera","title":"How Spark Connects to External Hive Metastore"},{"location":"data/external_hive_support/#how-snappydata-connects-to-hive-metastore","text":"The hive-site.xml configuration file provides information about the Hive configurations to SnappyData. You must copy this file into the SnappyData conf folder, configure the hive-site.xml with the required settings, and copy the appropriate JDBC driver into the SnappyData classpath. This establishes the connection between the SnappyData cluster and the Hive Metastore. After you have established the connection, you can access hive tables and run queries on it from the SnappyData cluster. For using Hive Metastore in SnappyData, do the following: Copy the hive-site.xml from <Hive installation folder> / conf directory to <TCDB installation folder> /conf folder or use the trimmed version of hive-site.xml . If you are using MySQL as the Metastore database, copy the mysql-connector jar file (for example, mysql-connector-java-5.1.48.jar ) from <Hive installation folder> /lib to the SnappyData classpath, that is <SnappyData installation folder> /jars/ . For any other Metastore database, copy the relevant connector jars to the SnappyData jars folder.","title":"How SnappyData Connects to Hive Metastore"},{"location":"data/external_hive_support/#example-trimmed-version-of-hive-sitexml","text":"<configuration> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:mysql://dev11:3306/metastore_db?createDatabaseIfNotExist=true </value> <description> JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide a database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. </description> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> hive </value> <description> Username to use against metastore database </description> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> Snappy!23 </value> <description> password to use against metastore database </description> </property> </configuration>","title":"Example - Trimmed Version of hive-site.xml"},{"location":"data/external_hive_support/#list-of-ecosystem-connectors","text":"The following connectors are tested with the corresponding ecosytems for SnappyData: Ecosystem Connector Example Apache Hive MySQL mysql-connector-java-5.1.48.jar Azure HDInsight MSSQL mssql-jdbc-7.0.0.jre8.jar AWS EMR MySQL mysql-connector-java-5.1.12.jar Cloudera MySQL mysql-connector-java-5.1.48.jar","title":"List of Ecosystem Connectors"},{"location":"data/external_hive_support/#accessing-hive-tables-from-snappydata-cluster","text":"After configuring the settings to connect the SnappyData cluster to an external Metastore, set the property spark.sql.catalogImplementation = hive . You can set this property only at the session-level. After setting this property, you can run queries on the Hive table, perform the join operations between Hive and SnappyData native tables, perform the join operations between Hive tables. If you set the property spark.sql.catalogImplementation = in-memory , you cannot access the Hive tables. SnappyData acts as the execution engine for Hive table query execution. You can refer the following examples to access and query the Hive tables: To point to the external Hive catalog from the snappy session, set the following property. This can be set only at session level and not at global level. snappy-sql> set spark.sql.catalogImplementation=hive; To point to SnappyData internal catalog from the snappy session. snappy-sql> set spark.sql.catalogImplementation=in-memory; To access Hive tables use the following command: snappy-sql> show tables in <database name>; It is mandatory to specify the Hive database to access the Hive tables from Snappy Session. If you have not created any other database, then you can use the default database. For example, in Hive, if you create database hiveDB , use the following command: snappy - sql > show tables in hiveDB ; To read the Hive tables from snappy: snappy - sql > SELECT FirstName , LastName FROM default . hive employees ORDER BY LastName ; To join SnappyData tables and Hive tables: snappy - sql > SELECT emp . EmployeeID , emp . FirstName , emp . LastName , o . OrderID , o . OrderDate FROM default . hive_employees emp JOIN snappy_orders o ON ( emp . EmployeeID = o . EmployeeID ) ORDER BY o . OrderDate ;","title":"Accessing Hive Tables from SnappyData Cluster"},{"location":"data/external_hive_support/#creating-and-querying-hive-tables","text":"To create or access, the hive tables, you must first create a database or schema in the external hive catalog from SnappyData. Else, the default database default is used. Go to the Snappy Shell prompt and enter the following command. Note that a DDL will only be executed against the external hive if it uses hive-specific extensions in the CREATE TABLE like \"row format\" shown below. Or else when \"using hive\" is specifically mentioned after the table schema. In other words, it is not possible to create Spark format EXTERNAL tables (e.g. a parquet table) in the external hive metastore. snappy > create database hivedb ; After creating the database or schema, you can create hive tables, load data into the hive tables, and run DDL queries on hive tables from SnappyData using one of the following methods:","title":"Creating and Querying Hive Tables"},{"location":"data/external_hive_support/#method-1","text":"In this method, you must use the schema or database name before the table name as shown in the following example: snappy > create table hivedb . hive_regions ( RegionID int , RegionDescription string ) row format delimited fields terminated by ',' tblproperties ( \"skip.header.line.count\" = \"1\" ); snappy > load data local inpath '/export/shared/QA_DATA/NW_1GB/regions.csv' overwrite into table hivedb . hive_regions ; snappy > select * from hivedb . hive_regions where RegionDescription <> 'RegionDescription' ; RegionID | RegionDescription -------------------------------------------------------------------------------------------------------------------------------------------- 1 | Eastern 2 | Western 3 | Northern 4 | Southern 4 rows selected","title":"Method 1"},{"location":"data/external_hive_support/#method-2","text":"In this method, you must first run the command use <database-name> as shown in the following example. If you use this method you need not mention the schema or database name. snappy > use hivedb ; snappy > create table hive_employee_territories ( EmployeeID int , TerritoryID string ) row format delimited fields terminated by ',' ; snappy > load data local inpath '/export/shared/QA_DATA/NW_1GB/employee_territories.csv' overwrite into table hive_employee_territories ; snappy > select * from hive_employee_territories limit 10 ; EmployeeID | TerritoryID -------------------------------------------------------------------------------------------------------------------------------------------- NULL | TerritoryID 1 | 06897 1 | 19713 2 | 01581 2 | 01730 2 | 01833 2 | 02116 2 | 02139 2 | 02184 2 | 40222 10 rows selected snappy > drop table hive_employee_territories ;","title":"Method 2"},{"location":"data/external_hive_support/#creating-and-querying-snappydata-tables","text":"After the hive catalog is visible, you can create and access the SnappyData tables as well. For accessing the tables, the following two methods are available.","title":"Creating and Querying SnappyData Tables"},{"location":"data/external_hive_support/#method-1_1","text":"Run the command set spark.sql.catalogImplementation=in-memory . After this run the command use <database-name> or use schema / <database name used while creating the table> . After above two steps, the external hive catalog is not visible from SnappyData and hence all the tables that are created and accessed are SnappyData tables only. The following examples demonstrates how to create and access SnappyData tables from an external hive catalog. snappy > set spark . sql . catalogImplementation = in - memory ; snappy > use app ; snappy > create table t1 ( id int ) using column ; snappy > insert into t1 select id from range ( 1000 ); 1000 rows inserted / updated / deleted snappy > select * from t1 order by id limit 10 ; id ----------- 0 1 2 3 4 5 6 7 8 9 10 rows selected snappy > show tables in app ; schemaName | tableName | isTemporary -----------------------------------------------------------------------------------------------------------------------------------------------------------------app |t1 |false 1 row selected snappy > drop table t1 ; snappy> set spark.sql.catalogImplementation=in-memory; snappy> create table app.t1(id int) using column; snappy> insert into app.t1 select id from range(10); 10 rows inserted/updated/deleted snappy> select * from app.t1; id ----------- 4 2 6 1 3 0 7 5 9 8 10 rows selected snappy> drop table app.t1; snappy> show tables in app; schemaName |tableName |isTemporary ----------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 rows selected","title":"Method 1"},{"location":"data/external_hive_support/#method-2_1","text":"If you want to create and access the SnappyData tables, while keeping the hive catalog visible, then use the <schemaname>.<tablename> in the DDL / DML statements as shown in following examples: In case you have run the command spark.sql.catalogImplementation=in-memory , then you can run the command that points to any hive catalog database. For Example: use hivedb; snappy > set spark . sql . catalogImplementation = hive ; snappy > create table app . t1 ( id int , name String ) using column ; snappy > show tables in app ; schemaName | tableName | isTemporary ------------------------------------------------------------------------------------------------------------------------ app | t1 | false 1 row selected snappy > insert into app . t1 select id , concat ( 'TIBCO_' , id ) from range ( 1000 ); 1000 rows inserted / updated / deleted snappy > select * from app . t1 where id > 995 order by id ; id | name 996 | TIBCO_996 997 | TIBCO_997 998 | TIBCO_998 999 | TIBCO_999 4 rows selected snappy > drop table app . t1 ; If you want to join the hive table and SnappyData table, then use the <schemaname>.<tablename> in the join query for both the hive table as well as for the SnappyData table. Example snappy > SELECT emp . EmployeeID , emp . FirstName , emp . LastName , o . OrderID , o . OrderDate FROM default . hive_employees emp JOIN app . snappy_orders o ON ( emp . EmployeeID = o . EmployeeID ) ORDER BY o . OrderID ;","title":"Method 2"},{"location":"data/external_hive_support/#troubleshooting","text":"Following are some issues that you may come across and their corresponding solutions / workarounds: If the property hive.execution.engine is set as Tez then the following exception is shown: java.lang.RuntimeException: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveExternalCatalog' This exception can occur when you have copied the hive-site.xml file to Conf folder and if you have provided the value of the xml property hive.execution.engine as Tez. This exception is caused by java.lang.ClassNotFoundException: org.apache.tez.dag.api.SessionNotRunning Workaround: In case you have copied all the contents from the hive-site.xml file to the SnappyData conf file, ensure that the value of the XML property hive.execution.engine is left empty. Unable to open a test connection to the given database. JDBC url = jdbc:mysql://dev11:3306/metastore_db?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app This error occurs when the MySQL process (Metadata server process) does not get started. Workaround : Ensure to start the Metastore service before connecting to Hive Beeline CLI. When the TIBCO Compute DB cluster, Hive, MySQL (Metadata Server) are running, and MySQL (Metadata Server) process stops abruptly, then Snappy shell becomes unresponsive. While accessing the Hive table, if you do not set the parameter javax.jdo.option.ConnectionPassword , then the following exception is shown: ERROR XJ001: (SQLState=XJ001 Severity=0) (Server=hostname:port Thread=ThriftProcessor-0) Java exception: 'Unable to open a test connection to the given database. JDBC url = jdbc:mysql:// : / ?createDatabaseIfNotExist=true, username = user. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------ java.sql.SQLException: Access denied for user 'user'@' ' (using password: YES) While accessing the hive table, if you do not set the parameter javax.jdo.option.ConnectionUserName , then the following exception is shown: ERROR XJ001: (SQLState=XJ001 Severity=0) (Server=hostname:port Thread=ThriftProcessor-0) Java exception: 'Unable to open a test connection to the given database. JDBC url = jdbc:mysql:// : / ?createDatabaseIfNotExist=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------ java.sql.SQLException: Access denied for user 'APP'@' ' (using password: YES) While accessing the hive table, if you do not set the parameter javax.jdo.option.ConnectionURL , then an exception is not shown. However, you cannot access the Hive tables. For example: snappy > show tables in default ; schemaName | tableName | isTemporary ------------------------------------------------------------------------------- 0 rows selected snappy > set spark . sql . catalogImplementation = hive ; snappy > show tables in default ; schemaName | tableName | isTemporary ------------------------------------------------------------------------------- 0 rows selected While using Azure HDInsight, if you do not set the parameter fs.azure.account.key.hadoopmetastohdistorage.blob.core.windows.net , the following exception is shown: ERROR 38000: (SQLState=38000 Severity=20000) (Server=hostname:port Thread=ThriftProcessor-0) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(22841) :51028, caused by org.apache.hadoop.fs.azure.AzureException: org.apache.hadoop.fs.azure.AzureException: Container hadoopmetastore-2019-12-13t09-18-59-359z in account hadoopmetastohdistorage.blob.core.windows.net not found, and we can't create it using anoynomous credentials.' was thrown while evaluating an expression.","title":"Troubleshooting"},{"location":"howto/","text":"Overview \u00b6 This section introduces you to several common operations such as starting a cluster, working with tables (load, query, update), working with streams and running approximate queries. Running the Examples: Topics in this section refer to source code examples that are shipped with the product. Instructions to run these examples can be found in the source code. Source code for these examples is located in the quickstart/src/main/scala/org/apache/spark/examples/snappydata and in quickstart/python directories of the SnappyData product distribution. You can run the examples in any of the following ways: In the Local Mode : By using the bin/run-example script (to run Scala examples) or by using the bin/spark-submit script (to run Python examples). These examples run colocated with Spark + SnappyData Store in the same JVM. As a Job : Many of the Scala examples are also implemented as a SnappyData job. In this case, examples can be submitted as a job to a running SnappyData cluster. Refer to the jobs section for details on how to run a job. Note SnappyData also supports Java API. Refer to the documentation for more details on Java API. The following topics are covered in this section: How to Start a SnappyData Cluster How to Check the Status of a SnappyData Cluster How to Stop a SnappyData Cluster How to Run a Spark Job inside the Cluster How to Access SnappyData Store from an existing Spark Installation using Smart Connector How to Use Snappy SQL shell (snappy-sql) How to Create Row Tables and Run Queries How to Create Column Tables and Run Queries How to Load Data into SnappyData Tables How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc) How to Perform a Colocated Join How to Connect using JDBC Driver How to Store and Query JSON Objects How to Store and Query Objects How to use Stream Processing with SnappyData How to use Transactions Isolation Levels How to use Synopsis Data Engine to Run Approximate Queries How to use Python to Create Tables and Run Queries How to connect using ODBC Driver How to connect to the Cluster from External Clients How to import data from a Hive Table into a SnappyData Table How to Export and Restore table data to HDFS How to use Apache Zeppelin with SnappyData How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster","title":"Overview"},{"location":"howto/#overview","text":"This section introduces you to several common operations such as starting a cluster, working with tables (load, query, update), working with streams and running approximate queries. Running the Examples: Topics in this section refer to source code examples that are shipped with the product. Instructions to run these examples can be found in the source code. Source code for these examples is located in the quickstart/src/main/scala/org/apache/spark/examples/snappydata and in quickstart/python directories of the SnappyData product distribution. You can run the examples in any of the following ways: In the Local Mode : By using the bin/run-example script (to run Scala examples) or by using the bin/spark-submit script (to run Python examples). These examples run colocated with Spark + SnappyData Store in the same JVM. As a Job : Many of the Scala examples are also implemented as a SnappyData job. In this case, examples can be submitted as a job to a running SnappyData cluster. Refer to the jobs section for details on how to run a job. Note SnappyData also supports Java API. Refer to the documentation for more details on Java API. The following topics are covered in this section: How to Start a SnappyData Cluster How to Check the Status of a SnappyData Cluster How to Stop a SnappyData Cluster How to Run a Spark Job inside the Cluster How to Access SnappyData Store from an existing Spark Installation using Smart Connector How to Use Snappy SQL shell (snappy-sql) How to Create Row Tables and Run Queries How to Create Column Tables and Run Queries How to Load Data into SnappyData Tables How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc) How to Perform a Colocated Join How to Connect using JDBC Driver How to Store and Query JSON Objects How to Store and Query Objects How to use Stream Processing with SnappyData How to use Transactions Isolation Levels How to use Synopsis Data Engine to Run Approximate Queries How to use Python to Create Tables and Run Queries How to connect using ODBC Driver How to connect to the Cluster from External Clients How to import data from a Hive Table into a SnappyData Table How to Export and Restore table data to HDFS How to use Apache Zeppelin with SnappyData How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster","title":"Overview"},{"location":"howto/check_status_cluster/","text":"How to Check the Status of the SnappyData Cluster \u00b6 You can check the status of a running cluster using the following command: $ ./sbin/snappy-status-all.sh SnappyData Locator pid: 9748 status: running SnappyData Server pid: 9887 status: running SnappyData Leader pid: 10468 status: running You can check the SnappyData UI by opening http://<leadHostname>:5050 in your browser, where <leadHostname> is the host name of your lead node. Use Snappy SQL shell to connect to the cluster and perform various SQL operations. Related Topics SnappyData Monitoring Console","title":"How to Check the Status of a SnappyData Cluster"},{"location":"howto/check_status_cluster/#how-to-check-the-status-of-the-snappydata-cluster","text":"You can check the status of a running cluster using the following command: $ ./sbin/snappy-status-all.sh SnappyData Locator pid: 9748 status: running SnappyData Server pid: 9887 status: running SnappyData Leader pid: 10468 status: running You can check the SnappyData UI by opening http://<leadHostname>:5050 in your browser, where <leadHostname> is the host name of your lead node. Use Snappy SQL shell to connect to the cluster and perform various SQL operations. Related Topics SnappyData Monitoring Console","title":"How to Check the Status of the SnappyData Cluster"},{"location":"howto/concurrent_apache_zeppelin_access_to_secure_snappydata/","text":"How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster \u00b6 Multiple users can concurrently access a secure SnappyData cluster by configuring the JDBC interpreter setting in Apache Zeppelin. The JDBC interpreter allows you to create a JDBC connection to a SnappyData cluster. Note Currently, only the %jdbc interpreter is supported with a secure SnappyData cluster. Each user accessing the secure SnappyData cluster should configure the %jdbc interpreter in Apache Zeppelin as described here. Step 1: Download, Install and Configure SnappyData \u00b6 Download and install SnappyData . Configure the SnappyData cluster with security enabled . Start the SnappyData cluster . Create a table and load data. Grant the required permissions for the users accessing the table. For example: snappy > GRANT SELECT ON Table airline TO user2 ; snappy > GRANT INSERT ON Table airline TO user3 ; snappy > GRANT UPDATE ON Table airline TO user4 ; To enable running EXEC SCALA also GRANT : snappy > GRANT PRIVILEGE EXEC SCALA TO user2 ; Note User requiring INSERT, UPDATE or DELETE permissions also require explicit SELECT permission on a table. Important Beware that granting EXEC SCALA privilege is overarching by design and essentially makes the user equivalent to the database adminstrator since scala code can be used to modify any data using internal APIs. Follow the remaining steps as given in How to Use Apache Zeppelin with SnappyData See also How to Use Apache Zeppelin with SnappyData How to connect using JDBC driver","title":"How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster"},{"location":"howto/concurrent_apache_zeppelin_access_to_secure_snappydata/#how-to-configure-apache-zeppelin-to-securely-and-concurrently-access-the-snappydata-cluster","text":"Multiple users can concurrently access a secure SnappyData cluster by configuring the JDBC interpreter setting in Apache Zeppelin. The JDBC interpreter allows you to create a JDBC connection to a SnappyData cluster. Note Currently, only the %jdbc interpreter is supported with a secure SnappyData cluster. Each user accessing the secure SnappyData cluster should configure the %jdbc interpreter in Apache Zeppelin as described here.","title":"How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster"},{"location":"howto/concurrent_apache_zeppelin_access_to_secure_snappydata/#step-1-download-install-and-configure-snappydata","text":"Download and install SnappyData . Configure the SnappyData cluster with security enabled . Start the SnappyData cluster . Create a table and load data. Grant the required permissions for the users accessing the table. For example: snappy > GRANT SELECT ON Table airline TO user2 ; snappy > GRANT INSERT ON Table airline TO user3 ; snappy > GRANT UPDATE ON Table airline TO user4 ; To enable running EXEC SCALA also GRANT : snappy > GRANT PRIVILEGE EXEC SCALA TO user2 ; Note User requiring INSERT, UPDATE or DELETE permissions also require explicit SELECT permission on a table. Important Beware that granting EXEC SCALA privilege is overarching by design and essentially makes the user equivalent to the database adminstrator since scala code can be used to modify any data using internal APIs. Follow the remaining steps as given in How to Use Apache Zeppelin with SnappyData See also How to Use Apache Zeppelin with SnappyData How to connect using JDBC driver","title":"Step 1: Download, Install and Configure SnappyData"},{"location":"howto/connect_jdbc_client_pool_driver/","text":"How to Connect with JDBC Client Pool Driver \u00b6 Note: I think this section should be merged with the other 'How to connect using JDBC' section (jags) \u00b6 JDBC client pool driver provides built-in connection pooling and relies on the non-pooled JDBC driver . The driver will initialize the pool when the first connection is created using this driver. Thereafter, for every request, the connection is returned from the pool instead of establishing a new connection with the server. We recommend using the pooled driver for low latency operations (e.g. a point lookup query) and when using the Spark JDBC data source API (see example below). When accessing SnappyData from Java frameworks such as Spring we recommend you use pooling provided in the framework and switch to using the non-pooled driver. (Jags) I don't understand this below section. It doesn't make sense with no context on what property you are talkign about. \u00b6 I don't know the exact semantics but perhaps this is the wording ..... The underlying pool is uniquely associated with the set of properties passed when creating the connection. If any of the properties change a new pool will be created. (Jags) it is super important for us to document the caveats - if you set a connection property like autocommit(false) other connection requests on this pool will also have this property set. Not sure if this issue exists. If so, critical to document and provide suggestions. Lizy, please verify. \u00b6 Important Internally, the JDBC client pool driver maintains a map, where the key is the property and the value is the connection pool. Therefore in a second request, if you pass a different property object, then internally it adds another record in the map. To connect to SnappyData Cluster using JDBC client pool driver , use the url of the form: jdbc:snappydata:pool://<host>:<port> The client pool driver class name is io.snappydata.jdbc.ClientPoolDriver . Where the <locatorHostName> is the hostname of the node on which the locator is started and <locatorClientPort> is the port on which the locator accepts client connections (default 1527). / Dependency section needs approval/discussion / Dependencies : Use the Maven/SBT dependencies for the latest released version of SnappyData. Example: Maven dependency <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client --> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-jdbc_2.11 </artifactId> <version> 1.1.0 </version> </dependency> Example: SBT dependency // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client libraryDependencies += \"io.snappydata\" % \"snappydata-jdbc_2.11\" % \"1.1.0\" Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . The following additional properties can be configured for JDBC client pool driver connection: Property Description pool.user The username to be passed to the JDBC client pool driver to establish a connection. pool.password The password to be passed to the JDBC client pool driver to establish a connection. pool.initialSize The initial number of connections that are created when the pool is started. Default value is max(256, availableProcessors * 8) . pool.maxActive The maximum number of active connections that can be allocated from this pool at a time. The default value is max(256, availableProcessors * 8) . pool.minIdle The minimum number of established connections that should be maintained in the client pool. Default value is derived from initialSize: max(256, availableProcessors * 8) . pool.maxIdle The maximum number of connections that should be maintained in the client pool. Default value is maxActive: max(256, availableProcessors * 8) . Idle connections are checked periodically, if enabled, and the connections that are idle for more than the time set in minEvictableIdleTimeMillis are released. pool.maxWait (int) The maximum waiting period, in milliseconds, for establishing a connection after which an exception is thrown. Default value is 30000 (30 seconds). pool.removeAbandoned Flag to remove the abandoned connections, in case they exceed the settings for removeAbandonedTimeout . If set to true a connection is considered abandoned and eligible for removal, if its no longer in use than the settings for removeAbandonedTimeout . Setting this to true can recover db connections from applications that fail to close a connection. The default value is false . pool.removeAbandonedTimeout Timeout in seconds before an abandoned connection, that was in use, can be removed. The default value is 60 seconds. The value should be set to the time required for the longest running query in your applications. pool.timeBetweenEvictionRunsMillis Time period required to sleep between runs of the idle connection validation/cleaner thread. You should always set this value above one second. This time period determines how often we check for idle and abandoned connections and how often to validate the idle connections. The default value is 5000 (5 seconds). pool.minEvictableIdleTimeMillis The minimum time period, in milliseconds, for which an object can be idle in the pool before it qualifies for eviction. The default value is 60000 (60 seconds). driver io.snappydata.jdbc.ClientPoolDriver This should be passed through Spark JDBC API for loading and using the driver. pool.testOnBorrow Indicates if the objects are validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and will attempt to borrow another. In order to have a more efficient validation, see pool.validationInterval . Default value is true . pool.validationInterval Avoid excess validation, only run validation at most at this frequency - time in milliseconds. If a connection is due for validation, but has been validated previously within this interval, it will not be validated again. The default value is 10000 (10 seconds). Example Code Snippet: val properties = new Properties () properties . setProperty ( \"pool.user\" , \"user\" ) properties . setProperty ( \"pool.password\" , \"pass\" ) properties . setProperty ( \"driver\" , \u201c\u201c io . snappydata . jdbc . ClientPoolDriver \u201d\u201d ) val builder = SparkSession . builder . appName ( \"app\" ) . master ( \"local[*]\" ) val spark : SparkSession = builder . getOrCreate val df = spark . read . jdbc ( \u201c jdbc : snappydata : pool : //localhost:1527\u201d, \"Table_X\", properties)","title":"How to Connect with JDBC Client Pool Driver"},{"location":"howto/connect_jdbc_client_pool_driver/#how-to-connect-with-jdbc-client-pool-driver","text":"","title":"How to Connect with JDBC Client Pool Driver"},{"location":"howto/connect_jdbc_client_pool_driver/#note-i-think-this-section-should-be-merged-with-the-other-how-to-connect-using-jdbc-section-jags","text":"JDBC client pool driver provides built-in connection pooling and relies on the non-pooled JDBC driver . The driver will initialize the pool when the first connection is created using this driver. Thereafter, for every request, the connection is returned from the pool instead of establishing a new connection with the server. We recommend using the pooled driver for low latency operations (e.g. a point lookup query) and when using the Spark JDBC data source API (see example below). When accessing SnappyData from Java frameworks such as Spring we recommend you use pooling provided in the framework and switch to using the non-pooled driver.","title":"Note: I think this section should be merged with the other 'How to connect using JDBC' section (jags)"},{"location":"howto/connect_jdbc_client_pool_driver/#jags-i-dont-understand-this-below-section-it-doesnt-make-sense-with-no-context-on-what-property-you-are-talkign-about","text":"I don't know the exact semantics but perhaps this is the wording ..... The underlying pool is uniquely associated with the set of properties passed when creating the connection. If any of the properties change a new pool will be created.","title":"(Jags) I don't understand this below section. It doesn't make sense with no context on what property you are talkign about."},{"location":"howto/connect_jdbc_client_pool_driver/#jags-it-is-super-important-for-us-to-document-the-caveats-if-you-set-a-connection-property-like-autocommitfalse-other-connection-requests-on-this-pool-will-also-have-this-property-set-not-sure-if-this-issue-exists-if-so-critical-to-document-and-provide-suggestions-lizy-please-verify","text":"Important Internally, the JDBC client pool driver maintains a map, where the key is the property and the value is the connection pool. Therefore in a second request, if you pass a different property object, then internally it adds another record in the map. To connect to SnappyData Cluster using JDBC client pool driver , use the url of the form: jdbc:snappydata:pool://<host>:<port> The client pool driver class name is io.snappydata.jdbc.ClientPoolDriver . Where the <locatorHostName> is the hostname of the node on which the locator is started and <locatorClientPort> is the port on which the locator accepts client connections (default 1527). / Dependency section needs approval/discussion / Dependencies : Use the Maven/SBT dependencies for the latest released version of SnappyData. Example: Maven dependency <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client --> <dependency> <groupId> io.snappydata </groupId> <artifactId> snappydata-jdbc_2.11 </artifactId> <version> 1.1.0 </version> </dependency> Example: SBT dependency // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client libraryDependencies += \"io.snappydata\" % \"snappydata-jdbc_2.11\" % \"1.1.0\" Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys . props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . The following additional properties can be configured for JDBC client pool driver connection: Property Description pool.user The username to be passed to the JDBC client pool driver to establish a connection. pool.password The password to be passed to the JDBC client pool driver to establish a connection. pool.initialSize The initial number of connections that are created when the pool is started. Default value is max(256, availableProcessors * 8) . pool.maxActive The maximum number of active connections that can be allocated from this pool at a time. The default value is max(256, availableProcessors * 8) . pool.minIdle The minimum number of established connections that should be maintained in the client pool. Default value is derived from initialSize: max(256, availableProcessors * 8) . pool.maxIdle The maximum number of connections that should be maintained in the client pool. Default value is maxActive: max(256, availableProcessors * 8) . Idle connections are checked periodically, if enabled, and the connections that are idle for more than the time set in minEvictableIdleTimeMillis are released. pool.maxWait (int) The maximum waiting period, in milliseconds, for establishing a connection after which an exception is thrown. Default value is 30000 (30 seconds). pool.removeAbandoned Flag to remove the abandoned connections, in case they exceed the settings for removeAbandonedTimeout . If set to true a connection is considered abandoned and eligible for removal, if its no longer in use than the settings for removeAbandonedTimeout . Setting this to true can recover db connections from applications that fail to close a connection. The default value is false . pool.removeAbandonedTimeout Timeout in seconds before an abandoned connection, that was in use, can be removed. The default value is 60 seconds. The value should be set to the time required for the longest running query in your applications. pool.timeBetweenEvictionRunsMillis Time period required to sleep between runs of the idle connection validation/cleaner thread. You should always set this value above one second. This time period determines how often we check for idle and abandoned connections and how often to validate the idle connections. The default value is 5000 (5 seconds). pool.minEvictableIdleTimeMillis The minimum time period, in milliseconds, for which an object can be idle in the pool before it qualifies for eviction. The default value is 60000 (60 seconds). driver io.snappydata.jdbc.ClientPoolDriver This should be passed through Spark JDBC API for loading and using the driver. pool.testOnBorrow Indicates if the objects are validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and will attempt to borrow another. In order to have a more efficient validation, see pool.validationInterval . Default value is true . pool.validationInterval Avoid excess validation, only run validation at most at this frequency - time in milliseconds. If a connection is due for validation, but has been validated previously within this interval, it will not be validated again. The default value is 10000 (10 seconds). Example Code Snippet: val properties = new Properties () properties . setProperty ( \"pool.user\" , \"user\" ) properties . setProperty ( \"pool.password\" , \"pass\" ) properties . setProperty ( \"driver\" , \u201c\u201c io . snappydata . jdbc . ClientPoolDriver \u201d\u201d ) val builder = SparkSession . builder . appName ( \"app\" ) . master ( \"local[*]\" ) val spark : SparkSession = builder . getOrCreate val df = spark . read . jdbc ( \u201c jdbc : snappydata : pool : //localhost:1527\u201d, \"Table_X\", properties)","title":"(Jags) it is super important for us to document the caveats - if you set a connection property like autocommit(false) other connection requests on this pool will also have this property set. Not sure if this issue exists. If so, critical to document and provide suggestions. Lizy, please verify."},{"location":"howto/connect_oss_vis_client_tools/","text":"How to Access SnappyData from Various SQL Client Tools \u00b6 You can use the following SQL client tools to access SnappyData: DbVisualizer SQL Workbench/J DBeaver SQuirreL SQL Client DbVisualizer \u00b6 DbVisualizer is a universal database tool for developers, DBAs and analysts to access databases. You can access SnappyData from DbVisualizer when you create a database connection using a database driver. To access SnappyData from DbVisualizer, do the following: Create a Database Driver \u00b6 To create a database driver, do the following: On the DbVisualizer home page, go to Tools > Driver Manager and click + . Enter the following details: Name: Provide a name for the driver. URL Format: Specify the URL format as jdbc:snappydata://server:port/ From the Driver jar Files , click the folder icon and select the client jar. Only after you select the client jar, the io.snappydata.jdbc.ClientDriver class is listed as an option in Driver Class drop-down list. For SnappyData, this jar is located in the build-artifacts/scala-2.11/distributions/ folder. Connecting to SnappyData from DbVisualizer \u00b6 To connect SnappyData from DbVisualizer, do the following: On the main dbviz UI, click the Create new database icon. You are given the options to either use or not use a wizard for the connection. Click Use Wizard . In the New Connection dialog box, enter a name that can be used to refer to the database connection. In the Select Database Driver dialog box, select the database driver that you have created. In the Database URL dialog box, type the complete URL. For example: jdbc:snappydata://localhost:1527/ . Enter the UserID and password. Select the Auto Commit option if you want to enable auto-commit in the SQL Commander. Click Finish . You can open the SQL Commander and run your queries. Note The steps provided here are specific to DbVisualizer 10.0.20 version. The steps can slightly vary in other versions. For secure connections, refer Creating a Secure Connection from JDBC Client Tools SQL Workbench/J \u00b6 SQL Workbench/J is a free, DBMS-independent, cross-platform SQL query tool which can be run on any operating system that provides a Java Runtime Environment. It can also work with a JDK (Java Development Kit). Prerequisites \u00b6 Install Java 8 or higher to run SQL Workbench/J. Using a 64-bit Java runtime is highly recommended. Download and install SQL Workbench/J. Connecting to SnappyData from SQL Workbench/J \u00b6 To connect SnappyData from SQL Workbench/J, do the following: Start the SnappyData cluster and Snappy shell in local mode. Start SQL Workbench/J. If you are using Microsoft Windows\u00ae, double-click the SQLWorkbench.exe executable to start. If you are using a 64-bit operating system and a 64-bit Java runtime, you have to use SQLWorkbench64.exe instead. If you are running Linux or another Unix\u00ae like operating system, you can use the supplied shell script sqlworkbench.sh to start the application. The Select Connection Profile dialog box is displayed. In the Default Group section, enter a name for the new profile and click the Save icon. Click Manage Drivers from the bottom left. The Manage driver dialog box is displayed. Enter the following details: Name : Provide a name for the driver. Library : Click the folder icon and select the JDBC Client jar. You must download the JDBC Client jar (snappydata-jdbc_2.11-1.3.1.jar) from the SnappyData website to your local machine. Classname : io.snappydata.jdbc.ClientDriver . Sample URL : jdbc:snappydata://server:port/ Click OK . The Select Connection Profile page is displayed. Do the following: Select the driver that was created. Provide the URL as jdbc:snappydata://localhost:1527/ Enter username and password. Click the Test button and then click OK . After you get a successful connection, you run queries in SnappyData from SQL WorkBench/J. For secure connections, refer Creating a Secure Connection from JDBC Client Tools DBeaver \u00b6 DBeaver is a graphical database management tool. You can access SnappyData from DBeaver. Download and install DBeaver, start the LDAP server and print the LDAP conf, and then connect to SnappyData from DBeaver. Download and Install DBeaver \u00b6 To download and install DBeaver, do the following: Go to the Download page of DBeaver. Choose an appropriate installer for the corresponding operating system. For example, for Linux Debian package, download from this link . Run the corresponding commands that are specified in the Install section on the Download page. Connecting to SnappyData from DBeaver \u00b6 Launch DBeaver and click New database connection . Select Hadoop / Big Data section from the left. Select SnappyData from the available list of databases and provide the following details: Hostname/IP Port Username / Password Test the connection and finish the setup of the database source. For secure connections, refer Creating a Secure Connection from JDBC Client Tools SQuirreL SQL Client \u00b6 The SQuirreL SQL Client client is a database administration tool that let you explore and interact with databases using a JDBC driver. You can access SnappyData from SQuirreL SQL. Download and install SQuirreL SQL Client, start the LDAP server and print the LDAP conf, and then connect to SnappyData from DBeaver. Download and Install SQuirrel \u00b6 To download and install SQuirrel, do the following: Download SQuirreL SQL Client. Go to the folder where the SQuirreL SQL Client jar is downloaded and run the following command to install SQuirreL SQL Client: java -jar <downloaded squirrel jar> Go to the SQuirreL SQL Client installation folder and run the following command: ./squirrel-sql.sh Connecting to SnappyData from SQuirreL SQL Client \u00b6 To connect SnappyData from SQuirreL SQL Client, do the following: Launch SQuirreL SQL Client. In the Drivers tab on the left side, click + sign to add a new driver. Provide the following details: Name Example URL(connection string) website URL Add the downloaded snappydata jdbc jar in the extra classpath tab and provide the class name to be used for the connection. jdbc jar: https://mvnrepository.com/artifact/io.snappydata/snappydata-jdbc_2.11/1.3.1 jdbc class: io.snappydata.jdbc.ClientPoolDriver Go to Aliases tab and then click + to add a new alias. Provide the following details: Name Driver (added in step 4) JDBC URL (For example: jdbc:snappydata:pool://localhost:1527) Username and password: (Default username / password: app/app) Test the connection and finish setup. You can run the following test queries: create table colTable(CustKey Integer not null, CustName String) using column options(); insert into colTable values(1, 'a'); insert into colTable values(2, 'a'); insert into colTable values(3, 'a'); select count(*) from colTable; create table rowTable(CustKey Integer NOT NULL PRIMARY KEY, CustName String) using row options(); insert into rowTable values(11, 'a'); insert into rowTable values(22, 'a'); insert into rowTable values(33, 'a'); update rowTable set CustName='d' where custkey = 1; select * from rowTable order by custkey; drop table if exists rowTable; drop table if exists colTable; show tables; For secure connections, refer Creating a Secure Connection from JDBC Client Tools Note When connecting to SnappyData, if a SQL client tool sets JDBC autocommit to false and transaction isolation level such as read committed or repeatable read is used, the unsupported operations such as those on column table will produce an error - Operations on column tables are not supported when query routing is disabled or autocommit is false. In such cases, connection property allow-explicit-commit=true can be used in the connection URL to avoid this error. Refer to configuration parameters section for details on this property. For example, JDBC URL: jdbc:snappydata://locatoHostName:1527/allow-explicit-commit=true Creating a Secure Connection from JDBC Client Tools \u00b6 If you already have an LDAP server, you can use the same to connect to SnappyData cluster or you can use the LDAP server that comes pre-configured with SnappyData. To start the pre-configured LDAP server of SnappyData, do the following: From the terminal, go to the location of ldap-test-server: cd $SNAPPY_HOME/store/ldap-test-server Run the following command to build: ./gradlew build Run the script: ./start-ldap-server.sh auth.ldif This starts the LDAP server and prints the LDAP conf. The printed LDAP conf contains the username and password of LDAP that should be used to connect from JDBC clients. Copy this into the leads/servers/locators conf files of SnappyData. Start the SnappyData cluster. When you are connecting from a JDBC client, ensure to provide the user name and password printed in step 3.","title":"How to Access SnappyData from Various SQL Client Tools"},{"location":"howto/connect_oss_vis_client_tools/#how-to-access-snappydata-from-various-sql-client-tools","text":"You can use the following SQL client tools to access SnappyData: DbVisualizer SQL Workbench/J DBeaver SQuirreL SQL Client","title":"How to Access SnappyData from Various SQL Client Tools"},{"location":"howto/connect_oss_vis_client_tools/#dbvisualizer","text":"DbVisualizer is a universal database tool for developers, DBAs and analysts to access databases. You can access SnappyData from DbVisualizer when you create a database connection using a database driver. To access SnappyData from DbVisualizer, do the following:","title":"DbVisualizer"},{"location":"howto/connect_oss_vis_client_tools/#create-a-database-driver","text":"To create a database driver, do the following: On the DbVisualizer home page, go to Tools > Driver Manager and click + . Enter the following details: Name: Provide a name for the driver. URL Format: Specify the URL format as jdbc:snappydata://server:port/ From the Driver jar Files , click the folder icon and select the client jar. Only after you select the client jar, the io.snappydata.jdbc.ClientDriver class is listed as an option in Driver Class drop-down list. For SnappyData, this jar is located in the build-artifacts/scala-2.11/distributions/ folder.","title":"Create a Database Driver"},{"location":"howto/connect_oss_vis_client_tools/#connecting-to-snappydata-from-dbvisualizer","text":"To connect SnappyData from DbVisualizer, do the following: On the main dbviz UI, click the Create new database icon. You are given the options to either use or not use a wizard for the connection. Click Use Wizard . In the New Connection dialog box, enter a name that can be used to refer to the database connection. In the Select Database Driver dialog box, select the database driver that you have created. In the Database URL dialog box, type the complete URL. For example: jdbc:snappydata://localhost:1527/ . Enter the UserID and password. Select the Auto Commit option if you want to enable auto-commit in the SQL Commander. Click Finish . You can open the SQL Commander and run your queries. Note The steps provided here are specific to DbVisualizer 10.0.20 version. The steps can slightly vary in other versions. For secure connections, refer Creating a Secure Connection from JDBC Client Tools","title":"Connecting to SnappyData from DbVisualizer"},{"location":"howto/connect_oss_vis_client_tools/#sql-workbenchj","text":"SQL Workbench/J is a free, DBMS-independent, cross-platform SQL query tool which can be run on any operating system that provides a Java Runtime Environment. It can also work with a JDK (Java Development Kit).","title":"SQL Workbench/J"},{"location":"howto/connect_oss_vis_client_tools/#prerequisites","text":"Install Java 8 or higher to run SQL Workbench/J. Using a 64-bit Java runtime is highly recommended. Download and install SQL Workbench/J.","title":"Prerequisites"},{"location":"howto/connect_oss_vis_client_tools/#connecting-to-snappydata-from-sql-workbenchj","text":"To connect SnappyData from SQL Workbench/J, do the following: Start the SnappyData cluster and Snappy shell in local mode. Start SQL Workbench/J. If you are using Microsoft Windows\u00ae, double-click the SQLWorkbench.exe executable to start. If you are using a 64-bit operating system and a 64-bit Java runtime, you have to use SQLWorkbench64.exe instead. If you are running Linux or another Unix\u00ae like operating system, you can use the supplied shell script sqlworkbench.sh to start the application. The Select Connection Profile dialog box is displayed. In the Default Group section, enter a name for the new profile and click the Save icon. Click Manage Drivers from the bottom left. The Manage driver dialog box is displayed. Enter the following details: Name : Provide a name for the driver. Library : Click the folder icon and select the JDBC Client jar. You must download the JDBC Client jar (snappydata-jdbc_2.11-1.3.1.jar) from the SnappyData website to your local machine. Classname : io.snappydata.jdbc.ClientDriver . Sample URL : jdbc:snappydata://server:port/ Click OK . The Select Connection Profile page is displayed. Do the following: Select the driver that was created. Provide the URL as jdbc:snappydata://localhost:1527/ Enter username and password. Click the Test button and then click OK . After you get a successful connection, you run queries in SnappyData from SQL WorkBench/J. For secure connections, refer Creating a Secure Connection from JDBC Client Tools","title":"Connecting to SnappyData from SQL Workbench/J"},{"location":"howto/connect_oss_vis_client_tools/#dbeaver","text":"DBeaver is a graphical database management tool. You can access SnappyData from DBeaver. Download and install DBeaver, start the LDAP server and print the LDAP conf, and then connect to SnappyData from DBeaver.","title":"DBeaver"},{"location":"howto/connect_oss_vis_client_tools/#download-and-install-dbeaver","text":"To download and install DBeaver, do the following: Go to the Download page of DBeaver. Choose an appropriate installer for the corresponding operating system. For example, for Linux Debian package, download from this link . Run the corresponding commands that are specified in the Install section on the Download page.","title":"Download and Install DBeaver"},{"location":"howto/connect_oss_vis_client_tools/#connecting-to-snappydata-from-dbeaver","text":"Launch DBeaver and click New database connection . Select Hadoop / Big Data section from the left. Select SnappyData from the available list of databases and provide the following details: Hostname/IP Port Username / Password Test the connection and finish the setup of the database source. For secure connections, refer Creating a Secure Connection from JDBC Client Tools","title":"Connecting to SnappyData from DBeaver"},{"location":"howto/connect_oss_vis_client_tools/#squirrel-sql-client","text":"The SQuirreL SQL Client client is a database administration tool that let you explore and interact with databases using a JDBC driver. You can access SnappyData from SQuirreL SQL. Download and install SQuirreL SQL Client, start the LDAP server and print the LDAP conf, and then connect to SnappyData from DBeaver.","title":"SQuirreL SQL Client"},{"location":"howto/connect_oss_vis_client_tools/#download-and-install-squirrel","text":"To download and install SQuirrel, do the following: Download SQuirreL SQL Client. Go to the folder where the SQuirreL SQL Client jar is downloaded and run the following command to install SQuirreL SQL Client: java -jar <downloaded squirrel jar> Go to the SQuirreL SQL Client installation folder and run the following command: ./squirrel-sql.sh","title":"Download and Install SQuirrel"},{"location":"howto/connect_oss_vis_client_tools/#connecting-to-snappydata-from-squirrel-sql-client","text":"To connect SnappyData from SQuirreL SQL Client, do the following: Launch SQuirreL SQL Client. In the Drivers tab on the left side, click + sign to add a new driver. Provide the following details: Name Example URL(connection string) website URL Add the downloaded snappydata jdbc jar in the extra classpath tab and provide the class name to be used for the connection. jdbc jar: https://mvnrepository.com/artifact/io.snappydata/snappydata-jdbc_2.11/1.3.1 jdbc class: io.snappydata.jdbc.ClientPoolDriver Go to Aliases tab and then click + to add a new alias. Provide the following details: Name Driver (added in step 4) JDBC URL (For example: jdbc:snappydata:pool://localhost:1527) Username and password: (Default username / password: app/app) Test the connection and finish setup. You can run the following test queries: create table colTable(CustKey Integer not null, CustName String) using column options(); insert into colTable values(1, 'a'); insert into colTable values(2, 'a'); insert into colTable values(3, 'a'); select count(*) from colTable; create table rowTable(CustKey Integer NOT NULL PRIMARY KEY, CustName String) using row options(); insert into rowTable values(11, 'a'); insert into rowTable values(22, 'a'); insert into rowTable values(33, 'a'); update rowTable set CustName='d' where custkey = 1; select * from rowTable order by custkey; drop table if exists rowTable; drop table if exists colTable; show tables; For secure connections, refer Creating a Secure Connection from JDBC Client Tools Note When connecting to SnappyData, if a SQL client tool sets JDBC autocommit to false and transaction isolation level such as read committed or repeatable read is used, the unsupported operations such as those on column table will produce an error - Operations on column tables are not supported when query routing is disabled or autocommit is false. In such cases, connection property allow-explicit-commit=true can be used in the connection URL to avoid this error. Refer to configuration parameters section for details on this property. For example, JDBC URL: jdbc:snappydata://locatoHostName:1527/allow-explicit-commit=true","title":"Connecting to SnappyData from SQuirreL SQL Client"},{"location":"howto/connect_oss_vis_client_tools/#creating-a-secure-connection-from-jdbc-client-tools","text":"If you already have an LDAP server, you can use the same to connect to SnappyData cluster or you can use the LDAP server that comes pre-configured with SnappyData. To start the pre-configured LDAP server of SnappyData, do the following: From the terminal, go to the location of ldap-test-server: cd $SNAPPY_HOME/store/ldap-test-server Run the following command to build: ./gradlew build Run the script: ./start-ldap-server.sh auth.ldif This starts the LDAP server and prints the LDAP conf. The printed LDAP conf contains the username and password of LDAP that should be used to connect from JDBC clients. Copy this into the leads/servers/locators conf files of SnappyData. Start the SnappyData cluster. When you are connecting from a JDBC client, ensure to provide the user name and password printed in step 3.","title":"Creating a Secure Connection from JDBC Client Tools"},{"location":"howto/connect_to_the_cluster_from_external_clients/","text":"How to Connect to the Cluster from External Network \u00b6 You can also connect to the SnappyData cluster from a different network as a client (DbVisualizer, SQuirreL SQL etc.). For example, to connect to a cluster on AWS from your local machine set the following properties in the conf/locators and conf/servers files: client-bind-address : Set the hostname or IP address to which the locator or server listens on, for JDBC/ODBC/thrift client connections. hostname-for-clients : Set the IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where the servers/locators are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in client-bind-address resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required. Note By default, the locator or server binds to localhost. You may need to set either or both these properties to enable connection from external clients. If not set, external client connections may fail. Port Settings: Locator or server listens on the default port 1527 for client connections. Ensure that this port is open in your firewall settings. You can also change the default port by setting the client-port property in the conf/locators and conf/servers . Note For ODBC clients, you must use the host and port details of the server and not the locator.","title":"How to Connect to the Cluster from External Network"},{"location":"howto/connect_to_the_cluster_from_external_clients/#how-to-connect-to-the-cluster-from-external-network","text":"You can also connect to the SnappyData cluster from a different network as a client (DbVisualizer, SQuirreL SQL etc.). For example, to connect to a cluster on AWS from your local machine set the following properties in the conf/locators and conf/servers files: client-bind-address : Set the hostname or IP address to which the locator or server listens on, for JDBC/ODBC/thrift client connections. hostname-for-clients : Set the IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where the servers/locators are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in client-bind-address resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required. Note By default, the locator or server binds to localhost. You may need to set either or both these properties to enable connection from external clients. If not set, external client connections may fail. Port Settings: Locator or server listens on the default port 1527 for client connections. Ensure that this port is open in your firewall settings. You can also change the default port by setting the client-port property in the conf/locators and conf/servers . Note For ODBC clients, you must use the host and port details of the server and not the locator.","title":"How to Connect to the Cluster from External Network"},{"location":"howto/connect_using_jdbc_driver/","text":"How to Connect using JDBC Driver \u00b6 You can connect to and execute queries against SnappyData cluster using JDBC driver. The connection URL typically points to one of the locators. The locator passes the information of all available servers, based on which the driver automatically connects to one of the servers. To connect to the SnappyData cluster using JDBC, use URL of the form jdbc:snappydata://<locatorHostName>:<locatorClientPort>/ Where the <locatorHostName> is the hostname of the node on which the locator is started and <locatorClientPort> is the port on which the locator accepts client connections (default 1527). You can use Maven or SBT dependencies to get the latest SnappyData JBDC driver which is used for establishing the JDBC connection with SnappyData. Other than this you can also directly download the JDBC driver from the SnappyData release page. Using Maven/SBT Dependencies \u00b6 You can use the Maven or the SBT dependencies to get the latest released version of SnappyData JDBC driver. Example: Maven dependency <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client --> <dependency> <groupId>io.snappydata</groupId> <artifactId>snappydata-jdbc_2.11</artifactId> <version>1.3.1</version> </dependency> Example: SBT dependency // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client libraryDependencies += \"io.snappydata\" % \"snappydata-jdbc_2.11\" % \"1.3.1\" Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due to an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys.props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Dowloading SnappyData JDBC Driver Jar \u00b6 You can directly download the SnappyData JDBC driver from the latest SnappyData release page. Scroll down to download the SnappyData JDBC driver jar which is listed in the Description of download Artifacts > Assets section. Code Example \u00b6 Connect to a SnappyData cluster using JDBC on default client port The code snippet shows how to connect to a SnappyData cluster using JDBC on default client port 1527. The complete source code of the example is located at JDBCExample.scala val url: String = s\"jdbc:snappydata://localhost:1527/\" val conn1 = DriverManager.getConnection(url) val stmt1 = conn1.createStatement() // Creating a table (PARTSUPP) using JDBC connection stmt1.execute(\"DROP TABLE IF EXISTS APP.PARTSUPP\") stmt1.execute(\"CREATE TABLE APP.PARTSUPP ( \" + \"PS_PARTKEY INTEGER NOT NULL PRIMARY KEY,\" + \"PS_SUPPKEY INTEGER NOT NULL,\" + \"PS_AVAILQTY INTEGER NOT NULL,\" + \"PS_SUPPLYCOST DECIMAL(15,2) NOT NULL)\" + \"USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\") // Inserting records in PARTSUPP table via batch inserts val preparedStmt1 = conn1.prepareStatement(\"INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\") var x = 0 for (x <- 1 to 10) { preparedStmt1.setInt(1, x*100) preparedStmt1.setInt(2, x) preparedStmt1.setInt(3, x*1000) preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2)) preparedStmt1.addBatch() } preparedStmt1.executeBatch() preparedStmt1.close() Note If the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the io.snappydata.jdbc.ClientDriver class.","title":"How to Connect using JDBC Driver"},{"location":"howto/connect_using_jdbc_driver/#how-to-connect-using-jdbc-driver","text":"You can connect to and execute queries against SnappyData cluster using JDBC driver. The connection URL typically points to one of the locators. The locator passes the information of all available servers, based on which the driver automatically connects to one of the servers. To connect to the SnappyData cluster using JDBC, use URL of the form jdbc:snappydata://<locatorHostName>:<locatorClientPort>/ Where the <locatorHostName> is the hostname of the node on which the locator is started and <locatorClientPort> is the port on which the locator accepts client connections (default 1527). You can use Maven or SBT dependencies to get the latest SnappyData JBDC driver which is used for establishing the JDBC connection with SnappyData. Other than this you can also directly download the JDBC driver from the SnappyData release page.","title":"How to Connect using JDBC Driver"},{"location":"howto/connect_using_jdbc_driver/#using-mavensbt-dependencies","text":"You can use the Maven or the SBT dependencies to get the latest released version of SnappyData JDBC driver. Example: Maven dependency <!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client --> <dependency> <groupId>io.snappydata</groupId> <artifactId>snappydata-jdbc_2.11</artifactId> <version>1.3.1</version> </dependency> Example: SBT dependency // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client libraryDependencies += \"io.snappydata\" % \"snappydata-jdbc_2.11\" % \"1.3.1\" Note If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due to an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys.props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 .","title":"Using Maven/SBT Dependencies"},{"location":"howto/connect_using_jdbc_driver/#dowloading-snappydata-jdbc-driver-jar","text":"You can directly download the SnappyData JDBC driver from the latest SnappyData release page. Scroll down to download the SnappyData JDBC driver jar which is listed in the Description of download Artifacts > Assets section.","title":"Dowloading SnappyData JDBC Driver Jar"},{"location":"howto/connect_using_jdbc_driver/#code-example","text":"Connect to a SnappyData cluster using JDBC on default client port The code snippet shows how to connect to a SnappyData cluster using JDBC on default client port 1527. The complete source code of the example is located at JDBCExample.scala val url: String = s\"jdbc:snappydata://localhost:1527/\" val conn1 = DriverManager.getConnection(url) val stmt1 = conn1.createStatement() // Creating a table (PARTSUPP) using JDBC connection stmt1.execute(\"DROP TABLE IF EXISTS APP.PARTSUPP\") stmt1.execute(\"CREATE TABLE APP.PARTSUPP ( \" + \"PS_PARTKEY INTEGER NOT NULL PRIMARY KEY,\" + \"PS_SUPPKEY INTEGER NOT NULL,\" + \"PS_AVAILQTY INTEGER NOT NULL,\" + \"PS_SUPPLYCOST DECIMAL(15,2) NOT NULL)\" + \"USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\") // Inserting records in PARTSUPP table via batch inserts val preparedStmt1 = conn1.prepareStatement(\"INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\") var x = 0 for (x <- 1 to 10) { preparedStmt1.setInt(1, x*100) preparedStmt1.setInt(2, x) preparedStmt1.setInt(3, x*1000) preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2)) preparedStmt1.addBatch() } preparedStmt1.executeBatch() preparedStmt1.close() Note If the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the io.snappydata.jdbc.ClientDriver class.","title":"Code Example"},{"location":"howto/connect_using_odbc_driver/","text":"How to Connect using ODBC Driver \u00b6 You can connect to SnappyData Cluster using SnappyData ODBC Driver and can execute SQL queries by connecting to any of the servers in the cluster. Step 1: Installing Visual C++ Redistributable for Visual Studio 2015 and above \u00b6 To download and install the Visual C++ Redistributable for Visual Studio 2015 and above: Download Visual C++ Redistributable for Visual Studio 2015 and above Select Run to start the installation and follow the steps to complete the installation. Step 2: Installing SnappyData ODBC Driver \u00b6 To download and install the ODBC driver: Download the drivers zip file snappydata-odbc_1.3.0_win64.zip using the steps provided here . Extract snappydata-odbc_1.3.0_win64.zip . Depending on your Windows installation, extract the contents of the 32-bit or 64-bit version of the SnappyData ODBC Driver. Version ODBC Driver 32-bit for 64-bit platform snappydata-odbc_1.3.0_win64_x86.msi 64-bit for 64-bit platform snappydata-odbc_1.3.0_win64_x64.msi Double-click on the corresponding msi file, and follow the steps to complete the installation. Note Ensure that SnappyData is installed and the SnappyData cluster is running . Connecting to the SnappyData Cluster \u00b6 Once you have installed the SnappyData ODBC Driver, you can connect to SnappyData cluster in any of the following ways: Use the SnappyData Driver Connection URL: Driver=SnappyData ODBC Driver;server=<ServerIP>;port=<ServerPort>;user=<userName>;password=<password> Create a SnappyData DSN (Data Source Name) using the installed SnappyData ODBC Driver. Refer to the Windows documentation relevant to your operating system for more information on creating a DSN. When prompted, select the SnappyData ODBC Driver from the list of drivers and enter a Data Source name, SnappyData Server Host, Port, User Name and Password. Refer to the documentation for detailed information on Setting Up SnappyData ODBC Driver . Connecting Spotfire\u00ae Desktop to SnappyData \u00b6 Refer TIBCO Spotfire\u00ae Connectivity to SnappyData\u2122 for detailed instructions to access SnappyData using this connector. Also see: ODBC Supported APIs in SnappyData Driver","title":"How to Connect using ODBC Driver"},{"location":"howto/connect_using_odbc_driver/#how-to-connect-using-odbc-driver","text":"You can connect to SnappyData Cluster using SnappyData ODBC Driver and can execute SQL queries by connecting to any of the servers in the cluster.","title":"How to Connect using ODBC Driver"},{"location":"howto/connect_using_odbc_driver/#step-1-installing-visual-c-redistributable-for-visual-studio-2015-and-above","text":"To download and install the Visual C++ Redistributable for Visual Studio 2015 and above: Download Visual C++ Redistributable for Visual Studio 2015 and above Select Run to start the installation and follow the steps to complete the installation.","title":"Step 1: Installing Visual C++ Redistributable for Visual Studio 2015 and above"},{"location":"howto/connect_using_odbc_driver/#step-2-installing-snappydata-odbc-driver","text":"To download and install the ODBC driver: Download the drivers zip file snappydata-odbc_1.3.0_win64.zip using the steps provided here . Extract snappydata-odbc_1.3.0_win64.zip . Depending on your Windows installation, extract the contents of the 32-bit or 64-bit version of the SnappyData ODBC Driver. Version ODBC Driver 32-bit for 64-bit platform snappydata-odbc_1.3.0_win64_x86.msi 64-bit for 64-bit platform snappydata-odbc_1.3.0_win64_x64.msi Double-click on the corresponding msi file, and follow the steps to complete the installation. Note Ensure that SnappyData is installed and the SnappyData cluster is running .","title":"Step 2: Installing SnappyData ODBC Driver"},{"location":"howto/connect_using_odbc_driver/#connecting-to-the-snappydata-cluster","text":"Once you have installed the SnappyData ODBC Driver, you can connect to SnappyData cluster in any of the following ways: Use the SnappyData Driver Connection URL: Driver=SnappyData ODBC Driver;server=<ServerIP>;port=<ServerPort>;user=<userName>;password=<password> Create a SnappyData DSN (Data Source Name) using the installed SnappyData ODBC Driver. Refer to the Windows documentation relevant to your operating system for more information on creating a DSN. When prompted, select the SnappyData ODBC Driver from the list of drivers and enter a Data Source name, SnappyData Server Host, Port, User Name and Password. Refer to the documentation for detailed information on Setting Up SnappyData ODBC Driver .","title":"Connecting to the SnappyData Cluster"},{"location":"howto/connect_using_odbc_driver/#connecting-spotfire-desktop-to-snappydata","text":"Refer TIBCO Spotfire\u00ae Connectivity to SnappyData\u2122 for detailed instructions to access SnappyData using this connector. Also see: ODBC Supported APIs in SnappyData Driver","title":"Connecting Spotfire\u00ae Desktop to SnappyData"},{"location":"howto/connecttibcodv/","text":"How to Connect TIBCO\u00ae Data Virtualization to SnappyData \u00b6 TIBCO\u00ae Data Virtualization integrates disparate data sources in real-time instead of copying their data into a data warehouse. Do the following to connect TIBCO Data Virtulization (TDV) to SnappyData: Create a TDV Data Source Adapter Copy the JDBC Driver to the Adapter location Create a TDV Data Source for SnappyData instance Create a TDV Data Source Adapter \u00b6 In the TDV Studio go to File > New > Data Source . Select Custom Adapter > New Adapter and enter the following details as mentioned: Name: SnappyData Parent Adapter: Apache Hive 2.x Adapter Location: The location must default in a similar pattern to C:/Program Files/TIBCO/TDV Server 8.1/conf/adapters/custom/tibco_computedb Adapter Class Name: io.snappydata.jdbc.ClientDriver Connection URL Pattern: jdbc:snappydata:// : / Copy the JDBC Driver \u00b6 Download the JDBC driver from here . Copy the jar file to the Adapter location that is specified while creating the TDV Data Source Adapter. This location must default in a similar pattern to C:/Program Files/TIBCO/TDV Server 8.1/conf/adapters/custom/tibco_computedb . Restart the TDV Server to load the JDBC driver. Create a TDV Data Source for SnappyData Instance \u00b6 In the TDV Studio, go to File > New > Data Source . Find SnappyData and select Next . Enter the following details: Datasource Name: Name that is used to identify the instance of SnappyData. Host: URL of the server location. Port: usually 1527 or 1528 Database Name: This can be left blank. Login and Password details. Select Create and Introspect . Select the required tables and then click Next . Click Finished .","title":"How to Connect TIBCO\u00ae Data Virtualization to SnappyData"},{"location":"howto/connecttibcodv/#how-to-connect-tibco-data-virtualization-to-snappydata","text":"TIBCO\u00ae Data Virtualization integrates disparate data sources in real-time instead of copying their data into a data warehouse. Do the following to connect TIBCO Data Virtulization (TDV) to SnappyData: Create a TDV Data Source Adapter Copy the JDBC Driver to the Adapter location Create a TDV Data Source for SnappyData instance","title":"How to Connect TIBCO\u00ae Data Virtualization to SnappyData"},{"location":"howto/connecttibcodv/#create-a-tdv-data-source-adapter","text":"In the TDV Studio go to File > New > Data Source . Select Custom Adapter > New Adapter and enter the following details as mentioned: Name: SnappyData Parent Adapter: Apache Hive 2.x Adapter Location: The location must default in a similar pattern to C:/Program Files/TIBCO/TDV Server 8.1/conf/adapters/custom/tibco_computedb Adapter Class Name: io.snappydata.jdbc.ClientDriver Connection URL Pattern: jdbc:snappydata:// : /","title":"Create a TDV Data Source Adapter"},{"location":"howto/connecttibcodv/#copy-the-jdbc-driver","text":"Download the JDBC driver from here . Copy the jar file to the Adapter location that is specified while creating the TDV Data Source Adapter. This location must default in a similar pattern to C:/Program Files/TIBCO/TDV Server 8.1/conf/adapters/custom/tibco_computedb . Restart the TDV Server to load the JDBC driver.","title":"Copy the JDBC Driver"},{"location":"howto/connecttibcodv/#create-a-tdv-data-source-for-snappydata-instance","text":"In the TDV Studio, go to File > New > Data Source . Find SnappyData and select Next . Enter the following details: Datasource Name: Name that is used to identify the instance of SnappyData. Host: URL of the server location. Port: usually 1527 or 1528 Database Name: This can be left blank. Login and Password details. Select Create and Introspect . Select the required tables and then click Next . Click Finished .","title":"Create a TDV Data Source for SnappyData Instance"},{"location":"howto/connecttibcospotfire/","text":"How to Connect TIBCO Spotfire\u00ae Desktop to SnappyData \u00b6 TIBCO Spotfire\u00ae Desktop allows users to easily author and update ad-hoc analytics, applications, and dashboards. To connect TIBCO Spotfire\u00ae Desktop to SnappyData, setup and launch the SnappyData cluster. You can use any of the following methods to connect TIBCO Spotfire\u00ae Desktop to SnappyData: CDB Connector using Native ODBC driver CDB Connector using Simba ODBC Driver CDB Connector using Native ODBC Driver \u00b6 TIBCO recommends to use the native Spotfire Connector to connect TIBCO Spotfire\u00ae Desktop to SnappyData. This connector is based on native ODBC and is a faster connector. Refer TIBCO Spotfire\u00ae Connectivity to SnappyData\u2122 for detailed instructions to access SnappyData using this connector. CDB Connector using Simba ODBC Driver (Builtin CDB Connector in Spotfire 10.4) \u00b6 Users of Spotfire\u00ae 10.4 or later can use the builtin CDB connector in Spotfire\u00ae to connect TIBCO Spotfire\u00ae Desktop to SnappyData. Refer to Accessing Data from SnappyData for detailed instructions to access SnappyData using this connector.","title":"How to connect TIBCO Spotfire\u00ae Desktop to SnappyData"},{"location":"howto/connecttibcospotfire/#how-to-connect-tibco-spotfire-desktop-to-snappydata","text":"TIBCO Spotfire\u00ae Desktop allows users to easily author and update ad-hoc analytics, applications, and dashboards. To connect TIBCO Spotfire\u00ae Desktop to SnappyData, setup and launch the SnappyData cluster. You can use any of the following methods to connect TIBCO Spotfire\u00ae Desktop to SnappyData: CDB Connector using Native ODBC driver CDB Connector using Simba ODBC Driver","title":"How to Connect TIBCO Spotfire\u00ae Desktop to SnappyData"},{"location":"howto/connecttibcospotfire/#cdb-connector-using-native-odbc-driver","text":"TIBCO recommends to use the native Spotfire Connector to connect TIBCO Spotfire\u00ae Desktop to SnappyData. This connector is based on native ODBC and is a faster connector. Refer TIBCO Spotfire\u00ae Connectivity to SnappyData\u2122 for detailed instructions to access SnappyData using this connector.","title":"CDB Connector using Native ODBC Driver"},{"location":"howto/connecttibcospotfire/#cdb-connector-using-simba-odbc-driver-builtin-cdb-connector-in-spotfire-104","text":"Users of Spotfire\u00ae 10.4 or later can use the builtin CDB connector in Spotfire\u00ae to connect TIBCO Spotfire\u00ae Desktop to SnappyData. Refer to Accessing Data from SnappyData for detailed instructions to access SnappyData using this connector.","title":"CDB Connector using Simba ODBC Driver (Builtin CDB Connector in Spotfire 10.4)"},{"location":"howto/create_column_tables_and_run_queries/","text":"How to Create Column Tables and Run Queries \u00b6 Column tables organize and manage data in a columnar form such that modern day CPUs can traverse and run computations like a sum or an average fast (as the values are available in contiguous memory). Refer to the Row and column tables documentation for the complete list of attributes for column tables. Full source code, for example, to create and perform operations on column table can be found in CreateColumnTable.scala Create a Column Table using DataFrame API \u00b6 The code snippet below shows how to create a column table using DataFrame API. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"CreateColumnTable\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Define the table schema val tableSchema = StructType(Array(StructField(\"C_CUSTKEY\", IntegerType, false), StructField(\"C_NAME\", StringType, false), StructField(\"C_ADDRESS\", StringType, false), StructField(\"C_NATIONKEY\", IntegerType, false), StructField(\"C_PHONE\", StringType, false), StructField(\"C_ACCTBAL\", DecimalType(15, 2), false), StructField(\"C_MKTSEGMENT\", StringType, false), StructField(\"C_COMMENT\", StringType, false) )) Create the table and load data from CSV // props1 map specifies the properties for the table to be created // \"PARTITION_BY\" attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY) val props1 = Map(\"PARTITION_BY\" -> \"C_CUSTKEY\") snSession.createTable(\"CUSTOMER\", \"column\", tableSchema, props1) val tableSchema = snSession.table(\"CUSTOMER\").schema // insert some data in it // loading data in CUSTOMER table from a text file with delimited columns val customerDF = snSession.read.schema(schema = tableSchema).csv(\"quickstart/src/main/resources/customer.csv\") customerDF.write.insertInto(\"CUSTOMER\") Create a Column Table using SQL \u00b6 The same table can be created using SQL as shown below: snSession.sql(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER NOT NULL,\" + \"C_NAME VARCHAR(25) NOT NULL,\" + \"C_ADDRESS VARCHAR(40) NOT NULL,\" + \"C_NATIONKEY INTEGER NOT NULL,\" + \"C_PHONE VARCHAR(15) NOT NULL,\" + \"C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \"C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \"C_COMMENT VARCHAR(117) NOT NULL)\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") You can execute selected queries on a column table, join the column table with other tables, and append data to it.","title":"How to Create Column Tables and Run Queries"},{"location":"howto/create_column_tables_and_run_queries/#how-to-create-column-tables-and-run-queries","text":"Column tables organize and manage data in a columnar form such that modern day CPUs can traverse and run computations like a sum or an average fast (as the values are available in contiguous memory). Refer to the Row and column tables documentation for the complete list of attributes for column tables. Full source code, for example, to create and perform operations on column table can be found in CreateColumnTable.scala","title":"How to Create Column Tables and Run Queries"},{"location":"howto/create_column_tables_and_run_queries/#create-a-column-table-using-dataframe-api","text":"The code snippet below shows how to create a column table using DataFrame API. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"CreateColumnTable\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Define the table schema val tableSchema = StructType(Array(StructField(\"C_CUSTKEY\", IntegerType, false), StructField(\"C_NAME\", StringType, false), StructField(\"C_ADDRESS\", StringType, false), StructField(\"C_NATIONKEY\", IntegerType, false), StructField(\"C_PHONE\", StringType, false), StructField(\"C_ACCTBAL\", DecimalType(15, 2), false), StructField(\"C_MKTSEGMENT\", StringType, false), StructField(\"C_COMMENT\", StringType, false) )) Create the table and load data from CSV // props1 map specifies the properties for the table to be created // \"PARTITION_BY\" attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY) val props1 = Map(\"PARTITION_BY\" -> \"C_CUSTKEY\") snSession.createTable(\"CUSTOMER\", \"column\", tableSchema, props1) val tableSchema = snSession.table(\"CUSTOMER\").schema // insert some data in it // loading data in CUSTOMER table from a text file with delimited columns val customerDF = snSession.read.schema(schema = tableSchema).csv(\"quickstart/src/main/resources/customer.csv\") customerDF.write.insertInto(\"CUSTOMER\")","title":"Create a Column Table using DataFrame API"},{"location":"howto/create_column_tables_and_run_queries/#create-a-column-table-using-sql","text":"The same table can be created using SQL as shown below: snSession.sql(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER NOT NULL,\" + \"C_NAME VARCHAR(25) NOT NULL,\" + \"C_ADDRESS VARCHAR(40) NOT NULL,\" + \"C_NATIONKEY INTEGER NOT NULL,\" + \"C_PHONE VARCHAR(15) NOT NULL,\" + \"C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \"C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \"C_COMMENT VARCHAR(117) NOT NULL)\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") You can execute selected queries on a column table, join the column table with other tables, and append data to it.","title":"Create a Column Table using SQL"},{"location":"howto/create_row_tables_and_run_queries/","text":"How to Create Row Tables and Run Queries \u00b6 Each record in a Row table is managed in contiguous memory, and therefore, optimized for selective queries (For example. key based point lookup ) or updates. A row table can either be replicated to all nodes or partitioned across nodes. It can be created by using DataFrame API or using SQL. Refer to the Row and column tables documentation for complete list of attributes for row tables. Full source code, for example, to create and perform operations on replicated and partitioned row table can be found in CreateReplicatedRowTable.scala and CreatePartitionedRowTable.scala Create a Row Table using DataFrame API: \u00b6 The code snippet below shows how to create a replicated row table using API. Get a SnappySession val spark: SparkSession = SparkSession .builder .appName(\"CreateReplicatedRowTable\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) import org.apache.spark.sql.types._ Create the Table using API : First, define the table schema and then create the table using createTable API val schema = StructType(Array(StructField(\"S_SUPPKEY\", IntegerType, false), StructField(\"S_NAME\", StringType, false), StructField(\"S_ADDRESS\", StringType, false), StructField(\"S_NATIONKEY\", IntegerType, false), StructField(\"S_PHONE\", StringType, false), StructField(\"S_ACCTBAL\", DecimalType(15, 2), false), StructField(\"S_COMMENT\", StringType, false) )) // props1 map specifies the properties for the table to be created // \"PERSISTENCE\" flag indicates that the table data should be persisted to // disk asynchronously val props1 = Map(\"PERSISTENCE\" -> \"asynchronous\") // create a row table using createTable API snSession.createTable(\"SUPPLIER\", \"row\", schema, props1) Creating a Row table using SQL : The same table can be created using SQL as shown below: // First drop the table if it exists snSession.sql(\"DROP TABLE IF EXISTS SUPPLIER\") // Create a row table using SQL // \"PERSISTENCE\" that the table data should be persisted to disk asynchronously // For complete list of attributes refer the documentation snSession.sql( \"CREATE TABLE SUPPLIER ( \" + \"S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \" + \"S_NAME STRING NOT NULL, \" + \"S_ADDRESS STRING NOT NULL, \" + \"S_NATIONKEY INTEGER NOT NULL, \" + \"S_PHONE STRING NOT NULL, \" + \"S_ACCTBAL DECIMAL(15, 2) NOT NULL, \" + \"S_COMMENT STRING NOT NULL \" + \") USING ROW OPTIONS (PERSISTENCE 'asynchronous')\") You can perform various operations such as inset data, mutate it (update/delete), select data from the table. All these operations can be done either through APIs or by using SQL queries. For example: To insert data in the SUPPLIER table: snSession.sql(\"INSERT INTO SUPPLIER VALUES(1, 'SUPPLIER1', 'CHICAGO, IL', 0, '555-543-789', 10000, ' ')\") snSession.sql(\"INSERT INTO SUPPLIER VALUES(2, 'SUPPLIER2', 'BOSTON, MA', 0, '555-234-489', 20000, ' ')\") snSession.sql(\"INSERT INTO SUPPLIER VALUES(3, 'SUPPLIER3', 'NEWYORK, NY', 0, '555-743-785', 34000, ' ')\") snSession.sql(\"INSERT INTO SUPPLIER VALUES(4, 'SUPPLIER4', 'SANHOSE, CA', 0, '555-321-098', 1000, ' ')\") To print the contents of the SUPPLIER table: var tableData = snSession.sql(\"SELECT * FROM SUPPLIER\").collect() tableData.foreach(println) To update the table account balance for SUPPLIER4: snSession.sql(\"UPDATE SUPPLIER SET S_ACCTBAL = 50000 WHERE S_NAME = 'SUPPLIER4'\") To print contents of the SUPPLIER table after update tableData = snSession.sql(\"SELECT * FROM SUPPLIER\").collect() tableData.foreach(println) To delete the records for SUPPLIER2 and SUPPLIER3 snSession.sql(\"DELETE FROM SUPPLIER WHERE S_NAME = 'SUPPLIER2' OR S_NAME = 'SUPPLIER3'\") To print the contents of the SUPPLIER table after delete tableData = snSession.sql(\"SELECT * FROM SUPPLIER\").collect() tableData.foreach(println)","title":"How to Create Row Tables and Run Queries"},{"location":"howto/create_row_tables_and_run_queries/#how-to-create-row-tables-and-run-queries","text":"Each record in a Row table is managed in contiguous memory, and therefore, optimized for selective queries (For example. key based point lookup ) or updates. A row table can either be replicated to all nodes or partitioned across nodes. It can be created by using DataFrame API or using SQL. Refer to the Row and column tables documentation for complete list of attributes for row tables. Full source code, for example, to create and perform operations on replicated and partitioned row table can be found in CreateReplicatedRowTable.scala and CreatePartitionedRowTable.scala","title":"How to Create Row Tables and Run Queries"},{"location":"howto/create_row_tables_and_run_queries/#create-a-row-table-using-dataframe-api","text":"The code snippet below shows how to create a replicated row table using API. Get a SnappySession val spark: SparkSession = SparkSession .builder .appName(\"CreateReplicatedRowTable\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) import org.apache.spark.sql.types._ Create the Table using API : First, define the table schema and then create the table using createTable API val schema = StructType(Array(StructField(\"S_SUPPKEY\", IntegerType, false), StructField(\"S_NAME\", StringType, false), StructField(\"S_ADDRESS\", StringType, false), StructField(\"S_NATIONKEY\", IntegerType, false), StructField(\"S_PHONE\", StringType, false), StructField(\"S_ACCTBAL\", DecimalType(15, 2), false), StructField(\"S_COMMENT\", StringType, false) )) // props1 map specifies the properties for the table to be created // \"PERSISTENCE\" flag indicates that the table data should be persisted to // disk asynchronously val props1 = Map(\"PERSISTENCE\" -> \"asynchronous\") // create a row table using createTable API snSession.createTable(\"SUPPLIER\", \"row\", schema, props1) Creating a Row table using SQL : The same table can be created using SQL as shown below: // First drop the table if it exists snSession.sql(\"DROP TABLE IF EXISTS SUPPLIER\") // Create a row table using SQL // \"PERSISTENCE\" that the table data should be persisted to disk asynchronously // For complete list of attributes refer the documentation snSession.sql( \"CREATE TABLE SUPPLIER ( \" + \"S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \" + \"S_NAME STRING NOT NULL, \" + \"S_ADDRESS STRING NOT NULL, \" + \"S_NATIONKEY INTEGER NOT NULL, \" + \"S_PHONE STRING NOT NULL, \" + \"S_ACCTBAL DECIMAL(15, 2) NOT NULL, \" + \"S_COMMENT STRING NOT NULL \" + \") USING ROW OPTIONS (PERSISTENCE 'asynchronous')\") You can perform various operations such as inset data, mutate it (update/delete), select data from the table. All these operations can be done either through APIs or by using SQL queries. For example: To insert data in the SUPPLIER table: snSession.sql(\"INSERT INTO SUPPLIER VALUES(1, 'SUPPLIER1', 'CHICAGO, IL', 0, '555-543-789', 10000, ' ')\") snSession.sql(\"INSERT INTO SUPPLIER VALUES(2, 'SUPPLIER2', 'BOSTON, MA', 0, '555-234-489', 20000, ' ')\") snSession.sql(\"INSERT INTO SUPPLIER VALUES(3, 'SUPPLIER3', 'NEWYORK, NY', 0, '555-743-785', 34000, ' ')\") snSession.sql(\"INSERT INTO SUPPLIER VALUES(4, 'SUPPLIER4', 'SANHOSE, CA', 0, '555-321-098', 1000, ' ')\") To print the contents of the SUPPLIER table: var tableData = snSession.sql(\"SELECT * FROM SUPPLIER\").collect() tableData.foreach(println) To update the table account balance for SUPPLIER4: snSession.sql(\"UPDATE SUPPLIER SET S_ACCTBAL = 50000 WHERE S_NAME = 'SUPPLIER4'\") To print contents of the SUPPLIER table after update tableData = snSession.sql(\"SELECT * FROM SUPPLIER\").collect() tableData.foreach(println) To delete the records for SUPPLIER2 and SUPPLIER3 snSession.sql(\"DELETE FROM SUPPLIER WHERE S_NAME = 'SUPPLIER2' OR S_NAME = 'SUPPLIER3'\") To print the contents of the SUPPLIER table after delete tableData = snSession.sql(\"SELECT * FROM SUPPLIER\").collect() tableData.foreach(println)","title":"Create a Row Table using DataFrame API:"},{"location":"howto/export_hdfs/","text":"How to Export and Restore Table Data using HDFS \u00b6 In SnappyData, table data is stored in memory and on disk (depending on the configuration). As SnappyData supports Spark APIs, table data can be exported to HDFS using Spark APIs. This can be used to backup your tables to HDFS. Tip When performing a backup of your tables to HDFS, it is a good practice to export data during a period of low activity in your system. The export does not block any activities in the distributed system, but it does use file system resources on all hosts in your distributed system and can affect performance. For example, as shown below you can create a DataFrame for a table and save it as parquet file. // created a DataFrame for table \"APP.CUSTOMER\" val df = snappySession.table(\"APP.CUSTOMER\") // save it as parquet file on HDFS df.write.parquet(\"hdfs://127.0.0.1:9000/customer\") Refer to How to Run Spark Code inside the Cluster to understand how to write a SnappyData job. The above can be added to the runSnappyJob() function of the SnappyData job. You can also import this data back into SnappyData tables. For example using SQL, create an external table and import the data: snappy> CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING parquet OPTIONS (path 'hdfs://127.0.0.1:9000/customer', header 'true', inferSchema 'true'); snappy> insert into customer select * from CUSTOMER_STAGING_1; Or by using APIs (as a part of SnappyData job). Refer to How to Run Spark Code inside the Cluster for more information. // create a DataFrame using parquet val df2 = snappySession.read.parquet(\"hdfs://127.0.0.1:9000/customer\") // insetert the data into table df2.write.mode(SaveMode.Append).saveAsTable(\"APP.CUSTOMER\") Note Snappydata supports kerberized Hadoop cluster in Smart connector mode only. You need to set HADOOP_CONF_DIR in spark-env.sh and snappy-env.sh . Currently the Embedded mode(Snappy job and Snappy shell) and Smart Connector with standalone mode are NOT supported. Smart connector with local and YARN mode are supported.","title":"How to Export and Restore Table Data using HDFS"},{"location":"howto/export_hdfs/#how-to-export-and-restore-table-data-using-hdfs","text":"In SnappyData, table data is stored in memory and on disk (depending on the configuration). As SnappyData supports Spark APIs, table data can be exported to HDFS using Spark APIs. This can be used to backup your tables to HDFS. Tip When performing a backup of your tables to HDFS, it is a good practice to export data during a period of low activity in your system. The export does not block any activities in the distributed system, but it does use file system resources on all hosts in your distributed system and can affect performance. For example, as shown below you can create a DataFrame for a table and save it as parquet file. // created a DataFrame for table \"APP.CUSTOMER\" val df = snappySession.table(\"APP.CUSTOMER\") // save it as parquet file on HDFS df.write.parquet(\"hdfs://127.0.0.1:9000/customer\") Refer to How to Run Spark Code inside the Cluster to understand how to write a SnappyData job. The above can be added to the runSnappyJob() function of the SnappyData job. You can also import this data back into SnappyData tables. For example using SQL, create an external table and import the data: snappy> CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING parquet OPTIONS (path 'hdfs://127.0.0.1:9000/customer', header 'true', inferSchema 'true'); snappy> insert into customer select * from CUSTOMER_STAGING_1; Or by using APIs (as a part of SnappyData job). Refer to How to Run Spark Code inside the Cluster for more information. // create a DataFrame using parquet val df2 = snappySession.read.parquet(\"hdfs://127.0.0.1:9000/customer\") // insetert the data into table df2.write.mode(SaveMode.Append).saveAsTable(\"APP.CUSTOMER\") Note Snappydata supports kerberized Hadoop cluster in Smart connector mode only. You need to set HADOOP_CONF_DIR in spark-env.sh and snappy-env.sh . Currently the Embedded mode(Snappy job and Snappy shell) and Smart Connector with standalone mode are NOT supported. Smart connector with local and YARN mode are supported.","title":"How to Export and Restore Table Data using HDFS"},{"location":"howto/import_from_hive_table/","text":"How to Import Data from Hive Table into SnappyData Table \u00b6 Using a Snappy session, you can read an existing hive tables that are defined in an external hive catalog, use hive tables as external tables from SnappySession for queries, including joins with tables defined in SnappyData catalog, and also define new Hive table or view to be stored in external hive catalog. Using a SnappySession, you can also define a new hive table or view to be stored in an external hive catalog. When working with Hive, one must instantiate Snappy session with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. If the underlying storage for Hive is HDFS, you can configure Hive with Snappy session. For this, you must place hive-site.xml , core-site.xml (for security configuration) and hdfs-site.xml (for HDFS configuration) files in the conf/ folder of Snappy. In addition to this, you must configure spark-env.sh file into the conf/ folder. The content in the hadoop_spark-env.sh file should be as follows: export SPARK_DIST_CLASSPATH=$(/home/user/hadoop-2.7.3/bin/hadoop classpath) Snappy has been tested with default hive database i.e. embedded derby database. User can also use and configure the remote metastore as well like SQL. In the hive-site xml, you must configure the parameters as per the requirement. With derby as database, the following are the hive-site.xml configuration: <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:derby:;databaseName=metastore_db;create=true</value> <description> JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. </description> </property> If you want to setup remote meta store instead of using default database derby, you can use the following configuration: <property> <name>hive.metastore.uris</name> <value>thrift:/hostname:9083</value> <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description> </property> Run the following steps to test Snappy with Apache Hadoop: Start the Hadoop daemons. Start the Hive thrift server. Start the Snappy-shell. After starting the Snappy Shell, you can do the following: # To point to external hive catalog from snappy session, set the below property. set spark.sql.catalogImplementation=hive. snappy-sql> set spark.sql.catalogImplementation=hive; This property can be set at the session level and global level. # To point to Snappy internal catalog from snappy session. set spark.sql.catalogImplementation=in-memory. snappy-sql> set spark.sql.catalogImplementation=in-memory; # To access hive tables use below command. snappy-sql> show tables in default; Please note that it is mandatory to specify the schema \u2018default\u2019 if any other schema is not created. If any other schema is created then it is mandatory to use the created schema name. For example, if schema / database hiveDB created then use, snappy-sql> show tables in hiveDB; # To read the hive tables from snappy. snappy-sql> SELECT FirstName, LastName FROM default.hiveemployees ORDER BY LastName; # To join Snappy tables and Hive tables. snappy-sql> SELECT emp.EmployeeID, emp.FirstName, emp.LastName, o.OrderID, o.OrderDate FROM default.hive_employees emp JOIN snappy_orders o ON (emp.EmployeeID = o.EmployeeID) ORDER BY o.OrderDate; # To create the hive table and insert the data into it from Snappy. snappy-sql> create table if not exists default.t1(id int) row format delimited fields terminated by ','; snappy-sql> insert into default.t1 select id, concat(id) from range(100); If you have not configure any of the configuration files mentioned above( hive-site.xml, core-site.xml, hdfs-site.xml) and started the Hadoop and Hive daemons, you will see the following error in SnappyData: No Datastore found in the Distributed System for 'execution on remote node null'. Note You must configure the configuration files hive-site.xml and core-site.xml as per your requirements. If you have connected to Hive and Hadoop and in case the configuration files get removed or deleted, errors or exceptions will not be shown. Therefore, you cannot perform any DDL and DML operations in Hive. For more details, refer the following links: https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started","title":"How to Import Data from Hive Table into SnappyData Table"},{"location":"howto/import_from_hive_table/#how-to-import-data-from-hive-table-into-snappydata-table","text":"Using a Snappy session, you can read an existing hive tables that are defined in an external hive catalog, use hive tables as external tables from SnappySession for queries, including joins with tables defined in SnappyData catalog, and also define new Hive table or view to be stored in external hive catalog. Using a SnappySession, you can also define a new hive table or view to be stored in an external hive catalog. When working with Hive, one must instantiate Snappy session with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. If the underlying storage for Hive is HDFS, you can configure Hive with Snappy session. For this, you must place hive-site.xml , core-site.xml (for security configuration) and hdfs-site.xml (for HDFS configuration) files in the conf/ folder of Snappy. In addition to this, you must configure spark-env.sh file into the conf/ folder. The content in the hadoop_spark-env.sh file should be as follows: export SPARK_DIST_CLASSPATH=$(/home/user/hadoop-2.7.3/bin/hadoop classpath) Snappy has been tested with default hive database i.e. embedded derby database. User can also use and configure the remote metastore as well like SQL. In the hive-site xml, you must configure the parameters as per the requirement. With derby as database, the following are the hive-site.xml configuration: <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:derby:;databaseName=metastore_db;create=true</value> <description> JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. </description> </property> If you want to setup remote meta store instead of using default database derby, you can use the following configuration: <property> <name>hive.metastore.uris</name> <value>thrift:/hostname:9083</value> <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description> </property> Run the following steps to test Snappy with Apache Hadoop: Start the Hadoop daemons. Start the Hive thrift server. Start the Snappy-shell. After starting the Snappy Shell, you can do the following: # To point to external hive catalog from snappy session, set the below property. set spark.sql.catalogImplementation=hive. snappy-sql> set spark.sql.catalogImplementation=hive; This property can be set at the session level and global level. # To point to Snappy internal catalog from snappy session. set spark.sql.catalogImplementation=in-memory. snappy-sql> set spark.sql.catalogImplementation=in-memory; # To access hive tables use below command. snappy-sql> show tables in default; Please note that it is mandatory to specify the schema \u2018default\u2019 if any other schema is not created. If any other schema is created then it is mandatory to use the created schema name. For example, if schema / database hiveDB created then use, snappy-sql> show tables in hiveDB; # To read the hive tables from snappy. snappy-sql> SELECT FirstName, LastName FROM default.hiveemployees ORDER BY LastName; # To join Snappy tables and Hive tables. snappy-sql> SELECT emp.EmployeeID, emp.FirstName, emp.LastName, o.OrderID, o.OrderDate FROM default.hive_employees emp JOIN snappy_orders o ON (emp.EmployeeID = o.EmployeeID) ORDER BY o.OrderDate; # To create the hive table and insert the data into it from Snappy. snappy-sql> create table if not exists default.t1(id int) row format delimited fields terminated by ','; snappy-sql> insert into default.t1 select id, concat(id) from range(100); If you have not configure any of the configuration files mentioned above( hive-site.xml, core-site.xml, hdfs-site.xml) and started the Hadoop and Hive daemons, you will see the following error in SnappyData: No Datastore found in the Distributed System for 'execution on remote node null'. Note You must configure the configuration files hive-site.xml and core-site.xml as per your requirements. If you have connected to Hive and Hadoop and in case the configuration files get removed or deleted, errors or exceptions will not be shown. Therefore, you cannot perform any DDL and DML operations in Hive. For more details, refer the following links: https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started","title":"How to Import Data from Hive Table into SnappyData Table"},{"location":"howto/load_data_from_external_data_stores/","text":"How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc) \u00b6 SnappyData comes bundled with the libraries to access HDFS (Apache compatible). You can load your data using SQL or DataFrame API. Example - Loading data from CSV file using SQL \u00b6 // Create an external table based on CSV file CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING csv OPTIONS (path '../../quickstart/src/main/resources/customer_with_headers.csv', header 'true', inferSchema 'true'); // Create a SnappyData table and load data into CUSTOMER table CREATE TABLE CUSTOMER using column options() as (select * from CUSTOMER_STAGING_1); Tip Similarly, you can create an external table for all data sources and use SQL \"insert into\" query to load data. For more information on creating external tables refer to, CREATE EXTERNAL TABLE Example - Loading CSV Files from HDFS using API \u00b6 The example below demonstrates how you can read CSV files from HDFS using an API: val dataDF=snc.read.option(\"header\",\"true\").csv (\"hdfs://namenode-uri:port/path/to/customer_with_headers.csv\") // Drop table if it exists snc.sql(\"drop table if exists CUSTOMER\") // Load data into table dataDF.write.format(\"column\").saveAsTable(\"CUSTOMER\") Example - Loading and Enriching CSV Data from HDFS \u00b6 The example below demonstrates how you can load and enrich CSV Data from HDFS: val dataDF = snappy.read.option(\"header\", \"true\") .csv(\"hdfs://namenode-uri:port/path/to/customers.csv\") // Drop table if it exists and create it with only required fields snappy.sql(\"drop table if exists CUSTOMER\") snappy.sql(\"create table CUSTOMER(C_CUSTKEY INTEGER NOT NULL\" + \", C_NAME VARCHAR(25) NOT NULL,\" + \" C_ADDRESS VARCHAR(40) NOT NULL,\" + \" C_NATIONKEY INTEGER NOT NULL,\" + \" C_PHONE VARCHAR(15) NOT NULL,\" + \" C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \" C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \" C_COMMENT VARCHAR(117) NOT NULL) using column options()\") // Project and transform data from df and load it in table. import snappy.implicits._ dataDF.select($\"C_CUSTKEY\", $\"C_NAME\", $\"C_ADDRESS\", $\"C_NATIONKEY\", $\"C_PHONE\", $\"C_ACCTBAL\" + 100, $\"C_MKTSEGMENT\", $\"C_COMMENT\".substr(1, 5).alias(\"SHORT_COMMENT\")).write.insertInto(\"CUSTOMER\") Example - Loading from Hive \u00b6 As SnappyData manages the catalog at all times and it is not possible to configure an external Hive catalog service like in Spark when using a SnappySession. But, it is still possible to access Hive using the native SparkSession (with enableHiveSupport set to true ). Here is an example using the SparkSession(spark object below) to access a Hive table as a DataFrame, then converted to an RDD so it can be passed to a SnappySession to store it in a SnappyData Table. val ds = spark.table(\"hiveTable\") val rdd = ds.rdd val session = new SnappySession(sparkContext) val df = session.createDataFrame(rdd, ds.schema) df.write.format(\"column\").saveAsTable(\"columnTable\") Importing Data using JDBC from a relational DB \u00b6 Note Before you begin, you must install the corresponding JDBC driver. Refer to Deploying Third Party Connectors . The example below demonstrates how to connect to any SQL database using JDBC: Verify and load the SQL Driver: Class.forName(\"com.mysql.jdbc.Driver\") Specify all the properties to access the database import java.util.Properties val jdbcUsername = \"USER_NAME\" val jdbcPassword = \"PASSWORD\" val jdbcHostname = \"HOSTNAME\" val jdbcPort = 3306 val jdbcDatabase =\"DATABASE\" val jdbcUrl = s\"jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?user=${jdbcUsername}&password=${jdbcPassword}&relaxAutoCommit=true\" val connectionProperties = new Properties() connectionProperties.put(\"user\", \"USERNAME\") connectionProperties.put(\"password\", \"PASSWORD\") Fetch the table meta data from the RDB and creates equivalent column tables val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword) connection.isClosed() val md:DatabaseMetaData = connection.getMetaData(); val rs:ResultSet = md.getTables(null, null, \"%\", null); while (rs.next()) { val tableName=rs.getString(3) val df=snc.read.jdbc(jdbcUrl, tableName, connectionProperties) df.printSchema df.show() // Create and load a column table with same schema as that of source table df.write.format(\"column\").mode(SaveMode.Append).saveAsTable(tableName) } Using SQL to access external RDB tables You can also use plain SQL to access any external RDB using external tables. Create external table on RDBMS table and query it directly from SnappyData as described below: snc.sql(\"drop table if exists external_table\") snc.sql(s\"CREATE external TABLE external_table USING jdbc OPTIONS (dbtable 'tweet', driver 'com.mysql.jdbc.Driver', user 'root', password 'root', url '$jdbcUrl')\") snc.sql(\"select * from external_table\").show Refer to the Spark SQL JDBC source access for how to parallelize access when dealing with large data sets . Loading Data from NoSQL store (Cassandra) \u00b6 The example below demonstrates how you can load data from a NoSQL store: Note Before you begin, you must install the corresponding Spark-Cassandra connector jar. Refer to Deploying Third Party Connectors . val df = snc.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"CUSTOMER\", \"keyspace\" -> \"test\")) .load df.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"CUSTOMER\") snc.sql(\"select * from CUSTOMER\").show","title":"How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)"},{"location":"howto/load_data_from_external_data_stores/#how-to-load-data-from-external-data-stores-eg-hdfs-cassandra-hive-etc","text":"SnappyData comes bundled with the libraries to access HDFS (Apache compatible). You can load your data using SQL or DataFrame API.","title":"How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)"},{"location":"howto/load_data_from_external_data_stores/#example-loading-data-from-csv-file-using-sql","text":"// Create an external table based on CSV file CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING csv OPTIONS (path '../../quickstart/src/main/resources/customer_with_headers.csv', header 'true', inferSchema 'true'); // Create a SnappyData table and load data into CUSTOMER table CREATE TABLE CUSTOMER using column options() as (select * from CUSTOMER_STAGING_1); Tip Similarly, you can create an external table for all data sources and use SQL \"insert into\" query to load data. For more information on creating external tables refer to, CREATE EXTERNAL TABLE","title":"Example - Loading data from CSV file using SQL"},{"location":"howto/load_data_from_external_data_stores/#example-loading-csv-files-from-hdfs-using-api","text":"The example below demonstrates how you can read CSV files from HDFS using an API: val dataDF=snc.read.option(\"header\",\"true\").csv (\"hdfs://namenode-uri:port/path/to/customer_with_headers.csv\") // Drop table if it exists snc.sql(\"drop table if exists CUSTOMER\") // Load data into table dataDF.write.format(\"column\").saveAsTable(\"CUSTOMER\")","title":"Example - Loading CSV Files from HDFS using API"},{"location":"howto/load_data_from_external_data_stores/#example-loading-and-enriching-csv-data-from-hdfs","text":"The example below demonstrates how you can load and enrich CSV Data from HDFS: val dataDF = snappy.read.option(\"header\", \"true\") .csv(\"hdfs://namenode-uri:port/path/to/customers.csv\") // Drop table if it exists and create it with only required fields snappy.sql(\"drop table if exists CUSTOMER\") snappy.sql(\"create table CUSTOMER(C_CUSTKEY INTEGER NOT NULL\" + \", C_NAME VARCHAR(25) NOT NULL,\" + \" C_ADDRESS VARCHAR(40) NOT NULL,\" + \" C_NATIONKEY INTEGER NOT NULL,\" + \" C_PHONE VARCHAR(15) NOT NULL,\" + \" C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \" C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \" C_COMMENT VARCHAR(117) NOT NULL) using column options()\") // Project and transform data from df and load it in table. import snappy.implicits._ dataDF.select($\"C_CUSTKEY\", $\"C_NAME\", $\"C_ADDRESS\", $\"C_NATIONKEY\", $\"C_PHONE\", $\"C_ACCTBAL\" + 100, $\"C_MKTSEGMENT\", $\"C_COMMENT\".substr(1, 5).alias(\"SHORT_COMMENT\")).write.insertInto(\"CUSTOMER\")","title":"Example - Loading and Enriching CSV Data from HDFS"},{"location":"howto/load_data_from_external_data_stores/#example-loading-from-hive","text":"As SnappyData manages the catalog at all times and it is not possible to configure an external Hive catalog service like in Spark when using a SnappySession. But, it is still possible to access Hive using the native SparkSession (with enableHiveSupport set to true ). Here is an example using the SparkSession(spark object below) to access a Hive table as a DataFrame, then converted to an RDD so it can be passed to a SnappySession to store it in a SnappyData Table. val ds = spark.table(\"hiveTable\") val rdd = ds.rdd val session = new SnappySession(sparkContext) val df = session.createDataFrame(rdd, ds.schema) df.write.format(\"column\").saveAsTable(\"columnTable\")","title":"Example - Loading from Hive"},{"location":"howto/load_data_from_external_data_stores/#importing-data-using-jdbc-from-a-relational-db","text":"Note Before you begin, you must install the corresponding JDBC driver. Refer to Deploying Third Party Connectors . The example below demonstrates how to connect to any SQL database using JDBC: Verify and load the SQL Driver: Class.forName(\"com.mysql.jdbc.Driver\") Specify all the properties to access the database import java.util.Properties val jdbcUsername = \"USER_NAME\" val jdbcPassword = \"PASSWORD\" val jdbcHostname = \"HOSTNAME\" val jdbcPort = 3306 val jdbcDatabase =\"DATABASE\" val jdbcUrl = s\"jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?user=${jdbcUsername}&password=${jdbcPassword}&relaxAutoCommit=true\" val connectionProperties = new Properties() connectionProperties.put(\"user\", \"USERNAME\") connectionProperties.put(\"password\", \"PASSWORD\") Fetch the table meta data from the RDB and creates equivalent column tables val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword) connection.isClosed() val md:DatabaseMetaData = connection.getMetaData(); val rs:ResultSet = md.getTables(null, null, \"%\", null); while (rs.next()) { val tableName=rs.getString(3) val df=snc.read.jdbc(jdbcUrl, tableName, connectionProperties) df.printSchema df.show() // Create and load a column table with same schema as that of source table df.write.format(\"column\").mode(SaveMode.Append).saveAsTable(tableName) } Using SQL to access external RDB tables You can also use plain SQL to access any external RDB using external tables. Create external table on RDBMS table and query it directly from SnappyData as described below: snc.sql(\"drop table if exists external_table\") snc.sql(s\"CREATE external TABLE external_table USING jdbc OPTIONS (dbtable 'tweet', driver 'com.mysql.jdbc.Driver', user 'root', password 'root', url '$jdbcUrl')\") snc.sql(\"select * from external_table\").show Refer to the Spark SQL JDBC source access for how to parallelize access when dealing with large data sets .","title":"Importing Data using JDBC from a relational DB"},{"location":"howto/load_data_from_external_data_stores/#loading-data-from-nosql-store-cassandra","text":"The example below demonstrates how you can load data from a NoSQL store: Note Before you begin, you must install the corresponding Spark-Cassandra connector jar. Refer to Deploying Third Party Connectors . val df = snc.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"CUSTOMER\", \"keyspace\" -> \"test\")) .load df.write.format(\"column\").mode(SaveMode.Append).saveAsTable(\"CUSTOMER\") snc.sql(\"select * from CUSTOMER\").show","title":"Loading Data from NoSQL store (Cassandra)"},{"location":"howto/load_data_into_snappydata_tables/","text":"How to Load Data into SnappyData Tables \u00b6 SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. By integrating the loading mechanism with the Query engine (Catalyst optimizer) it is often possible to push down filters and projections all the way to the data source minimizing data transfer. Here is the list of important features: Support for many Sources There is built-in support for many data sources as well as data formats. Data can be accessed from S3, file system, HDFS, Hive, RDB, etc. And the loaders have built-in support to handle CSV, Parquet, ORC, Avro, JSON, Java/Scala Objects, etc as the data formats. Access virtually any modern data store Virtually all major data providers have a native Spark connector that complies with the Data Sources API. For e.g. you can load data from any RDB like Amazon Redshift, Cassandra, Redis, Elastic Search, Neo4J, etc. While these connectors are not built-in, you can easily deploy these connectors as dependencies into a SnappyData cluster. All the connectors are typically registered in spark-packages.org Avoid Schema wrangling Spark supports schema inference. Which means, all you need to do is point to the external source in your 'create table' DDL (or Spark SQL API) and schema definition is learned by reading in the data. There is no need to explicitly define each column and type. This is extremely useful when dealing with disparate, complex and wide data sets. Read nested, sparse data sets When data is accessed from a source, the schema inference occurs by not just reading a header but often by reading the entire data set. For instance, when reading JSON files the structure could change from document to document. The inference engine builds up the schema as it reads each record and keeps unioning them to create a unified schema. This approach allows developers to become very productive with disparate data sets. Load using Spark API or SQL You can use SQL to point to any data source or use the native Spark Scala/Java API to load. For instance, you can first create an external table . CREATE EXTERNAL TABLE <tablename> USING <any-data-source-supported> OPTIONS <options> Next, use it in any SQL query or DDL. For example, CREATE EXTERNAL TABLE STAGING_CUSTOMER USING parquet OPTIONS(path 'quickstart/src/main/resources/customerparquet') CREATE TABLE CUSTOMER USING column OPTIONS(buckets '8') AS ( SELECT * FROM STAGING_CUSTOMER) Example - Load from CSV You can either explicitly define the schema or infer the schema and the column data types. To infer the column names, we need the CSV header to specify the names. In this example we don't have the names, so we explicitly define the schema. // Get a SnappySession in a local cluster val spark: SparkSession = SparkSession .builder .appName(\"CreateColumnTable\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) We explicitly define the table definition first .... snSession.sql(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER NOT NULL,\" + \"C_NAME VARCHAR(25) NOT NULL,\" + \"C_ADDRESS VARCHAR(40) NOT NULL,\" + \"C_NATIONKEY INTEGER NOT NULL,\" + \"C_PHONE VARCHAR(15) NOT NULL,\" + \"C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \"C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \"C_COMMENT VARCHAR(117) NOT NULL)\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") Load data in the CUSTOMER table from a CSV file by using Data Sources API val tableSchema = snSession.table(\"CUSTOMER\").schema val customerDF = snSession.read.schema(schema = tableSchema).csv(s\"$dataFolder/customer.csv\") customerDF.write.insertInto(\"CUSTOMER\") The Spark SQL programming guide provides a full description of the Data Sources API Example - Load from Parquet files val customerDF = snSession.read.parquet(s\"$dataDir/customer_parquet\") customerDF.write.insertInto(\"CUSTOMER\") Inferring schema from data file A schema for the table can be inferred from the data file. Data is first introspected to learn the schema (column names and types) without requring this input from the user. The example below illustrates reading a parquet data source and creates a new columnar table in SnappyData. The schema is automatically defined when the Parquet data files are read. val customerDF = snSession.read.parquet(s\"quickstart/src/main/resources/customerparquet\") // props1 map specifies the properties for the table to be created // \"PARTITION_BY\" attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY) val props1 = Map(\"PARTITION_BY\" -> \"C_CUSTKEY\") customerDF.write.format(\"column\").mode(\"append\").options(props1).saveAsTable(\"CUSTOMER\") In the code snippet below a schema is inferred from a CSV file. Column names are derived from the header in the file. val customer_csv_DF = snSession.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\").csv(\"quickstart/src/main/resources/customer_with_headers.csv\") // props1 map specifies the properties for the table to be created // \"PARTITION_BY\" attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY), // For complete list of attributes refer the documentation val props1 = Map(\"PARTITION_BY\" -> \"C_CUSTKEY\") customer_csv_DF.write.format(\"column\").mode(\"append\").options(props1).saveAsTable(\"CUSTOMER\") The source code to load the data from a CSV/Parquet files is in CreateColumnTable.scala . Example - reading JSON documents As mentioned before when dealing with JSON you have two challenges - (1) the data can be highly nested (2) the structure of the documents can keep changing. Here is a simple example that loads multiple JSON records that show dealing with schema changes across documents - WorkingWithJson.scala Note When loading data from sources like CSV or Parquet the files would need to be accessible from all the cluster members in SnappyData. Make sure it is NFS mounted or made accessible through the Cloud solution (shared storage like S3).","title":"How to Load Data into SnappyData Tables"},{"location":"howto/load_data_into_snappydata_tables/#how-to-load-data-into-snappydata-tables","text":"SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. By integrating the loading mechanism with the Query engine (Catalyst optimizer) it is often possible to push down filters and projections all the way to the data source minimizing data transfer. Here is the list of important features: Support for many Sources There is built-in support for many data sources as well as data formats. Data can be accessed from S3, file system, HDFS, Hive, RDB, etc. And the loaders have built-in support to handle CSV, Parquet, ORC, Avro, JSON, Java/Scala Objects, etc as the data formats. Access virtually any modern data store Virtually all major data providers have a native Spark connector that complies with the Data Sources API. For e.g. you can load data from any RDB like Amazon Redshift, Cassandra, Redis, Elastic Search, Neo4J, etc. While these connectors are not built-in, you can easily deploy these connectors as dependencies into a SnappyData cluster. All the connectors are typically registered in spark-packages.org Avoid Schema wrangling Spark supports schema inference. Which means, all you need to do is point to the external source in your 'create table' DDL (or Spark SQL API) and schema definition is learned by reading in the data. There is no need to explicitly define each column and type. This is extremely useful when dealing with disparate, complex and wide data sets. Read nested, sparse data sets When data is accessed from a source, the schema inference occurs by not just reading a header but often by reading the entire data set. For instance, when reading JSON files the structure could change from document to document. The inference engine builds up the schema as it reads each record and keeps unioning them to create a unified schema. This approach allows developers to become very productive with disparate data sets. Load using Spark API or SQL You can use SQL to point to any data source or use the native Spark Scala/Java API to load. For instance, you can first create an external table . CREATE EXTERNAL TABLE <tablename> USING <any-data-source-supported> OPTIONS <options> Next, use it in any SQL query or DDL. For example, CREATE EXTERNAL TABLE STAGING_CUSTOMER USING parquet OPTIONS(path 'quickstart/src/main/resources/customerparquet') CREATE TABLE CUSTOMER USING column OPTIONS(buckets '8') AS ( SELECT * FROM STAGING_CUSTOMER) Example - Load from CSV You can either explicitly define the schema or infer the schema and the column data types. To infer the column names, we need the CSV header to specify the names. In this example we don't have the names, so we explicitly define the schema. // Get a SnappySession in a local cluster val spark: SparkSession = SparkSession .builder .appName(\"CreateColumnTable\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) We explicitly define the table definition first .... snSession.sql(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER NOT NULL,\" + \"C_NAME VARCHAR(25) NOT NULL,\" + \"C_ADDRESS VARCHAR(40) NOT NULL,\" + \"C_NATIONKEY INTEGER NOT NULL,\" + \"C_PHONE VARCHAR(15) NOT NULL,\" + \"C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \"C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \"C_COMMENT VARCHAR(117) NOT NULL)\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") Load data in the CUSTOMER table from a CSV file by using Data Sources API val tableSchema = snSession.table(\"CUSTOMER\").schema val customerDF = snSession.read.schema(schema = tableSchema).csv(s\"$dataFolder/customer.csv\") customerDF.write.insertInto(\"CUSTOMER\") The Spark SQL programming guide provides a full description of the Data Sources API Example - Load from Parquet files val customerDF = snSession.read.parquet(s\"$dataDir/customer_parquet\") customerDF.write.insertInto(\"CUSTOMER\") Inferring schema from data file A schema for the table can be inferred from the data file. Data is first introspected to learn the schema (column names and types) without requring this input from the user. The example below illustrates reading a parquet data source and creates a new columnar table in SnappyData. The schema is automatically defined when the Parquet data files are read. val customerDF = snSession.read.parquet(s\"quickstart/src/main/resources/customerparquet\") // props1 map specifies the properties for the table to be created // \"PARTITION_BY\" attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY) val props1 = Map(\"PARTITION_BY\" -> \"C_CUSTKEY\") customerDF.write.format(\"column\").mode(\"append\").options(props1).saveAsTable(\"CUSTOMER\") In the code snippet below a schema is inferred from a CSV file. Column names are derived from the header in the file. val customer_csv_DF = snSession.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\").csv(\"quickstart/src/main/resources/customer_with_headers.csv\") // props1 map specifies the properties for the table to be created // \"PARTITION_BY\" attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY), // For complete list of attributes refer the documentation val props1 = Map(\"PARTITION_BY\" -> \"C_CUSTKEY\") customer_csv_DF.write.format(\"column\").mode(\"append\").options(props1).saveAsTable(\"CUSTOMER\") The source code to load the data from a CSV/Parquet files is in CreateColumnTable.scala . Example - reading JSON documents As mentioned before when dealing with JSON you have two challenges - (1) the data can be highly nested (2) the structure of the documents can keep changing. Here is a simple example that loads multiple JSON records that show dealing with schema changes across documents - WorkingWithJson.scala Note When loading data from sources like CSV or Parquet the files would need to be accessible from all the cluster members in SnappyData. Make sure it is NFS mounted or made accessible through the Cloud solution (shared storage like S3).","title":"How to Load Data into SnappyData Tables"},{"location":"howto/perform_a_colocated_join/","text":"How to Perform a Colocated Join \u00b6 When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData server. Colocating the data of two tables based on a partitioning column's value is a best practice if you frequently perform queries on those tables that join on that column. When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. Code Example: ORDERS table is colocated with CUSTOMER table A partitioned table can be colocated with another partitioned table by using the \"COLOCATE_WITH\" attribute in the table options. For example, in the code snippet below, the ORDERS table is colocated with the CUSTOMER table. The complete source for this example can be found in the file ColocatedJoinExample.scala Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"ColocatedJoinExample\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create Table Customer: snSession.sql(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER NOT NULL,\" + \"C_NAME VARCHAR(25) NOT NULL,\" + \"C_ADDRESS VARCHAR(40) NOT NULL,\" + \"C_NATIONKEY INTEGER NOT NULL,\" + \"C_PHONE VARCHAR(15) NOT NULL,\" + \"C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \"C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \"C_COMMENT VARCHAR(117) NOT NULL)\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") Create Table Orders: snSession.sql(\"CREATE TABLE ORDERS ( \" + \"O_ORDERKEY INTEGER NOT NULL,\" + \"O_CUSTKEY INTEGER NOT NULL,\" + \"O_ORDERSTATUS CHAR(1) NOT NULL,\" + \"O_TOTALPRICE DECIMAL(15,2) NOT NULL,\" + \"O_ORDERDATE DATE NOT NULL,\" + \"O_ORDERPRIORITY CHAR(15) NOT NULL,\" + \"O_CLERK CHAR(15) NOT NULL,\" + \"O_SHIPPRIORITY INTEGER NOT NULL,\" + \"O_COMMENT VARCHAR(79) NOT NULL) \" + \"USING COLUMN OPTIONS (PARTITION_BY 'O_CUSTKEY', \" + \"COLOCATE_WITH 'CUSTOMER' )\") Perform a Colocate join: // Selecting orders for all customers val result = snSession.sql(\"SELECT C_CUSTKEY, C_NAME, O_ORDERKEY, O_ORDERSTATUS, O_ORDERDATE, \" + \"O_TOTALPRICE FROM CUSTOMER, ORDERS WHERE C_CUSTKEY = O_CUSTKEY\").collect()","title":"How to Perform a Colocated Join"},{"location":"howto/perform_a_colocated_join/#how-to-perform-a-colocated-join","text":"When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData server. Colocating the data of two tables based on a partitioning column's value is a best practice if you frequently perform queries on those tables that join on that column. When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. Code Example: ORDERS table is colocated with CUSTOMER table A partitioned table can be colocated with another partitioned table by using the \"COLOCATE_WITH\" attribute in the table options. For example, in the code snippet below, the ORDERS table is colocated with the CUSTOMER table. The complete source for this example can be found in the file ColocatedJoinExample.scala Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"ColocatedJoinExample\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create Table Customer: snSession.sql(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER NOT NULL,\" + \"C_NAME VARCHAR(25) NOT NULL,\" + \"C_ADDRESS VARCHAR(40) NOT NULL,\" + \"C_NATIONKEY INTEGER NOT NULL,\" + \"C_PHONE VARCHAR(15) NOT NULL,\" + \"C_ACCTBAL DECIMAL(15,2) NOT NULL,\" + \"C_MKTSEGMENT VARCHAR(10) NOT NULL,\" + \"C_COMMENT VARCHAR(117) NOT NULL)\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") Create Table Orders: snSession.sql(\"CREATE TABLE ORDERS ( \" + \"O_ORDERKEY INTEGER NOT NULL,\" + \"O_CUSTKEY INTEGER NOT NULL,\" + \"O_ORDERSTATUS CHAR(1) NOT NULL,\" + \"O_TOTALPRICE DECIMAL(15,2) NOT NULL,\" + \"O_ORDERDATE DATE NOT NULL,\" + \"O_ORDERPRIORITY CHAR(15) NOT NULL,\" + \"O_CLERK CHAR(15) NOT NULL,\" + \"O_SHIPPRIORITY INTEGER NOT NULL,\" + \"O_COMMENT VARCHAR(79) NOT NULL) \" + \"USING COLUMN OPTIONS (PARTITION_BY 'O_CUSTKEY', \" + \"COLOCATE_WITH 'CUSTOMER' )\") Perform a Colocate join: // Selecting orders for all customers val result = snSession.sql(\"SELECT C_CUSTKEY, C_NAME, O_ORDERKEY, O_ORDERSTATUS, O_ORDERDATE, \" + \"O_TOTALPRICE FROM CUSTOMER, ORDERS WHERE C_CUSTKEY = O_CUSTKEY\").collect()","title":"How to Perform a Colocated Join"},{"location":"howto/run_spark_job_inside_cluster/","text":"How to Run Spark Job inside the Cluster \u00b6 Spark program that runs inside a SnappyData cluster is implemented as a SnappyData job. Implementing a Job : A SnappyData job is a class or object that implements SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. In the runSnappyJob method of the job, you implement the logic for your Spark program using SnappySession object instance passed to it. You can perform all operations such as create a table, load data, execute queries using the SnappySession. Any of the Spark APIs can be invoked by a SnappyJob. class CreatePartitionedRowTable extends SnappySQLJob { /** SnappyData uses this as an entry point to execute Snappy jobs. **/ def runSnappyJob(sc: SnappySession, jobConfig: Config): Any /** SnappyData calls this function to validate the job input and reject invalid job requests. You can implement custom validations here, for example, validating the configuration parameters **/ def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation } Dependencies : To compile your job, use the Maven/SBT dependencies for the latest released version of SnappyData. Example: Maven dependency : // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 <dependency> <groupId>io.snappydata</groupId> <artifactId>snappydata-cluster_2.11</artifactId> <version>1.3.1</version> </dependency> Example: SBT dependency : // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" Note If your project fails while resolving the above dependency (ie. it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys.props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Running the Job : Once you create a jar file for SnappyData job, use the ./bin/snappy-job.sh to submit the job in the SnappyData cluster, and then run the job. This is similar to spark-submit for any Spark application. For example, to run the job implemented in CreatePartitionedRowTable.scala you can use the following command. The command submits the job and runs it as: # first change the directory to the SnappyData product directory $ cd $SNAPPY_HOME $ ./bin/snappy-job.sh submit --app-name CreatePartitionedRowTable --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable --app-jar examples/jars/quickstart.jar --lead localhost:8090 In the above command, quickstart.jar contains the program and is bundled in the product distribution and the --lead option specifies the host name of the lead node along with the port on which it accepts jobs (default 8090). Output : It returns output similar to: { \"status\": \"STARTED\", \"result\": { \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\", \"context\": \"snappyContext1452598154529305363\" } } Check Status : You can check the status of the job using the Job ID listed above: ./bin/snappy-job.sh status --lead localhost:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 Refer to the Building SnappyData applications using Spark API section of the documentation for more details.","title":"How to Run Spark Job inside the Cluster"},{"location":"howto/run_spark_job_inside_cluster/#how-to-run-spark-job-inside-the-cluster","text":"Spark program that runs inside a SnappyData cluster is implemented as a SnappyData job. Implementing a Job : A SnappyData job is a class or object that implements SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. In the runSnappyJob method of the job, you implement the logic for your Spark program using SnappySession object instance passed to it. You can perform all operations such as create a table, load data, execute queries using the SnappySession. Any of the Spark APIs can be invoked by a SnappyJob. class CreatePartitionedRowTable extends SnappySQLJob { /** SnappyData uses this as an entry point to execute Snappy jobs. **/ def runSnappyJob(sc: SnappySession, jobConfig: Config): Any /** SnappyData calls this function to validate the job input and reject invalid job requests. You can implement custom validations here, for example, validating the configuration parameters **/ def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation } Dependencies : To compile your job, use the Maven/SBT dependencies for the latest released version of SnappyData. Example: Maven dependency : // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 <dependency> <groupId>io.snappydata</groupId> <artifactId>snappydata-cluster_2.11</artifactId> <version>1.3.1</version> </dependency> Example: SBT dependency : // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.3.1\" Note If your project fails while resolving the above dependency (ie. it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. As a workaround, add the below code to the build.sbt : val workaround = { sys.props += \"packaging.type\" -> \"jar\" () } For more details, refer https://github.com/sbt/sbt/issues/3618 . Running the Job : Once you create a jar file for SnappyData job, use the ./bin/snappy-job.sh to submit the job in the SnappyData cluster, and then run the job. This is similar to spark-submit for any Spark application. For example, to run the job implemented in CreatePartitionedRowTable.scala you can use the following command. The command submits the job and runs it as: # first change the directory to the SnappyData product directory $ cd $SNAPPY_HOME $ ./bin/snappy-job.sh submit --app-name CreatePartitionedRowTable --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable --app-jar examples/jars/quickstart.jar --lead localhost:8090 In the above command, quickstart.jar contains the program and is bundled in the product distribution and the --lead option specifies the host name of the lead node along with the port on which it accepts jobs (default 8090). Output : It returns output similar to: { \"status\": \"STARTED\", \"result\": { \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\", \"context\": \"snappyContext1452598154529305363\" } } Check Status : You can check the status of the job using the Job ID listed above: ./bin/snappy-job.sh status --lead localhost:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 Refer to the Building SnappyData applications using Spark API section of the documentation for more details.","title":"How to Run Spark Job inside the Cluster"},{"location":"howto/spark_installation_using_smart_connector/","text":"How to Access SnappyData Store from an existing Spark Installation using Smart Connector \u00b6 SnappyData comes with a Smart Connector that enables Spark applications to work with the SnappyData cluster, from any compatible Spark cluster. You can use any distribution that is compatible with Apache Spark 2.1.1 to 2.1.3. To connect from a Spark 2.4 based cluster, you may use JDBC connector as described here . The Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. This is similar to how Spark applications today work with stores like Cassandra, Redis, etc. For more information on the various modes, refer to the SnappyData Smart Connector section of the documentation. Code Example \u00b6 The code example for this mode is in SmartConnectorExample.scala Configure a SnappySession : The code below shows how to initialize a SparkSession. Here the property snappydata.connection instructs the connector to acquire cluster connectivity, catalog metadata and register it locally in the Spark cluster. The values consists of locator host and JDBC client port on which the locator listens for connections (default 1527). val spark: SparkSession = SparkSession .builder .appName(\"SmartConnectorExample\") // It can be any master URL .master(\"local[4]\") // snappydata.connection property enables the application to interact with SnappyData store .config(\"snappydata.connection\", \"localhost:1527\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create Table and Run Queries : You can now create tables and run queries in SnappyData store using your Apache Spark program. // reading an already created SnappyStore table SNAPPY_COL_TABLE val colTable = snSession.table(\"SNAPPY_COL_TABLE\") colTable.show(10) snSession.dropTable(\"TestColumnTable\", ifExists = true) // Creating a table from a DataFrame val dataFrame = snSession.range(1000).selectExpr(\"id\", \"floor(rand() * 10000) as k\") snSession.sql(\"create table TestColumnTable (id bigint not null, k bigint not null) using column\") // insert data in TestColumnTable dataFrame.write.insertInto(\"TestColumnTable\") Running a Smart Connector Application \u00b6 Start a SnappyData cluster and create a table. $ ./sbin/snappy-start-all.sh $ ./bin/snappy SnappyData version 1.3.1 snappy> connect client 'localhost:1527'; Using CONNECTION0 snappy> CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN; snappy> insert into SNAPPY_COL_TABLE VALUES(1,1); 1 row inserted/updated/deleted snappy> insert into SNAPPY_COL_TABLE VALUES(2,2); 1 row inserted/updated/deleted exit; The Smart Connector Application can now connect to this SnappyData cluster. The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside the SnappyData cluster. SnappyData package has to be specified along with the application jar to run the Smart Connector application. $ ./bin/spark-submit --master local[*] --conf snappydata.connection=localhost:1527 --class org.apache.spark.examples.snappydata.SmartConnectorExample --packages io.snappydata:snappydata-spark-connector_2.11:1.3.1 $SNAPPY_HOME/examples/jars/quickstart.jar Execute a Smart Connector Application \u00b6 Start a SnappyData cluster and create a table inside it. $ ./sbin/snappy-start-all.sh $ ./bin/snappy SnappyData version 1.3.1 snappy> connect client 'localhost:1527'; Using CONNECTION0 snappy> CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN; snappy> insert into SNAPPY_COL_TABLE VALUES(1,1); 1 row inserted/updated/deleted snappy> insert into SNAPPY_COL_TABLE VALUES(2,2); 1 row inserted/updated/deleted exit; A Smart Connector Application can now connect to this SnappyData cluster. The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside SnappyData cluster. SnappyData package has to be specified along with the application jar to run the Smart Connector application. $ ./bin/spark-submit --master local[*] --conf spark.snappydata.connection=localhost:1527 --class org.apache.spark.examples.snappydata.SmartConnectorExample --packages io.snappydata:snappydata-spark-connector_2.11:1.3.1 $SNAPPY_HOME/examples/jars/quickstart.jar","title":"How to Access SnappyData Store from existing Spark Installation using Smart Connector"},{"location":"howto/spark_installation_using_smart_connector/#how-to-access-snappydata-store-from-an-existing-spark-installation-using-smart-connector","text":"SnappyData comes with a Smart Connector that enables Spark applications to work with the SnappyData cluster, from any compatible Spark cluster. You can use any distribution that is compatible with Apache Spark 2.1.1 to 2.1.3. To connect from a Spark 2.4 based cluster, you may use JDBC connector as described here . The Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. This is similar to how Spark applications today work with stores like Cassandra, Redis, etc. For more information on the various modes, refer to the SnappyData Smart Connector section of the documentation.","title":"How to Access SnappyData Store from an existing Spark Installation using Smart Connector"},{"location":"howto/spark_installation_using_smart_connector/#code-example","text":"The code example for this mode is in SmartConnectorExample.scala Configure a SnappySession : The code below shows how to initialize a SparkSession. Here the property snappydata.connection instructs the connector to acquire cluster connectivity, catalog metadata and register it locally in the Spark cluster. The values consists of locator host and JDBC client port on which the locator listens for connections (default 1527). val spark: SparkSession = SparkSession .builder .appName(\"SmartConnectorExample\") // It can be any master URL .master(\"local[4]\") // snappydata.connection property enables the application to interact with SnappyData store .config(\"snappydata.connection\", \"localhost:1527\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create Table and Run Queries : You can now create tables and run queries in SnappyData store using your Apache Spark program. // reading an already created SnappyStore table SNAPPY_COL_TABLE val colTable = snSession.table(\"SNAPPY_COL_TABLE\") colTable.show(10) snSession.dropTable(\"TestColumnTable\", ifExists = true) // Creating a table from a DataFrame val dataFrame = snSession.range(1000).selectExpr(\"id\", \"floor(rand() * 10000) as k\") snSession.sql(\"create table TestColumnTable (id bigint not null, k bigint not null) using column\") // insert data in TestColumnTable dataFrame.write.insertInto(\"TestColumnTable\")","title":"Code Example"},{"location":"howto/spark_installation_using_smart_connector/#running-a-smart-connector-application","text":"Start a SnappyData cluster and create a table. $ ./sbin/snappy-start-all.sh $ ./bin/snappy SnappyData version 1.3.1 snappy> connect client 'localhost:1527'; Using CONNECTION0 snappy> CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN; snappy> insert into SNAPPY_COL_TABLE VALUES(1,1); 1 row inserted/updated/deleted snappy> insert into SNAPPY_COL_TABLE VALUES(2,2); 1 row inserted/updated/deleted exit; The Smart Connector Application can now connect to this SnappyData cluster. The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside the SnappyData cluster. SnappyData package has to be specified along with the application jar to run the Smart Connector application. $ ./bin/spark-submit --master local[*] --conf snappydata.connection=localhost:1527 --class org.apache.spark.examples.snappydata.SmartConnectorExample --packages io.snappydata:snappydata-spark-connector_2.11:1.3.1 $SNAPPY_HOME/examples/jars/quickstart.jar","title":"Running a Smart Connector Application"},{"location":"howto/spark_installation_using_smart_connector/#execute-a-smart-connector-application","text":"Start a SnappyData cluster and create a table inside it. $ ./sbin/snappy-start-all.sh $ ./bin/snappy SnappyData version 1.3.1 snappy> connect client 'localhost:1527'; Using CONNECTION0 snappy> CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN; snappy> insert into SNAPPY_COL_TABLE VALUES(1,1); 1 row inserted/updated/deleted snappy> insert into SNAPPY_COL_TABLE VALUES(2,2); 1 row inserted/updated/deleted exit; A Smart Connector Application can now connect to this SnappyData cluster. The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside SnappyData cluster. SnappyData package has to be specified along with the application jar to run the Smart Connector application. $ ./bin/spark-submit --master local[*] --conf spark.snappydata.connection=localhost:1527 --class org.apache.spark.examples.snappydata.SmartConnectorExample --packages io.snappydata:snappydata-spark-connector_2.11:1.3.1 $SNAPPY_HOME/examples/jars/quickstart.jar","title":"Execute a Smart Connector Application"},{"location":"howto/start_snappy_cluster/","text":"How to Start a SnappyData Cluster \u00b6 Starting SnappyData Cluster on a Single Machine \u00b6 If you have downloaded and extracted the SnappyData product distribution, navigate to the SnappyData product root directory. Start the Cluster : Run the ./sbin/snappy-start-all.sh script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server. The Hive Thrift server also starts by default. $ ./sbin/snappy-start-all.sh It may take 30 seconds or more to bootstrap the entire cluster on your local machine. An additional 10 seconds is required to start the Hive Thrift server. To avoid this additional 10 seconds, you can set the snappydata.hiveServer.enabled to false. Sample Output : The sample output for snappy-start-all.sh is displayed as: Logs generated in /home/cbhatt/snappydata-1.3.1-bin/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 10813 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/cbhatt/snappydata-1.3.1-bin/work/localhost-server-1/snappyserver.log SnappyData Server pid: 11018 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/cbhatt/snappydata-1.3.1-bin/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 11213 status: running Distributed system now has 3 members. Starting hive thrift server (session=snappy) Starting job server on: 0.0.0.0[8090] Starting the SnappyData Cluster on Multiple Hosts \u00b6 To start the cluster on multiple hosts: The easiest way to run SnappyData on multiple nodes is to use a shared file system such as NFS on all the nodes. You can also extract the product distribution on each node of the cluster. If all nodes have NFS access, install SnappyData on any one of the nodes. Create the configuration files using the templates provided in the conf folder. Copy the existing template files ( servers.template , locators.template and leads.template ) and rename them to servers , locators , leads . Edit the files to include the hostnames on which to start the server, locator, and lead. Refer to the configuration section for more information on properties. Start the cluster using ./sbin/snappy-start-all.sh . SnappyData starts the cluster using SSH. Note It is recommended that you set up passwordless SSH on all hosts in the cluster. Refer to the documentation for more details on installation and cluster configuration . Starting Individual Components \u00b6 Instead of starting SnappyData cluster using the snappy-start-all.sh script, individual components can be started on a system locally using the following commands: Tip All configuration parameters are provided as command line arguments rather than reading from a configuration file. For example, you can run any of the following commands depending on the individual component that you want to start. $ ./bin/snappy locator start -dir=/node-a/locator1 $ ./bin/snappy server start -dir=/node-b/server1 -locators=localhost[10334] -heap-size=16g $ ./bin/snappy leader start -dir=/node-c/lead1 -locators=localhost[10334] -spark.executor.cores=32 Note The path mentioned for -dir should exist. Otherwise, the command will fail with FileNotFoundException . Executing Commands on Selected Cluster Component \u00b6 Syntax \u00b6 cluster-util.sh (--on-locators|--on-servers|--on-leads|--on-all) [-y] (--copy-conf | --run \"<cmd-to-run-on-selected-nodes>\") Description \u00b6 You can use the cluster-util.sh utility to execute a given command on selected members of the cluster. The script relies on the entries you specify in locators, servers, and leads files in the conf directory to identify the members of the cluster. -on-locators If specified, the given command is executed on locators. --on-servers If specified, the given command is executed on servers. --on-leads If specified, the given command is executed on leads. --on-all If specified, the given command is executed on all the member in the cluster. -y If specified, the script does not prompt for the confirmation to execute the command on each member node. --copy-conf This is a shortcut command. When you specify this comand, the log4j2.properties, snappy-env.sh and spark-env.sh configuration files are copied from the local machine to all the members. These files are copied only in the following conditions: If these are absent in the destination member If their content is different. In latter case, a backup of the file is taken in conf/backup directory, on the destination member, before copy. --run <cmd-to-run-on-selected-nodes> If specified, the given command(s) is executed on specified members. Command to be executed specified after --run`` must be in double-quotes. Example \u00b6 1.To copy configuration files on all servers \u201c./sbin/cluster-util.sh --on-servers --copy-conf 2.To run \u201cls\u201d command on all servers with -y option \u201c./sbin/cluster-util.sh --on-servers -y --run \u201cls\u201d","title":"How to Start a SnappyData Cluster"},{"location":"howto/start_snappy_cluster/#how-to-start-a-snappydata-cluster","text":"","title":"How to Start a SnappyData Cluster"},{"location":"howto/start_snappy_cluster/#starting-snappydata-cluster-on-a-single-machine","text":"If you have downloaded and extracted the SnappyData product distribution, navigate to the SnappyData product root directory. Start the Cluster : Run the ./sbin/snappy-start-all.sh script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server. The Hive Thrift server also starts by default. $ ./sbin/snappy-start-all.sh It may take 30 seconds or more to bootstrap the entire cluster on your local machine. An additional 10 seconds is required to start the Hive Thrift server. To avoid this additional 10 seconds, you can set the snappydata.hiveServer.enabled to false. Sample Output : The sample output for snappy-start-all.sh is displayed as: Logs generated in /home/cbhatt/snappydata-1.3.1-bin/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 10813 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/cbhatt/snappydata-1.3.1-bin/work/localhost-server-1/snappyserver.log SnappyData Server pid: 11018 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/cbhatt/snappydata-1.3.1-bin/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 11213 status: running Distributed system now has 3 members. Starting hive thrift server (session=snappy) Starting job server on: 0.0.0.0[8090]","title":"Starting SnappyData Cluster on a Single Machine"},{"location":"howto/start_snappy_cluster/#starting-the-snappydata-cluster-on-multiple-hosts","text":"To start the cluster on multiple hosts: The easiest way to run SnappyData on multiple nodes is to use a shared file system such as NFS on all the nodes. You can also extract the product distribution on each node of the cluster. If all nodes have NFS access, install SnappyData on any one of the nodes. Create the configuration files using the templates provided in the conf folder. Copy the existing template files ( servers.template , locators.template and leads.template ) and rename them to servers , locators , leads . Edit the files to include the hostnames on which to start the server, locator, and lead. Refer to the configuration section for more information on properties. Start the cluster using ./sbin/snappy-start-all.sh . SnappyData starts the cluster using SSH. Note It is recommended that you set up passwordless SSH on all hosts in the cluster. Refer to the documentation for more details on installation and cluster configuration .","title":"Starting the SnappyData Cluster on Multiple Hosts"},{"location":"howto/start_snappy_cluster/#starting-individual-components","text":"Instead of starting SnappyData cluster using the snappy-start-all.sh script, individual components can be started on a system locally using the following commands: Tip All configuration parameters are provided as command line arguments rather than reading from a configuration file. For example, you can run any of the following commands depending on the individual component that you want to start. $ ./bin/snappy locator start -dir=/node-a/locator1 $ ./bin/snappy server start -dir=/node-b/server1 -locators=localhost[10334] -heap-size=16g $ ./bin/snappy leader start -dir=/node-c/lead1 -locators=localhost[10334] -spark.executor.cores=32 Note The path mentioned for -dir should exist. Otherwise, the command will fail with FileNotFoundException .","title":"Starting Individual Components"},{"location":"howto/start_snappy_cluster/#executing-commands-on-selected-cluster-component","text":"","title":"Executing Commands on Selected Cluster Component"},{"location":"howto/start_snappy_cluster/#syntax","text":"cluster-util.sh (--on-locators|--on-servers|--on-leads|--on-all) [-y] (--copy-conf | --run \"<cmd-to-run-on-selected-nodes>\")","title":"Syntax"},{"location":"howto/start_snappy_cluster/#description","text":"You can use the cluster-util.sh utility to execute a given command on selected members of the cluster. The script relies on the entries you specify in locators, servers, and leads files in the conf directory to identify the members of the cluster. -on-locators If specified, the given command is executed on locators. --on-servers If specified, the given command is executed on servers. --on-leads If specified, the given command is executed on leads. --on-all If specified, the given command is executed on all the member in the cluster. -y If specified, the script does not prompt for the confirmation to execute the command on each member node. --copy-conf This is a shortcut command. When you specify this comand, the log4j2.properties, snappy-env.sh and spark-env.sh configuration files are copied from the local machine to all the members. These files are copied only in the following conditions: If these are absent in the destination member If their content is different. In latter case, a backup of the file is taken in conf/backup directory, on the destination member, before copy. --run <cmd-to-run-on-selected-nodes> If specified, the given command(s) is executed on specified members. Command to be executed specified after --run`` must be in double-quotes.","title":"Description"},{"location":"howto/start_snappy_cluster/#example","text":"1.To copy configuration files on all servers \u201c./sbin/cluster-util.sh --on-servers --copy-conf 2.To run \u201cls\u201d command on all servers with -y option \u201c./sbin/cluster-util.sh --on-servers -y --run \u201cls\u201d","title":"Example"},{"location":"howto/stop_snappy_cluster/","text":"How to Stop a SnappyData Cluster \u00b6 Stopping the Cluster \u00b6 You can stop the cluster using the ./sbin/snappy-stop-all.sh command: $ ./sbin/snappy-stop-all.sh The SnappyData Leader has stopped. The SnappyData Server has stopped. The SnappyData Locator has stopped. Note Ensure that all write operations on column table have finished execution when you stop a cluster, else it can lead to a partial write. Stopping Individual Components \u00b6 Instead of stopping the SnappyData cluster using the snappy-stop-all.sh script, individual components can be stopped on a system locally using the following commands: Tip All configuration parameters are provided as command line arguments rather than reading from a configuration file. $ ./bin/snappy locator stop -dir=/node-a/locator1 $ ./bin/snappy server stop -dir=/node-b/server1 $ ./bin/snappy leader stop -dir=/node-c/lead1","title":"How to Stop a SnappyData Cluster"},{"location":"howto/stop_snappy_cluster/#how-to-stop-a-snappydata-cluster","text":"","title":"How to Stop a SnappyData Cluster"},{"location":"howto/stop_snappy_cluster/#stopping-the-cluster","text":"You can stop the cluster using the ./sbin/snappy-stop-all.sh command: $ ./sbin/snappy-stop-all.sh The SnappyData Leader has stopped. The SnappyData Server has stopped. The SnappyData Locator has stopped. Note Ensure that all write operations on column table have finished execution when you stop a cluster, else it can lead to a partial write.","title":"Stopping the Cluster"},{"location":"howto/stop_snappy_cluster/#stopping-individual-components","text":"Instead of stopping the SnappyData cluster using the snappy-stop-all.sh script, individual components can be stopped on a system locally using the following commands: Tip All configuration parameters are provided as command line arguments rather than reading from a configuration file. $ ./bin/snappy locator stop -dir=/node-a/locator1 $ ./bin/snappy server stop -dir=/node-b/server1 $ ./bin/snappy leader stop -dir=/node-c/lead1","title":"Stopping Individual Components"},{"location":"howto/store_and_query_json_objects/","text":"How to Store and Query JSON Objects \u00b6 You can insert JSON data in SnappyData tables and execute queries on the tables. Code Example: Loads JSON data from a JSON file into a column table and executes query The code snippet loads JSON data from a JSON file into a column table and executes the query against it. The source code for JSON example is located at WorkingWithJson.scala . After creating SnappySession, the JSON file is read using Spark API and loaded into a SnappyData table. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"WorkingWithJson\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create a DataFrame from the JSON file : val some_people_path = s\"quickstart/src/main/resources/some_people.json\" // Read a JSON file using Spark API val people = snSession.read.json(some_people_path) people.printSchema() Create a SnappyData table and insert the JSON data in it using the DataFrame : //Drop the table if it exists snSession.dropTable(\"people\", ifExists = true) //Create a columnar table with the Json DataFrame schema snSession.createTable(tableName = \"people\", provider = \"column\", schema = people.schema, options = Map.empty[String,String], allowExisting = false) // Write the created DataFrame to the columnar table people.write.insertInto(\"people\") Append more data from a second JSON file : // Append more people to the column table val more_people_path = s\"quickstart/src/main/resources/more_people.json\" //Explicitly passing schema to handle record level field mismatch // e.g. some records have \"district\" field while some do not. val morePeople = snSession.read.schema(people.schema).json(more_people_path) morePeople.write.insertInto(\"people\") //print schema of the table println(\"Print Schema of the table\\n################\") println(snSession.table(\"people\").schema) Execute queries and return the results // Query it like any other table val nameAndAddress = snSession.sql(\"SELECT \" + \"name, \" + \"address.city, \" + \"address.state, \" + \"address.district, \" + \"address.lane \" + \"FROM people\") nameAndAddress.toJSON.show()","title":"Store and query json objects"},{"location":"howto/store_and_query_json_objects/#how-to-store-and-query-json-objects","text":"You can insert JSON data in SnappyData tables and execute queries on the tables. Code Example: Loads JSON data from a JSON file into a column table and executes query The code snippet loads JSON data from a JSON file into a column table and executes the query against it. The source code for JSON example is located at WorkingWithJson.scala . After creating SnappySession, the JSON file is read using Spark API and loaded into a SnappyData table. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"WorkingWithJson\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create a DataFrame from the JSON file : val some_people_path = s\"quickstart/src/main/resources/some_people.json\" // Read a JSON file using Spark API val people = snSession.read.json(some_people_path) people.printSchema() Create a SnappyData table and insert the JSON data in it using the DataFrame : //Drop the table if it exists snSession.dropTable(\"people\", ifExists = true) //Create a columnar table with the Json DataFrame schema snSession.createTable(tableName = \"people\", provider = \"column\", schema = people.schema, options = Map.empty[String,String], allowExisting = false) // Write the created DataFrame to the columnar table people.write.insertInto(\"people\") Append more data from a second JSON file : // Append more people to the column table val more_people_path = s\"quickstart/src/main/resources/more_people.json\" //Explicitly passing schema to handle record level field mismatch // e.g. some records have \"district\" field while some do not. val morePeople = snSession.read.schema(people.schema).json(more_people_path) morePeople.write.insertInto(\"people\") //print schema of the table println(\"Print Schema of the table\\n################\") println(snSession.table(\"people\").schema) Execute queries and return the results // Query it like any other table val nameAndAddress = snSession.sql(\"SELECT \" + \"name, \" + \"address.city, \" + \"address.state, \" + \"address.district, \" + \"address.lane \" + \"FROM people\") nameAndAddress.toJSON.show()","title":"How to Store and Query JSON Objects"},{"location":"howto/store_and_query_objects/","text":"How to Store and Query Objects \u00b6 You can use domain object to load data into SnappyData tables and select the data by executing queries against the table. Code Example: Insert Person objects into the column table The code snippet below inserts Person objects into a column table. The source code for this example is located at WorkingWithObjects.scala . After creating SnappySession, the Person objects are inserted using Spark API and loads into a SnappyData table. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"CreateReplicatedRowTable\") .master(\"local[4]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create DataFrame objects : //Import the implicits for automatic conversion between Objects to DataSets. import snSession.implicits._ // Create a Dataset using Spark APIs val people = Seq(Person(\"Tom\", Address(\"Columbus\", \"Ohio\"), Map(\"frnd1\"-> \"8998797979\", \"frnd2\" -> \"09878786886\")) , Person(\"Ned\", Address(\"San Diego\", \"California\"), Map.empty[String,String])).toDS() Create a SnappyData table and insert data into it : //Drop the table if it exists. snSession.dropTable(\"Persons\", ifExists = true) //Create a columnar table with a Struct to store Address snSession.sql(\"CREATE table Persons(name String, address Struct<city: String, state:String>, \" + \"emergencyContacts Map<String,String>) using column options()\") // Write the created DataFrame to the columnar table. people.write.insertInto(\"Persons\") //print schema of the table println(\"Print Schema of the table\\n################\") println(snSession.table(\"Persons\").schema) // Append more people to the column table val morePeople = Seq(Person(\"Jon Snow\", Address(\"Columbus\", \"Ohio\"), Map.empty[String,String]), Person(\"Rob Stark\", Address(\"San Diego\", \"California\"), Map.empty[String,String]), Person(\"Michael\", Address(\"Null\", \"California\"), Map.empty[String,String])).toDS() morePeople.write.insertInto(\"Persons\") Execute query on the table and return results : // Query it like any other table val nameAndAddress = snSession.sql(\"SELECT name, address, emergencyContacts FROM Persons\") //Reconstruct the objects from obtained Row val allPersons = nameAndAddress.as[Person] //allPersons is a Spark Dataset of Person objects. // Use of the Dataset APIs to transform, query this data set.","title":"How to Store and Query Objects"},{"location":"howto/store_and_query_objects/#how-to-store-and-query-objects","text":"You can use domain object to load data into SnappyData tables and select the data by executing queries against the table. Code Example: Insert Person objects into the column table The code snippet below inserts Person objects into a column table. The source code for this example is located at WorkingWithObjects.scala . After creating SnappySession, the Person objects are inserted using Spark API and loads into a SnappyData table. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"CreateReplicatedRowTable\") .master(\"local[4]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) Create DataFrame objects : //Import the implicits for automatic conversion between Objects to DataSets. import snSession.implicits._ // Create a Dataset using Spark APIs val people = Seq(Person(\"Tom\", Address(\"Columbus\", \"Ohio\"), Map(\"frnd1\"-> \"8998797979\", \"frnd2\" -> \"09878786886\")) , Person(\"Ned\", Address(\"San Diego\", \"California\"), Map.empty[String,String])).toDS() Create a SnappyData table and insert data into it : //Drop the table if it exists. snSession.dropTable(\"Persons\", ifExists = true) //Create a columnar table with a Struct to store Address snSession.sql(\"CREATE table Persons(name String, address Struct<city: String, state:String>, \" + \"emergencyContacts Map<String,String>) using column options()\") // Write the created DataFrame to the columnar table. people.write.insertInto(\"Persons\") //print schema of the table println(\"Print Schema of the table\\n################\") println(snSession.table(\"Persons\").schema) // Append more people to the column table val morePeople = Seq(Person(\"Jon Snow\", Address(\"Columbus\", \"Ohio\"), Map.empty[String,String]), Person(\"Rob Stark\", Address(\"San Diego\", \"California\"), Map.empty[String,String]), Person(\"Michael\", Address(\"Null\", \"California\"), Map.empty[String,String])).toDS() morePeople.write.insertInto(\"Persons\") Execute query on the table and return results : // Query it like any other table val nameAndAddress = snSession.sql(\"SELECT name, address, emergencyContacts FROM Persons\") //Reconstruct the objects from obtained Row val allPersons = nameAndAddress.as[Person] //allPersons is a Spark Dataset of Person objects. // Use of the Dataset APIs to transform, query this data set.","title":"How to Store and Query Objects"},{"location":"howto/store_retrieve_complex_datatypes_JDBC/","text":"How to Store and Retrieve Complex Data Types in JDBC Programs \u00b6 If you want to store/retrieve objects for complex data types (Array, Map and Struct) using JDBC programs, SnappyData provides com.pivotal.gemfirexd.snappy.ComplexTypeSerializer utility class to serialize/deserialize those objects: A column of type ARRAY can store array of Java objects (Object[]), typed arrays, java.util.Collection, and scala.collection.Seq. A column of type MAP can store java.util.Map or scala.collection.Map. A column of type STRUCT can store array of Java objects (Object[]), typed arrays, java.util.Collection, scala.collection.Seq, or scala.Product. Note Complex data types are supported only for column tables. Code Example: Storing and Retrieving Array Data in a JDBC program \u00b6 The following scala code snippets show how to perform insert and select operations on columns of complex data types. Complete source code for example is available at JDBCWithComplexTypes.scala // create a JDBC connection val url: String = s\"jdbc:snappydata://localhost:1527/\" val conn = DriverManager.getConnection(url) val stmt = conn.createStatement() // create a table with a column of type array stmt.execute(\"CREATE TABLE TABLE_WITH_COMPLEX_TYPES (col1 Int, col2 Array<Decimal>) USING column options()\") Inserting Data \u00b6 Insert a single row having a complex type (array) val arrDecimal = Array(Decimal(\"4.92\"), Decimal(\"51.98\")) val pstmt = conn.prepareStatement(\"insert into TABLE_WITH_COMPLEX_TYPES values (?, ?)\") Create a serializer that can be used to serialize array data and insert into the table. val serializer1 = ComplexTypeSerializer.create(tableName, \"col2\", conn) pstmt.setInt(1, 1) pstmt.setBytes(2, serializer1.serialize(arrDecimal)) pstmt.execute pstmt.close() Selecting Data \u00b6 // Select array data as a JSON string var rs = stmt.executeQuery(s\"SELECT * FROM $tableName\") while (rs.next()) { // read the column as a String val res1 = rs.getString(\"col2\") println(s\"res1 = $res1\") // alternate way, read the same column as a Clob val res2 = rs.getClob(\"col2\") println(s\"res2 = \" + res2.getSubString(1, res2.length.asInstanceOf[Int])) } // reading array data as BLOB and Bytes and then forming a Scala array val serializer = ComplexTypeSerializer.create(tableName, \"col2\", conn) rs = stmt.executeQuery(s\"SELECT * FROM $tableName --+ complexTypeAsJson(0)\") while (rs.next()) { // read the same column as a byte[] and then deserialize it into an Array val res1 = serializer.deserialize(rs.getBytes(\"col2\")) println(s\"res1 = $res1\") // alternate way, read the same column as a Blob an then deserialize it into an Array val res2 = serializer.deserialize(rs.getBlob(\"col2\")) println(s\"res2 = $res2\") } See also: How to connect using JDBC driver","title":"How to Store and Retrieve Complex Data Types in JDBC Programs"},{"location":"howto/store_retrieve_complex_datatypes_JDBC/#how-to-store-and-retrieve-complex-data-types-in-jdbc-programs","text":"If you want to store/retrieve objects for complex data types (Array, Map and Struct) using JDBC programs, SnappyData provides com.pivotal.gemfirexd.snappy.ComplexTypeSerializer utility class to serialize/deserialize those objects: A column of type ARRAY can store array of Java objects (Object[]), typed arrays, java.util.Collection, and scala.collection.Seq. A column of type MAP can store java.util.Map or scala.collection.Map. A column of type STRUCT can store array of Java objects (Object[]), typed arrays, java.util.Collection, scala.collection.Seq, or scala.Product. Note Complex data types are supported only for column tables.","title":"How to Store and Retrieve Complex Data Types in JDBC Programs"},{"location":"howto/store_retrieve_complex_datatypes_JDBC/#code-example-storing-and-retrieving-array-data-in-a-jdbc-program","text":"The following scala code snippets show how to perform insert and select operations on columns of complex data types. Complete source code for example is available at JDBCWithComplexTypes.scala // create a JDBC connection val url: String = s\"jdbc:snappydata://localhost:1527/\" val conn = DriverManager.getConnection(url) val stmt = conn.createStatement() // create a table with a column of type array stmt.execute(\"CREATE TABLE TABLE_WITH_COMPLEX_TYPES (col1 Int, col2 Array<Decimal>) USING column options()\")","title":"Code Example: Storing and Retrieving Array Data in a JDBC program"},{"location":"howto/store_retrieve_complex_datatypes_JDBC/#inserting-data","text":"Insert a single row having a complex type (array) val arrDecimal = Array(Decimal(\"4.92\"), Decimal(\"51.98\")) val pstmt = conn.prepareStatement(\"insert into TABLE_WITH_COMPLEX_TYPES values (?, ?)\") Create a serializer that can be used to serialize array data and insert into the table. val serializer1 = ComplexTypeSerializer.create(tableName, \"col2\", conn) pstmt.setInt(1, 1) pstmt.setBytes(2, serializer1.serialize(arrDecimal)) pstmt.execute pstmt.close()","title":"Inserting Data"},{"location":"howto/store_retrieve_complex_datatypes_JDBC/#selecting-data","text":"// Select array data as a JSON string var rs = stmt.executeQuery(s\"SELECT * FROM $tableName\") while (rs.next()) { // read the column as a String val res1 = rs.getString(\"col2\") println(s\"res1 = $res1\") // alternate way, read the same column as a Clob val res2 = rs.getClob(\"col2\") println(s\"res2 = \" + res2.getSubString(1, res2.length.asInstanceOf[Int])) } // reading array data as BLOB and Bytes and then forming a Scala array val serializer = ComplexTypeSerializer.create(tableName, \"col2\", conn) rs = stmt.executeQuery(s\"SELECT * FROM $tableName --+ complexTypeAsJson(0)\") while (rs.next()) { // read the same column as a byte[] and then deserialize it into an Array val res1 = serializer.deserialize(rs.getBytes(\"col2\")) println(s\"res1 = $res1\") // alternate way, read the same column as a Blob an then deserialize it into an Array val res2 = serializer.deserialize(rs.getBlob(\"col2\")) println(s\"res2 = $res2\") } See also: How to connect using JDBC driver","title":"Selecting Data"},{"location":"howto/tableauconnect/","text":"How to Connect Tableau to SnappyData \u00b6 Download and install SnappyData Enterprise edition to connect Tableau to SnappyData. You can connect Tableau using one of the following options: Thrift Server compatible with Apache HiveServer2 (HS2) SnappyData ODBC driver Connect Tableau using Thrift Server \u00b6 Use the following steps to connect Tableau to SnappyData using Thrift Server that is compatible with Apache HiveServer2. This is also fully compatible with Spark's Thrift. Check the system requirements , download and install SnappyData, and then start the SnappyData cluster . Thrift server is enabled by default. Download and install Tableau Desktop v2018.3.x or higher from the Tableau Download page . You may also need to register your product. Open the Tableau Desktop application, on the left panel, from the To A Server section, select Spark SQL connector option. In the Spark SQL configuration dialog box, enter the following details: Enter the host/IP of the Lead node in SnappyData cluster. The default port used by the Hive thrift server is 10000. Select SparkThriftServer option from Type dropdown. Select username and password option from the Authentication dropdown. Set Transport field to SASL . Provide a username/password. You could choose to use APP/APP for username/password if authentication was not configured in the cluster. Note For more information about Spark SQL configurations, click here . Click the Sign In button to connect to SnappyData. Tableau displays the page where you can browse and select Schema and Tables as per your requirements to create data visualizations. Note If you have not installed the Simba Spark ODBC Driver on your system already, the Sign In button is disabled. To enable it, click the Download and Install the drivers link and install the Simba Spark ODBC Driver. After this, the Sign in button is enabled. Handling Large Size Tableau Extracts in SnappyData \u00b6 When you are using the Tableau extract feature and if your extracted data set will be large, you may need to do the following: Set the max result size allowed by SnappyData By default, SnappyData will terminate a query whose result exceeds 1GB. You can set the spark.driver.MaxResultSize property on the Lead node and bounce your cluster. Configure streaming of the result set to Tableau from SnappyData Hive server Tableau permits Initial SQL to be sent to the server when creating a data source connection as described here . In the Initial SQL dialog box, type the following: set spark.sql.thriftServer.incrementalCollect=true Connect Tableau using SnappyData ODBC Driver \u00b6 Get the latest version of SnappyData and SnappyData ODBC driver from SnappyData Release page . Use the following instructions to connect Tableau using SnappyData ODBC driver: Step 1: Setup SnappyData ODBC Driver \u00b6 Follow the instructions provided here to setup SnappyData ODBC Driver. Step 2: Install Tableau Desktop (10.1 or Higher) \u00b6 To install Tableau desktop: Download Tableau Desktop . Depending on your Windows installation, download the 32-bit or 64-bit version of the installer. Follow the steps to complete the installation and ensure that you register and activate your product. Step 3: Connect Tableau Desktop to SnappyData Server \u00b6 When using Tableau with the SnappyData ODBC Driver for the first time, you must add the odbc-snappydata.tdc file that is available in the downloaded snappydata-odbc_1.3.0_win64.zip . To connect the Tableau Desktop to the SnappyData Server: Copy the odbc-snappydata.tdc file to the < User_Home_Path >/Documents/My Tableau Repository/Datasources directory. Open the Tableau Desktop application. On the Start Page, a. Under Connect > To a Server , click Other Databases (ODBC) . The Other Databases (ODBC) window is displayed. b. In the DSN drop-down list, select the name that you provided for your SnappyData ODBC connection (for example snappydsn ), and then click Connect . When the connection to the SnappyData server is established, the Sign In option is enabled. Click Sign In to log into Tableau. From the Schema drop-down list, select a schema. For example, app . All tables from the selected schema are listed. Select the required table(s) and drag it to the canvas. A view generated using the selected tables is displayed. If you make changes to the table, click Update Now to refresh and view your changes. In the Worksheets tab, click sheet to start the analysis. On this screen, you can click and drag a field from the Dimensions area to Rows or Columns . Refer to the Tableau documentation for more information on data visualization.","title":"How to Connect Tableau to SnappyData"},{"location":"howto/tableauconnect/#how-to-connect-tableau-to-snappydata","text":"Download and install SnappyData Enterprise edition to connect Tableau to SnappyData. You can connect Tableau using one of the following options: Thrift Server compatible with Apache HiveServer2 (HS2) SnappyData ODBC driver","title":"How to Connect Tableau to SnappyData"},{"location":"howto/tableauconnect/#connect-tableau-using-thrift-server","text":"Use the following steps to connect Tableau to SnappyData using Thrift Server that is compatible with Apache HiveServer2. This is also fully compatible with Spark's Thrift. Check the system requirements , download and install SnappyData, and then start the SnappyData cluster . Thrift server is enabled by default. Download and install Tableau Desktop v2018.3.x or higher from the Tableau Download page . You may also need to register your product. Open the Tableau Desktop application, on the left panel, from the To A Server section, select Spark SQL connector option. In the Spark SQL configuration dialog box, enter the following details: Enter the host/IP of the Lead node in SnappyData cluster. The default port used by the Hive thrift server is 10000. Select SparkThriftServer option from Type dropdown. Select username and password option from the Authentication dropdown. Set Transport field to SASL . Provide a username/password. You could choose to use APP/APP for username/password if authentication was not configured in the cluster. Note For more information about Spark SQL configurations, click here . Click the Sign In button to connect to SnappyData. Tableau displays the page where you can browse and select Schema and Tables as per your requirements to create data visualizations. Note If you have not installed the Simba Spark ODBC Driver on your system already, the Sign In button is disabled. To enable it, click the Download and Install the drivers link and install the Simba Spark ODBC Driver. After this, the Sign in button is enabled.","title":"Connect Tableau using Thrift Server"},{"location":"howto/tableauconnect/#handling-large-size-tableau-extracts-in-snappydata","text":"When you are using the Tableau extract feature and if your extracted data set will be large, you may need to do the following: Set the max result size allowed by SnappyData By default, SnappyData will terminate a query whose result exceeds 1GB. You can set the spark.driver.MaxResultSize property on the Lead node and bounce your cluster. Configure streaming of the result set to Tableau from SnappyData Hive server Tableau permits Initial SQL to be sent to the server when creating a data source connection as described here . In the Initial SQL dialog box, type the following: set spark.sql.thriftServer.incrementalCollect=true","title":"Handling Large Size Tableau Extracts in SnappyData"},{"location":"howto/tableauconnect/#connect-tableau-using-snappydata-odbc-driver","text":"Get the latest version of SnappyData and SnappyData ODBC driver from SnappyData Release page . Use the following instructions to connect Tableau using SnappyData ODBC driver:","title":"Connect Tableau using SnappyData ODBC Driver"},{"location":"howto/tableauconnect/#step-1-setup-snappydata-odbc-driver","text":"Follow the instructions provided here to setup SnappyData ODBC Driver.","title":"Step 1: Setup SnappyData ODBC Driver"},{"location":"howto/tableauconnect/#step-2-install-tableau-desktop-101-or-higher","text":"To install Tableau desktop: Download Tableau Desktop . Depending on your Windows installation, download the 32-bit or 64-bit version of the installer. Follow the steps to complete the installation and ensure that you register and activate your product.","title":"Step 2: Install Tableau Desktop (10.1 or Higher)"},{"location":"howto/tableauconnect/#step-3-connect-tableau-desktop-to-snappydata-server","text":"When using Tableau with the SnappyData ODBC Driver for the first time, you must add the odbc-snappydata.tdc file that is available in the downloaded snappydata-odbc_1.3.0_win64.zip . To connect the Tableau Desktop to the SnappyData Server: Copy the odbc-snappydata.tdc file to the < User_Home_Path >/Documents/My Tableau Repository/Datasources directory. Open the Tableau Desktop application. On the Start Page, a. Under Connect > To a Server , click Other Databases (ODBC) . The Other Databases (ODBC) window is displayed. b. In the DSN drop-down list, select the name that you provided for your SnappyData ODBC connection (for example snappydsn ), and then click Connect . When the connection to the SnappyData server is established, the Sign In option is enabled. Click Sign In to log into Tableau. From the Schema drop-down list, select a schema. For example, app . All tables from the selected schema are listed. Select the required table(s) and drag it to the canvas. A view generated using the selected tables is displayed. If you make changes to the table, click Update Now to refresh and view your changes. In the Worksheets tab, click sheet to start the analysis. On this screen, you can click and drag a field from the Dimensions area to Rows or Columns . Refer to the Tableau documentation for more information on data visualization.","title":"Step 3: Connect Tableau Desktop to SnappyData Server"},{"location":"howto/use_apache_zeppelin_with_snappydata/","text":"Using Apache Zeppelin with SnappyData \u00b6 Step 1: Download, Install and Configure SnappyData \u00b6 Download and install SnappyData Configure the SnappyData Cluster Start the SnappyData cluster Extract the contents of the Zeppelin 0.8.2 binary package . Then cd into the extracted zeppelin-0.8.2-bin-netinst directory. Note that while these instructions work with any version of Zeppelin, the demo notebooks installed later have been created and tested only on Zeppelin 0.8.2 and may not work correctly on other versions. Install a couple of additional interpreters (angular is used by display panels of the sample notebooks installed later): ZEPPELIN_INTERPRETER_DEP_MVNREPO=https://repo1.maven.org/maven2 ./bin/install-interpreter.sh --name angular,jdbc If you are using the all binary package from zeppelin instead of the netinst package linked in the previous step, then you can skip this step. Copy the SnappyData JDBC client jar inside the interpreter/jdbc directory. Download the predefined SnappyData notebooks with configuration notebooks_embedded_zeppelin.tar.gz . Extract the contents of the compressed tar file (tar xzf) in the Zeppelin installation on your local machine. Start the Zeppelin daemon using the command: ./bin/zeppelin-daemon.sh start To ensure that the installation is successful, log into the Zeppelin UI ( http://localhost:8080 or :8080) from your web browser. Refer here for instructions to configure Apache Zeppelin for securely accessing SnappyData Cluster. Step 2: Configure Interpreter Settings \u00b6 Log on to Zeppelin from your web browser and select Interpreter from the Settings option. This will require a user having administrator privileges, which is set to admin by default. See zeppelin-dir/conf/shiro.ini file for the default admin password and other users and update the file to use your preferred authentication scheme as required. Click on edit in the jdbc interpreter section. Configure the interpreter properties. The table below lists the properties required for SnappyData. Property Value Description default.driver io.snappydata.jdbc.ClientDriver Specify the JDBC driver for SnappyData default.url jdbc:snappydata://localhost:1527 Specify the JDBC URL for SnappyData cluster in the format jdbc:snappydata://<locator_hostname>:1527 default.user SQL user name or app If security is enabled in the SnappyData cluster, then the configured user name else app default.password SQL user password or app If security is enabled in the SnappyData cluster, then the password of the user else can be anything default.splitQueries true Each query in a paragraph is executed apart and returns the result zeppelin.jdbc.concurrent.use true Specify the Zeppelin scheduler to be used. Select True for Fair and False for FIFO zeppelin.jdbc.interpolation true If interpolation of ZeppelinContext objects into the paragraph text is allowed If required, edit other properties, and then click Save to apply your changes. FAQs \u00b6 I am on the homepage, what should I do next? If you are using SnappyData for the first time, you can start with the QuickStart notebooks to start exploring the capabilities of the product. If you have used SnappyData earlier and just want to explore the new interface, you can download data from external sources using the notebooks in the External Data Sources section. I get an error when I run a paragraph? By design, the anonymous user is not allowed to execute notebooks. You may clone the notebook and proceed in the cloned notebook. Do I need to change any setting in Zeppelin to work with the multi-node cluster? Yes, but this requires Zeppelin\u2019s admin user access. By default, you access the Zeppelin notebooks as an anonymous user. For admin user access, click the Interpreter tab and enter your credentials in the Login box. You can find the admin credentials in the zeppelin-dir/conf/shiro.ini file. Update the appropriate IP of a server node in the jdbc URL (highlighted in the following image). Do these notebooks depend on specific Zeppelin version? Yes, these notebooks were developed on Zeppelin version 0.8.2. Are there any configuration files of Zeppelin that I need to be aware of? For advanced multi-user settings, refer to the zeppelin-site.xml and shiro.ini . For more details and options, refer to the Apache Zeppelin documentation. Is Zeppelin the only interface to interact with SnappyData? No, if you prefer a command-line interface, then the product provides two command-line interfaces. The SQL interface, which can be accessed using ./bin/snappy and the experimental scala interpreter can be invoked using ./bin/snappy-scala . You can also use standard JDBC tools like SQuirreL SQL, DbVisualizer, DBeaver etc. See the respective tool documentation to configure SnappyData JDBC driver (snappydata-jdbc_2.11-1.3.1.jar). For BI tools, refer to TIBCO Spotfire\u00ae and Tableau . How to configure Apache Zeppelin to securely and concurrently access the SnappyData Cluster? Refer to How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster .","title":"How to Use Apache Zeppelin with SnappyData"},{"location":"howto/use_apache_zeppelin_with_snappydata/#using-apache-zeppelin-with-snappydata","text":"","title":"Using Apache Zeppelin with SnappyData"},{"location":"howto/use_apache_zeppelin_with_snappydata/#step-1-download-install-and-configure-snappydata","text":"Download and install SnappyData Configure the SnappyData Cluster Start the SnappyData cluster Extract the contents of the Zeppelin 0.8.2 binary package . Then cd into the extracted zeppelin-0.8.2-bin-netinst directory. Note that while these instructions work with any version of Zeppelin, the demo notebooks installed later have been created and tested only on Zeppelin 0.8.2 and may not work correctly on other versions. Install a couple of additional interpreters (angular is used by display panels of the sample notebooks installed later): ZEPPELIN_INTERPRETER_DEP_MVNREPO=https://repo1.maven.org/maven2 ./bin/install-interpreter.sh --name angular,jdbc If you are using the all binary package from zeppelin instead of the netinst package linked in the previous step, then you can skip this step. Copy the SnappyData JDBC client jar inside the interpreter/jdbc directory. Download the predefined SnappyData notebooks with configuration notebooks_embedded_zeppelin.tar.gz . Extract the contents of the compressed tar file (tar xzf) in the Zeppelin installation on your local machine. Start the Zeppelin daemon using the command: ./bin/zeppelin-daemon.sh start To ensure that the installation is successful, log into the Zeppelin UI ( http://localhost:8080 or :8080) from your web browser. Refer here for instructions to configure Apache Zeppelin for securely accessing SnappyData Cluster.","title":"Step 1: Download, Install and Configure SnappyData"},{"location":"howto/use_apache_zeppelin_with_snappydata/#step-2-configure-interpreter-settings","text":"Log on to Zeppelin from your web browser and select Interpreter from the Settings option. This will require a user having administrator privileges, which is set to admin by default. See zeppelin-dir/conf/shiro.ini file for the default admin password and other users and update the file to use your preferred authentication scheme as required. Click on edit in the jdbc interpreter section. Configure the interpreter properties. The table below lists the properties required for SnappyData. Property Value Description default.driver io.snappydata.jdbc.ClientDriver Specify the JDBC driver for SnappyData default.url jdbc:snappydata://localhost:1527 Specify the JDBC URL for SnappyData cluster in the format jdbc:snappydata://<locator_hostname>:1527 default.user SQL user name or app If security is enabled in the SnappyData cluster, then the configured user name else app default.password SQL user password or app If security is enabled in the SnappyData cluster, then the password of the user else can be anything default.splitQueries true Each query in a paragraph is executed apart and returns the result zeppelin.jdbc.concurrent.use true Specify the Zeppelin scheduler to be used. Select True for Fair and False for FIFO zeppelin.jdbc.interpolation true If interpolation of ZeppelinContext objects into the paragraph text is allowed If required, edit other properties, and then click Save to apply your changes.","title":"Step 2: Configure Interpreter Settings"},{"location":"howto/use_apache_zeppelin_with_snappydata/#faqs","text":"I am on the homepage, what should I do next? If you are using SnappyData for the first time, you can start with the QuickStart notebooks to start exploring the capabilities of the product. If you have used SnappyData earlier and just want to explore the new interface, you can download data from external sources using the notebooks in the External Data Sources section. I get an error when I run a paragraph? By design, the anonymous user is not allowed to execute notebooks. You may clone the notebook and proceed in the cloned notebook. Do I need to change any setting in Zeppelin to work with the multi-node cluster? Yes, but this requires Zeppelin\u2019s admin user access. By default, you access the Zeppelin notebooks as an anonymous user. For admin user access, click the Interpreter tab and enter your credentials in the Login box. You can find the admin credentials in the zeppelin-dir/conf/shiro.ini file. Update the appropriate IP of a server node in the jdbc URL (highlighted in the following image). Do these notebooks depend on specific Zeppelin version? Yes, these notebooks were developed on Zeppelin version 0.8.2. Are there any configuration files of Zeppelin that I need to be aware of? For advanced multi-user settings, refer to the zeppelin-site.xml and shiro.ini . For more details and options, refer to the Apache Zeppelin documentation. Is Zeppelin the only interface to interact with SnappyData? No, if you prefer a command-line interface, then the product provides two command-line interfaces. The SQL interface, which can be accessed using ./bin/snappy and the experimental scala interpreter can be invoked using ./bin/snappy-scala . You can also use standard JDBC tools like SQuirreL SQL, DbVisualizer, DBeaver etc. See the respective tool documentation to configure SnappyData JDBC driver (snappydata-jdbc_2.11-1.3.1.jar). For BI tools, refer to TIBCO Spotfire\u00ae and Tableau . How to configure Apache Zeppelin to securely and concurrently access the SnappyData Cluster? Refer to How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster .","title":"FAQs"},{"location":"howto/use_python_to_create_tables_and_run_queries/","text":"How to use Python to Create Tables and Run Queries \u00b6 Developers can write programs in Python to use SnappyData features. First create a SnappySession : from pyspark.sql.snappy import SnappySession from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(appName).setMaster(master) sc = SparkContext(conf=conf) snappy = SnappySession(sc) Create table using SnappySession : For complete list of table attributes refer the Programming Guide # Creating partitioned table PARTSUPP using SQL snappy.sql(\"DROP TABLE IF EXISTS PARTSUPP\") # \"PARTITION_BY\" attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY), snappy.sql(\"CREATE TABLE PARTSUPP ( \" + \"PS_PARTKEY INTEGER NOT NULL PRIMARY KEY,\" + \"PS_SUPPKEY INTEGER NOT NULL,\" + \"PS_AVAILQTY INTEGER NOT NULL,\" + \"PS_SUPPLYCOST DECIMAL(15,2) NOT NULL)\" + \"USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY' )\") Inserting data in table using INSERT query : snappy.sql(\"INSERT INTO PARTSUPP VALUES(100, 1, 5000, 100)\") snappy.sql(\"INSERT INTO PARTSUPP VALUES(200, 2, 50, 10)\") snappy.sql(\"INSERT INTO PARTSUPP VALUES(300, 3, 1000, 20)\") snappy.sql(\"INSERT INTO PARTSUPP VALUES(400, 4, 200, 30)\") # Printing the contents of the PARTSUPP table snappy.sql(\"SELECT * FROM PARTSUPP\").show() Update the data using SQL : # Update the available quantity for PARTKEY 100 snappy.sql(\"UPDATE PARTSUPP SET PS_AVAILQTY = 50000 WHERE PS_PARTKEY = 100\") # Printing the contents of the PARTSUPP table after update snappy.sql(\"SELECT * FROM PARTSUPP\").show() Delete records from the table : # Delete the records for PARTKEY 400 snappy.sql(\"DELETE FROM PARTSUPP WHERE PS_PARTKEY = 400\") # Printing the contents of the PARTSUPP table after delete snappy.sql(\"SELECT * FROM PARTSUPP\").show() Create table using API : This same table can be created by using createTable API. First create a schema and then create the table, and then mutate the table data using API: # drop the table if it exists snappy.dropTable('PARTSUPP', True) schema = StructType([StructField('PS_PARTKEY', IntegerType(), False), StructField('PS_SUPPKEY', IntegerType(), False), StructField('PS_AVAILQTY', IntegerType(),False), StructField('PS_SUPPLYCOST', DecimalType(15, 2), False) ]) # \"PARTITION_BY\" attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY) snappy.createTable('PARTSUPP', 'row', schema, False, PARTITION_BY = 'PS_PARTKEY') # Inserting data in PARTSUPP table using DataFrame tuples = [(100, 1, 5000, Decimal(100)), (200, 2, 50, Decimal(10)), (300, 3, 1000, Decimal(20)), (400, 4, 200, Decimal(30))] rdd = sc.parallelize(tuples) tuplesDF = snappy.createDataFrame(rdd, schema) tuplesDF.write.insertInto(\"PARTSUPP\") #Printing the contents of the PARTSUPP table snappy.sql(\"SELECT * FROM PARTSUPP\").show() # Update the available quantity for PARTKEY 100 snappy.update(\"PARTSUPP\", \"PS_PARTKEY =100\", [50000], [\"PS_AVAILQTY\"]) # Printing the contents of the PARTSUPP table after update snappy.sql(\"SELECT * FROM PARTSUPP\").show() # Delete the records for PARTKEY 400 snappy.delete(\"PARTSUPP\", \"PS_PARTKEY =400\") # Printing the contents of the PARTSUPP table after delete snappy.sql(\"SELECT * FROM PARTSUPP\").show() The complete source code for the above example is in CreateTable.py Related Topics: Running Python Applications","title":"How to use Python to Create Tables and Run Queries"},{"location":"howto/use_python_to_create_tables_and_run_queries/#how-to-use-python-to-create-tables-and-run-queries","text":"Developers can write programs in Python to use SnappyData features. First create a SnappySession : from pyspark.sql.snappy import SnappySession from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(appName).setMaster(master) sc = SparkContext(conf=conf) snappy = SnappySession(sc) Create table using SnappySession : For complete list of table attributes refer the Programming Guide # Creating partitioned table PARTSUPP using SQL snappy.sql(\"DROP TABLE IF EXISTS PARTSUPP\") # \"PARTITION_BY\" attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY), snappy.sql(\"CREATE TABLE PARTSUPP ( \" + \"PS_PARTKEY INTEGER NOT NULL PRIMARY KEY,\" + \"PS_SUPPKEY INTEGER NOT NULL,\" + \"PS_AVAILQTY INTEGER NOT NULL,\" + \"PS_SUPPLYCOST DECIMAL(15,2) NOT NULL)\" + \"USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY' )\") Inserting data in table using INSERT query : snappy.sql(\"INSERT INTO PARTSUPP VALUES(100, 1, 5000, 100)\") snappy.sql(\"INSERT INTO PARTSUPP VALUES(200, 2, 50, 10)\") snappy.sql(\"INSERT INTO PARTSUPP VALUES(300, 3, 1000, 20)\") snappy.sql(\"INSERT INTO PARTSUPP VALUES(400, 4, 200, 30)\") # Printing the contents of the PARTSUPP table snappy.sql(\"SELECT * FROM PARTSUPP\").show() Update the data using SQL : # Update the available quantity for PARTKEY 100 snappy.sql(\"UPDATE PARTSUPP SET PS_AVAILQTY = 50000 WHERE PS_PARTKEY = 100\") # Printing the contents of the PARTSUPP table after update snappy.sql(\"SELECT * FROM PARTSUPP\").show() Delete records from the table : # Delete the records for PARTKEY 400 snappy.sql(\"DELETE FROM PARTSUPP WHERE PS_PARTKEY = 400\") # Printing the contents of the PARTSUPP table after delete snappy.sql(\"SELECT * FROM PARTSUPP\").show() Create table using API : This same table can be created by using createTable API. First create a schema and then create the table, and then mutate the table data using API: # drop the table if it exists snappy.dropTable('PARTSUPP', True) schema = StructType([StructField('PS_PARTKEY', IntegerType(), False), StructField('PS_SUPPKEY', IntegerType(), False), StructField('PS_AVAILQTY', IntegerType(),False), StructField('PS_SUPPLYCOST', DecimalType(15, 2), False) ]) # \"PARTITION_BY\" attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY) snappy.createTable('PARTSUPP', 'row', schema, False, PARTITION_BY = 'PS_PARTKEY') # Inserting data in PARTSUPP table using DataFrame tuples = [(100, 1, 5000, Decimal(100)), (200, 2, 50, Decimal(10)), (300, 3, 1000, Decimal(20)), (400, 4, 200, Decimal(30))] rdd = sc.parallelize(tuples) tuplesDF = snappy.createDataFrame(rdd, schema) tuplesDF.write.insertInto(\"PARTSUPP\") #Printing the contents of the PARTSUPP table snappy.sql(\"SELECT * FROM PARTSUPP\").show() # Update the available quantity for PARTKEY 100 snappy.update(\"PARTSUPP\", \"PS_PARTKEY =100\", [50000], [\"PS_AVAILQTY\"]) # Printing the contents of the PARTSUPP table after update snappy.sql(\"SELECT * FROM PARTSUPP\").show() # Delete the records for PARTKEY 400 snappy.delete(\"PARTSUPP\", \"PS_PARTKEY =400\") # Printing the contents of the PARTSUPP table after delete snappy.sql(\"SELECT * FROM PARTSUPP\").show() The complete source code for the above example is in CreateTable.py Related Topics: Running Python Applications","title":"How to use Python to Create Tables and Run Queries"},{"location":"howto/use_snappy_shell/","text":"How to Use Snappy SQL shell (snappy-sql) \u00b6 The Snappy SQL shell can be used to execute SQL on SnappyData cluster. In the background, snappy-sql uses JDBC connections to execute SQL. Connect to a SnappyData Cluster : Use the snappy-sql and connect client commands on the Snappy SQL shell as follows: $ ./bin/snappy-sql snappy> connect client '<locatorHostName>:1527'; Here, the <locatorHostName> is the host name of the node on which the locator is started and 1527 is the default port on which the locator listens for connections. Execute SQL queries : Once connected, you can execute SQL queries using snappy-sql snappy> CREATE TABLE APP.PARTSUPP (PS_PARTKEY INTEGER NOT NULL PRIMARY KEY, PS_SUPPKEY INTEGER NOT NULL, PS_AVAILQTY INTEGER NOT NULL, PS_SUPPLYCOST DECIMAL(15,2) NOT NULL) USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') ; snappy> INSERT INTO APP.PARTSUPP VALUES(100, 1, 5000, 100); snappy> INSERT INTO APP.PARTSUPP VALUES(200, 2, 50, 10); snappy> SELECT * FROM APP.PARTSUPP; PS_PARTKEY |PS_SUPPKEY |PS_AVAILQTY|PS_SUPPLYCOST ----------------------------------------------------- 100 |1 |5000 |100.00 200 |2 |50 |10.00 2 rows selected View the members of cluster : Use the show members command. snappy> show members; ID |HOST |KIND |STATUS |NETSERVERS |SERVERGROUPS --------------------------------------------------------------------------------------------------------------------------------------------------------------- 192.168.63.1(21412)<v1>:61964 |192.168.63.1 |datastore |RUNNING |localhost/127.0.0.1[1528] | 192.168.63.1(21594)<v2>:29474 |192.168.63.1 |primary lead |RUNNING | | localhost(21262)<v0>:22770 |localhost |locator |RUNNING |localhost/127.0.0.1[1527] | 3 rows selected View the list tables in a schema : Use show tables in <schema> command. snappy> show tables in app; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE|REMARKS ----------------------------------------------------------------------------------- APP |PARTSUPP |TABLE | 1 row selected","title":"How to Use Snappy SQL shell (snappy-sql)"},{"location":"howto/use_snappy_shell/#how-to-use-snappy-sql-shell-snappy-sql","text":"The Snappy SQL shell can be used to execute SQL on SnappyData cluster. In the background, snappy-sql uses JDBC connections to execute SQL. Connect to a SnappyData Cluster : Use the snappy-sql and connect client commands on the Snappy SQL shell as follows: $ ./bin/snappy-sql snappy> connect client '<locatorHostName>:1527'; Here, the <locatorHostName> is the host name of the node on which the locator is started and 1527 is the default port on which the locator listens for connections. Execute SQL queries : Once connected, you can execute SQL queries using snappy-sql snappy> CREATE TABLE APP.PARTSUPP (PS_PARTKEY INTEGER NOT NULL PRIMARY KEY, PS_SUPPKEY INTEGER NOT NULL, PS_AVAILQTY INTEGER NOT NULL, PS_SUPPLYCOST DECIMAL(15,2) NOT NULL) USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') ; snappy> INSERT INTO APP.PARTSUPP VALUES(100, 1, 5000, 100); snappy> INSERT INTO APP.PARTSUPP VALUES(200, 2, 50, 10); snappy> SELECT * FROM APP.PARTSUPP; PS_PARTKEY |PS_SUPPKEY |PS_AVAILQTY|PS_SUPPLYCOST ----------------------------------------------------- 100 |1 |5000 |100.00 200 |2 |50 |10.00 2 rows selected View the members of cluster : Use the show members command. snappy> show members; ID |HOST |KIND |STATUS |NETSERVERS |SERVERGROUPS --------------------------------------------------------------------------------------------------------------------------------------------------------------- 192.168.63.1(21412)<v1>:61964 |192.168.63.1 |datastore |RUNNING |localhost/127.0.0.1[1528] | 192.168.63.1(21594)<v2>:29474 |192.168.63.1 |primary lead |RUNNING | | localhost(21262)<v0>:22770 |localhost |locator |RUNNING |localhost/127.0.0.1[1527] | 3 rows selected View the list tables in a schema : Use show tables in <schema> command. snappy> show tables in app; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE|REMARKS ----------------------------------------------------------------------------------- APP |PARTSUPP |TABLE | 1 row selected","title":"How to Use Snappy SQL shell (snappy-sql)"},{"location":"howto/use_stream_processing_with_snappydata/","text":"How to use Stream Processing with SnappyData \u00b6 SnappyData supports both the older Spark Streaming model (based on DStreams) as well as the newer Structured Streaming model . Unlike the Spark streaming DStreams model, that is based on RDDs, SnappyData supports Spark SQL in both models. Structured Streaming \u00b6 The SnappyData structured streaming programming model is the same as Spark structured streaming . The only difference is support for ingesting streaming dataframes into SnappyData tables through a built-in Sink . SnappyData provides a build-in output Sink which simplifies ingestion of streaming dataframes into SnappyData tables. The Sink supports idempotent writes, ensuring consistency of data when failures occur, as well as support for all mutation operations such as inserts, appends, updates, puts, and deletes. The output data source name for SnappyData is snappysink . A minimal code example for structured streaming with socket source and Snappy Sink is available here . You can also refer to Structured Streaming Quickstart guide . For more examples, refer to structured streaming examples . The following examples are shown: Example Description CDCExample.scala An example explaining CDC (change data capture) use case with SnappyData streaming Sink. CSVFileSourceExampleWithSnappySink.scala An example of structured streaming depicting CSV file processing with Snappy Sink. CSVKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from kafka source using snappy Sink. JSONFileSourceExampleWithSnappySink.scala An example of structured streaming depicting JSON file processing with Snappy Sink. JSONKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from Kafka source using Snappy Sink SocketSourceExample.scala An example showing usage of structured streaming with console Sink. SocketSourceExampleWithSnappySink.scala An example showing usage of structured streaming with SnappyData. The topic included the following sections: Using SnappyData Structured Streaming API Handling Inserts, Updates and Deletes Event Processing Order Sink State Table Overriding Default Sink Behavior Resetting a Streaming Query Best Practices for Structured Streaming Limitations Using SnappyData Structured Streaming API \u00b6 The following code snippet, from the example, explains the usage of SnappyData's Structured Streaming API : val streamingQuery = structDF .filter(_.signal > 10) // so transformation on input dataframe .writeStream .format(\"snappysink\") // Required to ingest into SnappyData tables .queryName(\"Devices\") // Required when using snappysink. Must be unique across the SnappyData cluster. .trigger(ProcessingTime(\"1 seconds\")) .option(\"tableName\", \"devices\") // Required: name of the snappy table where data will be ingested. .option(\"checkpointLocation\", checkpointDirectory) .start() SnappyData Specific options \u00b6 The following are SnappyData specific options which can be configured for Structured Streaming: Options Description tableName Name of the SnappyData table where the streaming data is ingested. The property is case-insensitive and is mandatory. stateTableSchema Name of the schema under which SnappyData\u2019s internal state table will be created. This table is used to track the progress of the streaming queries and enables snappy sink to behave in an idempotent manner when streaming query is restarted after abrupt failures or planned down time. This is a mandatory property when security is enabled for the SnappyData cluster. When security is disabled, snappy sink uses APP schema by default to store the sink state table. conflation This is an optional boolean property with the default value set to false . Conflation is enabled only when you set this property to true . If this property is set to true and if the incoming streaming batch contains multiple events on the same key, Snappy Sink automatically reduces this to a single operation. This is typically the last operation on any given key for the batch that is being processed. This property is only applicable when the _eventType column is available (see below ) and the target table has Keys defined. For more information, see here . sinkCallback This is an optional property which is used to override default Snappy Sink behavior. To override the default behavior, client codes should implement SnappySinkCallback trait and pass the fully qualified name of the implementing class against this property value. Handling Inserts, Updates and Deletes \u00b6 A common use case for streaming is capturing writes into another store (Operational database such as RDB or NoSQL DB) and streaming the events through Kafka, applying Spark transformations, and ingesting into an analytics datastore such as SnappyData. This pattern is commonly referred to as Change-Data-Capture (CDC) . To support this use case, Snappy Sink supports events to signal if these are Inserts, Updates, or Deletes. The application is required to inject a column called _eventType as described below. To support CDC , the source DataFrame must have the following: An IntegerType column named _eventType . The value in the _eventType column can be any of the following: 0 for insert events 1 for update events 2 for delete events In case the input data is following a different convention for event types, then it must be transformed to match the above-mentioned format. Note Records which have _eventType value other than the above-mentioned ones are skipped. The target SnappyData table must have key columns defined for a column table or primary key defined for a row table. An example explaining the CDC use case is available here . If the _eventType column is not provided as part of source dataframe, then the following is observed: In a target table with key columns/primary key defined, the put into operation is applied to all events. In a target table without key columns/primary key defined, the insert operation is applied to all the events. Event Processing Order \u00b6 Currently, the ordering of events across partitions is not supported. Event processing occurs independently in each partition. Hence, you must ensure that in your application all the events, that are associated with a key, are always delivered on the same partition (shard on the key). If your incoming stream is not partitioned on the key column, the application should first repartition the dataframe on the key column. You can ignore this requirement, if your incoming streams are continuously appending. For example, time series or when replacing data where ordering is irrelevant. The writes occur by grouping the events in the following manner (for performance optimization of the columnar store) and is only applicable when your DataFrame has an _eventType column: Processes all delete events (deletes relevant records from target table) Processes all insert events (inserts relevant records into the target table Processes all update events (applies PutInto operation) If the _eventType column is not provided as part of source dataframe, then the events are processed in the following manner: * If key columns/primary keys are defined for the target table, then all the events are treated as update events and put into operation is performed for all events. * If key columns/primary keys are not defined for the target table, then all the events are treated as insert events and insert operation is applied for all events. If conflation property is set to true , Snappy Sink will first conflate the incoming batch independently by each partition. The conflation of events is performed in the following steps: * Group all the events in the given partition by key. * Convert inserts into put into operations if the event type of the last event for a key is of insert type and there are more than one events for the same key. * Keep the last event for each key and drop remaining events. This results in a batch, where there is at most a single entry per key. By default the conflation property is set to false . Therefore, the event processing semantics only ensures consistency when incoming events in a batch are for the unique key column(s). For example: If an incoming batch contains an Insert(key1) event followed by a Delete(key1) event, the record for key1 is shown in the target table after the batch is processed. This is because all the Delete events are processed before Insert events as per the event processing order explained here . In such cases, you should enable the Conflation by setting the conflation property to true. Now, if a batch contains Insert(key1) event followed by a Dele te(key1) event, then SnappyData Sink conflates these two events into a single event by selecting the last event which is Delete(key1) and only that event is processed for key1 . Processing Delete(key1) event without processing Insert(key1) event does not result in a failure, as Delete events are ignored if corresponding records do not exist in the target table. Sink State Table \u00b6 A replicated row table with name snappysys_internal____sink_state_table is created by Snappy Sink under schema specified by the stateTableSchema option if the table does not exist. If the stateTableSchema is not specified then the sink state table is created under the APP schema. During the processing of each batch, this state is updated. This table is used by Snappy Sink to maintain the state of the streaming queries. This state is important to maintain the idempotency of the sink In case of stream failures. The Sink State table contains the following fields: Name Type Comment stream_query_id varchar(200) Primary Key. Name of the streaming query batch_id long Batch id of the most recent batch picked up for processing. Behavior of Sink State Table in a Secure cluster \u00b6 When security is enabled for the cluster, the stateTableSchema becomes a mandatory option. Also, when you submit the streaming job, you must have the necessary permissions on the schema specified by stateTableSchema option. Maintaining Idempotency In Case Of Stream Failures \u00b6 When stream execution fails, it is possible that streaming batch was half processed. Hence next time whenever the stream is started, Spark picks the half processed batch again for processing. This can lead to extraneous records in the target table if the batch contains insert events. To overcome this, Snappy Sink keeps the state of a stream query execution as part of the Sink State table. Note The key columns in a column table are merely a hint (used to perform put into and delete operations) and does not enforce a unique constraint such as a primary key in case of a row table. Using this state, Snappy Sink can detect whether a batch is a duplicate batch. If a batch is a duplicate batch then Snappy Sink processes all insert events from the batch using put into operation. This ensures that no duplicate records are inserted into the target table. Note The above-mentioned behavior is applicable only when the key columns are defined on the target table as key columns are necessary to apply put into operation. When key columns are not defined on the target table, Snappy Sink does not behave in an idempotent manner and it can lead to duplicate records in the target table when the streaming query is restarted after stream failure. Overriding Default Sink Behavior \u00b6 If required, applications can override the default Snappy Sink semantics by implementing org.apache.spark.sql.streaming.SnappySinkCallback and passing the fully qualified name of the implementing class as a value of sinkCallback option of Snappy Sink . SnappySinkCallback trait contains one method which needs to be implemented by the implementing class. This method is called for each streaming batch after checking the possibility of batch duplication which is indicated by possibleDuplicate flag. A duplicate batch might be picked up for processing in case of failure. In the case of batch duplication, this method should handle batch in an idempotent manner in order to avoid data inconsistency. def process(snappySession: SnappySession, sinkProps: Map[String, String], batchId: Long, df: Dataset[Row], possibleDuplicate: Boolean = false): Unit Resetting a Streaming Query \u00b6 Progress of a streaming query is saved as part of the checkpoint directory by Spark. On top of this Snappy Sink also maintains an internal state as part of the state table to ensure idempotency of the sink. Hence to reset a streaming query, the following actions must be taken to clean te state of the streaming query: Note When you use the following steps you may permanently lose the state of the streaming query. Delete the checkpoint directory. (or start streaming query with different checkpoint directory.) Clear the state from the state table using following sql: delete from [state_table_schema].snappysys_internal____sink_state_table where stream_query_id = <query_name>; [state_table_schema] is the schema passed as part of stateTableSchema option of snappy sink. It should be skipped if stateTableSchema option was not provided while defining snappy sink. <query_name> is the name of the query provided while defining the sink. Best Practices for Structured Streaming \u00b6 Refer to the Best Practices for Structured Streaming Considerations . Limitations \u00b6 Limitations of Snappy Sink are as follows: When the data coming from the source is not partitioned by key columns, then using Snappy Sink may result in inconsistent data. This is because each partition independently processes the data using the above-mentioned logic . When key columns are not defined on the target table and the input dataframe does not contain _eventType column, then Snappy Sink cannot guarantee idempotent behavior. This is because inserts cannot be converted into put into , as there are no key columns on the table. In such a scenario, Snappy Sink may insert duplicate records after an abrupt failure of the streaming job. The default Snappy Sink implementation does not support partial records for updates. Which means that there is no support to merge updates on a few columns into the store. For all update events, the incoming records must provide values into all the columns of the target table. Spark Streaming DStreams Model \u00b6 SnappyData\u2019s streaming functionality builds on top of Spark Streaming and is primarily aimed at making it simpler to build streaming applications and to integrate with the built-in store. In SnappyData, you can define streams declaratively from any SQL client, register continuous queries on streams, mutate SnappyData tables based on the streaming data. For more information on streaming, refer to this section . Code Sample \u00b6 Code example for streaming is in StreamingExample.scala . The code snippets in the following sections show how to declare a stream table, register continuous queries(CQ), and update SnappyData table using the stream data. Using Stream Processing with SnappyData \u00b6 First get a SnappySession and a SnappyStreamingContext : Here SnappyStreamingContext is initialized in a batch duration of one second. val spark: SparkSession = SparkSession .builder .appName(getClass.getSimpleName) .master(\"local[*]\") .getOrCreate val snsc = new SnappyStreamingContext(spark.sparkContext, Seconds(1)) The example starts an embedded Kafka instance on which a few messages are published. SnappyData processes these message and updates a table based on the stream data. The SQL below shows how to declare a stream table using SQL. The rowConverter attribute specifies a class used to return Row objects from the received stream messages. snsc.sql( \"create stream table adImpressionStream (\" + \" time_stamp timestamp,\" + \" publisher string,\" + \" advertiser string,\" + \" website string,\" + \" geo string,\" + \" bid double,\" + \" cookie string) \" + \" using kafka_stream options(\" + \" rowConverter 'org.apache.spark.examples.snappydata.RowsConverter',\" + s\" kafkaParams 'bootstrap.servers->$add;\" + \"key.deserializer->org.apache.kafka.common.serialization.StringDeserializer;\" + \"value.deserializer->org.apache.kafka.common.serialization.StringDeserializer;\" + s\"group.id->$groupId;auto.offset.reset->earliest',\" + s\" startingOffsets '$startingOffsets', \" + s\" subscribe '$topic')\" ) RowsConverter decodes a stream message consisting of comma-separated fields and forms a Row object from it. class RowsConverter extends StreamToRowsConverter with Serializable { override def toRows(message: Any): Seq[Row] = { val log = message.asInstanceOf[String] val fields = log.split(\",\") val rows = Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong), fields(1), fields(2), fields(3), fields(4), fields(5).toDouble, fields(6) ))) rows } } To create a row table that is updated based on the streaming data : snsc.sql(\"create table publisher_bid_counts(publisher string, bidCount int) using row\") To declare a continuous query that is executed on the streaming data : This query returns a number of bids per publisher in one batch. val resultStream: SchemaDStream = snsc.registerCQ(\"select publisher, count(bid) as bidCount from \" + \"adImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher\") To process that the result of above continuous query to update the row table publisher_bid_counts : // this conf is used to get a JDBC connection val conf = new ConnectionConfBuilder(snsc.snappySession).build() resultStream.foreachDataFrame(df => { println(\"Data received in streaming window\") df.show() println(\"Updating table publisher_bid_counts\") val conn = ConnectionUtil.getConnection(conf) val result = df.collect() val stmt = conn.prepareStatement(\"update publisher_bid_counts set \" + s\"bidCount = bidCount + ? where publisher = ?\") result.foreach(row => { val publisher = row.getString(0) val bidCount = row.getLong(1) stmt.setLong(1, bidCount) stmt.setString(2, publisher) stmt.addBatch() } ) stmt.executeBatch() conn.close() } }) To display the total bids by each publisher by querying publisher_bid_counts table : snsc.snappySession.sql(\"select publisher, bidCount from publisher_bid_counts\").show()","title":"How to use Stream Processing with SnappyData"},{"location":"howto/use_stream_processing_with_snappydata/#how-to-use-stream-processing-with-snappydata","text":"SnappyData supports both the older Spark Streaming model (based on DStreams) as well as the newer Structured Streaming model . Unlike the Spark streaming DStreams model, that is based on RDDs, SnappyData supports Spark SQL in both models.","title":"How to use Stream Processing with SnappyData"},{"location":"howto/use_stream_processing_with_snappydata/#structured-streaming","text":"The SnappyData structured streaming programming model is the same as Spark structured streaming . The only difference is support for ingesting streaming dataframes into SnappyData tables through a built-in Sink . SnappyData provides a build-in output Sink which simplifies ingestion of streaming dataframes into SnappyData tables. The Sink supports idempotent writes, ensuring consistency of data when failures occur, as well as support for all mutation operations such as inserts, appends, updates, puts, and deletes. The output data source name for SnappyData is snappysink . A minimal code example for structured streaming with socket source and Snappy Sink is available here . You can also refer to Structured Streaming Quickstart guide . For more examples, refer to structured streaming examples . The following examples are shown: Example Description CDCExample.scala An example explaining CDC (change data capture) use case with SnappyData streaming Sink. CSVFileSourceExampleWithSnappySink.scala An example of structured streaming depicting CSV file processing with Snappy Sink. CSVKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from kafka source using snappy Sink. JSONFileSourceExampleWithSnappySink.scala An example of structured streaming depicting JSON file processing with Snappy Sink. JSONKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from Kafka source using Snappy Sink SocketSourceExample.scala An example showing usage of structured streaming with console Sink. SocketSourceExampleWithSnappySink.scala An example showing usage of structured streaming with SnappyData. The topic included the following sections: Using SnappyData Structured Streaming API Handling Inserts, Updates and Deletes Event Processing Order Sink State Table Overriding Default Sink Behavior Resetting a Streaming Query Best Practices for Structured Streaming Limitations","title":"Structured Streaming"},{"location":"howto/use_stream_processing_with_snappydata/#using-snappydata-structured-streaming-api","text":"The following code snippet, from the example, explains the usage of SnappyData's Structured Streaming API : val streamingQuery = structDF .filter(_.signal > 10) // so transformation on input dataframe .writeStream .format(\"snappysink\") // Required to ingest into SnappyData tables .queryName(\"Devices\") // Required when using snappysink. Must be unique across the SnappyData cluster. .trigger(ProcessingTime(\"1 seconds\")) .option(\"tableName\", \"devices\") // Required: name of the snappy table where data will be ingested. .option(\"checkpointLocation\", checkpointDirectory) .start()","title":"Using SnappyData Structured Streaming API"},{"location":"howto/use_stream_processing_with_snappydata/#snappydata-specific-options","text":"The following are SnappyData specific options which can be configured for Structured Streaming: Options Description tableName Name of the SnappyData table where the streaming data is ingested. The property is case-insensitive and is mandatory. stateTableSchema Name of the schema under which SnappyData\u2019s internal state table will be created. This table is used to track the progress of the streaming queries and enables snappy sink to behave in an idempotent manner when streaming query is restarted after abrupt failures or planned down time. This is a mandatory property when security is enabled for the SnappyData cluster. When security is disabled, snappy sink uses APP schema by default to store the sink state table. conflation This is an optional boolean property with the default value set to false . Conflation is enabled only when you set this property to true . If this property is set to true and if the incoming streaming batch contains multiple events on the same key, Snappy Sink automatically reduces this to a single operation. This is typically the last operation on any given key for the batch that is being processed. This property is only applicable when the _eventType column is available (see below ) and the target table has Keys defined. For more information, see here . sinkCallback This is an optional property which is used to override default Snappy Sink behavior. To override the default behavior, client codes should implement SnappySinkCallback trait and pass the fully qualified name of the implementing class against this property value.","title":"SnappyData Specific options"},{"location":"howto/use_stream_processing_with_snappydata/#handling-inserts-updates-and-deletes","text":"A common use case for streaming is capturing writes into another store (Operational database such as RDB or NoSQL DB) and streaming the events through Kafka, applying Spark transformations, and ingesting into an analytics datastore such as SnappyData. This pattern is commonly referred to as Change-Data-Capture (CDC) . To support this use case, Snappy Sink supports events to signal if these are Inserts, Updates, or Deletes. The application is required to inject a column called _eventType as described below. To support CDC , the source DataFrame must have the following: An IntegerType column named _eventType . The value in the _eventType column can be any of the following: 0 for insert events 1 for update events 2 for delete events In case the input data is following a different convention for event types, then it must be transformed to match the above-mentioned format. Note Records which have _eventType value other than the above-mentioned ones are skipped. The target SnappyData table must have key columns defined for a column table or primary key defined for a row table. An example explaining the CDC use case is available here . If the _eventType column is not provided as part of source dataframe, then the following is observed: In a target table with key columns/primary key defined, the put into operation is applied to all events. In a target table without key columns/primary key defined, the insert operation is applied to all the events.","title":"Handling Inserts, Updates and Deletes"},{"location":"howto/use_stream_processing_with_snappydata/#event-processing-order","text":"Currently, the ordering of events across partitions is not supported. Event processing occurs independently in each partition. Hence, you must ensure that in your application all the events, that are associated with a key, are always delivered on the same partition (shard on the key). If your incoming stream is not partitioned on the key column, the application should first repartition the dataframe on the key column. You can ignore this requirement, if your incoming streams are continuously appending. For example, time series or when replacing data where ordering is irrelevant. The writes occur by grouping the events in the following manner (for performance optimization of the columnar store) and is only applicable when your DataFrame has an _eventType column: Processes all delete events (deletes relevant records from target table) Processes all insert events (inserts relevant records into the target table Processes all update events (applies PutInto operation) If the _eventType column is not provided as part of source dataframe, then the events are processed in the following manner: * If key columns/primary keys are defined for the target table, then all the events are treated as update events and put into operation is performed for all events. * If key columns/primary keys are not defined for the target table, then all the events are treated as insert events and insert operation is applied for all events. If conflation property is set to true , Snappy Sink will first conflate the incoming batch independently by each partition. The conflation of events is performed in the following steps: * Group all the events in the given partition by key. * Convert inserts into put into operations if the event type of the last event for a key is of insert type and there are more than one events for the same key. * Keep the last event for each key and drop remaining events. This results in a batch, where there is at most a single entry per key. By default the conflation property is set to false . Therefore, the event processing semantics only ensures consistency when incoming events in a batch are for the unique key column(s). For example: If an incoming batch contains an Insert(key1) event followed by a Delete(key1) event, the record for key1 is shown in the target table after the batch is processed. This is because all the Delete events are processed before Insert events as per the event processing order explained here . In such cases, you should enable the Conflation by setting the conflation property to true. Now, if a batch contains Insert(key1) event followed by a Dele te(key1) event, then SnappyData Sink conflates these two events into a single event by selecting the last event which is Delete(key1) and only that event is processed for key1 . Processing Delete(key1) event without processing Insert(key1) event does not result in a failure, as Delete events are ignored if corresponding records do not exist in the target table.","title":"Event Processing Order"},{"location":"howto/use_stream_processing_with_snappydata/#sink-state-table","text":"A replicated row table with name snappysys_internal____sink_state_table is created by Snappy Sink under schema specified by the stateTableSchema option if the table does not exist. If the stateTableSchema is not specified then the sink state table is created under the APP schema. During the processing of each batch, this state is updated. This table is used by Snappy Sink to maintain the state of the streaming queries. This state is important to maintain the idempotency of the sink In case of stream failures. The Sink State table contains the following fields: Name Type Comment stream_query_id varchar(200) Primary Key. Name of the streaming query batch_id long Batch id of the most recent batch picked up for processing.","title":"Sink State Table"},{"location":"howto/use_stream_processing_with_snappydata/#behavior-of-sink-state-table-in-a-secure-cluster","text":"When security is enabled for the cluster, the stateTableSchema becomes a mandatory option. Also, when you submit the streaming job, you must have the necessary permissions on the schema specified by stateTableSchema option.","title":"Behavior of Sink State Table in a Secure cluster"},{"location":"howto/use_stream_processing_with_snappydata/#maintaining-idempotency-in-case-of-stream-failures","text":"When stream execution fails, it is possible that streaming batch was half processed. Hence next time whenever the stream is started, Spark picks the half processed batch again for processing. This can lead to extraneous records in the target table if the batch contains insert events. To overcome this, Snappy Sink keeps the state of a stream query execution as part of the Sink State table. Note The key columns in a column table are merely a hint (used to perform put into and delete operations) and does not enforce a unique constraint such as a primary key in case of a row table. Using this state, Snappy Sink can detect whether a batch is a duplicate batch. If a batch is a duplicate batch then Snappy Sink processes all insert events from the batch using put into operation. This ensures that no duplicate records are inserted into the target table. Note The above-mentioned behavior is applicable only when the key columns are defined on the target table as key columns are necessary to apply put into operation. When key columns are not defined on the target table, Snappy Sink does not behave in an idempotent manner and it can lead to duplicate records in the target table when the streaming query is restarted after stream failure.","title":"Maintaining Idempotency In Case Of Stream Failures"},{"location":"howto/use_stream_processing_with_snappydata/#overriding-default-sink-behavior","text":"If required, applications can override the default Snappy Sink semantics by implementing org.apache.spark.sql.streaming.SnappySinkCallback and passing the fully qualified name of the implementing class as a value of sinkCallback option of Snappy Sink . SnappySinkCallback trait contains one method which needs to be implemented by the implementing class. This method is called for each streaming batch after checking the possibility of batch duplication which is indicated by possibleDuplicate flag. A duplicate batch might be picked up for processing in case of failure. In the case of batch duplication, this method should handle batch in an idempotent manner in order to avoid data inconsistency. def process(snappySession: SnappySession, sinkProps: Map[String, String], batchId: Long, df: Dataset[Row], possibleDuplicate: Boolean = false): Unit","title":"Overriding Default Sink Behavior"},{"location":"howto/use_stream_processing_with_snappydata/#resetting-a-streaming-query","text":"Progress of a streaming query is saved as part of the checkpoint directory by Spark. On top of this Snappy Sink also maintains an internal state as part of the state table to ensure idempotency of the sink. Hence to reset a streaming query, the following actions must be taken to clean te state of the streaming query: Note When you use the following steps you may permanently lose the state of the streaming query. Delete the checkpoint directory. (or start streaming query with different checkpoint directory.) Clear the state from the state table using following sql: delete from [state_table_schema].snappysys_internal____sink_state_table where stream_query_id = <query_name>; [state_table_schema] is the schema passed as part of stateTableSchema option of snappy sink. It should be skipped if stateTableSchema option was not provided while defining snappy sink. <query_name> is the name of the query provided while defining the sink.","title":"Resetting a Streaming Query"},{"location":"howto/use_stream_processing_with_snappydata/#best-practices-for-structured-streaming","text":"Refer to the Best Practices for Structured Streaming Considerations .","title":"Best Practices for Structured Streaming"},{"location":"howto/use_stream_processing_with_snappydata/#limitations","text":"Limitations of Snappy Sink are as follows: When the data coming from the source is not partitioned by key columns, then using Snappy Sink may result in inconsistent data. This is because each partition independently processes the data using the above-mentioned logic . When key columns are not defined on the target table and the input dataframe does not contain _eventType column, then Snappy Sink cannot guarantee idempotent behavior. This is because inserts cannot be converted into put into , as there are no key columns on the table. In such a scenario, Snappy Sink may insert duplicate records after an abrupt failure of the streaming job. The default Snappy Sink implementation does not support partial records for updates. Which means that there is no support to merge updates on a few columns into the store. For all update events, the incoming records must provide values into all the columns of the target table.","title":"Limitations"},{"location":"howto/use_stream_processing_with_snappydata/#spark-streaming-dstreams-model","text":"SnappyData\u2019s streaming functionality builds on top of Spark Streaming and is primarily aimed at making it simpler to build streaming applications and to integrate with the built-in store. In SnappyData, you can define streams declaratively from any SQL client, register continuous queries on streams, mutate SnappyData tables based on the streaming data. For more information on streaming, refer to this section .","title":"Spark Streaming DStreams Model"},{"location":"howto/use_stream_processing_with_snappydata/#code-sample","text":"Code example for streaming is in StreamingExample.scala . The code snippets in the following sections show how to declare a stream table, register continuous queries(CQ), and update SnappyData table using the stream data.","title":"Code Sample"},{"location":"howto/use_stream_processing_with_snappydata/#using-stream-processing-with-snappydata","text":"First get a SnappySession and a SnappyStreamingContext : Here SnappyStreamingContext is initialized in a batch duration of one second. val spark: SparkSession = SparkSession .builder .appName(getClass.getSimpleName) .master(\"local[*]\") .getOrCreate val snsc = new SnappyStreamingContext(spark.sparkContext, Seconds(1)) The example starts an embedded Kafka instance on which a few messages are published. SnappyData processes these message and updates a table based on the stream data. The SQL below shows how to declare a stream table using SQL. The rowConverter attribute specifies a class used to return Row objects from the received stream messages. snsc.sql( \"create stream table adImpressionStream (\" + \" time_stamp timestamp,\" + \" publisher string,\" + \" advertiser string,\" + \" website string,\" + \" geo string,\" + \" bid double,\" + \" cookie string) \" + \" using kafka_stream options(\" + \" rowConverter 'org.apache.spark.examples.snappydata.RowsConverter',\" + s\" kafkaParams 'bootstrap.servers->$add;\" + \"key.deserializer->org.apache.kafka.common.serialization.StringDeserializer;\" + \"value.deserializer->org.apache.kafka.common.serialization.StringDeserializer;\" + s\"group.id->$groupId;auto.offset.reset->earliest',\" + s\" startingOffsets '$startingOffsets', \" + s\" subscribe '$topic')\" ) RowsConverter decodes a stream message consisting of comma-separated fields and forms a Row object from it. class RowsConverter extends StreamToRowsConverter with Serializable { override def toRows(message: Any): Seq[Row] = { val log = message.asInstanceOf[String] val fields = log.split(\",\") val rows = Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong), fields(1), fields(2), fields(3), fields(4), fields(5).toDouble, fields(6) ))) rows } } To create a row table that is updated based on the streaming data : snsc.sql(\"create table publisher_bid_counts(publisher string, bidCount int) using row\") To declare a continuous query that is executed on the streaming data : This query returns a number of bids per publisher in one batch. val resultStream: SchemaDStream = snsc.registerCQ(\"select publisher, count(bid) as bidCount from \" + \"adImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher\") To process that the result of above continuous query to update the row table publisher_bid_counts : // this conf is used to get a JDBC connection val conf = new ConnectionConfBuilder(snsc.snappySession).build() resultStream.foreachDataFrame(df => { println(\"Data received in streaming window\") df.show() println(\"Updating table publisher_bid_counts\") val conn = ConnectionUtil.getConnection(conf) val result = df.collect() val stmt = conn.prepareStatement(\"update publisher_bid_counts set \" + s\"bidCount = bidCount + ? where publisher = ?\") result.foreach(row => { val publisher = row.getString(0) val bidCount = row.getLong(1) stmt.setLong(1, bidCount) stmt.setString(2, publisher) stmt.addBatch() } ) stmt.executeBatch() conn.close() } }) To display the total bids by each publisher by querying publisher_bid_counts table : snsc.snappySession.sql(\"select publisher, bidCount from publisher_bid_counts\").show()","title":"Using Stream Processing with SnappyData"},{"location":"howto/use_synopsis_data_engine_to_run_approximate_queries/","text":"How to use Approximate Query Processing (AQP) to Run Approximate Queries \u00b6 Approximate Query Processing (AQP) uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time. For more information on AQP, refer to AQP documentation . Code Example : The complete code example for AQP is in SynopsisDataExample.scala . The code below creates a sample table and executes queries that run on the sample table. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"SynopsisDataExample\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) The base column table(AIRLINE) is created from temporary parquet table as follows : // Create temporary staging table to load parquet data snSession.sql(\"CREATE EXTERNAL TABLE STAGING_AIRLINE \" + \"USING parquet OPTIONS(path \" + s\"'${dataFolder}/airlineParquetData')\") // Create a column table AIRLINE snSession.sql(\"CREATE TABLE AIRLINE USING column AS (SELECT Year AS Year_, \" + \"Month AS Month_ , DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, \" + \"CRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, \" + \"CRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance, \" + \"TaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay, \" + \"WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, \" + \"ArrDelaySlot FROM STAGING_AIRLINE)\") Create a sample table for the above base table : Attribute 'qcs' in the statement below specifies the columns used for stratification and attribute 'fraction' specifies how big the sample needs to be (3% of the base table AIRLINE in this case). For more information on Approximate Query Processing, refer to the AQP documentation . snSession.sql(\"CREATE SAMPLE TABLE AIRLINE_SAMPLE ON AIRLINE OPTIONS\" + \"(qcs 'UniqueCarrier, Year_, Month_', fraction '0.03') \" + \"AS (SELECT Year_, Month_ , DayOfMonth, \" + \"DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier, \" + \"FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, \" + \"ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, \" + \"Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, \" + \"NASDelay, SecurityDelay, LateAircraftDelay, ArrDelaySlot FROM AIRLINE)\") Execute queries that return approximate results using sample tables : The query below returns airlines by number of flights in descending order. The 'with error 0.20' clause in the query below signals query engine to execute the query on the sample table instead of the base table and maximum 20% error is allowed. var result = snSession.sql(\"select count(*) flightRecCount, description AirlineName, \" + \"UniqueCarrier carrierCode ,Year_ from airline , airlineref where \" + \"airline.UniqueCarrier = airlineref.code group by \" + \"UniqueCarrier,description, Year_ order by flightRecCount desc limit \" + \"10 with error 0.20\").collect() result.foreach(r => println(r(0) + \", \" + r(1) + \", \" + r(2) + \", \" + r(3))) Join the sample table with a reference table : You can join the sample table with a reference table to execute queries. The example below illustrates how a reference table (AIRLINEREF) is created as from a parquet data file. // create temporary staging table to load parquet data snSession.sql(\"CREATE EXTERNAL TABLE STAGING_AIRLINEREF USING \" + \"parquet OPTIONS(path \" + s\"'${dataFolder}/airportcodeParquetData')\") snSession.sql(\"CREATE TABLE AIRLINEREF USING row AS (SELECT CODE, \" + \"DESCRIPTION FROM STAGING_AIRLINEREF)\") Join the sample table and reference table to find out which airlines arrive on schedule : result = snSession.sql(\"select AVG(ArrDelay) arrivalDelay, \" + \"relative_error(arrivalDelay) rel_err, description AirlineName, \" + \"UniqueCarrier carrier from airline, airlineref \" + \"where airline.UniqueCarrier = airlineref.Code \" + \"group by UniqueCarrier, description order by arrivalDelay \" + \"with error\").collect() result.foreach(r => println(r(0) + \", \" + r(1) + \", \" + r(2) + \", \" + r(3)))","title":"How to use Approximate Query Processing (AQP) to Run Approximate Queries"},{"location":"howto/use_synopsis_data_engine_to_run_approximate_queries/#how-to-use-approximate-query-processing-aqp-to-run-approximate-queries","text":"Approximate Query Processing (AQP) uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time. For more information on AQP, refer to AQP documentation . Code Example : The complete code example for AQP is in SynopsisDataExample.scala . The code below creates a sample table and executes queries that run on the sample table. Get a SnappySession : val spark: SparkSession = SparkSession .builder .appName(\"SynopsisDataExample\") .master(\"local[*]\") .getOrCreate val snSession = new SnappySession(spark.sparkContext) The base column table(AIRLINE) is created from temporary parquet table as follows : // Create temporary staging table to load parquet data snSession.sql(\"CREATE EXTERNAL TABLE STAGING_AIRLINE \" + \"USING parquet OPTIONS(path \" + s\"'${dataFolder}/airlineParquetData')\") // Create a column table AIRLINE snSession.sql(\"CREATE TABLE AIRLINE USING column AS (SELECT Year AS Year_, \" + \"Month AS Month_ , DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, \" + \"CRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, \" + \"CRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance, \" + \"TaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay, \" + \"WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, \" + \"ArrDelaySlot FROM STAGING_AIRLINE)\") Create a sample table for the above base table : Attribute 'qcs' in the statement below specifies the columns used for stratification and attribute 'fraction' specifies how big the sample needs to be (3% of the base table AIRLINE in this case). For more information on Approximate Query Processing, refer to the AQP documentation . snSession.sql(\"CREATE SAMPLE TABLE AIRLINE_SAMPLE ON AIRLINE OPTIONS\" + \"(qcs 'UniqueCarrier, Year_, Month_', fraction '0.03') \" + \"AS (SELECT Year_, Month_ , DayOfMonth, \" + \"DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier, \" + \"FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, \" + \"ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, \" + \"Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, \" + \"NASDelay, SecurityDelay, LateAircraftDelay, ArrDelaySlot FROM AIRLINE)\") Execute queries that return approximate results using sample tables : The query below returns airlines by number of flights in descending order. The 'with error 0.20' clause in the query below signals query engine to execute the query on the sample table instead of the base table and maximum 20% error is allowed. var result = snSession.sql(\"select count(*) flightRecCount, description AirlineName, \" + \"UniqueCarrier carrierCode ,Year_ from airline , airlineref where \" + \"airline.UniqueCarrier = airlineref.code group by \" + \"UniqueCarrier,description, Year_ order by flightRecCount desc limit \" + \"10 with error 0.20\").collect() result.foreach(r => println(r(0) + \", \" + r(1) + \", \" + r(2) + \", \" + r(3))) Join the sample table with a reference table : You can join the sample table with a reference table to execute queries. The example below illustrates how a reference table (AIRLINEREF) is created as from a parquet data file. // create temporary staging table to load parquet data snSession.sql(\"CREATE EXTERNAL TABLE STAGING_AIRLINEREF USING \" + \"parquet OPTIONS(path \" + s\"'${dataFolder}/airportcodeParquetData')\") snSession.sql(\"CREATE TABLE AIRLINEREF USING row AS (SELECT CODE, \" + \"DESCRIPTION FROM STAGING_AIRLINEREF)\") Join the sample table and reference table to find out which airlines arrive on schedule : result = snSession.sql(\"select AVG(ArrDelay) arrivalDelay, \" + \"relative_error(arrivalDelay) rel_err, description AirlineName, \" + \"UniqueCarrier carrier from airline, airlineref \" + \"where airline.UniqueCarrier = airlineref.Code \" + \"group by UniqueCarrier, description order by arrivalDelay \" + \"with error\").collect() result.foreach(r => println(r(0) + \", \" + r(1) + \", \" + r(2) + \", \" + r(3)))","title":"How to use Approximate Query Processing (AQP) to Run Approximate Queries"},{"location":"howto/use_transactions_isolation_levels/","text":"How to use Transactions Isolation Levels \u00b6 SnappyData supports transaction isolation levels when using JDBC or ODBC connections. The default transaction level in SnappyData is set to NONE. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads. SnappyData also supports READ_COMMITTED and REPEATABLE_READ transaction isolation levels. A detailed description of the transaction's semantics in SnappyData can be found in the Overview of SnappyData Distributed Transactions section. Note If you set the isolation level to READ_COMMITTED or REPEATABLE_READ , queries on column table report an error if autocommit is set to off ( false ). Queries on column tables are supported when isolation level is set to NONE . SnappyData internally sets autocommit to true in this case. Queries on row tables are supported when autocommit is set to false and isolation level is set to other READ_COMMITTED or REPEATABLE_READ . Examples \u00b6 Note Before you try these examples, ensure that you have started the SnappyData cluster . The following examples provide JDBC example code snippets that explain how to use transactions isolation levels. Example 1 \u00b6 For row tables, autocommit can be set to false or true import java.sql.{Connection, Statement} ... ... val url: String = \"jdbc:snappydata://1527/\" val conn1 = DriverManager.getConnection(url) val stmt1 = conn1.createStatement() // create a row table stmt1.execute(\"CREATE TABLE APP.PARTSUPP ( \" + \"PS_PARTKEY INTEGER NOT NULL PRIMARY KEY,\" + \"PS_SUPPKEY INTEGER NOT NULL,\" + \"PS_AVAILQTY INTEGER NOT NULL,\" + \"PS_SUPPLYCOST DECIMAL(15,2) NOT NULL)\" + \"USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\") // set the tx isolation level conn1.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED) // set autocommit to false conn1.setAutoCommit(false) val preparedStmt1 = conn1.prepareStatement(\"INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\") for (x <- 1 to 10) { preparedStmt1.setInt(1, x*100) preparedStmt1.setInt(2, x) preparedStmt1.setInt(3, x*1000) preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2)) preparedStmt1.executeUpdate() } // commit the transaction conn1.commit() val rs1 = stmt1.executeQuery(\"SELECT * FROM APP.PARTSUPP\") while (rs1.next()) { println(rs1.getInt(1) + \",\" + rs1.getInt(2) + \",\" + rs1.getInt(3)) } rs1.close() stmt1.close() conn1.close() Example 2 \u00b6 For column tables, autocommit must be set to true , otherwise, an error is reported when the query is executed. val conn2 = DriverManager.getConnection(url) val stmt2 = conn2.createStatement() // create a column table stmt2.execute(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER,\" + \"C_NAME VARCHAR(25),\" + \"C_ADDRESS VARCHAR(40),\" + \"C_NATIONKEY INTEGER,\" + \"C_PHONE VARCHAR(15),\" + \"C_ACCTBAL DECIMAL(15,2),\" + \"C_MKTSEGMENT VARCHAR(10),\" + \"C_COMMENT VARCHAR(117))\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") // set the tx isolation level conn2.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED) // set autocommit to true otherwise opeartions on column table will error out conn2.setAutoCommit(true) stmt2.execute(\"INSERT INTO CUSTOMER VALUES(20000, 'Customer20000', \" + \"'Chicago, IL', 1, '555-101-782', 3500, 'MKTSEGMENT', '')\") stmt2.execute(\"INSERT INTO CUSTOMER VALUES(30000, 'Customer30000', \" + \"'San Hose, CA', 1, '555-201-562', 4500, 'MKTSEGMENT', '')\") val rs2 = stmt2.executeQuery(\"SELECT * FROM APP.CUSTOMER\") while (rs2.next()) { println(rs2.getInt(1) + \",\" + rs2.getString(2)) } rs2.close() Unsupported operations when autocommit is set to false for column tables \u00b6 // if autocommit is set to false, queries throw an error if column tables are involved conn2.setAutoCommit(false) // invalid query stmt2.execute(\"SELECT * FROM APP.CUSTOMER\") // the above statement throws an error as given below EXCEPTION: java.sql.SQLException: (SQLState=XJ218 Severity=20000) (Server=localhost/127.0.0.1[25299] Thread=pool-14-thread-3) Operations on column tables are not supported when query routing is disabled or autocommit is false More information Overview of SnappyData Distributed Transactions Best Practices for SnappyData Distributed Transactions","title":"How to use Transactions Isolation Levels"},{"location":"howto/use_transactions_isolation_levels/#how-to-use-transactions-isolation-levels","text":"SnappyData supports transaction isolation levels when using JDBC or ODBC connections. The default transaction level in SnappyData is set to NONE. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads. SnappyData also supports READ_COMMITTED and REPEATABLE_READ transaction isolation levels. A detailed description of the transaction's semantics in SnappyData can be found in the Overview of SnappyData Distributed Transactions section. Note If you set the isolation level to READ_COMMITTED or REPEATABLE_READ , queries on column table report an error if autocommit is set to off ( false ). Queries on column tables are supported when isolation level is set to NONE . SnappyData internally sets autocommit to true in this case. Queries on row tables are supported when autocommit is set to false and isolation level is set to other READ_COMMITTED or REPEATABLE_READ .","title":"How to use Transactions Isolation Levels"},{"location":"howto/use_transactions_isolation_levels/#examples","text":"Note Before you try these examples, ensure that you have started the SnappyData cluster . The following examples provide JDBC example code snippets that explain how to use transactions isolation levels.","title":"Examples"},{"location":"howto/use_transactions_isolation_levels/#example-1","text":"For row tables, autocommit can be set to false or true import java.sql.{Connection, Statement} ... ... val url: String = \"jdbc:snappydata://1527/\" val conn1 = DriverManager.getConnection(url) val stmt1 = conn1.createStatement() // create a row table stmt1.execute(\"CREATE TABLE APP.PARTSUPP ( \" + \"PS_PARTKEY INTEGER NOT NULL PRIMARY KEY,\" + \"PS_SUPPKEY INTEGER NOT NULL,\" + \"PS_AVAILQTY INTEGER NOT NULL,\" + \"PS_SUPPLYCOST DECIMAL(15,2) NOT NULL)\" + \"USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\") // set the tx isolation level conn1.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED) // set autocommit to false conn1.setAutoCommit(false) val preparedStmt1 = conn1.prepareStatement(\"INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\") for (x <- 1 to 10) { preparedStmt1.setInt(1, x*100) preparedStmt1.setInt(2, x) preparedStmt1.setInt(3, x*1000) preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2)) preparedStmt1.executeUpdate() } // commit the transaction conn1.commit() val rs1 = stmt1.executeQuery(\"SELECT * FROM APP.PARTSUPP\") while (rs1.next()) { println(rs1.getInt(1) + \",\" + rs1.getInt(2) + \",\" + rs1.getInt(3)) } rs1.close() stmt1.close() conn1.close()","title":"Example 1"},{"location":"howto/use_transactions_isolation_levels/#example-2","text":"For column tables, autocommit must be set to true , otherwise, an error is reported when the query is executed. val conn2 = DriverManager.getConnection(url) val stmt2 = conn2.createStatement() // create a column table stmt2.execute(\"CREATE TABLE CUSTOMER ( \" + \"C_CUSTKEY INTEGER,\" + \"C_NAME VARCHAR(25),\" + \"C_ADDRESS VARCHAR(40),\" + \"C_NATIONKEY INTEGER,\" + \"C_PHONE VARCHAR(15),\" + \"C_ACCTBAL DECIMAL(15,2),\" + \"C_MKTSEGMENT VARCHAR(10),\" + \"C_COMMENT VARCHAR(117))\" + \"USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\") // set the tx isolation level conn2.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED) // set autocommit to true otherwise opeartions on column table will error out conn2.setAutoCommit(true) stmt2.execute(\"INSERT INTO CUSTOMER VALUES(20000, 'Customer20000', \" + \"'Chicago, IL', 1, '555-101-782', 3500, 'MKTSEGMENT', '')\") stmt2.execute(\"INSERT INTO CUSTOMER VALUES(30000, 'Customer30000', \" + \"'San Hose, CA', 1, '555-201-562', 4500, 'MKTSEGMENT', '')\") val rs2 = stmt2.executeQuery(\"SELECT * FROM APP.CUSTOMER\") while (rs2.next()) { println(rs2.getInt(1) + \",\" + rs2.getString(2)) } rs2.close()","title":"Example 2"},{"location":"howto/use_transactions_isolation_levels/#unsupported-operations-when-autocommit-is-set-to-false-for-column-tables","text":"// if autocommit is set to false, queries throw an error if column tables are involved conn2.setAutoCommit(false) // invalid query stmt2.execute(\"SELECT * FROM APP.CUSTOMER\") // the above statement throws an error as given below EXCEPTION: java.sql.SQLException: (SQLState=XJ218 Severity=20000) (Server=localhost/127.0.0.1[25299] Thread=pool-14-thread-3) Operations on column tables are not supported when query routing is disabled or autocommit is false More information Overview of SnappyData Distributed Transactions Best Practices for SnappyData Distributed Transactions","title":"Unsupported operations when autocommit is set to false for column tables"},{"location":"howto/using_snappydata_for_any_spark_dist/","text":"How to use SnappyData for any Spark Distribution \u00b6 The snappydat-jdbc Spark package adds extensions to Spark\u2019s inbuilt JDBC data source provider to work better with SnappyData. This allows SnappyData to be treated as a regular JDBC data source with all versions of Spark which are greater or equal to 2.1, while also providing speed to direct SnappyData embedded cluster for many types of queries. Following is a sample of Spark JDBC extension setup and usage: Include the snappydata-jdbc package in the Spark job with spark-submit or spark-shell: $SPARK_HOME/bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar Set the session properties. The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in spark-defaults.conf along with all the other Spark properties. Following is a sample code of configuring the properties in SparkConf : $SPARK_HOME/bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.user=<user> --conf spark.snappydata.password=<password> Overloads of the above methods accepting user+password and host+port is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the DataFrameReader.jdbc method. Import the required implicits in the job/shell code as follows: import io.snappydata.sql.implicits._ After the required session properties are set (connection and user/password etc.), you can run the queries/DMLs without any other configuration as shown here: // execute DDL spark.snappyExecute(\"create table testTable1 (id long, data string) using column\") // DML spark.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\") // bulk insert from external table in embedded mode spark.snappyExecute(\"insert into testTable1 select * from externalTable1\") // query val dataSet = spark.snappyQuery(\u201cselect count(*) from testTable1\u201d) For Java When using Java, the wrapper must be created explicitly as shown in the following sample: import org.apache.spark.sql.*; JdbcExecute exec = new JdbcExecute(spark); exec.snappyExecute(\u201ccreate table testTable1 (id long, data string) using column\u201d); exec.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\"); DataFrame df = exec.snappyQuery(...); ...","title":"How to use SnappyData for any Spark Distribution"},{"location":"howto/using_snappydata_for_any_spark_dist/#how-to-use-snappydata-for-any-spark-distribution","text":"The snappydat-jdbc Spark package adds extensions to Spark\u2019s inbuilt JDBC data source provider to work better with SnappyData. This allows SnappyData to be treated as a regular JDBC data source with all versions of Spark which are greater or equal to 2.1, while also providing speed to direct SnappyData embedded cluster for many types of queries. Following is a sample of Spark JDBC extension setup and usage: Include the snappydata-jdbc package in the Spark job with spark-submit or spark-shell: $SPARK_HOME/bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar Set the session properties. The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in spark-defaults.conf along with all the other Spark properties. Following is a sample code of configuring the properties in SparkConf : $SPARK_HOME/bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.user=<user> --conf spark.snappydata.password=<password> Overloads of the above methods accepting user+password and host+port is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the DataFrameReader.jdbc method. Import the required implicits in the job/shell code as follows: import io.snappydata.sql.implicits._ After the required session properties are set (connection and user/password etc.), you can run the queries/DMLs without any other configuration as shown here: // execute DDL spark.snappyExecute(\"create table testTable1 (id long, data string) using column\") // DML spark.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\") // bulk insert from external table in embedded mode spark.snappyExecute(\"insert into testTable1 select * from externalTable1\") // query val dataSet = spark.snappyQuery(\u201cselect count(*) from testTable1\u201d) For Java When using Java, the wrapper must be created explicitly as shown in the following sample: import org.apache.spark.sql.*; JdbcExecute exec = new JdbcExecute(spark); exec.snappyExecute(\u201ccreate table testTable1 (id long, data string) using column\u201d); exec.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\"); DataFrame df = exec.snappyQuery(...); ...","title":"How to use SnappyData for any Spark Distribution"},{"location":"install/","text":"Provisioning SnappyData \u00b6 The SnappyData Community Edition is Apache 2.0 licensed. It is a free, open-source version of the product that can be downloaded by anyone. The erstwhile Enterprise Edition of the product, which is sold by TIBCO Software under the name TIBCO ComputeDB\u2122 , includes everything that is offered in the Community Edition along with additional capabilities that are closed source and only available as part of a licensed subscription. Starting with 1.3.0 release, all components that were previously closed source are now OSS (except for the GemFire connector), and there is only the Community Edition that is released. For more information on the capabilities of the Community Edition and differences from the previous Enterprise Edition, see Community Edition (Open Source) . Download SnappyData Community Edition \u00b6 Download the SnappyData 1.3.1 Community Edition (Open Source) from the release page, which lists the latest and previous releases of SnappyData. The packages are available in compressed files (.tar format). SnappyData 1.3.1 Release download link SnappyData Provisioning Options \u00b6 Prerequisites \u00b6 Before you start the installation, make sure that Java SE Development Kit 8 is installed, and the JAVA_HOME environment variable is set on each computer. The following options are available for provisioning SnappyData: On-Premise Amazon Web Services (AWS) Kubernetes Docker Building from Source Configuring the Limit for Open Files and Threads/Processes \u00b6 On a Linux system, you can set the limit of open files and thread processes in the /etc/security/limits.conf file. A minimum of 8192 is recommended for open file descriptors limit and >128K is recommended for the number of active threads. A typical configuration used for SnappyData servers and leads can appear as follows: snappydata hard nofile 32768 snappydata soft nofile 32768 snappydata hard nproc unlimited snappydata soft nproc 524288 snappydata hard sigpending unlimited snappydata soft sigpending 524288 * snappydata is the user running SnappyData. Recent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) require the NOFILE limit to be increased in systemd configuration too. Edit /etc/systemd/system.conf as root, search for #DefaultLimitNOFILE under the [Manager] section. Uncomment and change it to DefaultLimitNOFILE=32768 . Reboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with \"ulimit -a -S\" (soft limits) and \"ulimit -a -H\" (hard limits).","title":"Provisioning SnappyData"},{"location":"install/#provisioning-snappydata","text":"The SnappyData Community Edition is Apache 2.0 licensed. It is a free, open-source version of the product that can be downloaded by anyone. The erstwhile Enterprise Edition of the product, which is sold by TIBCO Software under the name TIBCO ComputeDB\u2122 , includes everything that is offered in the Community Edition along with additional capabilities that are closed source and only available as part of a licensed subscription. Starting with 1.3.0 release, all components that were previously closed source are now OSS (except for the GemFire connector), and there is only the Community Edition that is released. For more information on the capabilities of the Community Edition and differences from the previous Enterprise Edition, see Community Edition (Open Source) .","title":"Provisioning SnappyData"},{"location":"install/#download-snappydata-community-edition","text":"Download the SnappyData 1.3.1 Community Edition (Open Source) from the release page, which lists the latest and previous releases of SnappyData. The packages are available in compressed files (.tar format). SnappyData 1.3.1 Release download link","title":"Download SnappyData Community Edition"},{"location":"install/#snappydata-provisioning-options","text":"","title":"SnappyData Provisioning Options"},{"location":"install/#prerequisites","text":"Before you start the installation, make sure that Java SE Development Kit 8 is installed, and the JAVA_HOME environment variable is set on each computer. The following options are available for provisioning SnappyData: On-Premise Amazon Web Services (AWS) Kubernetes Docker Building from Source","title":"Prerequisites"},{"location":"install/#configuring-the-limit-for-open-files-and-threadsprocesses","text":"On a Linux system, you can set the limit of open files and thread processes in the /etc/security/limits.conf file. A minimum of 8192 is recommended for open file descriptors limit and >128K is recommended for the number of active threads. A typical configuration used for SnappyData servers and leads can appear as follows: snappydata hard nofile 32768 snappydata soft nofile 32768 snappydata hard nproc unlimited snappydata soft nproc 524288 snappydata hard sigpending unlimited snappydata soft sigpending 524288 * snappydata is the user running SnappyData. Recent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) require the NOFILE limit to be increased in systemd configuration too. Edit /etc/systemd/system.conf as root, search for #DefaultLimitNOFILE under the [Manager] section. Uncomment and change it to DefaultLimitNOFILE=32768 . Reboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with \"ulimit -a -S\" (soft limits) and \"ulimit -a -H\" (hard limits).","title":"Configuring the Limit for Open Files and Threads/Processes"},{"location":"install/building_from_source/","text":"Building from Source \u00b6 Note Building SnappyData requires JDK 8 installation ( Oracle Java SE ). Build all Components of SnappyData \u00b6 Master > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew product The product is in build-artifacts/scala-2.11/snappy To build product artifacts in all supported formats (tarball, zip, rpm, deb): > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew cleanAll distProduct The artifacts are in build-artifacts/scala-2.11/distributions You can also add the flags -PenablePublish -PR.enable to get them in the form as in an official SnappyData distributions but that also requires an installation of R as noted below. To build all product artifacts that are in the official SnappyData distributions: > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew cleanAll product copyShadowJars distTar -PenablePublish -PR.enable The artifacts are in build-artifacts/scala-2.11/distributions Building SparkR with the -PR.enable flag requires R 3.x or 4.x to be installed locally. At least the following R packages along with their dependencies also need to be installed: knitr , markdown , rmarkdown , e1071 , testthat The R build also needs a base installation of texlive or an equivalent TeX distribution for pdflatex and related tools to build the documentation. On distributions like Ubuntu/Debian/Arch you can install them from repositories (for Arch some packages need AUR). For example on recent Ubuntu/Debian: sudo apt install texlive r-cran-knitr r-cran-markdown r-cran-rmarkdown r-cran-e1071 r-cran-testthat Official builds are published to maven using the publishMaven task. Docs \u00b6 Documentation is created using the publishDocs target. You need an installation of mkdocs with its material , minify and 'mike' plugins to build and publish the docs. The standard way to install these is using python package installer pip . This is present in the repositories of all distributions e.g. sudo apt install python3-pip on Ubuntu/Debian based distributions and sudo yum install python3-pip on CentOS/RHEL/Fedora based ones. Other distributions will have a similar way to install the package (e.g. sudo pacman -S python-pip on Arch-like distributions). Once you have pip installed, then you can install the required packages using: pip3 install mkdocs mkdocs-material mkdocs-minify-plugin mike Note On some newer distributions, the executable is pip instead of pip3 though latter is usually still available as a symlink. Then you can run the target that will build the documentation as below: ./gradlew publishDocs -PenablePublish This will publish the docs for the current release version to the public site assuming you have the requisite write permissions to the repository. If there are docs for multiple releases, then they will appear in the drop-down at the top. If you want to see the docs locally, then uncomment the last mike serve line in publish-site.sh and the site will be hosted locally in localhost:8000 . Note that locally hosted site may have some subtle differences from the one published on the site, so its best to check the final published output on the site. Repository Layout \u00b6 core - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between spark and store (GemFireXD). For example, SnappyContext, row and column store, streaming additions etc. cluster - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc. This component depends on core and store . The code in the cluster depends on the core but not the other way round. spark - Apache Spark code with SnappyData enhancements. store - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch. spark-jobserver - Fork of spark-jobserver project with some additions to integrate with SnappyData. aqp - Approximate Query Processing (AQP) module of SnappyData. snappy-connectors - Connector for Apache Geode and a Change-Data-Capture (CDC) connector. The spark , store , spark-jobserver , aqp , and snappy-connectors directories are required to be clones of the respective SnappyData repositories and are integrated into the top-level SnappyData project as git submodules. When working with submodules, updating the repositories follows the normal git submodules . One can add some aliases in gitconfig to aid pull/push as follows: [alias] spull = !git pull && git submodule sync --recursive && git submodule update --init --recursive spush = push --recurse-submodules=on-demand The above aliases can serve as useful shortcuts to pull and push all projects from top-level snappydata repository. Building \u00b6 Gradle is the build tool used for all the SnappyData projects. Changes to Apache Spark and spark-jobserver forks include the addition of Gradle build scripts to allow building them independently as well as a sub-project of SnappyData. The only requirement for the build is a JDK 8 installation. The Gradle wrapper script downloads all the other build dependencies as required. If you do not want to deal with sub-modules and only work on a SnappyData project, you can clone only the SnappyData repository (without the --recursive option) and the build pulls those SnappyData project jar dependencies from Maven central. If working on all the separate projects integrated inside the top-level SnappyData clone, the Gradle build recognizes the same and build those projects too and includes the same in the top-level product distribution jar. The spark and store submodules can also be built and published independently. Useful build and test targets: ./gradlew assemble - build all the sources ./gradlew testClasses - build all the tests ./gradlew product - build and place the product distribution (in build-artifacts/scala_2.11/snappy) ./gradlew distTar - create a tar.gz archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew distZip - create a zip archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew buildAll - build all sources, tests, product, packages (all targets above) ./gradlew checkAll - run testsuites of snappydata components ./gradlew cleanAll - clean all build and test output ./gradlew runQuickstart - run the quickstart suite (the \"Getting Started\" section of docs) ./gradlew precheckin - cleanAll, buildAll, scalaStyle, build docs, and run full snappydata testsuite including quickstart ./gradlew precheckin -Pstore - cleanAll, buildAll, scalaStyle, build docs, run full snappydata testsuite including quickstart and also full SnappyData store testsuite ./gradlew buildDtests - To build the Distributed tests The default build directory is build-artifacts/scala-2.11 for projects. An exception is store project, where the default build directory is build-artifacts/ ; where; os is linux on Linux systems, osx on Mac, windows on Windows. The usual Gradle test run targets ( test , check ) work as expected for JUnit tests. Separate targets have been provided for running Scala tests ( scalaTest ) while the check target runs both the JUnit and ScalaTests. One can run a single Scala test suite class with singleSuite option while running a single test within some suite works with the --tests option: > ./gradlew snappy-core:scalaTest -PsingleSuite=**.ColumnTableTest # run all tests in the class > ./gradlew snappy-core:scalaTest \\ > --tests \"Test the creation/dropping of table using SQL\" # run a single test (use full name) Running individual tests within some suite works using the --tests argument. All ScalaTest build targets can be found by running the following command (case sensitive): ./gradlew tasks --all | grep scalaTest Setting up IntelliJ IDEA with Gradle \u00b6 IntelliJ IDEA is the IDE commonly used by developers at SnappyData. Users who prefer to use Eclipse can try the Scala-IDE and Gradle support, however, it is recommended to use IntelliJ IDEA. Steps required for setting up SnappyData with all its components in IDEA are listed below. To import into IntelliJ IDEA: Upgrade IntelliJ IDEA to at least version 2016.x, preferably 2018.x or more, including the latest Scala plug-in. Older versions have trouble dealing with scala code particularly some of the code in Spark. Newer versions have trouble running tests with gradle import, since they do not honor the build output directory as set in gradle. Ensure JDK 8 is installed and IDEA can find it (either in PATH or via JAVA_HOME). Increase the Xmx to 2g or more (4g, if possible) in the IDEA global vmoptions (in product bin directory, files named idea64.vmoptions for 64-bit and idea.vmoptions for 32-bit). If using Java 8 release 144 or later, also add -Djdk.util.zip.ensureTrailingSlash=false to the global vmoptions file to fix an IDEA issue . Increase the available JVM heap size for IDEA. Open bin/idea64.vmoptions (assuming 64-bit JVM) and increase -Xmx option to be something like -Xmx2g for comfortable use. Select Import Project , and then select the SnappyData directory. Use external Gradle import. Click Next in the following screen. Clear the Create separate module per source set option, while other options can continue with the default. Click Next in the following screens. Note Ignore the \"Gradle location is unknown warning\" . Ensure that the JDK 8 installation has been selected. Ignore and dismiss the \"Unindexed remote Maven repositories found\" warning message if seen. When import is completed, Go to File> Settings> Editor> Code Style> Scala . Set the scheme as Project . In the same window, select Java code style and set the scheme as Project . Click OK to apply and close the window. Copy codeStyleSettings.xml located in the SnappyData top-level directory, to the .idea directory created by IDEA. Verify that the settings are now applied in File> Settings> Editor> Code Style> Java which should display indent as 2 and continuation indent as 4 (same as Scala). If the Gradle tab is not visible immediately, then select it from option available at the bottom-left of IDE. Click on that window list icon for the tabs to be displayed permanently. Generate Apache Avro and SnappyData required sources by expanding: snappydata_2.11> Tasks> other . Right-click on generateSources and run it. The Run option may not be available if indexing is still in progress, wait for indexing to complete, and then try again. The first run may take some time to complete, as it downloads the jar files and other required files. This step has to be done the first time, or if ./gradlew clean has been run, or if you have made changes to javacc/avro/messages.xml source files. Go to File> Settings> Build, Execution, Deployment> Build tools> Gradle . Enter -DideaBuild in the Gradle VM Options textbox. If you get unexpected Database not found or NullPointerException errors in SnappyData-store/GemFireXD layer, run the generateSources target (Gradle tab) again. If you get NullPointerException error when reading the spark-version-info.properties file, right-click and run the copyResourcesAll target from snappydata_2.11> Tasks> other (Gradle tab) to copy the required resources. Increase the compiler heap sizes or else the build can take a long time to complete, especially with integrated spark and store . In File> Settings> Build, Execution, Deployment> Compiler option increase the Build process heap size to 1536 or 2048. Similarly, in Languages & Frameworks> Scala Compiler Server option, increase the JVM maximum heap size to 1536 or 2048. Test the full build. For JUnit tests configuration also append /build-artifacts to the working directory. That is, open Run> Edit Configurations , expand Defaults and select JUnit , the working directory should be \\$MODULE_DIR\\$/build-artifacts . Likewise, append build-artifacts to the working directory for ScalaTest. Without this, all intermediate log and other files pollute the source tree and will have to be cleaned manually. If you see the following error while building the project, open module settings, select the module snappy-cluster_2.11 , go to its Dependencies tab and ensure that snappy-spark-unsafe_2.11 comes before spark-unsafe or just find snappy-spark-unsafe_2.11 and move it to the top. Error:(236, 18) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String if (source.getByte(i) == first && matchAt(source, target, i)) return true Error:(233, 24) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String val first = target.getByte(0) Even with the above, running unit tests in IDEA may result in more runtime errors due to unexpected slf4j versions. A more comprehensive way to correct, both the compilation and unit test problems in IDEA, is to update the snappy-cluster or for whichever module unit tests are to be run and have the TEST imports at the end. The easiest way to do that is to close IDEA, open the module IML file ( .idea/modules/cluster/snappy-cluster_2.11.iml in this case) in an editor. Search for scope=\"TEST\" and move all those lines to the bottom just before </component> close tag. The ordering of imports is no longer a problem in latest IDEA 2020.x and newer. Running a ScalaTest/JUnit \u00b6 Running Scala/JUnit tests from IntelliJ IDEA is straightforward. In newer IDEA releases, ensure that Intellij IDEA is used to run the tests instead of gradle, while gradle is used for the build. To do this, go to File->Settings->Build, Execution, Deployment->Build Tools->Gradle, select \"Run tests using\" to be \"Intellij IDEA\" rather than with gradle for the \"snappydata\" project. Also ensure that \"Build and run using\" is gradle rather than \"Intellij IDEA\". When selecting a run configuration for JUnit/ScalaTest, avoid selecting the Gradle one (green round icon) otherwise, an external Gradle process is launched that can start building the project again is not cleanly integrated with IDEA. Use the normal JUnit (red+green arrows icon) or ScalaTest (JUnit like with red overlay). For JUnit tests, ensure that the working directory is the top-level \\$MODULE_DIR\\$/build-artifacts as mentioned earlier. Otherwise, many SnappyData-store tests fail to find the resource files required in tests. They also pollute the files, so when launched, this allows those to go into build-artifacts that are easier to clean. For that reason, it is preferable to do the same for ScalaTests. Some of the tests use data files from the tests-common directory. For such tests, run the Gradle task snappydata_2.11> Tasks> other> copyResourcesAll to copy the resources in build area where IDEA runs can find it.","title":"Building SnappyData from Source files"},{"location":"install/building_from_source/#building-from-source","text":"Note Building SnappyData requires JDK 8 installation ( Oracle Java SE ).","title":"Building from Source"},{"location":"install/building_from_source/#build-all-components-of-snappydata","text":"Master > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew product The product is in build-artifacts/scala-2.11/snappy To build product artifacts in all supported formats (tarball, zip, rpm, deb): > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew cleanAll distProduct The artifacts are in build-artifacts/scala-2.11/distributions You can also add the flags -PenablePublish -PR.enable to get them in the form as in an official SnappyData distributions but that also requires an installation of R as noted below. To build all product artifacts that are in the official SnappyData distributions: > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew cleanAll product copyShadowJars distTar -PenablePublish -PR.enable The artifacts are in build-artifacts/scala-2.11/distributions Building SparkR with the -PR.enable flag requires R 3.x or 4.x to be installed locally. At least the following R packages along with their dependencies also need to be installed: knitr , markdown , rmarkdown , e1071 , testthat The R build also needs a base installation of texlive or an equivalent TeX distribution for pdflatex and related tools to build the documentation. On distributions like Ubuntu/Debian/Arch you can install them from repositories (for Arch some packages need AUR). For example on recent Ubuntu/Debian: sudo apt install texlive r-cran-knitr r-cran-markdown r-cran-rmarkdown r-cran-e1071 r-cran-testthat Official builds are published to maven using the publishMaven task.","title":"Build all Components of SnappyData"},{"location":"install/building_from_source/#docs","text":"Documentation is created using the publishDocs target. You need an installation of mkdocs with its material , minify and 'mike' plugins to build and publish the docs. The standard way to install these is using python package installer pip . This is present in the repositories of all distributions e.g. sudo apt install python3-pip on Ubuntu/Debian based distributions and sudo yum install python3-pip on CentOS/RHEL/Fedora based ones. Other distributions will have a similar way to install the package (e.g. sudo pacman -S python-pip on Arch-like distributions). Once you have pip installed, then you can install the required packages using: pip3 install mkdocs mkdocs-material mkdocs-minify-plugin mike Note On some newer distributions, the executable is pip instead of pip3 though latter is usually still available as a symlink. Then you can run the target that will build the documentation as below: ./gradlew publishDocs -PenablePublish This will publish the docs for the current release version to the public site assuming you have the requisite write permissions to the repository. If there are docs for multiple releases, then they will appear in the drop-down at the top. If you want to see the docs locally, then uncomment the last mike serve line in publish-site.sh and the site will be hosted locally in localhost:8000 . Note that locally hosted site may have some subtle differences from the one published on the site, so its best to check the final published output on the site.","title":"Docs"},{"location":"install/building_from_source/#repository-layout","text":"core - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between spark and store (GemFireXD). For example, SnappyContext, row and column store, streaming additions etc. cluster - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc. This component depends on core and store . The code in the cluster depends on the core but not the other way round. spark - Apache Spark code with SnappyData enhancements. store - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch. spark-jobserver - Fork of spark-jobserver project with some additions to integrate with SnappyData. aqp - Approximate Query Processing (AQP) module of SnappyData. snappy-connectors - Connector for Apache Geode and a Change-Data-Capture (CDC) connector. The spark , store , spark-jobserver , aqp , and snappy-connectors directories are required to be clones of the respective SnappyData repositories and are integrated into the top-level SnappyData project as git submodules. When working with submodules, updating the repositories follows the normal git submodules . One can add some aliases in gitconfig to aid pull/push as follows: [alias] spull = !git pull && git submodule sync --recursive && git submodule update --init --recursive spush = push --recurse-submodules=on-demand The above aliases can serve as useful shortcuts to pull and push all projects from top-level snappydata repository.","title":"Repository Layout"},{"location":"install/building_from_source/#building","text":"Gradle is the build tool used for all the SnappyData projects. Changes to Apache Spark and spark-jobserver forks include the addition of Gradle build scripts to allow building them independently as well as a sub-project of SnappyData. The only requirement for the build is a JDK 8 installation. The Gradle wrapper script downloads all the other build dependencies as required. If you do not want to deal with sub-modules and only work on a SnappyData project, you can clone only the SnappyData repository (without the --recursive option) and the build pulls those SnappyData project jar dependencies from Maven central. If working on all the separate projects integrated inside the top-level SnappyData clone, the Gradle build recognizes the same and build those projects too and includes the same in the top-level product distribution jar. The spark and store submodules can also be built and published independently. Useful build and test targets: ./gradlew assemble - build all the sources ./gradlew testClasses - build all the tests ./gradlew product - build and place the product distribution (in build-artifacts/scala_2.11/snappy) ./gradlew distTar - create a tar.gz archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew distZip - create a zip archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew buildAll - build all sources, tests, product, packages (all targets above) ./gradlew checkAll - run testsuites of snappydata components ./gradlew cleanAll - clean all build and test output ./gradlew runQuickstart - run the quickstart suite (the \"Getting Started\" section of docs) ./gradlew precheckin - cleanAll, buildAll, scalaStyle, build docs, and run full snappydata testsuite including quickstart ./gradlew precheckin -Pstore - cleanAll, buildAll, scalaStyle, build docs, run full snappydata testsuite including quickstart and also full SnappyData store testsuite ./gradlew buildDtests - To build the Distributed tests The default build directory is build-artifacts/scala-2.11 for projects. An exception is store project, where the default build directory is build-artifacts/ ; where; os is linux on Linux systems, osx on Mac, windows on Windows. The usual Gradle test run targets ( test , check ) work as expected for JUnit tests. Separate targets have been provided for running Scala tests ( scalaTest ) while the check target runs both the JUnit and ScalaTests. One can run a single Scala test suite class with singleSuite option while running a single test within some suite works with the --tests option: > ./gradlew snappy-core:scalaTest -PsingleSuite=**.ColumnTableTest # run all tests in the class > ./gradlew snappy-core:scalaTest \\ > --tests \"Test the creation/dropping of table using SQL\" # run a single test (use full name) Running individual tests within some suite works using the --tests argument. All ScalaTest build targets can be found by running the following command (case sensitive): ./gradlew tasks --all | grep scalaTest","title":"Building"},{"location":"install/building_from_source/#setting-up-intellij-idea-with-gradle","text":"IntelliJ IDEA is the IDE commonly used by developers at SnappyData. Users who prefer to use Eclipse can try the Scala-IDE and Gradle support, however, it is recommended to use IntelliJ IDEA. Steps required for setting up SnappyData with all its components in IDEA are listed below. To import into IntelliJ IDEA: Upgrade IntelliJ IDEA to at least version 2016.x, preferably 2018.x or more, including the latest Scala plug-in. Older versions have trouble dealing with scala code particularly some of the code in Spark. Newer versions have trouble running tests with gradle import, since they do not honor the build output directory as set in gradle. Ensure JDK 8 is installed and IDEA can find it (either in PATH or via JAVA_HOME). Increase the Xmx to 2g or more (4g, if possible) in the IDEA global vmoptions (in product bin directory, files named idea64.vmoptions for 64-bit and idea.vmoptions for 32-bit). If using Java 8 release 144 or later, also add -Djdk.util.zip.ensureTrailingSlash=false to the global vmoptions file to fix an IDEA issue . Increase the available JVM heap size for IDEA. Open bin/idea64.vmoptions (assuming 64-bit JVM) and increase -Xmx option to be something like -Xmx2g for comfortable use. Select Import Project , and then select the SnappyData directory. Use external Gradle import. Click Next in the following screen. Clear the Create separate module per source set option, while other options can continue with the default. Click Next in the following screens. Note Ignore the \"Gradle location is unknown warning\" . Ensure that the JDK 8 installation has been selected. Ignore and dismiss the \"Unindexed remote Maven repositories found\" warning message if seen. When import is completed, Go to File> Settings> Editor> Code Style> Scala . Set the scheme as Project . In the same window, select Java code style and set the scheme as Project . Click OK to apply and close the window. Copy codeStyleSettings.xml located in the SnappyData top-level directory, to the .idea directory created by IDEA. Verify that the settings are now applied in File> Settings> Editor> Code Style> Java which should display indent as 2 and continuation indent as 4 (same as Scala). If the Gradle tab is not visible immediately, then select it from option available at the bottom-left of IDE. Click on that window list icon for the tabs to be displayed permanently. Generate Apache Avro and SnappyData required sources by expanding: snappydata_2.11> Tasks> other . Right-click on generateSources and run it. The Run option may not be available if indexing is still in progress, wait for indexing to complete, and then try again. The first run may take some time to complete, as it downloads the jar files and other required files. This step has to be done the first time, or if ./gradlew clean has been run, or if you have made changes to javacc/avro/messages.xml source files. Go to File> Settings> Build, Execution, Deployment> Build tools> Gradle . Enter -DideaBuild in the Gradle VM Options textbox. If you get unexpected Database not found or NullPointerException errors in SnappyData-store/GemFireXD layer, run the generateSources target (Gradle tab) again. If you get NullPointerException error when reading the spark-version-info.properties file, right-click and run the copyResourcesAll target from snappydata_2.11> Tasks> other (Gradle tab) to copy the required resources. Increase the compiler heap sizes or else the build can take a long time to complete, especially with integrated spark and store . In File> Settings> Build, Execution, Deployment> Compiler option increase the Build process heap size to 1536 or 2048. Similarly, in Languages & Frameworks> Scala Compiler Server option, increase the JVM maximum heap size to 1536 or 2048. Test the full build. For JUnit tests configuration also append /build-artifacts to the working directory. That is, open Run> Edit Configurations , expand Defaults and select JUnit , the working directory should be \\$MODULE_DIR\\$/build-artifacts . Likewise, append build-artifacts to the working directory for ScalaTest. Without this, all intermediate log and other files pollute the source tree and will have to be cleaned manually. If you see the following error while building the project, open module settings, select the module snappy-cluster_2.11 , go to its Dependencies tab and ensure that snappy-spark-unsafe_2.11 comes before spark-unsafe or just find snappy-spark-unsafe_2.11 and move it to the top. Error:(236, 18) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String if (source.getByte(i) == first && matchAt(source, target, i)) return true Error:(233, 24) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String val first = target.getByte(0) Even with the above, running unit tests in IDEA may result in more runtime errors due to unexpected slf4j versions. A more comprehensive way to correct, both the compilation and unit test problems in IDEA, is to update the snappy-cluster or for whichever module unit tests are to be run and have the TEST imports at the end. The easiest way to do that is to close IDEA, open the module IML file ( .idea/modules/cluster/snappy-cluster_2.11.iml in this case) in an editor. Search for scope=\"TEST\" and move all those lines to the bottom just before </component> close tag. The ordering of imports is no longer a problem in latest IDEA 2020.x and newer.","title":"Setting up IntelliJ IDEA with Gradle"},{"location":"install/building_from_source/#running-a-scalatestjunit","text":"Running Scala/JUnit tests from IntelliJ IDEA is straightforward. In newer IDEA releases, ensure that Intellij IDEA is used to run the tests instead of gradle, while gradle is used for the build. To do this, go to File->Settings->Build, Execution, Deployment->Build Tools->Gradle, select \"Run tests using\" to be \"Intellij IDEA\" rather than with gradle for the \"snappydata\" project. Also ensure that \"Build and run using\" is gradle rather than \"Intellij IDEA\". When selecting a run configuration for JUnit/ScalaTest, avoid selecting the Gradle one (green round icon) otherwise, an external Gradle process is launched that can start building the project again is not cleanly integrated with IDEA. Use the normal JUnit (red+green arrows icon) or ScalaTest (JUnit like with red overlay). For JUnit tests, ensure that the working directory is the top-level \\$MODULE_DIR\\$/build-artifacts as mentioned earlier. Otherwise, many SnappyData-store tests fail to find the resource files required in tests. They also pollute the files, so when launched, this allows those to go into build-artifacts that are easier to clean. For that reason, it is preferable to do the same for ScalaTests. Some of the tests use data files from the tests-common directory. For such tests, run the Gradle task snappydata_2.11> Tasks> other> copyResourcesAll to copy the resources in build area where IDEA runs can find it.","title":"Running a ScalaTest/JUnit"},{"location":"install/install_on_premise/","text":"Install On-Premise \u00b6 SnappyData runs on UNIX-like systems (for example, Linux, Mac OS). With on-premises installation, SnappyData is installed and operated from your in-house computing infrastructure. For quick start instructions on Installing SnappyData on-premise, refer Getting Started with SnappyData On-premise . After installing SnappyData, follow the instructions here , to use the product from Apache Zeppelin. Single-Host Installation \u00b6 This is the simplest form of deployment and can be used for testing and POCs. Open the command prompt, go the location of the downloaded SnappyData file, and run the following command to extract the archive file. $ tar -xzf snappydata-<version-number>bin.tar.gz $ cd snappydata-<version-number>-bin/ Start a basic cluster with one data node, one lead, and one locator: ./sbin/snappy-start-all.sh For custom configuration and to start more nodes, refer configuring the SnappyData cluster . Multi-Host Installation \u00b6 For real-life use cases, you require multiple machines on which SnappyData must be deployed. You can start one or more SnappyData node on a single machine based on your machine size. Where there are multiple machines involved, you can deploy SnappyData on: Machines With Shared Path Machines Without a Shared Path Machines Without Passwordless SSH Machines With a Shared Path \u00b6 If all the machines in your cluster can share a path over an NFS or similar protocol, then use the following instructions: Prerequisites Ensure that the /etc/hosts correctly configures the host and IP address of each SnappyData member machine. Ensure that SSH is supported and you have configured all the machines to be accessed by passwordless SSH . If SSH is not supported then follow the instructions in the Machines Without Passwordless SSH section. To set up the cluster for machines with a shared path: Copy the downloaded binaries to the shared folder. Extract the downloaded archive file and go to SnappyData home directory. $ tar -xzf snappydata- -bin.tar.gz $ cd snappydata- -bin/ Configure the cluster as described in Configuring the Cluster . After configuring each of the members in the cluster, run the snappy-start-all.sh script: ./sbin/snappy-start-all.sh This creates a default folder named work and stores all SnappyData member's artifacts separately. The folder is identified by the name of the node. Tip For optimum performance, configure the -dir to a local directory and not to a network directory. When -dir property is configured for each member in the cluster, the artifacts of the respective members get created in the -dir folder. Machines Without a Shared Path \u00b6 In case all the machines in your cluster do not share a path over an NFS or similar protocol, then use the following instructions: Prerequisites Ensure that /etc/hosts correctly configures the host and IP Address of each SnappyData member machine. Ensure that SSH is supported and you have configured all the machines to be accessed by passwordless SSH . If SSH is not supported then follow the instructions in the Machines without passwordless SSH section. To set up the cluster for machines without a shared path: Copy and extract the downloaded binaries into each machine. Ensure to maintain the same directory structure on all the machines. For example, if you copy the binaries in /opt/snappydata/ on the first machine, then you must ensure to copy the binaries to /opt/snappydata/ on rest of the machines. Configure the cluster as described in Configuring the Cluster . Maintain one node as the controller node, where you can configure your cluster. Usually this is done in the lead node. On that machine, you can edit files such as servers, locators, and leads which are in the $SNAPPY_HOME/conf/ directory . Create a working directory on every machine, for each of the SnappyData member that you want to run. The member's working directory provides a default location for the logs, persistence, and status files of that member. For example, if you want to run both a locator and server member on the local machine, create separate directories for each member. Run the snappy-start-all.sh script: ./sbin/snappy-start-all.sh Machines Without Passwordless SSH \u00b6 In case the machines in your cluster do not share a common path as well as cannot be accessed by passwordless SSH , then you can use the following instructions to deploy SnappyData: To set up the cluster for machines without passwordless SSH: Copy and extract the downloaded binaries into each machine. The binaries can be placed in different directory structures. Configure each member separately. Note The scripts used for starting individual members in the cluster do not read from the conf file of each member, hence there is no need to edit the conf files for starting the members. These scripts will start the member with the default configuration properties. To override the default configuration, you can pass the properties as arguments to the above scripts. Start the members in the cluster one at a time. Start the locator first, then the servers, and finally the leads. Use the following scripts to start the members: $SNAPPY_HOME/sbin/snappy-locator.sh $SNAPPY_HOME/sbin/snappy-server.sh $SNAPPY_HOME/sbin/snappy-lead.sh Start Examples : $SNAPPY_HOME/sbin/snappy-locator.sh start -dir=/tmp/locator $SNAPPY_HOME/sbin/snappy-server.sh start -dir=/tmp/server -locators=\"localhost:10334\" $SNAPPY_HOME/sbin/snappy-lead.sh start -dir=/tmp/lead -locators=\"localhost:10334\" Stop Examples : $SNAPPY_HOME/sbin/snappy-locator.sh stop -dir=/tmp/locator $SNAPPY_HOME/sbin/snappy-server.sh stop -dir=/tmp/server $SNAPPY_HOME/sbin/snappy-lead.sh stop -dir=/tmp/lead","title":"Installing SnappyData On-Premise"},{"location":"install/install_on_premise/#install-on-premise","text":"SnappyData runs on UNIX-like systems (for example, Linux, Mac OS). With on-premises installation, SnappyData is installed and operated from your in-house computing infrastructure. For quick start instructions on Installing SnappyData on-premise, refer Getting Started with SnappyData On-premise . After installing SnappyData, follow the instructions here , to use the product from Apache Zeppelin.","title":"Install On-Premise"},{"location":"install/install_on_premise/#single-host-installation","text":"This is the simplest form of deployment and can be used for testing and POCs. Open the command prompt, go the location of the downloaded SnappyData file, and run the following command to extract the archive file. $ tar -xzf snappydata-<version-number>bin.tar.gz $ cd snappydata-<version-number>-bin/ Start a basic cluster with one data node, one lead, and one locator: ./sbin/snappy-start-all.sh For custom configuration and to start more nodes, refer configuring the SnappyData cluster .","title":"Single-Host Installation"},{"location":"install/install_on_premise/#multi-host-installation","text":"For real-life use cases, you require multiple machines on which SnappyData must be deployed. You can start one or more SnappyData node on a single machine based on your machine size. Where there are multiple machines involved, you can deploy SnappyData on: Machines With Shared Path Machines Without a Shared Path Machines Without Passwordless SSH","title":"Multi-Host Installation"},{"location":"install/install_on_premise/#machines-with-a-shared-path","text":"If all the machines in your cluster can share a path over an NFS or similar protocol, then use the following instructions: Prerequisites Ensure that the /etc/hosts correctly configures the host and IP address of each SnappyData member machine. Ensure that SSH is supported and you have configured all the machines to be accessed by passwordless SSH . If SSH is not supported then follow the instructions in the Machines Without Passwordless SSH section. To set up the cluster for machines with a shared path: Copy the downloaded binaries to the shared folder. Extract the downloaded archive file and go to SnappyData home directory. $ tar -xzf snappydata- -bin.tar.gz $ cd snappydata- -bin/ Configure the cluster as described in Configuring the Cluster . After configuring each of the members in the cluster, run the snappy-start-all.sh script: ./sbin/snappy-start-all.sh This creates a default folder named work and stores all SnappyData member's artifacts separately. The folder is identified by the name of the node. Tip For optimum performance, configure the -dir to a local directory and not to a network directory. When -dir property is configured for each member in the cluster, the artifacts of the respective members get created in the -dir folder.","title":"Machines With a Shared Path"},{"location":"install/install_on_premise/#machines-without-a-shared-path","text":"In case all the machines in your cluster do not share a path over an NFS or similar protocol, then use the following instructions: Prerequisites Ensure that /etc/hosts correctly configures the host and IP Address of each SnappyData member machine. Ensure that SSH is supported and you have configured all the machines to be accessed by passwordless SSH . If SSH is not supported then follow the instructions in the Machines without passwordless SSH section. To set up the cluster for machines without a shared path: Copy and extract the downloaded binaries into each machine. Ensure to maintain the same directory structure on all the machines. For example, if you copy the binaries in /opt/snappydata/ on the first machine, then you must ensure to copy the binaries to /opt/snappydata/ on rest of the machines. Configure the cluster as described in Configuring the Cluster . Maintain one node as the controller node, where you can configure your cluster. Usually this is done in the lead node. On that machine, you can edit files such as servers, locators, and leads which are in the $SNAPPY_HOME/conf/ directory . Create a working directory on every machine, for each of the SnappyData member that you want to run. The member's working directory provides a default location for the logs, persistence, and status files of that member. For example, if you want to run both a locator and server member on the local machine, create separate directories for each member. Run the snappy-start-all.sh script: ./sbin/snappy-start-all.sh","title":"Machines Without a Shared Path"},{"location":"install/install_on_premise/#machines-without-passwordless-ssh","text":"In case the machines in your cluster do not share a common path as well as cannot be accessed by passwordless SSH , then you can use the following instructions to deploy SnappyData: To set up the cluster for machines without passwordless SSH: Copy and extract the downloaded binaries into each machine. The binaries can be placed in different directory structures. Configure each member separately. Note The scripts used for starting individual members in the cluster do not read from the conf file of each member, hence there is no need to edit the conf files for starting the members. These scripts will start the member with the default configuration properties. To override the default configuration, you can pass the properties as arguments to the above scripts. Start the members in the cluster one at a time. Start the locator first, then the servers, and finally the leads. Use the following scripts to start the members: $SNAPPY_HOME/sbin/snappy-locator.sh $SNAPPY_HOME/sbin/snappy-server.sh $SNAPPY_HOME/sbin/snappy-lead.sh Start Examples : $SNAPPY_HOME/sbin/snappy-locator.sh start -dir=/tmp/locator $SNAPPY_HOME/sbin/snappy-server.sh start -dir=/tmp/server -locators=\"localhost:10334\" $SNAPPY_HOME/sbin/snappy-lead.sh start -dir=/tmp/lead -locators=\"localhost:10334\" Stop Examples : $SNAPPY_HOME/sbin/snappy-locator.sh stop -dir=/tmp/locator $SNAPPY_HOME/sbin/snappy-server.sh stop -dir=/tmp/server $SNAPPY_HOME/sbin/snappy-lead.sh stop -dir=/tmp/lead","title":"Machines Without Passwordless SSH"},{"location":"install/setting_up_cluster_on_amazon_web_services/","text":"Setting up Cluster on Amazon Web Services (AWS) \u00b6 Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3). You can set up SnappyData cluster on Amazon Web Services using one of the following options: EC2 Scripts AWS Management Console SnappyData EC2 Scripts \u00b6 The SnappyData EC2 scripts enable you to launch and manage SnappyData clusters quickly on Amazon EC2 instances. They also allow you to provide custom configuration for the cluster via SnappyData configuration files, before launching the cluster. The snappy-ec2 script is the entry point for these EC2 scripts and is derived from the spark-ec2 script available in Apache Spark 1.6 . The scripts are available on GitHub in the snappy-cloud-tools repository and also as a .tar.gz file on the release page file. Note The EC2 scripts are provided on an experimental basis. Feel free to try it out and provide your feedback as via GitHub issues. This section covers the following: * Prerequisites * Deploying SnappyData Cluster with EC2 Scripts * Cluster Management * Known Limitations Prerequisites \u00b6 Ensure that you have an existing AWS account with required permissions to launch EC2 resources Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster Refer to the Amazon Web Services EC2 documentation for more information on generating your own EC2 Key Pair . Using the AWS Secret Access Key and the Access Key ID, set the two environment variables, AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID . You can find information about generating these keys in the AWS IAM console page. If you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file. For example: export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112 export AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10 Ensure Python v 2.7 or later is installed on your local computer. Deploying SnappyData Cluster with EC2 Scripts \u00b6 In the command prompt, go to the directory where the snappydata-ec2- <version> .tar.gz is extracted or to the aws/ec2 directory where the SnappyData cloud tools repository is cloned locally. Syntax ./snappy-ec2 -k <your-key-name> -i <your-keyfile-path> <action> <your-cluster-name> [options] Here: <your-key-name> refers to the name of your EC2 key pair. <your-keyfile-path> refers to the path to the key (typically .pem) file. <action> refers to the action to be performed. Some of the available actions are launch , destroy , stop , start and reboot-cluster . Use launch action to create a new cluster while stop and start actions work on existing clusters. By default, the script starts one instance of a locator, lead, and server each. The script identifies each cluster by its unique cluster name that you provide and internally ties the members (locators, leads, and stores/servers) of the cluster with EC2 security groups, whose names are derived from the cluster name. When running the script, you can also specify options to configure the cluster such as the number of stores in the cluster and the region where the EC2 instances should be launched. Example ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin --region=us-west-1 launch my-cluster The above example launches a SnappyData cluster named my-cluster with 2 stores or servers. The locator is associated with security group named my-cluster-locator and the servers are associated with my-cluster-store security group. The cluster is launched in the N. California (us-west-1) region on AWS and has an Apache Zeppelin server running on the instance where the lead is running. The example assumes that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'. Assuming IAM role in the AWS EC2 Scripts \u00b6 An IAM user in AWS can gain additional (or different) permissions, or get permissions to perform actions in a different AWS account through EC2 scripts. You can configure the AWS EC2 scripts to use an IAM role by passing the following properties: assume-role-arn : The Amazon Resource Name (ARN) of the IAM role to be assumed. This IAM role's credentials are used to launch the cluster. If you are using the switch role functionality, this property is mandatory. assume-role-timeout : Timeout in seconds for the temporary credentials of the assumed IAM role, min is 900 seconds and max is 3600 seconds. assume-role-session-name : Name of this session in which this IAM role is assumed by the user. Example ./snappy-ec2 -k <your-key-name> -i <your-keyfile-path> stop snap_ec2_cluster --with-zeppelin --authorized-address=<Authorized IP Address> --assume-role-arn=<role-arn> --assume-role-timeout=<timeout> --assume-role-session-name=<name-for-session> Note By default, the cluster is launched in the N. Virginia (us-east-1) region on AWS. To launch the cluster in a specific region use option --region . Cluster Management \u00b6 This section covers the following: Using custom build Specifying Properties Stopping the Cluster Resuming the Cluster Adding Servers to the Cluster Listing Members of the Cluster Connecting to the Cluster Destroying the Cluster Starting Cluster with Apache Zeppelin More Options Using Custom build \u00b6 This script by default uses the SnappyData OSS build available on the GitHub releases page to launch the cluster. To select a version of the OSS build available on GitHub, use option --snappydata-version . You can also provide your SnappyData build to the script to launch the cluster, by using option --snappydata-tarball to the launch command. The build can be present either on a local filesystem or as a resource on the web. For example, to use SnappyData OSS build to launch the cluster, download the build tarball from https://github.com/TIBCOSoftware/snappydata/releases on your local machine and give its path as value to above option. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball=\"/home/ec2-user/snappydata/distributions/snappydata-1.3.1-bin.tar.gz\" Alternatively, you can also put your build file on a public web server and provide its URL to this option. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball=\"https://s3-us-east-2.amazonaws.com/mybucket/distributions/snappydata-1.3.1-bin.tar.gz\" The build file should be in .tar.gz format. Specifying Properties \u00b6 You can specify the configuration for the cluster via command-line options. Use --locator-conf to specify the configuration properties for all the locators in the cluster. Similarly, --server-conf and --lead-conf allows you to specify the configuration properties for servers and leads in the cluster, respectively. Following is a sample configuration for all the three processes in a SnappyData cluster: ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 launch my-cluster \\ --locator-conf=\"-peer-discovery-port=9999 -heap-size=1024m\" \\ --lead-conf=\"-spark.executor.cores=10 -heap-size=4096m -spark.ui.port=3333\" \\ --server-conf=\"-client-port=1530\" The utility also reads snappy-env.sh , if present in the directory where helper scripts are present. Note The earlier method of specifying the configuration properties by placing the actual configuration files in the directory, where helper scripts are available, is discontinued. Ensure that the configuration properties specified are correct. Otherwise, launching the SnappyData cluster may fail, but the EC2 instances would still be running. Stopping the Cluster \u00b6 When you stop a cluster, it shuts down the EC2 instances, and any data saved on the local instance stores is lost. However, the data saved on EBS volumes is retained, unless the spot-instances are used. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem stop cluster-name Resuming the Cluster \u00b6 When you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem start cluster-name Note The start command, or launch command with --resume option, ignores the --locators , --leads , or --stores options and launches the SnappyData cluster on existing instances. However, if the configuration options are provided, they are read and processed, thus overriding their values that were provided when the cluster was launched or started previously. Adding Servers to the Cluster \u00b6 This is not yet supported using the script. You must manually launch an instance with (cluster-name)-stores group and then use launch command with the --resume option. Listing Members of the Cluster \u00b6 To get the first locator's hostname: ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem get-locator cluster-name Use the get-lead command to get the first lead's hostname. Connecting to the Cluster \u00b6 You can connect to any instance of a cluster with SSH using the login command. It logs you into the first lead instance. You can then use SSH to connect to any other member of the cluster without a password. The SnappyData product directory is located at /opt/snappydata/ on all the members. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem login cluster-name Destroying the Cluster \u00b6 Destroying a cluster permanently destroys all the data on the local instance stores and on the attached EBS volumes. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem destroy cluster-name This also deletes the security groups created for this cluster. Starting Cluster with Apache Zeppelin \u00b6 Optionally, you can start an instance of the Apache Zeppelin server with the cluster. Apache Zeppelin provides a web-based interactive notebook that is pre-configured to communicate with the SnappyData cluster. The Zeppelin server is launched on the same EC2 instance where the lead node is running. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --with-zeppelin launch cluster-name More Options \u00b6 For a complete list of options provided by the script, run ./snappy-ec2 . The options are also provided below for quick reference. Usage: snappy-ec2 [options] <action> <cluster_name> <action> can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster Options: --version show program's version number and exit -h, --help show this help message and exit -s STORES, --stores=STORES Number of stores to launch (default: 1) --locators=LOCATORS Number of locator nodes to launch (default: 1) --leads=LEADS Number of lead nodes to launch (default: 1) -w WAIT, --wait=WAIT DEPRECATED (no longer necessary) - Seconds to wait for nodes to start -k KEY_PAIR, --key-pair=KEY_PAIR Name of the key pair to use on instances -i IDENTITY_FILE, --identity-file=IDENTITY_FILE SSH private key file to use for logging into instances -p PROFILE, --profile=PROFILE If you have multiple profiles (AWS or boto config), you can configure additional, named profiles by using this option (default: none) -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE Type of server and lead instance to launch (default: m4.large). WARNING: must be 64-bit; small instances won't work --locator-instance-type=LOCATOR_INSTANCE_TYPE Locator instance type (default: t2.medium) -r REGION, --region=REGION EC2 region used to launch instances in, or to find them in (default: us-east-1) -z ZONE, --zone=ZONE Availability zone to launch instances in, or 'all' to spread stores across multiple (an additional $0.01/Gb for bandwidthbetween zones applies) (default: a single zone chosen at random) -a AMI, --ami=AMI Amazon Machine Image ID to use --snappydata-tarball=SNAPPYDATA_TARBALL HTTP URL or local file path of the SnappyData distribution tarball with which the cluster will be launched. (default: ) --locator-conf=LOCATOR_CONF Configuration properties for locators (default: ) --server-conf=SERVER_CONF Configuration properties for servers (default: ) --lead-conf=LEAD_CONF Configuration properties for leads (default: ) -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION Version of SnappyData to use: 'X.Y.Z' (default: LATEST) --with-zeppelin Launch Apache Zeppelin server with the cluster. It'll be launched on the same instance where lead node will be running. --deploy-root-dir=DEPLOY_ROOT_DIR A directory to copy into / on the first locator. Must be absolute. Note that a trailing slash is handled as per rsync: If you omit it, the last directory of the --deploy-root-dir path will be created in / before copying its contents. If you append the trailing slash, the directory is not created and its contents are copied directly into /. (default: none). -D [ADDRESS:]PORT Use SSH dynamic port forwarding to create a SOCKS proxy at the given local address (for use with login) --resume Resume installation on a previously launched cluster (for debugging) --root-ebs-vol-size=SIZE Size (in GB) of root EBS volume for servers and leads. SnappyData is installed on root volume. --root-ebs-vol-size-locator=SIZE Size (in GB) of root EBS volume for locators. SnappyData is installed on root volume. --ebs-vol-size=SIZE Size (in GB) of each additional EBS volume to be attached. --ebs-vol-type=EBS_VOL_TYPE EBS volume type (e.g. 'gp2', 'standard'). --ebs-vol-num=EBS_VOL_NUM Number of EBS volumes to attach to each node as /vol[x]. The volumes will be deleted when the instances terminate. Only possible on EBS-backed AMIs. EBS volumes are only attached if --ebs-vol-size > 0. Only support up to 8 EBS volumes. --placement-group=PLACEMENT_GROUP Which placement group to try and launch instances into. Assumes placement group is already created. --spot-price=PRICE If specified, launch stores as spot instances with the given maximum price (in dollars) -u USER, --user=USER The SSH user you want to connect as (default: ec2-user) --delete-groups When destroying a cluster, delete the security groups that were created --use-existing-locator Launch fresh stores, but use an existing stopped locator if possible --user-data=USER_DATA Path to a user-data file (most AMIs interpret this as an initialization script) --authorized-address=AUTHORIZED_ADDRESS Address to authorize on created security groups (default: 0.0.0.0/0) --additional-security-group=ADDITIONAL_SECURITY_GROUP Additional security group to place the machines in --additional-tags=ADDITIONAL_TAGS Additional tags to set on the machines; tags are comma-separated, while name and value are colon separated; ex: \"Task:MySnappyProject,Env:production\" --copy-aws-credentials Add AWS credentials to hadoop configuration to allow Snappy to access S3 --subnet-id=SUBNET_ID VPC subnet to launch instances in --vpc-id=VPC_ID VPC to launch instances in --private-ips Use private IPs for instances rather than public if VPC/subnet requires that. --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR Whether instances should terminate when shut down or just stop --instance-profile-name=INSTANCE_PROFILE_NAME IAM profile name to launch instances under --assume-role-arn=The Amazon Resource Name (ARN) of the IAM role to be assumed. This IAM role's credentials are used to launch the cluster. If you are using the switch role functionality, this property is mandatory. --assume-role-timeout=Timeout in seconds for the temporary credentials of the assumed IAM role, min is 900 seconds and max is 3600 seconds. --assume-role-session-name=Name of this session in which this IAM role is assumed by the user. Known Limitations \u00b6 Launching the cluster on custom AMI (specified via --ami option) does not work if the user 'ec2-user' does not have sudo permissions. Support for option --user is incomplete. AWS Management Console \u00b6 You can launch a SnappyData cluster on Amazon EC2 instance(s) using Linux-based AMIs available on AWS. For more information on launching an EC2 instance, refer to the AWS documentation . This section covers the following: Prerequisites Launching the Instance and Cluster Accessing the Cluster Attention The AMIs of SnappyData are currently unavailable on AWS. Prerequisites \u00b6 Ensure that you have an existing AWS account with required permissions to launch the EC2 resources. Create an EC2 Key Pair in the region where you want to launch the SnappyData cluster. Deploying SnappyData Cluster with AWS Management Console \u00b6 To launch the instance and start the SnappyData cluster on EC2 instance(s): Open the Amazon EC2 console and sign in using your AWS login credentials. The current region is displayed at the top of the screen. Select the region where you want to launch the instance. Click Launch Instance from the Amazon EC2 console dashboard. On the Choose an Amazon Machine Image (AMI) page, select your preferred Linux-based AMI. For example, you can select Amazon Linux 2 AMI or Ubuntu Server 16.04 LTS . See this page for recommended Operating Systems. The AMIs with pre-installed SnappyData distribution are currently unavailable under AWS Marketplace or Community AMIs . On the Choose an Instance Type page, select the instance type as per the requirement of your use case and then click Review and Launch to launch the instance with default configurations. Note You can also continue customizing your instance before you launch the instance. Refer to the AWS documentation for more information. For the setup across multiple EC2 instances, specify the appropriate number for Number of instances field on Configure Instance page. For example, to launch a SnappyData cluster with 3 servers and 1 locator and 1 lead on separate instances, specify the number as 5. You can also launch locator and lead processes on a single EC2 instance, thereby reducing the instances to 4. On Configure Security Group page, ensure that you open ports 22 (for SSH access to the EC2 instance) and 5050 (to access SnappyData Monitoring Console) for public IP address of your laptop or client terminal. For the setup on multiple instances, you also must open all traffic between the instances in this security group. You can do that by adding a rule with the group id of this security group as value for Source . If you need to connect to the SnappyData cluster via a JDBC client application or tool, open ports 1527 and 1528 for the public IP of the host where your application/tool is running, in the security group. You are directed to the last step Review Instance Launch . Check the details of your instance, and click Launch . In the Select an existing key pair or create a new key pair dialog box, select your key pair. Click Launch . The Launch Status page is displayed. Click View Instances . The dashboard which lists the EC2 instances is displayed. Click Refresh to view the updated list and the status of the instance(s) you just created. Once the status of the instance changes to running , connect to the instance via SSH. You require: The private key (.pem) file of the key pair with which the instance was launched, The public DNS or IP address of the instance, and The username to connect with. It depends on the AMI you selected above. For example, it could be ec2-user for Amazon Linux AMIs or ubuntu for Ubuntu-based AMIs. Refer to the following documentation, for more information on accessing an EC2 instance . !!! Note * The public DNS/IP of the instance is available on the EC2 dashboard > **Instances** page. Select your EC2 instance and look for it in the lower half of the page. Download the required SnappyData distribution (.tar.gz) into the EC2 instance(s). You can find the latest SnappyData Community Edition (OSS) release here . When we make the SnappyData AMI available on AWS in future, it will have the distribution pre-installed. In that case, you can jump to step 4 below. Extract the tarball to /opt/snappydata/. tar -xvf snappydata-<version>-bin.tar.gz sudo mv snappydata-<version>-bin /opt/snappydata chown -R ec2-user:ec2-user /opt/snappydata Make Sure Java 8 is installed and set as default. For Amazon Linux 2018.03, you may need to uninstall Java 7 first. Below commands update OpenJDK to 8. sudo yum -y -q remove jre-1.7.0-openjdk sudo yum -y -q install java-1.8.0-openjdk-devel java -version # Ensure it prints correct Java version Repeat above three steps for all the instances launched. If you are launching the cluster across multiple EC2 instances, you need to 1) setup passwordless ssh access across these instances and 2) provide EC2 instance information in SnappyData's conf files. You can skip these two steps for a SnappyData cluster on a single EC2 instance. For step 2), at a minimum, provide private IP addresses of EC2 instances in appropriate conf files, viz. conf/locators , conf/servers and conf/leads . Sample conf files for a cluster with 3 servers, 1 locator and 1 lead are given below. Here the locator and lead processes are configured to run on the same EC2 instance. cat /opt/snappydata/conf/locators 172.16.32.180 cat /opt/snappydata/conf/servers 172.16.32.181 172.16.32.182 172.16.32.183 cat /opt/snappydata/conf/leads 172.16.32.180 Go to the /opt/snappydata directory. Run the following command to start your cluster. By default, it will launch a basic cluster with one data server, one lead, and one locator. ./sbin/snappy-start-all.sh After deploying SnappyData, follow the instructions here , to use the product from Apache Zeppelin. Accessing SnappyData Cluster \u00b6 Before you access the SnappyData cluster, you must configure cluster's security group to allow connections from your client host on required ports. In case you do not know the IP address of your client host, you can open these ports to the world (though, not recommended) by specifying 0.0.0.0/0 as Source against above port range in the security group. Note that in such a case, any unknown user on the internet can connect to your cluster, if your cluster does not have security enabled. So it is strongly recommended to add specific IP addresses as Source , in the format XXX.XXX.XXX.XXX/32 in your security group. The quickest way to connect to your SnappyData cluster is probably using the snappy shell utility packaged with the distribution. You can launch the snappy shell either from the same EC2 instance or from your laptop where you have SnappyData installed. Connecting to the cluster from the same EC2 instance: Launch the snappy shell. ./bin/snappy Important Before connecting to the cluster, make sure the security group attached to this EC2 instance has ports 1527-1528 open for the public IP of the same ec2 instance . Now, connect to the cluster using its private IP (you can also use the public DNS/IP instead): snappy> connect client '(private-ip-of-EC2-instance):1527'; To connect to the cluster running on multiple EC2 instances, you can use private IP of the EC2 instance where either the locator or any of the servers is running. Connecting to the cluster from your laptop (or any host outside AWS VPC): Launch the snappy shell: ${SNAPPY_HOME}/bin/snappy Important Before connecting to the cluster, make sure the security group attached to this EC2 instance has ports 1527-1528 open for the public IP of your laptop (i.e. the host with SnappyData installed). Now, connect to the cluster using the public DNS/IP of its EC2 instance: snappy> connect client '<public-ip-of-EC2-instance>:1527'; To connect to the cluster running on multiple EC2 instances, you can use public IP of the EC2 instance where either the locator or any of the servers is running.","title":"Setting-up SnappyData Cluster on AWS"},{"location":"install/setting_up_cluster_on_amazon_web_services/#setting-up-cluster-on-amazon-web-services-aws","text":"Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3). You can set up SnappyData cluster on Amazon Web Services using one of the following options: EC2 Scripts AWS Management Console","title":"Setting up Cluster on Amazon Web Services (AWS)"},{"location":"install/setting_up_cluster_on_amazon_web_services/#snappydata-ec2-scripts","text":"The SnappyData EC2 scripts enable you to launch and manage SnappyData clusters quickly on Amazon EC2 instances. They also allow you to provide custom configuration for the cluster via SnappyData configuration files, before launching the cluster. The snappy-ec2 script is the entry point for these EC2 scripts and is derived from the spark-ec2 script available in Apache Spark 1.6 . The scripts are available on GitHub in the snappy-cloud-tools repository and also as a .tar.gz file on the release page file. Note The EC2 scripts are provided on an experimental basis. Feel free to try it out and provide your feedback as via GitHub issues. This section covers the following: * Prerequisites * Deploying SnappyData Cluster with EC2 Scripts * Cluster Management * Known Limitations","title":"SnappyData EC2 Scripts"},{"location":"install/setting_up_cluster_on_amazon_web_services/#prerequisites","text":"Ensure that you have an existing AWS account with required permissions to launch EC2 resources Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster Refer to the Amazon Web Services EC2 documentation for more information on generating your own EC2 Key Pair . Using the AWS Secret Access Key and the Access Key ID, set the two environment variables, AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID . You can find information about generating these keys in the AWS IAM console page. If you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file. For example: export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112 export AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10 Ensure Python v 2.7 or later is installed on your local computer.","title":"Prerequisites"},{"location":"install/setting_up_cluster_on_amazon_web_services/#deploying-snappydata-cluster-with-ec2-scripts","text":"In the command prompt, go to the directory where the snappydata-ec2- <version> .tar.gz is extracted or to the aws/ec2 directory where the SnappyData cloud tools repository is cloned locally. Syntax ./snappy-ec2 -k <your-key-name> -i <your-keyfile-path> <action> <your-cluster-name> [options] Here: <your-key-name> refers to the name of your EC2 key pair. <your-keyfile-path> refers to the path to the key (typically .pem) file. <action> refers to the action to be performed. Some of the available actions are launch , destroy , stop , start and reboot-cluster . Use launch action to create a new cluster while stop and start actions work on existing clusters. By default, the script starts one instance of a locator, lead, and server each. The script identifies each cluster by its unique cluster name that you provide and internally ties the members (locators, leads, and stores/servers) of the cluster with EC2 security groups, whose names are derived from the cluster name. When running the script, you can also specify options to configure the cluster such as the number of stores in the cluster and the region where the EC2 instances should be launched. Example ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin --region=us-west-1 launch my-cluster The above example launches a SnappyData cluster named my-cluster with 2 stores or servers. The locator is associated with security group named my-cluster-locator and the servers are associated with my-cluster-store security group. The cluster is launched in the N. California (us-west-1) region on AWS and has an Apache Zeppelin server running on the instance where the lead is running. The example assumes that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.","title":"Deploying SnappyData Cluster with EC2 Scripts"},{"location":"install/setting_up_cluster_on_amazon_web_services/#assuming-iam-role-in-the-aws-ec2-scripts","text":"An IAM user in AWS can gain additional (or different) permissions, or get permissions to perform actions in a different AWS account through EC2 scripts. You can configure the AWS EC2 scripts to use an IAM role by passing the following properties: assume-role-arn : The Amazon Resource Name (ARN) of the IAM role to be assumed. This IAM role's credentials are used to launch the cluster. If you are using the switch role functionality, this property is mandatory. assume-role-timeout : Timeout in seconds for the temporary credentials of the assumed IAM role, min is 900 seconds and max is 3600 seconds. assume-role-session-name : Name of this session in which this IAM role is assumed by the user. Example ./snappy-ec2 -k <your-key-name> -i <your-keyfile-path> stop snap_ec2_cluster --with-zeppelin --authorized-address=<Authorized IP Address> --assume-role-arn=<role-arn> --assume-role-timeout=<timeout> --assume-role-session-name=<name-for-session> Note By default, the cluster is launched in the N. Virginia (us-east-1) region on AWS. To launch the cluster in a specific region use option --region .","title":"Assuming IAM role in the AWS EC2 Scripts"},{"location":"install/setting_up_cluster_on_amazon_web_services/#cluster-management","text":"This section covers the following: Using custom build Specifying Properties Stopping the Cluster Resuming the Cluster Adding Servers to the Cluster Listing Members of the Cluster Connecting to the Cluster Destroying the Cluster Starting Cluster with Apache Zeppelin More Options","title":"Cluster Management"},{"location":"install/setting_up_cluster_on_amazon_web_services/#using-custom-build","text":"This script by default uses the SnappyData OSS build available on the GitHub releases page to launch the cluster. To select a version of the OSS build available on GitHub, use option --snappydata-version . You can also provide your SnappyData build to the script to launch the cluster, by using option --snappydata-tarball to the launch command. The build can be present either on a local filesystem or as a resource on the web. For example, to use SnappyData OSS build to launch the cluster, download the build tarball from https://github.com/TIBCOSoftware/snappydata/releases on your local machine and give its path as value to above option. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball=\"/home/ec2-user/snappydata/distributions/snappydata-1.3.1-bin.tar.gz\" Alternatively, you can also put your build file on a public web server and provide its URL to this option. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball=\"https://s3-us-east-2.amazonaws.com/mybucket/distributions/snappydata-1.3.1-bin.tar.gz\" The build file should be in .tar.gz format.","title":"Using Custom build"},{"location":"install/setting_up_cluster_on_amazon_web_services/#specifying-properties","text":"You can specify the configuration for the cluster via command-line options. Use --locator-conf to specify the configuration properties for all the locators in the cluster. Similarly, --server-conf and --lead-conf allows you to specify the configuration properties for servers and leads in the cluster, respectively. Following is a sample configuration for all the three processes in a SnappyData cluster: ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 launch my-cluster \\ --locator-conf=\"-peer-discovery-port=9999 -heap-size=1024m\" \\ --lead-conf=\"-spark.executor.cores=10 -heap-size=4096m -spark.ui.port=3333\" \\ --server-conf=\"-client-port=1530\" The utility also reads snappy-env.sh , if present in the directory where helper scripts are present. Note The earlier method of specifying the configuration properties by placing the actual configuration files in the directory, where helper scripts are available, is discontinued. Ensure that the configuration properties specified are correct. Otherwise, launching the SnappyData cluster may fail, but the EC2 instances would still be running.","title":"Specifying Properties"},{"location":"install/setting_up_cluster_on_amazon_web_services/#stopping-the-cluster","text":"When you stop a cluster, it shuts down the EC2 instances, and any data saved on the local instance stores is lost. However, the data saved on EBS volumes is retained, unless the spot-instances are used. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem stop cluster-name","title":"Stopping the Cluster"},{"location":"install/setting_up_cluster_on_amazon_web_services/#resuming-the-cluster","text":"When you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem start cluster-name Note The start command, or launch command with --resume option, ignores the --locators , --leads , or --stores options and launches the SnappyData cluster on existing instances. However, if the configuration options are provided, they are read and processed, thus overriding their values that were provided when the cluster was launched or started previously.","title":"Resuming the Cluster"},{"location":"install/setting_up_cluster_on_amazon_web_services/#adding-servers-to-the-cluster","text":"This is not yet supported using the script. You must manually launch an instance with (cluster-name)-stores group and then use launch command with the --resume option.","title":"Adding Servers to the Cluster"},{"location":"install/setting_up_cluster_on_amazon_web_services/#listing-members-of-the-cluster","text":"To get the first locator's hostname: ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem get-locator cluster-name Use the get-lead command to get the first lead's hostname.","title":"Listing Members of the Cluster"},{"location":"install/setting_up_cluster_on_amazon_web_services/#connecting-to-the-cluster","text":"You can connect to any instance of a cluster with SSH using the login command. It logs you into the first lead instance. You can then use SSH to connect to any other member of the cluster without a password. The SnappyData product directory is located at /opt/snappydata/ on all the members. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem login cluster-name","title":"Connecting to the Cluster"},{"location":"install/setting_up_cluster_on_amazon_web_services/#destroying-the-cluster","text":"Destroying a cluster permanently destroys all the data on the local instance stores and on the attached EBS volumes. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem destroy cluster-name This also deletes the security groups created for this cluster.","title":"Destroying the Cluster"},{"location":"install/setting_up_cluster_on_amazon_web_services/#starting-cluster-with-apache-zeppelin","text":"Optionally, you can start an instance of the Apache Zeppelin server with the cluster. Apache Zeppelin provides a web-based interactive notebook that is pre-configured to communicate with the SnappyData cluster. The Zeppelin server is launched on the same EC2 instance where the lead node is running. ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --with-zeppelin launch cluster-name","title":"Starting Cluster with Apache Zeppelin"},{"location":"install/setting_up_cluster_on_amazon_web_services/#more-options","text":"For a complete list of options provided by the script, run ./snappy-ec2 . The options are also provided below for quick reference. Usage: snappy-ec2 [options] <action> <cluster_name> <action> can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster Options: --version show program's version number and exit -h, --help show this help message and exit -s STORES, --stores=STORES Number of stores to launch (default: 1) --locators=LOCATORS Number of locator nodes to launch (default: 1) --leads=LEADS Number of lead nodes to launch (default: 1) -w WAIT, --wait=WAIT DEPRECATED (no longer necessary) - Seconds to wait for nodes to start -k KEY_PAIR, --key-pair=KEY_PAIR Name of the key pair to use on instances -i IDENTITY_FILE, --identity-file=IDENTITY_FILE SSH private key file to use for logging into instances -p PROFILE, --profile=PROFILE If you have multiple profiles (AWS or boto config), you can configure additional, named profiles by using this option (default: none) -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE Type of server and lead instance to launch (default: m4.large). WARNING: must be 64-bit; small instances won't work --locator-instance-type=LOCATOR_INSTANCE_TYPE Locator instance type (default: t2.medium) -r REGION, --region=REGION EC2 region used to launch instances in, or to find them in (default: us-east-1) -z ZONE, --zone=ZONE Availability zone to launch instances in, or 'all' to spread stores across multiple (an additional $0.01/Gb for bandwidthbetween zones applies) (default: a single zone chosen at random) -a AMI, --ami=AMI Amazon Machine Image ID to use --snappydata-tarball=SNAPPYDATA_TARBALL HTTP URL or local file path of the SnappyData distribution tarball with which the cluster will be launched. (default: ) --locator-conf=LOCATOR_CONF Configuration properties for locators (default: ) --server-conf=SERVER_CONF Configuration properties for servers (default: ) --lead-conf=LEAD_CONF Configuration properties for leads (default: ) -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION Version of SnappyData to use: 'X.Y.Z' (default: LATEST) --with-zeppelin Launch Apache Zeppelin server with the cluster. It'll be launched on the same instance where lead node will be running. --deploy-root-dir=DEPLOY_ROOT_DIR A directory to copy into / on the first locator. Must be absolute. Note that a trailing slash is handled as per rsync: If you omit it, the last directory of the --deploy-root-dir path will be created in / before copying its contents. If you append the trailing slash, the directory is not created and its contents are copied directly into /. (default: none). -D [ADDRESS:]PORT Use SSH dynamic port forwarding to create a SOCKS proxy at the given local address (for use with login) --resume Resume installation on a previously launched cluster (for debugging) --root-ebs-vol-size=SIZE Size (in GB) of root EBS volume for servers and leads. SnappyData is installed on root volume. --root-ebs-vol-size-locator=SIZE Size (in GB) of root EBS volume for locators. SnappyData is installed on root volume. --ebs-vol-size=SIZE Size (in GB) of each additional EBS volume to be attached. --ebs-vol-type=EBS_VOL_TYPE EBS volume type (e.g. 'gp2', 'standard'). --ebs-vol-num=EBS_VOL_NUM Number of EBS volumes to attach to each node as /vol[x]. The volumes will be deleted when the instances terminate. Only possible on EBS-backed AMIs. EBS volumes are only attached if --ebs-vol-size > 0. Only support up to 8 EBS volumes. --placement-group=PLACEMENT_GROUP Which placement group to try and launch instances into. Assumes placement group is already created. --spot-price=PRICE If specified, launch stores as spot instances with the given maximum price (in dollars) -u USER, --user=USER The SSH user you want to connect as (default: ec2-user) --delete-groups When destroying a cluster, delete the security groups that were created --use-existing-locator Launch fresh stores, but use an existing stopped locator if possible --user-data=USER_DATA Path to a user-data file (most AMIs interpret this as an initialization script) --authorized-address=AUTHORIZED_ADDRESS Address to authorize on created security groups (default: 0.0.0.0/0) --additional-security-group=ADDITIONAL_SECURITY_GROUP Additional security group to place the machines in --additional-tags=ADDITIONAL_TAGS Additional tags to set on the machines; tags are comma-separated, while name and value are colon separated; ex: \"Task:MySnappyProject,Env:production\" --copy-aws-credentials Add AWS credentials to hadoop configuration to allow Snappy to access S3 --subnet-id=SUBNET_ID VPC subnet to launch instances in --vpc-id=VPC_ID VPC to launch instances in --private-ips Use private IPs for instances rather than public if VPC/subnet requires that. --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR Whether instances should terminate when shut down or just stop --instance-profile-name=INSTANCE_PROFILE_NAME IAM profile name to launch instances under --assume-role-arn=The Amazon Resource Name (ARN) of the IAM role to be assumed. This IAM role's credentials are used to launch the cluster. If you are using the switch role functionality, this property is mandatory. --assume-role-timeout=Timeout in seconds for the temporary credentials of the assumed IAM role, min is 900 seconds and max is 3600 seconds. --assume-role-session-name=Name of this session in which this IAM role is assumed by the user.","title":"More Options"},{"location":"install/setting_up_cluster_on_amazon_web_services/#known-limitations","text":"Launching the cluster on custom AMI (specified via --ami option) does not work if the user 'ec2-user' does not have sudo permissions. Support for option --user is incomplete.","title":"Known Limitations"},{"location":"install/setting_up_cluster_on_amazon_web_services/#aws-management-console","text":"You can launch a SnappyData cluster on Amazon EC2 instance(s) using Linux-based AMIs available on AWS. For more information on launching an EC2 instance, refer to the AWS documentation . This section covers the following: Prerequisites Launching the Instance and Cluster Accessing the Cluster Attention The AMIs of SnappyData are currently unavailable on AWS.","title":"AWS Management Console"},{"location":"install/setting_up_cluster_on_amazon_web_services/#prerequisites_1","text":"Ensure that you have an existing AWS account with required permissions to launch the EC2 resources. Create an EC2 Key Pair in the region where you want to launch the SnappyData cluster.","title":"Prerequisites"},{"location":"install/setting_up_cluster_on_amazon_web_services/#deploying-snappydata-cluster-with-aws-management-console","text":"To launch the instance and start the SnappyData cluster on EC2 instance(s): Open the Amazon EC2 console and sign in using your AWS login credentials. The current region is displayed at the top of the screen. Select the region where you want to launch the instance. Click Launch Instance from the Amazon EC2 console dashboard. On the Choose an Amazon Machine Image (AMI) page, select your preferred Linux-based AMI. For example, you can select Amazon Linux 2 AMI or Ubuntu Server 16.04 LTS . See this page for recommended Operating Systems. The AMIs with pre-installed SnappyData distribution are currently unavailable under AWS Marketplace or Community AMIs . On the Choose an Instance Type page, select the instance type as per the requirement of your use case and then click Review and Launch to launch the instance with default configurations. Note You can also continue customizing your instance before you launch the instance. Refer to the AWS documentation for more information. For the setup across multiple EC2 instances, specify the appropriate number for Number of instances field on Configure Instance page. For example, to launch a SnappyData cluster with 3 servers and 1 locator and 1 lead on separate instances, specify the number as 5. You can also launch locator and lead processes on a single EC2 instance, thereby reducing the instances to 4. On Configure Security Group page, ensure that you open ports 22 (for SSH access to the EC2 instance) and 5050 (to access SnappyData Monitoring Console) for public IP address of your laptop or client terminal. For the setup on multiple instances, you also must open all traffic between the instances in this security group. You can do that by adding a rule with the group id of this security group as value for Source . If you need to connect to the SnappyData cluster via a JDBC client application or tool, open ports 1527 and 1528 for the public IP of the host where your application/tool is running, in the security group. You are directed to the last step Review Instance Launch . Check the details of your instance, and click Launch . In the Select an existing key pair or create a new key pair dialog box, select your key pair. Click Launch . The Launch Status page is displayed. Click View Instances . The dashboard which lists the EC2 instances is displayed. Click Refresh to view the updated list and the status of the instance(s) you just created. Once the status of the instance changes to running , connect to the instance via SSH. You require: The private key (.pem) file of the key pair with which the instance was launched, The public DNS or IP address of the instance, and The username to connect with. It depends on the AMI you selected above. For example, it could be ec2-user for Amazon Linux AMIs or ubuntu for Ubuntu-based AMIs. Refer to the following documentation, for more information on accessing an EC2 instance . !!! Note * The public DNS/IP of the instance is available on the EC2 dashboard > **Instances** page. Select your EC2 instance and look for it in the lower half of the page. Download the required SnappyData distribution (.tar.gz) into the EC2 instance(s). You can find the latest SnappyData Community Edition (OSS) release here . When we make the SnappyData AMI available on AWS in future, it will have the distribution pre-installed. In that case, you can jump to step 4 below. Extract the tarball to /opt/snappydata/. tar -xvf snappydata-<version>-bin.tar.gz sudo mv snappydata-<version>-bin /opt/snappydata chown -R ec2-user:ec2-user /opt/snappydata Make Sure Java 8 is installed and set as default. For Amazon Linux 2018.03, you may need to uninstall Java 7 first. Below commands update OpenJDK to 8. sudo yum -y -q remove jre-1.7.0-openjdk sudo yum -y -q install java-1.8.0-openjdk-devel java -version # Ensure it prints correct Java version Repeat above three steps for all the instances launched. If you are launching the cluster across multiple EC2 instances, you need to 1) setup passwordless ssh access across these instances and 2) provide EC2 instance information in SnappyData's conf files. You can skip these two steps for a SnappyData cluster on a single EC2 instance. For step 2), at a minimum, provide private IP addresses of EC2 instances in appropriate conf files, viz. conf/locators , conf/servers and conf/leads . Sample conf files for a cluster with 3 servers, 1 locator and 1 lead are given below. Here the locator and lead processes are configured to run on the same EC2 instance. cat /opt/snappydata/conf/locators 172.16.32.180 cat /opt/snappydata/conf/servers 172.16.32.181 172.16.32.182 172.16.32.183 cat /opt/snappydata/conf/leads 172.16.32.180 Go to the /opt/snappydata directory. Run the following command to start your cluster. By default, it will launch a basic cluster with one data server, one lead, and one locator. ./sbin/snappy-start-all.sh After deploying SnappyData, follow the instructions here , to use the product from Apache Zeppelin.","title":"Deploying SnappyData Cluster with AWS Management Console"},{"location":"install/setting_up_cluster_on_amazon_web_services/#accessing-snappydata-cluster","text":"Before you access the SnappyData cluster, you must configure cluster's security group to allow connections from your client host on required ports. In case you do not know the IP address of your client host, you can open these ports to the world (though, not recommended) by specifying 0.0.0.0/0 as Source against above port range in the security group. Note that in such a case, any unknown user on the internet can connect to your cluster, if your cluster does not have security enabled. So it is strongly recommended to add specific IP addresses as Source , in the format XXX.XXX.XXX.XXX/32 in your security group. The quickest way to connect to your SnappyData cluster is probably using the snappy shell utility packaged with the distribution. You can launch the snappy shell either from the same EC2 instance or from your laptop where you have SnappyData installed. Connecting to the cluster from the same EC2 instance: Launch the snappy shell. ./bin/snappy Important Before connecting to the cluster, make sure the security group attached to this EC2 instance has ports 1527-1528 open for the public IP of the same ec2 instance . Now, connect to the cluster using its private IP (you can also use the public DNS/IP instead): snappy> connect client '(private-ip-of-EC2-instance):1527'; To connect to the cluster running on multiple EC2 instances, you can use private IP of the EC2 instance where either the locator or any of the servers is running. Connecting to the cluster from your laptop (or any host outside AWS VPC): Launch the snappy shell: ${SNAPPY_HOME}/bin/snappy Important Before connecting to the cluster, make sure the security group attached to this EC2 instance has ports 1527-1528 open for the public IP of your laptop (i.e. the host with SnappyData installed). Now, connect to the cluster using the public DNS/IP of its EC2 instance: snappy> connect client '<public-ip-of-EC2-instance>:1527'; To connect to the cluster running on multiple EC2 instances, you can use public IP of the EC2 instance where either the locator or any of the servers is running.","title":"Accessing SnappyData Cluster"},{"location":"install/system_requirements/","text":"System Requirements \u00b6 In this section, we discuss the hardware, software, and network requirements for SnappyData. Hardware Requirements \u00b6 SnappyData turns Apache Spark into a mission-critical, elastic scalable in-memory data store. This allows users to run Spark workloads and classic database workloads on SnappyData. Memory : SnappyData works well with anywhere from 8GB of memory to TBs of memory. While exact memory requirements depend on the end user application, we recommend allocating no more than 75% of the memory to SnappyData. We recommend using a machine with at least 8GB of RAM when working with SnappyData. Note It is recommended to have a minimum of 8GB memory for server-grade machines. CPU Cores : SnappyData is a highly multi-threaded system and can take advantage of CPU cores to deliver higher throughput. It has been tested with multi-core multi-CPU machines. We recommend using machines with at least 16 cores when working with SnappyData. The degree of parallelism you can achieve with SnappyData directly depends on the number of cores, as higher core machines perform better than lower core machines. Network : SnappyData is a clustered scale-out in-memory data store and both jobs and queries use the network extensively to complete their job. Since data is mostly available in-memory, queries and jobs typically get CPU and/or network bound. We recommend running SnappyData on at least a 1GB network for testing and use a 10GB network for production scenarios. Disk : SnappyData overflows data to local disk files and tables can be configured for persistence. We recommend using flash storage for optimal performance for SnappyData shared nothing persistence. Data can be saved out to stores like HDFS and S3 using SnappyData DataFrame APIs. Operating Systems Supported \u00b6 Operating System Version Red Hat Enterprise Linux RHEL 6, 7 and later (Minimum recommended kernel version: 3.10.0-693.2.2.el7.x86_64) Ubuntu Ubuntu Server 14.04 and later CentOS CentOS 6, 7 and later (Minimum recommended kernel version: 3.10.0-693.2.2.el7.x86_64) Host Machine Requirements \u00b6 Requirements for each host: A supported Oracle Java SE 8 JDK installation. Required minimum version: 1.8.0_144 (see SNAP-2017 , SNAP-1999 , SNAP-1911 , SNAP-1375 for crashes reported with earlier versions). Recommended is the latest stable release version. Alternatively equivalent Java version >= 1.8.0_144 from OpenJDK distributions (Linux vendor build, AdoptOpenJDK or equivalent). Recommended is the latest stable release version. A full JDK installation is required. The latest version of Bash shell. A file system that supports long file names. TCP/IP. System clock set to the correct time. For each Linux host, the hostname and host files must be properly configured. See the system manual pages for hostname and host settings. For each Linux host, configure the swap to be in the range of 16-64GB to allow for swapping out of unused pages. Time synchronization service such as Network Time Protocol (NTP). cURL must be installed on lead nodes for snappy scripts to work. On Red Hat based systems it can be installed using sudo yum install curl while on Debian/Ubuntu based systems, you can install using sudo apt-get install curl command. Note For troubleshooting, you must run a time synchronization service on all hosts. Synchronized time stamps allow you to merge log messages from different hosts, for an accurate chronological history of a distributed run. If you deploy SnappyData on a virtualized host, consult the documentation provided with the platform, for system requirements and recommended best practices, for running Java and latency-sensitive workloads. Python Integration using pyspark \u00b6 The Python pyspark module has the same requirements as in Apache Spark. The numpy package is required by many modules of pyspark including the examples shipped with SnappyData. On recent Red Hat based systems, it can be installed using sudo yum install numpy or sudo yum install python2-numpy commands. Whereas, on Debian/Ubuntu based systems, you can install using the sudo apt-get install python-numpy command. Some of the python APIs can use SciPy to optimize some algorithms (in linalg package), and some others need Pandas. On recent Red Hat based systems SciPy can be installed using sudo yum install scipy command. Whereas, on Debian/Ubuntu based systems you can install using the sudo apt-get install python-scipy command. Likewise, Pandas on recent Red Hat based systems can be installed using sudo yum installed python-pandas command, while on Debian/Ubuntu based systems it can be installed using the sudo apt-get install python-pandas command. On Red Hat based systems, some of the above Python packages may be available only after enabling the EPEL repository. If these are not available in the repositories for your OS version or if using EPEL is not an option, then you can use pip . Refer to the respective project documentation for details and alternative options such as Anaconda. Alternatively Python 3 can also be used but the version should be <= 3.7. Consult your distribution documentation to install the equivalent python 3 packages for numpy , scipy and pandas . Or you can use conda/mamba to set up the required python environment. Filesystem Type for Linux Platforms \u00b6 For optimum disk-store performance, we recommend the use of local filesystem for disk data storage and not over NFS.","title":"System Requirements"},{"location":"install/system_requirements/#system-requirements","text":"In this section, we discuss the hardware, software, and network requirements for SnappyData.","title":"System Requirements"},{"location":"install/system_requirements/#hardware-requirements","text":"SnappyData turns Apache Spark into a mission-critical, elastic scalable in-memory data store. This allows users to run Spark workloads and classic database workloads on SnappyData. Memory : SnappyData works well with anywhere from 8GB of memory to TBs of memory. While exact memory requirements depend on the end user application, we recommend allocating no more than 75% of the memory to SnappyData. We recommend using a machine with at least 8GB of RAM when working with SnappyData. Note It is recommended to have a minimum of 8GB memory for server-grade machines. CPU Cores : SnappyData is a highly multi-threaded system and can take advantage of CPU cores to deliver higher throughput. It has been tested with multi-core multi-CPU machines. We recommend using machines with at least 16 cores when working with SnappyData. The degree of parallelism you can achieve with SnappyData directly depends on the number of cores, as higher core machines perform better than lower core machines. Network : SnappyData is a clustered scale-out in-memory data store and both jobs and queries use the network extensively to complete their job. Since data is mostly available in-memory, queries and jobs typically get CPU and/or network bound. We recommend running SnappyData on at least a 1GB network for testing and use a 10GB network for production scenarios. Disk : SnappyData overflows data to local disk files and tables can be configured for persistence. We recommend using flash storage for optimal performance for SnappyData shared nothing persistence. Data can be saved out to stores like HDFS and S3 using SnappyData DataFrame APIs.","title":"Hardware  Requirements"},{"location":"install/system_requirements/#operating-systems-supported","text":"Operating System Version Red Hat Enterprise Linux RHEL 6, 7 and later (Minimum recommended kernel version: 3.10.0-693.2.2.el7.x86_64) Ubuntu Ubuntu Server 14.04 and later CentOS CentOS 6, 7 and later (Minimum recommended kernel version: 3.10.0-693.2.2.el7.x86_64)","title":"Operating Systems Supported"},{"location":"install/system_requirements/#host-machine-requirements","text":"Requirements for each host: A supported Oracle Java SE 8 JDK installation. Required minimum version: 1.8.0_144 (see SNAP-2017 , SNAP-1999 , SNAP-1911 , SNAP-1375 for crashes reported with earlier versions). Recommended is the latest stable release version. Alternatively equivalent Java version >= 1.8.0_144 from OpenJDK distributions (Linux vendor build, AdoptOpenJDK or equivalent). Recommended is the latest stable release version. A full JDK installation is required. The latest version of Bash shell. A file system that supports long file names. TCP/IP. System clock set to the correct time. For each Linux host, the hostname and host files must be properly configured. See the system manual pages for hostname and host settings. For each Linux host, configure the swap to be in the range of 16-64GB to allow for swapping out of unused pages. Time synchronization service such as Network Time Protocol (NTP). cURL must be installed on lead nodes for snappy scripts to work. On Red Hat based systems it can be installed using sudo yum install curl while on Debian/Ubuntu based systems, you can install using sudo apt-get install curl command. Note For troubleshooting, you must run a time synchronization service on all hosts. Synchronized time stamps allow you to merge log messages from different hosts, for an accurate chronological history of a distributed run. If you deploy SnappyData on a virtualized host, consult the documentation provided with the platform, for system requirements and recommended best practices, for running Java and latency-sensitive workloads.","title":"Host Machine Requirements"},{"location":"install/system_requirements/#python-integration-using-pyspark","text":"The Python pyspark module has the same requirements as in Apache Spark. The numpy package is required by many modules of pyspark including the examples shipped with SnappyData. On recent Red Hat based systems, it can be installed using sudo yum install numpy or sudo yum install python2-numpy commands. Whereas, on Debian/Ubuntu based systems, you can install using the sudo apt-get install python-numpy command. Some of the python APIs can use SciPy to optimize some algorithms (in linalg package), and some others need Pandas. On recent Red Hat based systems SciPy can be installed using sudo yum install scipy command. Whereas, on Debian/Ubuntu based systems you can install using the sudo apt-get install python-scipy command. Likewise, Pandas on recent Red Hat based systems can be installed using sudo yum installed python-pandas command, while on Debian/Ubuntu based systems it can be installed using the sudo apt-get install python-pandas command. On Red Hat based systems, some of the above Python packages may be available only after enabling the EPEL repository. If these are not available in the repositories for your OS version or if using EPEL is not an option, then you can use pip . Refer to the respective project documentation for details and alternative options such as Anaconda. Alternatively Python 3 can also be used but the version should be <= 3.7. Consult your distribution documentation to install the equivalent python 3 packages for numpy , scipy and pandas . Or you can use conda/mamba to set up the required python environment.","title":"Python Integration using pyspark"},{"location":"install/system_requirements/#filesystem-type-for-linux-platforms","text":"For optimum disk-store performance, we recommend the use of local filesystem for disk data storage and not over NFS.","title":"Filesystem Type for Linux Platforms"},{"location":"install/upgrade/","text":"Upgrade Instructions \u00b6 This guide provides information for upgrading systems running an earlier version of SnappyData. We assume that you have SnappyData already installed, and you are upgrading to the latest version of SnappyData. Before you begin the upgrade, ensure that you understand the new features and any specific requirements for that release. Before You Upgrade \u00b6 Confirm that your system meets the hardware and software requirements described in System Requirements section. Backup the existing environment: Create a backup of the locator, lead, and server configuration files that exist in the conf folder located in the SnappyData home directory. Stop the cluster and verify that all members are stopped: You can shut down the cluster using the sbin/snappy-stop-all.sh command. To ensure that all the members have been shut down correctly, use the sbin/snappy-status-all.sh command. Create a backup of the operational disk store files for all members in the distributed system. Reinstall SnappyData: After you have stopped the cluster, install the latest version of SnappyData . Reconfigure your cluster using the locator, lead, and server configuration files you backed up in step 1. To ensure that the restore script (restore.sh) copies files back to their original locations, make sure that the disk files are available at the original location before restarting the cluster with the latest version of SnappyData. Upgrading to SnappyData 1.3.1 from 1.0.1 or Earlier Versions \u00b6 When you upgrade to SnappyData 1.3.1 from product version 1.0.1 or earlier versions, it is recommended to save all the table data as parquet files, recreate the tables in the new cluster, and then load data from the saved parquet files. Before taking the backup ensure that no operations are currently running on the system. Ensure to cleanup the data from the previous cluster and start the new cluster from a clean directory. For example: # Creating parquet files in older product version 1.0.1 or prior: snappy> create external table table1Parquet using parquet options (path '<path-to-parquet-file>') as select * from table1; snappy> drop table table1; snappy> drop table table1Parquet; # Creating tables from parquet files in SnappyData 1.3.1 snappy> create external table table1_parquet using parquet options (path '<path-to-parquet-file>') ; snappy> create table table1(...); snappy> insert into table1 select * from table1_parquet; Use a path for the Parquet file that has enough space to hold the table data. Once the re-import has completed successfully, make sure that the Parquet files are deleted explicitly. Note Upgrade to SnappyData from 1.0.2 or later versions can be done directly. Migrating to the SnappyData ODBC Driver \u00b6 If you have been using TIBCO ComputeDB\u2122 ODBC Driver from 1.2.0 or earlier releases, then you can install the SnappyData ODBC Driver alongside it without any conflicts. The product name as well as the ODBC Driver name for the two are TIBCO ComputeDB and SnappyData respectively that do not overlap with one another. The older drivers and their DSNs will continue to work against SnappyData 1.3.1 without any changes. While the older driver will continue to work, migrating to the new SnappyData ODBC Driver is highly recommended to avail the new features and bug fixes over the TIBCO ComputeDB\u2122 ODBC Driver releases. This includes new options AutoReconnect , CredentialManager and DefaultSchema , new APIs SQLCancel/SQLCancelHandle, fixes to APIs like SQLPutData, SQLBindParameter, SQLGetDiagRec, SQLGetDiagField and SQLGetInfo, updated dependencies among others. See the Release Notes for more details. Migration to the SnappyData ODBC Driver will involve installation from the new MSI installer (see docs ), then creating new DSNs that have their keys and values copied from the previous ones. While just changing the DSN driver to SnappyData in the registry will work, it is recommended to create it afresh with the ODBC Setup UI that will allow setting the new options easily.","title":"Upgrading SnappyData"},{"location":"install/upgrade/#upgrade-instructions","text":"This guide provides information for upgrading systems running an earlier version of SnappyData. We assume that you have SnappyData already installed, and you are upgrading to the latest version of SnappyData. Before you begin the upgrade, ensure that you understand the new features and any specific requirements for that release.","title":"Upgrade Instructions"},{"location":"install/upgrade/#before-you-upgrade","text":"Confirm that your system meets the hardware and software requirements described in System Requirements section. Backup the existing environment: Create a backup of the locator, lead, and server configuration files that exist in the conf folder located in the SnappyData home directory. Stop the cluster and verify that all members are stopped: You can shut down the cluster using the sbin/snappy-stop-all.sh command. To ensure that all the members have been shut down correctly, use the sbin/snappy-status-all.sh command. Create a backup of the operational disk store files for all members in the distributed system. Reinstall SnappyData: After you have stopped the cluster, install the latest version of SnappyData . Reconfigure your cluster using the locator, lead, and server configuration files you backed up in step 1. To ensure that the restore script (restore.sh) copies files back to their original locations, make sure that the disk files are available at the original location before restarting the cluster with the latest version of SnappyData.","title":"Before You Upgrade"},{"location":"install/upgrade/#upgrading-to-snappydata-131-from-101-or-earlier-versions","text":"When you upgrade to SnappyData 1.3.1 from product version 1.0.1 or earlier versions, it is recommended to save all the table data as parquet files, recreate the tables in the new cluster, and then load data from the saved parquet files. Before taking the backup ensure that no operations are currently running on the system. Ensure to cleanup the data from the previous cluster and start the new cluster from a clean directory. For example: # Creating parquet files in older product version 1.0.1 or prior: snappy> create external table table1Parquet using parquet options (path '<path-to-parquet-file>') as select * from table1; snappy> drop table table1; snappy> drop table table1Parquet; # Creating tables from parquet files in SnappyData 1.3.1 snappy> create external table table1_parquet using parquet options (path '<path-to-parquet-file>') ; snappy> create table table1(...); snappy> insert into table1 select * from table1_parquet; Use a path for the Parquet file that has enough space to hold the table data. Once the re-import has completed successfully, make sure that the Parquet files are deleted explicitly. Note Upgrade to SnappyData from 1.0.2 or later versions can be done directly.","title":"Upgrading to SnappyData 1.3.1 from 1.0.1 or Earlier Versions"},{"location":"install/upgrade/#migrating-to-the-snappydata-odbc-driver","text":"If you have been using TIBCO ComputeDB\u2122 ODBC Driver from 1.2.0 or earlier releases, then you can install the SnappyData ODBC Driver alongside it without any conflicts. The product name as well as the ODBC Driver name for the two are TIBCO ComputeDB and SnappyData respectively that do not overlap with one another. The older drivers and their DSNs will continue to work against SnappyData 1.3.1 without any changes. While the older driver will continue to work, migrating to the new SnappyData ODBC Driver is highly recommended to avail the new features and bug fixes over the TIBCO ComputeDB\u2122 ODBC Driver releases. This includes new options AutoReconnect , CredentialManager and DefaultSchema , new APIs SQLCancel/SQLCancelHandle, fixes to APIs like SQLPutData, SQLBindParameter, SQLGetDiagRec, SQLGetDiagField and SQLGetInfo, updated dependencies among others. See the Release Notes for more details. Migration to the SnappyData ODBC Driver will involve installation from the new MSI installer (see docs ), then creating new DSNs that have their keys and values copied from the previous ones. While just changing the DSN driver to SnappyData in the registry will work, it is recommended to create it afresh with the ODBC Setup UI that will allow setting the new options easily.","title":"Migrating to the SnappyData ODBC Driver"},{"location":"isight/key_components/","text":"Key Components \u00b6 This section provides a brief description of the key terms used in this document. Amazon Web Services (AWS ): Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3). SnappyData Cluster : A database cluster which has three main components - Locator, Server and Lead Apache Zeppelin : Apache Zeppelin is a web-based notebook that enables interactive data analytics. It allows you to make data-driven, interactive and collaborative documents with SQL queries or directly use the Spark API to process data. Interpreters : A software module which is loaded into Apache Zeppelin upon startup. Interpreters allow various third party products including SnappyData to interact with Apache Zeppelin. The SnappyData interpreter gives users the ability to execute SQL queries or use the data frame API to visualize data.","title":"Key Components"},{"location":"isight/key_components/#key-components","text":"This section provides a brief description of the key terms used in this document. Amazon Web Services (AWS ): Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3). SnappyData Cluster : A database cluster which has three main components - Locator, Server and Lead Apache Zeppelin : Apache Zeppelin is a web-based notebook that enables interactive data analytics. It allows you to make data-driven, interactive and collaborative documents with SQL queries or directly use the Spark API to process data. Interpreters : A software module which is loaded into Apache Zeppelin upon startup. Interpreters allow various third party products including SnappyData to interact with Apache Zeppelin. The SnappyData interpreter gives users the ability to execute SQL queries or use the data frame API to visualize data.","title":"Key Components"},{"location":"isight/quick_start_steps/","text":"Setting Up SnappyData Cloud Cluster \u00b6 To set up the SnappyData Cloud Cluster follow these easy steps that can get you started quickly: Deploying SnappyData Cloud Cluster Deploying SnappyData Cloud Cluster using SnappyData CloudBuilder Deploying SnappyData Cloud Cluster on AWS using scripts Using Apache Zeppelin Using Predefined Notebook Creating your own Notebook Loading Data from AWS S3 Monitoring SnappyData Cloud Cluster This section discusses the steps required for setting up and deploying SnappyData Cloud Cluster on AWS using the SnappyData CloudBuilder and using a script. Prerequisites \u00b6 Before you begin: Ensure that you have an existing AWS account with required permissions to launch EC2 resources with CloudFormation Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins. Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster Deploying SnappyData Cloud Cluster with SnappyData CloudBuilder \u00b6 SnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData Cloud cluster. In this release, the configuration supports launching the cluster on a single EC2 instance. It is recommended that you select an instance type with higher processing power and more memory for this cluster, as it would be running four processes (locator, lead, a data server and an Apache Zeppelin server) on it. This method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started! Step 1: Launch SnappyData CloudBuilder \u00b6 Step 2: Define your Cluster \u00b6 Community Edition users \u00b6 Pick your version Select the Community option. Pick your Instance Select an instance based on the capacity that you require. Enter your email : Provide your email address. Click Generate . The next page is displayed where you can Select the Region and Launch your Cluster . Enterprise Edition users \u00b6 Pick your version Select the Enterprise option. Make Locators & Leads Highly Available? Select HA/Non-HA for the Locators Select HA/Non-HA for the Leads Currently, Amazon Elastic Block Storage (EBS) is provided. Pick total Memory & Disk (GB): Memory : Click and drag the bar to select the required memory. Disk (3x Memory Recommended) : Click and drag the bar to select the required disk size. Recommended Cluster : Select an instance based on the required capacity. Add servers to support high availability? : Select this option to add servers to support high availability. Do your workloads have high query volumes? : Select this option if your workloads have high query volumes. Click the Edit Nodes option to modify the number of nodes. Enter your email address and select the Agree to terms of service check-box. Click Generate . The next page is displayed where you can Select the Region and Launch your Cluster . Step 3: Select the Region and Launch your Cluster \u00b6 On this page, select the AWS region, and then click Launch Cluster to launch your single-node cluster. Note The region you select must match the EC2 Key Pair you created. If you are not already logged into AWS, you are redirected to the AWS sign-in page. On the Select Template page, the URL for the Amazon S3 template is pre-populated. Click Next to continue. On the Specify Details page: Stack name : You can change the stack name. Note The stack name must contain only letters, numbers, dashes and should start with an alpha character. KeyPairName : Enter a name of an existing EC2 KeyPair. This enables SSH access to the cluster. Refer to the Amazon documentation for more information on generating your own EC2 Key Pair . VPCID : From the drop-down list, select default Virtual Private Cloud (VPC) ID of your selected region. Your instances are launched within this VPC. Click Next to continue. Specify the tags (key-value pairs) for resources in your stack or leave the field empty and click Next . On the Review page, verify the details and click Create to create a stack. Note This operation may take a few minutes to complete. The next page lists the existing stacks. Click Refresh to view the updated list and the status of the stack creation. When the cluster has started, the status of the stack changes to CREATE_COMPLETE . Click on the Outputs tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration. Note If the status of the stack displays ROLLBACK_IN_PROGRESS or DELETE_COMPLETE , the stack creation may have failed. Some common problems that might have caused the failure are: Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS. Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the SnappyData CloudBuilder creation steps. Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.* Warning To stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it. For more information, refer to the Apache Zeppelin section or refer to the Apache Zeppelin documentation . Deploying SnappyData Cloud Cluster on AWS using Scripts \u00b6 SnappyData provides a script ( snappy-ec2 ) that allows you to launch and manage SnappyData clusters on Amazon Elastic Compute Cloud (EC2). The scripts are available on GitHub in the snappy-cloud-tools repository and also on the Release page . The package is available in compressed files ( snappydata-ec2- <version> .tar.gz ). Extract the contents to a location on your computer. For more information on setting up the cluster using the EC2 script, refer to Using SnappyData EC2 scripts . Loading Data from AWS S3 \u00b6 SnappyData provides you with predefined buckets which contain datasets. When data is loaded, the table reads from the files available at the specified external location (AWS S3). Note The Amazon S3 buckets and files are private by default. Ensure that you set the permissions required to make the data publicly accessible. Please refer to the documentation provided by Amazon S3 for detailed information on creating a bucket, adding files and setting required permissions. You can also find AWS related information on the AWS homepage, from the Account > Security Credentials > Access Credentials option. Information related to the Bucket Name and Folder Location can be found on the AWS S3 site. If the Secret Access Key contains a slash, it causes the Spark job to fail. When you create the Secret Access Key, ensure that it only contains alpha-numeric characters. When reading or writing CSV/Parquet to and from S3, the ConnectionPoolTimeoutException error may be reported. To avoid this error, in the Spark context, set the value of the fs.s3a.connection.maximum property to a number greater than the possible number of partitions. For example, snc.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.maximum\", \"1000\") To access data from AWS S3, copy the aws-java-sdk- <version_number> and hadoop-aws- <version_number> files (available in the Maven repository), to the jars directory (snappydata- <version_number> -bin/jars) in the SnappyData home directory. To define a table that references the data in AWS S3, create a paragraph in the following format: %sql DROP TABLE IF EXISTS <table_name> ; CREATE EXTERNAL TABLE <table_name> USING parquet OPTIONS(path 's3a://<AWS_ACCESS_KEY_ID>:<AWS_SECRET_ACCESS_KEY>@<bucket_Name>/<folder_name>'); The values are: Property Description/Value <table_name> The name of the table <AWS_SECRET_ACCESS_KEY>:<AWS_ACCESS_KEY_ID> Security credentials used to authenticate and authorize calls that you make to AWS. <bucket_Name> The name of the bucket where the folder is located. Default value: zeppelindemo <folder_name> The folder name where the data is stored. Default value: nytaxifaredata Using Apache Zeppelin \u00b6 Apache Zeppelin provides web-based notebooks for data exploration. A notebook consists of one or more paragraphs, and each paragraph consists of a section each for code and results. Launch Apache Zeppelin from the web browser by accessing the host and port associated with your Apache Zeppelin server. For example, http:// <zeppelin_host> : <port_number> . The welcome page which lists existing notebooks is displayed. SnappyData provides predefined notebooks which are displayed on the home page after you have logged into Apache Zeppelin. For more information, see Using Predefined Notebooks . Refer to the Apache Zeppelin documentation , for more information. Functioning of the SnappyData Interpreter and SnappyData Cluster \u00b6 When you execute a paragraph in the Apache Zeppelin UI, the query is sent to the Apache Zeppelin Server. The query is then received by the SnappyData Interpreter which is running on the Lead node in the cluster. When the query has completed execution, the results are sent from the SnappyData Interpreter (which is running on the Lead node) to the Apache Zeppelin server. Finally, the results are displayed in the Zeppelin UI. Connecting the SnappyData Interpreter to the SnappyData cluster is represented in the below figure. Important Note \u00b6 The %snappydata.* interpreters described in the sections below are no longer preferred due to being unsupported on secure clusters. The standard %jdbc interpreter with support for EXEC SCALA provides equivalent functionality for both secure and insecure clusters. Refer to How to Use Apache Zeppelin with SnappyData for more details. The previous way noted below can still useful for AQP queries with the show-instant-results-first directive as described in the sections below, but it works only for insecure clusters and for all other cases, use of %jdbc interpreter should be preferred. Using the Interpreter \u00b6 SnappyData Interpreter group consists of the interpreters %snappydata.spark and %snappydata.sql . To use an interpreter, add the associated interpreter directive with the format, %<Interpreter_name> at the beginning of a paragraph in your note. In a paragraph, use one of the interpreters, and then enter required commands. Note The SnappyData Interpreter provides a basic auto-completion functionality. Press (Ctrl+.) on the keyboard to view a list of suggestions. It is recommended that you use the SQL interpreter to run queries on the SnappyData cluster, as an out of memory error may be reported with running the Scala interpreter. Each paragraph has its own SnappyData context. When you set a property on one paragraph, the property is applicable only to that paragraph and not to other paragraphs in the notebook. SQL Interpreter \u00b6 The %snappydata.sql code specifies the default SQL interpreter. This interpreter is used to execute SQL queries on SnappyData cluster. Multi-Line Statements \u00b6 Multi-line statements ,as well as multiple statements on the same line, are also supported as long as they are separated by a semicolon. However, only the result of the last query is displayed. SnappyData provides a list of connection-specific SQL properties that can be applied to the paragraph that is executed. In the following example, spark.sql.shuffle.partitions allows you to specify the number of partitions to be used for this query: %sql set spark.sql.shuffle.partitions=6; select medallion,avg(trip_distance) as avgTripDist from nyctaxi group by medallion order by medallion desc limit 100 with error SnappyData Directives in Apache Zeppelin \u00b6 You can execute approximate queries on SnappyData cluster by using the %sql show-instant-results-first directive. In this case, the query is first executed on the sample table and the approximate result is displayed, after which the query is run on the base table. Once the query is complete, the approximate result is replaced with the actual result. In the following example, you can see that the query is first executed on the sample table, and the time required to execute the query is displayed. At the same time, the query is executed on the base table, and the total time required to execute the query on the base table is displayed. %sql show-instant-results-first select avg(trip_time_in_secs/60) tripTime, hour(pickup_datetime), count(*) howManyTrips, absolute_error(tripTime) from nyctaxi where pickup_latitude < 40.767588 and pickup_latitude > 40.749775 and pickup_longitude > -74.001632 and pickup_longitude < -73.974595 and dropoff_latitude > 40.716800 and dropoff_latitude < 40.717776 and dropoff_longitude > -74.017682 and dropoff_longitude < -74.000945 group by hour(pickup_datetime); Note This directive works only for the SQL interpreter and an error may be displayed for the Scala interpreter. Scala Interpreter \u00b6 The %snappydata.spark code specifies the default Scala interpreter. This interpreter is used to write Scala code in the paragraph. SnappyContext is injected in this interpreter and can be accessed using variable snc . Using Predefined Notebooks \u00b6 SnappyData provides predefined notebooks which contains definitions that are stored in a single file. The overview notebook provides a brief introduction to the SnappyData product features. It also provides additional predefined notebooks, Quickstart, Performance, NYC TAXI Analytics and Airline Data Analytics. When you launch Apache Zeppelin in the browser, the welcome page displays the existing notebooks. Open a notebook and run any of the paragraphs to analyze data and view the result. Creating Notebooks - Try it Yourself! \u00b6 Log on to Apache Zeppelin, create a notebook and insert a new paragraph. Bind the interpreter by setting the default interpreter . Use %snappydata.spark for SnappyData interpreter or use %snappydata.sql for SQL interpreter. Download a data set you want to use and create tables as mentioned below. Examples of Queries and Results \u00b6 This section provides you with examples you can use in a paragraph. In this example, you can create tables using an external dataset from AWS S3. In this example, you can execute a query on a base table using the SQL interpreter. It returns the number of rides per week. In this example, you can execute a query on a sample table using the SQL interpreter. It returns the number of rides per week In this example, you are processing data using the SnappyData Scala interpreter. Apache Zeppelin allows you to dynamically create input fields. To create a text input field, use ${fieldname} . In this example, the input forms are, ${taxiin=60} or taxiout > ${taxiout=60} Monitoring the SnappyData Cloud Cluster \u00b6 You can monitor the SnappyData cluster using SnappyData Pulse, which enables you to observe and record the performance and the activities on the SnappyData cluster. It also displays useful information about SnappyData that includes a list of scheduler stages and tasks, summary of tables and memory usage etc. For more information, see SnappyData Pulse . Accessing the Console \u00b6 To access the SnappyData Pulse UI from the Apache Zeppelin notebook: Click on the SnappyData Pulse UI link provided in the paragraph. Once you have logged in, you can start monitoring the SnappyData cluster.","title":"Setting Up SnappyData Cloud Cluster"},{"location":"isight/quick_start_steps/#setting-up-snappydata-cloud-cluster","text":"To set up the SnappyData Cloud Cluster follow these easy steps that can get you started quickly: Deploying SnappyData Cloud Cluster Deploying SnappyData Cloud Cluster using SnappyData CloudBuilder Deploying SnappyData Cloud Cluster on AWS using scripts Using Apache Zeppelin Using Predefined Notebook Creating your own Notebook Loading Data from AWS S3 Monitoring SnappyData Cloud Cluster This section discusses the steps required for setting up and deploying SnappyData Cloud Cluster on AWS using the SnappyData CloudBuilder and using a script.","title":"Setting Up SnappyData Cloud Cluster"},{"location":"isight/quick_start_steps/#prerequisites","text":"Before you begin: Ensure that you have an existing AWS account with required permissions to launch EC2 resources with CloudFormation Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins. Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster","title":"Prerequisites"},{"location":"isight/quick_start_steps/#deploying-snappydata-cloud-cluster-with-snappydata-cloudbuilder","text":"SnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData Cloud cluster. In this release, the configuration supports launching the cluster on a single EC2 instance. It is recommended that you select an instance type with higher processing power and more memory for this cluster, as it would be running four processes (locator, lead, a data server and an Apache Zeppelin server) on it. This method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started!","title":"Deploying SnappyData Cloud Cluster with SnappyData CloudBuilder"},{"location":"isight/quick_start_steps/#step-1-launch-snappydata-cloudbuilder","text":"","title":"Step 1: Launch SnappyData CloudBuilder"},{"location":"isight/quick_start_steps/#step-2-define-your-cluster","text":"","title":"Step 2: Define your Cluster"},{"location":"isight/quick_start_steps/#community-edition-users","text":"Pick your version Select the Community option. Pick your Instance Select an instance based on the capacity that you require. Enter your email : Provide your email address. Click Generate . The next page is displayed where you can Select the Region and Launch your Cluster .","title":"Community Edition users"},{"location":"isight/quick_start_steps/#enterprise-edition-users","text":"Pick your version Select the Enterprise option. Make Locators & Leads Highly Available? Select HA/Non-HA for the Locators Select HA/Non-HA for the Leads Currently, Amazon Elastic Block Storage (EBS) is provided. Pick total Memory & Disk (GB): Memory : Click and drag the bar to select the required memory. Disk (3x Memory Recommended) : Click and drag the bar to select the required disk size. Recommended Cluster : Select an instance based on the required capacity. Add servers to support high availability? : Select this option to add servers to support high availability. Do your workloads have high query volumes? : Select this option if your workloads have high query volumes. Click the Edit Nodes option to modify the number of nodes. Enter your email address and select the Agree to terms of service check-box. Click Generate . The next page is displayed where you can Select the Region and Launch your Cluster .","title":"Enterprise Edition users"},{"location":"isight/quick_start_steps/#step-3-select-the-region-and-launch-your-cluster","text":"On this page, select the AWS region, and then click Launch Cluster to launch your single-node cluster. Note The region you select must match the EC2 Key Pair you created. If you are not already logged into AWS, you are redirected to the AWS sign-in page. On the Select Template page, the URL for the Amazon S3 template is pre-populated. Click Next to continue. On the Specify Details page: Stack name : You can change the stack name. Note The stack name must contain only letters, numbers, dashes and should start with an alpha character. KeyPairName : Enter a name of an existing EC2 KeyPair. This enables SSH access to the cluster. Refer to the Amazon documentation for more information on generating your own EC2 Key Pair . VPCID : From the drop-down list, select default Virtual Private Cloud (VPC) ID of your selected region. Your instances are launched within this VPC. Click Next to continue. Specify the tags (key-value pairs) for resources in your stack or leave the field empty and click Next . On the Review page, verify the details and click Create to create a stack. Note This operation may take a few minutes to complete. The next page lists the existing stacks. Click Refresh to view the updated list and the status of the stack creation. When the cluster has started, the status of the stack changes to CREATE_COMPLETE . Click on the Outputs tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration. Note If the status of the stack displays ROLLBACK_IN_PROGRESS or DELETE_COMPLETE , the stack creation may have failed. Some common problems that might have caused the failure are: Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS. Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the SnappyData CloudBuilder creation steps. Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.* Warning To stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it. For more information, refer to the Apache Zeppelin section or refer to the Apache Zeppelin documentation .","title":"Step 3: Select the Region and Launch your Cluster"},{"location":"isight/quick_start_steps/#deploying-snappydata-cloud-cluster-on-aws-using-scripts","text":"SnappyData provides a script ( snappy-ec2 ) that allows you to launch and manage SnappyData clusters on Amazon Elastic Compute Cloud (EC2). The scripts are available on GitHub in the snappy-cloud-tools repository and also on the Release page . The package is available in compressed files ( snappydata-ec2- <version> .tar.gz ). Extract the contents to a location on your computer. For more information on setting up the cluster using the EC2 script, refer to Using SnappyData EC2 scripts .","title":"Deploying SnappyData Cloud Cluster on AWS using Scripts"},{"location":"isight/quick_start_steps/#loading-data-from-aws-s3","text":"SnappyData provides you with predefined buckets which contain datasets. When data is loaded, the table reads from the files available at the specified external location (AWS S3). Note The Amazon S3 buckets and files are private by default. Ensure that you set the permissions required to make the data publicly accessible. Please refer to the documentation provided by Amazon S3 for detailed information on creating a bucket, adding files and setting required permissions. You can also find AWS related information on the AWS homepage, from the Account > Security Credentials > Access Credentials option. Information related to the Bucket Name and Folder Location can be found on the AWS S3 site. If the Secret Access Key contains a slash, it causes the Spark job to fail. When you create the Secret Access Key, ensure that it only contains alpha-numeric characters. When reading or writing CSV/Parquet to and from S3, the ConnectionPoolTimeoutException error may be reported. To avoid this error, in the Spark context, set the value of the fs.s3a.connection.maximum property to a number greater than the possible number of partitions. For example, snc.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.maximum\", \"1000\") To access data from AWS S3, copy the aws-java-sdk- <version_number> and hadoop-aws- <version_number> files (available in the Maven repository), to the jars directory (snappydata- <version_number> -bin/jars) in the SnappyData home directory. To define a table that references the data in AWS S3, create a paragraph in the following format: %sql DROP TABLE IF EXISTS <table_name> ; CREATE EXTERNAL TABLE <table_name> USING parquet OPTIONS(path 's3a://<AWS_ACCESS_KEY_ID>:<AWS_SECRET_ACCESS_KEY>@<bucket_Name>/<folder_name>'); The values are: Property Description/Value <table_name> The name of the table <AWS_SECRET_ACCESS_KEY>:<AWS_ACCESS_KEY_ID> Security credentials used to authenticate and authorize calls that you make to AWS. <bucket_Name> The name of the bucket where the folder is located. Default value: zeppelindemo <folder_name> The folder name where the data is stored. Default value: nytaxifaredata","title":"Loading Data from AWS S3"},{"location":"isight/quick_start_steps/#using-apache-zeppelin","text":"Apache Zeppelin provides web-based notebooks for data exploration. A notebook consists of one or more paragraphs, and each paragraph consists of a section each for code and results. Launch Apache Zeppelin from the web browser by accessing the host and port associated with your Apache Zeppelin server. For example, http:// <zeppelin_host> : <port_number> . The welcome page which lists existing notebooks is displayed. SnappyData provides predefined notebooks which are displayed on the home page after you have logged into Apache Zeppelin. For more information, see Using Predefined Notebooks . Refer to the Apache Zeppelin documentation , for more information.","title":"Using Apache Zeppelin"},{"location":"isight/quick_start_steps/#functioning-of-the-snappydata-interpreter-and-snappydata-cluster","text":"When you execute a paragraph in the Apache Zeppelin UI, the query is sent to the Apache Zeppelin Server. The query is then received by the SnappyData Interpreter which is running on the Lead node in the cluster. When the query has completed execution, the results are sent from the SnappyData Interpreter (which is running on the Lead node) to the Apache Zeppelin server. Finally, the results are displayed in the Zeppelin UI. Connecting the SnappyData Interpreter to the SnappyData cluster is represented in the below figure.","title":"Functioning of the SnappyData Interpreter and SnappyData Cluster"},{"location":"isight/quick_start_steps/#important-note","text":"The %snappydata.* interpreters described in the sections below are no longer preferred due to being unsupported on secure clusters. The standard %jdbc interpreter with support for EXEC SCALA provides equivalent functionality for both secure and insecure clusters. Refer to How to Use Apache Zeppelin with SnappyData for more details. The previous way noted below can still useful for AQP queries with the show-instant-results-first directive as described in the sections below, but it works only for insecure clusters and for all other cases, use of %jdbc interpreter should be preferred.","title":"Important Note"},{"location":"isight/quick_start_steps/#using-the-interpreter","text":"SnappyData Interpreter group consists of the interpreters %snappydata.spark and %snappydata.sql . To use an interpreter, add the associated interpreter directive with the format, %<Interpreter_name> at the beginning of a paragraph in your note. In a paragraph, use one of the interpreters, and then enter required commands. Note The SnappyData Interpreter provides a basic auto-completion functionality. Press (Ctrl+.) on the keyboard to view a list of suggestions. It is recommended that you use the SQL interpreter to run queries on the SnappyData cluster, as an out of memory error may be reported with running the Scala interpreter. Each paragraph has its own SnappyData context. When you set a property on one paragraph, the property is applicable only to that paragraph and not to other paragraphs in the notebook.","title":"Using the Interpreter"},{"location":"isight/quick_start_steps/#sql-interpreter","text":"The %snappydata.sql code specifies the default SQL interpreter. This interpreter is used to execute SQL queries on SnappyData cluster.","title":"SQL Interpreter"},{"location":"isight/quick_start_steps/#multi-line-statements","text":"Multi-line statements ,as well as multiple statements on the same line, are also supported as long as they are separated by a semicolon. However, only the result of the last query is displayed. SnappyData provides a list of connection-specific SQL properties that can be applied to the paragraph that is executed. In the following example, spark.sql.shuffle.partitions allows you to specify the number of partitions to be used for this query: %sql set spark.sql.shuffle.partitions=6; select medallion,avg(trip_distance) as avgTripDist from nyctaxi group by medallion order by medallion desc limit 100 with error","title":"Multi-Line Statements"},{"location":"isight/quick_start_steps/#snappydata-directives-in-apache-zeppelin","text":"You can execute approximate queries on SnappyData cluster by using the %sql show-instant-results-first directive. In this case, the query is first executed on the sample table and the approximate result is displayed, after which the query is run on the base table. Once the query is complete, the approximate result is replaced with the actual result. In the following example, you can see that the query is first executed on the sample table, and the time required to execute the query is displayed. At the same time, the query is executed on the base table, and the total time required to execute the query on the base table is displayed. %sql show-instant-results-first select avg(trip_time_in_secs/60) tripTime, hour(pickup_datetime), count(*) howManyTrips, absolute_error(tripTime) from nyctaxi where pickup_latitude < 40.767588 and pickup_latitude > 40.749775 and pickup_longitude > -74.001632 and pickup_longitude < -73.974595 and dropoff_latitude > 40.716800 and dropoff_latitude < 40.717776 and dropoff_longitude > -74.017682 and dropoff_longitude < -74.000945 group by hour(pickup_datetime); Note This directive works only for the SQL interpreter and an error may be displayed for the Scala interpreter.","title":"SnappyData Directives in Apache Zeppelin"},{"location":"isight/quick_start_steps/#scala-interpreter","text":"The %snappydata.spark code specifies the default Scala interpreter. This interpreter is used to write Scala code in the paragraph. SnappyContext is injected in this interpreter and can be accessed using variable snc .","title":"Scala Interpreter"},{"location":"isight/quick_start_steps/#using-predefined-notebooks","text":"SnappyData provides predefined notebooks which contains definitions that are stored in a single file. The overview notebook provides a brief introduction to the SnappyData product features. It also provides additional predefined notebooks, Quickstart, Performance, NYC TAXI Analytics and Airline Data Analytics. When you launch Apache Zeppelin in the browser, the welcome page displays the existing notebooks. Open a notebook and run any of the paragraphs to analyze data and view the result.","title":"Using Predefined Notebooks"},{"location":"isight/quick_start_steps/#creating-notebooks-try-it-yourself","text":"Log on to Apache Zeppelin, create a notebook and insert a new paragraph. Bind the interpreter by setting the default interpreter . Use %snappydata.spark for SnappyData interpreter or use %snappydata.sql for SQL interpreter. Download a data set you want to use and create tables as mentioned below.","title":"Creating Notebooks - Try it Yourself!"},{"location":"isight/quick_start_steps/#examples-of-queries-and-results","text":"This section provides you with examples you can use in a paragraph. In this example, you can create tables using an external dataset from AWS S3. In this example, you can execute a query on a base table using the SQL interpreter. It returns the number of rides per week. In this example, you can execute a query on a sample table using the SQL interpreter. It returns the number of rides per week In this example, you are processing data using the SnappyData Scala interpreter. Apache Zeppelin allows you to dynamically create input fields. To create a text input field, use ${fieldname} . In this example, the input forms are, ${taxiin=60} or taxiout > ${taxiout=60}","title":"Examples of Queries and Results"},{"location":"isight/quick_start_steps/#monitoring-the-snappydata-cloud-cluster","text":"You can monitor the SnappyData cluster using SnappyData Pulse, which enables you to observe and record the performance and the activities on the SnappyData cluster. It also displays useful information about SnappyData that includes a list of scheduler stages and tasks, summary of tables and memory usage etc. For more information, see SnappyData Pulse .","title":"Monitoring the SnappyData Cloud Cluster"},{"location":"isight/quick_start_steps/#accessing-the-console","text":"To access the SnappyData Pulse UI from the Apache Zeppelin notebook: Click on the SnappyData Pulse UI link provided in the paragraph. Once you have logged in, you can start monitoring the SnappyData cluster.","title":"Accessing the Console"},{"location":"isight/the_technology_powering_isight_cloud/","text":"The Technology Powering SnappyData CloudBuilder \u00b6 SnappyData CloudBuilder uses the SnappyData Synopsis Engine to deliver blazing fast responses to queries that have long processing times. Analytic queries typically aim to provide aggregate information and involve full table or partial table scans. The cost of these queries is directly proportional to the amount of data that needs to be scanned. Analytics queries also often involve distributed joins of a dimension table with one or more fact tables. The cost of pruning these queries down to the final result is directly proportional to the size of the data involved. Distributed joins involve lots of data movement making such queries extremely expensive in traditional systems that process the entire data set. The Synopsis Data Engine offers a breakthrough solution to these problems by building out stratified samples of the most common columns used in queries, as well as other probabilistic data structures like count-min-sketch, bloom filters etc. The use of these structures, along with extensions to the querying engine allow users to get almost-perfect answers to complex queries in a fraction of the time it used to take to answer these queries. For more information on SDE and sampling techniques used by SnappyData, refer to the SDE documentation .","title":"The Technology Powering <!--iSight Cloud-->SnappyData CloudBuilder"},{"location":"isight/the_technology_powering_isight_cloud/#the-technology-powering-snappydata-cloudbuilder","text":"SnappyData CloudBuilder uses the SnappyData Synopsis Engine to deliver blazing fast responses to queries that have long processing times. Analytic queries typically aim to provide aggregate information and involve full table or partial table scans. The cost of these queries is directly proportional to the amount of data that needs to be scanned. Analytics queries also often involve distributed joins of a dimension table with one or more fact tables. The cost of pruning these queries down to the final result is directly proportional to the size of the data involved. Distributed joins involve lots of data movement making such queries extremely expensive in traditional systems that process the entire data set. The Synopsis Data Engine offers a breakthrough solution to these problems by building out stratified samples of the most common columns used in queries, as well as other probabilistic data structures like count-min-sketch, bloom filters etc. The use of these structures, along with extensions to the querying engine allow users to get almost-perfect answers to complex queries in a fraction of the time it used to take to answer these queries. For more information on SDE and sampling techniques used by SnappyData, refer to the SDE documentation .","title":"The Technology Powering SnappyData CloudBuilder"},{"location":"migration/migration-0.8-0.9/","text":"Migrating from Version 0.8 to Version 0.9 \u00b6 Note Upgrade of on-disk data files is not supported for this release. This document only contains instructions for users migrating from SnappyData 0.8 to SnappyData 0.9. After you have re-configured your cluster, you must reload your data into SnappyData tables. Memory Management: Heap and Off-Heap \u00b6 SnappyData can now be configured to use both off-heap and on-heap storage. The memory-size and heap-size properties control the off-heap and on-heap sizes of the SnappyData server process. Row tables are always stored on on-heap storage. You can now configure column tables to use off-heap storage. Off-heap storage is also recommended for production environments. Several artifacts in the product, however, require on-heap memory, and therefore minimum heap size is also required in such cases. For example: To use row tables: According to the row table size requirements, configure the heap size. Currently, row tables in SnappyData do not use off-heap memory. To read-write Parquet and CSV: Parquet and CSV read-write are memory consuming activities, and still, use heap memory. Ensure that you provide sufficient heap memory in such cases. When most of your data reside in column tables, use off-heap memory. They are faster and put less pressure on garbage collection threads. The following properties have been added for memory management: Properties Description memory-size The total memory that can be used by the node for column storage and execution in off-heap. The default value is 0 (OFF_HEAP is not used by default). critical-heap-percentage The portion of memory that is reserved for system use, unaccounted on-the-fly objects, JVM, GC overhead etc. The default value is 90% and can be increased to 95% or similar for large heaps (>20G). This is only applicable to heap-size accounting. Off-heap configuration with memory-size uses the entire available memory and does not have any reserved area. spark.memory.storageFraction The fraction of total storage memory that is immune to eviction. Execution can never grow into this portion of memory. If set to a higher value, less working memory may be available for execution, and tasks may overflow to disk. It is recommended that you do not modify the default setting. spark.memory.storageMaxFraction Specifies how much off-heap memory can be consumed by storage. The default is 0.95 of the total memory-size (off-heap size). Beyond this, all data in storage gets evicted to disk. The split between execution and storage is governed by the spark.memory.storageFraction property, but storage grows into execution space up to this limit if space is available in the execution area. However, if execution requires space, then it evicts storage to the spark.memory.storageFraction limit. Normally you do not need to modify this property even if queries are expected to take lots of execution space. It is better to use the spark.memory.storageFraction property to control the split between storage and execution. Tables Persistent To Disk By Default \u00b6 In the previous releases (0.8 and earlier), tables were stored in memory by default, and users had to configure the persistence clause to store data on disk. From this release onwards, all tables persist to disk by default and can be explicitly turned OFF for pure memory-only tables by specifying the persistence option as none . Changes to Properties \u00b6 In this release, the following changes have been made to the properties * Memory management New Properties Deleted Properties . Make sure that you familiarise yourself with changes before you re-configure your cluster. New Property \u00b6 Property Description snappydata.connection This property points to thrift network server running on a locator or a data server of a running SnappyData cluster. The value of this property is a combination of locator or server and JDBC client port on which the thrift network server listens for connections (The port that is specified by the client-port property and defaults to 1527 or the next available port on the locator/server). Deleted Property \u00b6 Property Description snappydata.store.locators Instructs the connector to acquire cluster connectivity, catalog metadata and registers it locally in the Spark cluster. This property has been deleted and replaced with `snappydata.connection .","title":"Migrating from Version 0.8 to Version 0.9"},{"location":"migration/migration-0.8-0.9/#migrating-from-version-08-to-version-09","text":"Note Upgrade of on-disk data files is not supported for this release. This document only contains instructions for users migrating from SnappyData 0.8 to SnappyData 0.9. After you have re-configured your cluster, you must reload your data into SnappyData tables.","title":"Migrating from Version 0.8 to Version 0.9"},{"location":"migration/migration-0.8-0.9/#memory-management-heap-and-off-heap","text":"SnappyData can now be configured to use both off-heap and on-heap storage. The memory-size and heap-size properties control the off-heap and on-heap sizes of the SnappyData server process. Row tables are always stored on on-heap storage. You can now configure column tables to use off-heap storage. Off-heap storage is also recommended for production environments. Several artifacts in the product, however, require on-heap memory, and therefore minimum heap size is also required in such cases. For example: To use row tables: According to the row table size requirements, configure the heap size. Currently, row tables in SnappyData do not use off-heap memory. To read-write Parquet and CSV: Parquet and CSV read-write are memory consuming activities, and still, use heap memory. Ensure that you provide sufficient heap memory in such cases. When most of your data reside in column tables, use off-heap memory. They are faster and put less pressure on garbage collection threads. The following properties have been added for memory management: Properties Description memory-size The total memory that can be used by the node for column storage and execution in off-heap. The default value is 0 (OFF_HEAP is not used by default). critical-heap-percentage The portion of memory that is reserved for system use, unaccounted on-the-fly objects, JVM, GC overhead etc. The default value is 90% and can be increased to 95% or similar for large heaps (>20G). This is only applicable to heap-size accounting. Off-heap configuration with memory-size uses the entire available memory and does not have any reserved area. spark.memory.storageFraction The fraction of total storage memory that is immune to eviction. Execution can never grow into this portion of memory. If set to a higher value, less working memory may be available for execution, and tasks may overflow to disk. It is recommended that you do not modify the default setting. spark.memory.storageMaxFraction Specifies how much off-heap memory can be consumed by storage. The default is 0.95 of the total memory-size (off-heap size). Beyond this, all data in storage gets evicted to disk. The split between execution and storage is governed by the spark.memory.storageFraction property, but storage grows into execution space up to this limit if space is available in the execution area. However, if execution requires space, then it evicts storage to the spark.memory.storageFraction limit. Normally you do not need to modify this property even if queries are expected to take lots of execution space. It is better to use the spark.memory.storageFraction property to control the split between storage and execution.","title":"Memory Management: Heap and Off-Heap"},{"location":"migration/migration-0.8-0.9/#tables-persistent-to-disk-by-default","text":"In the previous releases (0.8 and earlier), tables were stored in memory by default, and users had to configure the persistence clause to store data on disk. From this release onwards, all tables persist to disk by default and can be explicitly turned OFF for pure memory-only tables by specifying the persistence option as none .","title":"Tables Persistent To Disk By Default"},{"location":"migration/migration-0.8-0.9/#changes-to-properties","text":"In this release, the following changes have been made to the properties * Memory management New Properties Deleted Properties . Make sure that you familiarise yourself with changes before you re-configure your cluster.","title":"Changes to Properties"},{"location":"migration/migration-0.8-0.9/#new-property","text":"Property Description snappydata.connection This property points to thrift network server running on a locator or a data server of a running SnappyData cluster. The value of this property is a combination of locator or server and JDBC client port on which the thrift network server listens for connections (The port that is specified by the client-port property and defaults to 1527 or the next available port on the locator/server).","title":"New Property"},{"location":"migration/migration-0.8-0.9/#deleted-property","text":"Property Description snappydata.store.locators Instructs the connector to acquire cluster connectivity, catalog metadata and registers it locally in the Spark cluster. This property has been deleted and replaced with `snappydata.connection .","title":"Deleted Property"},{"location":"migration/migration-0.9-01_rc/","text":"Instruction for Migrating from SnappyData 0.8-1.0.0-RC1.1 Release \u00b6 Changes to Properties \u00b6 In this release, the following changes have been made to properties: New Properties Updated Properties Deleted Properties Make sure that you familiarise yourself with changes before you re-configure your cluster. New Properties \u00b6 Property Description Updated Properties \u00b6 Property Description Deleted Properties \u00b6 Property Description","title":"Instruction for Migrating from SnappyData 0.8-1.0.0-RC1.1 Release"},{"location":"migration/migration-0.9-01_rc/#instruction-for-migrating-from-snappydata-08-100-rc11-release","text":"","title":"Instruction for Migrating from SnappyData 0.8-1.0.0-RC1.1 Release"},{"location":"migration/migration-0.9-01_rc/#changes-to-properties","text":"In this release, the following changes have been made to properties: New Properties Updated Properties Deleted Properties Make sure that you familiarise yourself with changes before you re-configure your cluster.","title":"Changes to Properties"},{"location":"migration/migration-0.9-01_rc/#new-properties","text":"Property Description","title":"New Properties"},{"location":"migration/migration-0.9-01_rc/#updated-properties","text":"Property Description","title":"Updated Properties"},{"location":"migration/migration-0.9-01_rc/#deleted-properties","text":"Property Description","title":"Deleted Properties"},{"location":"migration/migration/","text":"Migration Guide \u00b6 This guide provides information related to the migration of systems running an earlier version of SnappyData to the latest version of SnappyData. We assume that you have SnappyData already installed, and you are migrating to the latest version of SnappyData. Before you begin migrating, ensure that you understand the new features and any specific requirements for that release. For more information see, Migrating from SnappyData version 0.8 to version 0.9 Before you begin migration : Backup the existing environment: Make sure you create a backup of the locator, lead, and server configuration files that exist in the conf folder located in the SnappyData home directory. Stop the cluster and verify that all members are stopped: You can shutdown the cluster using the sbin/snappy-stop-all.sh command. To ensure that all the members have been shut down correctly, use the sbin/snappy-status-all.sh command. Re-install SnappyData: After you have stopped the cluster, install the latest version of SnappyData . Reconfigure your cluster.","title":"Migration Guide"},{"location":"migration/migration/#migration-guide","text":"This guide provides information related to the migration of systems running an earlier version of SnappyData to the latest version of SnappyData. We assume that you have SnappyData already installed, and you are migrating to the latest version of SnappyData. Before you begin migrating, ensure that you understand the new features and any specific requirements for that release. For more information see, Migrating from SnappyData version 0.8 to version 0.9 Before you begin migration : Backup the existing environment: Make sure you create a backup of the locator, lead, and server configuration files that exist in the conf folder located in the SnappyData home directory. Stop the cluster and verify that all members are stopped: You can shutdown the cluster using the sbin/snappy-stop-all.sh command. To ensure that all the members have been shut down correctly, use the sbin/snappy-status-all.sh command. Re-install SnappyData: After you have stopped the cluster, install the latest version of SnappyData . Reconfigure your cluster.","title":"Migration Guide"},{"location":"migration/upgrade_gemfirexd_standalone/","text":"Manually Upgrading from GemFire XD 1.4.x to SnappyData RowStore 1.0.0 \u00b6 In this document, you can find information on how to manually upgrade GemFire XD 1.4.x to SnappyData 1.0.0/RowStore 1.6. It is a step-by-step guide for configuring and customizing your system. This guide assumes you have a basic understanding of your Linux system. Prerequisites Upgrade Process Prerequisites \u00b6 Download the distribution file from the SnappyData Release page (snappydata-1.0.0) Ensure that before moving into production, you have tested your development systems thoroughly with the new version Upgrade Process \u00b6 Pre-Upgrade Tasks Downloading and Extracting Product Distribution Starting the Upgrade Verifying Upgrade is Successful Step1: Pre-Upgrade Tasks \u00b6 Shutting the System \u00b6 Use the shut-down-all and locator stop commands to stop all members. For example: $ gfxd shut-down-all -locators=localhost[10101] $ gfxd locator stop -dir=$HOME/locator Backing Up all Data Directories \u00b6 In this step, you create a backup of all the data directories. For example: $cp -R node1-server /backup/snappydata/ $cp -R node2-server /backup/snappydata/ Here node1-server and node2-server are the data directories of GemFire XD servers. Step 2: Downloading and Extracting Product Distribution \u00b6 Download the distribution binary file (snappydata-1.0.0/RowStore1.6.0) from the SnappyData Release page (if not done already), and extract the contents of the file to a suitable location on your computer. For example for Linux: $ unzip snappydata-1.0.0-bin.zip -d <path_to_product> Here the is the directory where you want to install the product. Repeat this step to install RowStore on each different computer where you want to run a SnappyData RowStore member. Alternatively, SnappyData RowStore can also be installed on an NFS location accessible to all members. If the PATH variable is set to the path of GemFire XD, ensure that you update it to the snappydata-1.0.0-bin . Add the SnappyData RowStore bin and sbin directories to your path, as described below: $ export PATH=$PATH:/path_to_product/snappydata-1.0.0-bin/bin:/path_to_product/snappydata-1.0.0-bin/sbin Step 3: Starting the Upgrade \u00b6 Setting up Passwordless SSH Creating configuration files for SnappyData servers and locators Starting the SnappyData RowStore cluster Setting up Passwordless SSH \u00b6 SnappyData provides utility scripts to start/stop cluster (located in the sbin directory of the product distribution). These scripts use SSH to execute commands on different machines on which SnappyData servers are to be started/stopped. It is recommended that you setup passwordless SSH on the system where these scripts are executed. For more information see Configuring SSH Login without Password . Creating configuration files for SnappyData servers and locators \u00b6 In this step, you create configuration files for servers and locators and then use those to start the SnappyData RowStore cluster (locators and servers) using the backed up data directories. If the NFS location is not accessible to all members, you need to follow the process of each machine. If passwordless SSH is set up, the cluster can be started from one host. These configuration files contain hostname of the node where a locator/server is to be started along with startup properties for it. To configure the locators: In the /snappydata-1.0.0-bin/conf directory, make a copy of the locators.template , and rename it to locators . Edit the locator file to set it to the previous location of the GemFire XD locator (using the -dir option) localhost -dir=<log-dir>/snappydata/locator If there are two locators running on different nodes, add the line twice to set the locator directories: node1 -dir=<log-dir>/snappydata/node1-locator node2 -dir=<log-dir>/snappydata/node2-locator To configure the servers: In /snappydata-1.0.0-bin/conf , make a copy of the servers.template file, and rename it to servers . Edit the servers file to set it to the previous location of the GemFire XD server (using the -dir option). You can use -server-group option to configure server groups. localhost -dir=<log-dir>/snappydata/server -server-groups=cache If there are two servers running on different nodes, add the line twice the set the server directories: node1 -dir=<log-dir>/snappydata/node1-server -server-groups=cache node2 -dir=<log-dir>/snappydata/node2-server -server-groups=cache Note The -heap-size property replaces GemFire XD -initial-heap and/or -max-heap properties. SnappyData no longer supports the -initial-heap and -max-heap properties . In GemFire XD, if you are using a properties file ( gemfirexd.properties ) for configuring various properties such as authentication, please rename the file to snappydata.properties . Also, the property prefix has been changed to snappydata instead of gemfirexd . Alternatively, you may add these properties to above-mentioned conf/locators and conf/servers files in front of each hostname. Starting the SnappyData RowStore Cluster \u00b6 After creating the configuration files start SnappyData RowStore using the following command: snappy-start-all rowstore The above command starts the SnappyData servers and locators with properties as mentioned in the conf/servers and conf/locators files. The rowstore option instructs SnappyData not to start the lead node, which is used when SnappyData is used for SnappyData specific extensions (like column store, running spark jobs etc.) Note Ensure that the distributed system is running by executing the command snappy-status-all . For example: $snappy-status-all SnappyData Locator pid: 13750 status: running SnappyData Server pid: 13897 status: running SnappyData Server pid: 13905 status: running Distributed system now has 2 members. Other members: localhost(13750:locator)<v0>:10811 SnappyData Leader pid: 0 status: stopped Step 4: Verifying Upgrade is Successful \u00b6 The system is started and you may fire queries on the snappy-shell to check the status of the upgrade. The gfxd shell is replaced by the snappy-sql command in SnappyData. For example: snappy-sql rowstore >gfxd version 1.6.0 gfxd> connect client '127.0.0.1:1527'; Using CONNECTION0 snappy> show tables; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE |REMARKS ------------------------------------------------------------------------------------- SYS |ASYNCEVENTLISTENERS |SYSTEM TABLE| SYS |GATEWAYRECEIVERS |SYSTEM TABLE| SYS |GATEWAYSENDERS |SYSTEM TABLE| SYS |SYSALIASES |SYSTEM TABLE| SYS |SYSCHECKS |SYSTEM TABLE| SYS |SYSCOLPERMS |SYSTEM TABLE| SYS |SYSCOLUMNS |SYSTEM TABLE| SYS |SYSCONGLOMERATES |SYSTEM TABLE| SYS |SYSCONSTRAINTS |SYSTEM TABLE| SYS |SYSDEPENDS |SYSTEM TABLE| SYS |SYSDISKSTORES |SYSTEM TABLE| SYS |SYSFILES |SYSTEM TABLE| SYS |SYSFOREIGNKEYS |SYSTEM TABLE| SYS |SYSHDFSSTORES |SYSTEM TABLE| SYS |SYSKEYS |SYSTEM TABLE| SYS |SYSROLES |SYSTEM TABLE| SYS |SYSROUTINEPERMS |SYSTEM TABLE| SYS |SYSSCHEMAS |SYSTEM TABLE| SYS |SYSSTATEMENTS |SYSTEM TABLE| SYS |SYSSTATISTICS |SYSTEM TABLE| SYS |SYSTABLEPERMS |SYSTEM TABLE| SYS |SYSTABLES |SYSTEM TABLE| SYS |SYSTRIGGERS |SYSTEM TABLE| SYS |SYSVIEWS |SYSTEM TABLE| SYSIBM |SYSDUMMY1 |SYSTEM TABLE| You can execute show members query to see the running members: snappy> show members; The show members command displays all members of the cluster (such as locators, datastores) in its output. You can execute queries on the tables (such as 'select count(*) from table') to verify the contents. Post-Upgrade Tasks \u00b6 After you have upgraded, perform these tasks: The JDBC connection string prefix has changed from jdbc:gemfirexd to jdbc:snappydata (the old jdbc:gemfirexd prefix works for backward compatibility reasons). Update any custom or third-party JDBC clients to use the correct connection string format. Run all required applications to test that your development system is functioning correctly with the upgraded version of SnappyData.","title":"Manually Upgrading from GemFire XD 1.4.x to SnappyData RowStore 1.0.0"},{"location":"migration/upgrade_gemfirexd_standalone/#manually-upgrading-from-gemfire-xd-14x-to-snappydata-rowstore-100","text":"In this document, you can find information on how to manually upgrade GemFire XD 1.4.x to SnappyData 1.0.0/RowStore 1.6. It is a step-by-step guide for configuring and customizing your system. This guide assumes you have a basic understanding of your Linux system. Prerequisites Upgrade Process","title":"Manually Upgrading from GemFire XD 1.4.x to SnappyData RowStore 1.0.0"},{"location":"migration/upgrade_gemfirexd_standalone/#prerequisites","text":"Download the distribution file from the SnappyData Release page (snappydata-1.0.0) Ensure that before moving into production, you have tested your development systems thoroughly with the new version","title":"Prerequisites"},{"location":"migration/upgrade_gemfirexd_standalone/#upgrade-process","text":"Pre-Upgrade Tasks Downloading and Extracting Product Distribution Starting the Upgrade Verifying Upgrade is Successful","title":"Upgrade Process"},{"location":"migration/upgrade_gemfirexd_standalone/#step1-pre-upgrade-tasks","text":"","title":"Step1: Pre-Upgrade Tasks"},{"location":"migration/upgrade_gemfirexd_standalone/#shutting-the-system","text":"Use the shut-down-all and locator stop commands to stop all members. For example: $ gfxd shut-down-all -locators=localhost[10101] $ gfxd locator stop -dir=$HOME/locator","title":"Shutting the System"},{"location":"migration/upgrade_gemfirexd_standalone/#backing-up-all-data-directories","text":"In this step, you create a backup of all the data directories. For example: $cp -R node1-server /backup/snappydata/ $cp -R node2-server /backup/snappydata/ Here node1-server and node2-server are the data directories of GemFire XD servers.","title":"Backing Up all Data Directories"},{"location":"migration/upgrade_gemfirexd_standalone/#step-2-downloading-and-extracting-product-distribution","text":"Download the distribution binary file (snappydata-1.0.0/RowStore1.6.0) from the SnappyData Release page (if not done already), and extract the contents of the file to a suitable location on your computer. For example for Linux: $ unzip snappydata-1.0.0-bin.zip -d <path_to_product> Here the is the directory where you want to install the product. Repeat this step to install RowStore on each different computer where you want to run a SnappyData RowStore member. Alternatively, SnappyData RowStore can also be installed on an NFS location accessible to all members. If the PATH variable is set to the path of GemFire XD, ensure that you update it to the snappydata-1.0.0-bin . Add the SnappyData RowStore bin and sbin directories to your path, as described below: $ export PATH=$PATH:/path_to_product/snappydata-1.0.0-bin/bin:/path_to_product/snappydata-1.0.0-bin/sbin","title":"Step 2: Downloading and Extracting Product Distribution"},{"location":"migration/upgrade_gemfirexd_standalone/#step-3-starting-the-upgrade","text":"Setting up Passwordless SSH Creating configuration files for SnappyData servers and locators Starting the SnappyData RowStore cluster","title":"Step 3: Starting the Upgrade"},{"location":"migration/upgrade_gemfirexd_standalone/#setting-up-passwordless-ssh","text":"SnappyData provides utility scripts to start/stop cluster (located in the sbin directory of the product distribution). These scripts use SSH to execute commands on different machines on which SnappyData servers are to be started/stopped. It is recommended that you setup passwordless SSH on the system where these scripts are executed. For more information see Configuring SSH Login without Password .","title":"Setting up Passwordless SSH"},{"location":"migration/upgrade_gemfirexd_standalone/#creating-configuration-files-for-snappydata-servers-and-locators","text":"In this step, you create configuration files for servers and locators and then use those to start the SnappyData RowStore cluster (locators and servers) using the backed up data directories. If the NFS location is not accessible to all members, you need to follow the process of each machine. If passwordless SSH is set up, the cluster can be started from one host. These configuration files contain hostname of the node where a locator/server is to be started along with startup properties for it. To configure the locators: In the /snappydata-1.0.0-bin/conf directory, make a copy of the locators.template , and rename it to locators . Edit the locator file to set it to the previous location of the GemFire XD locator (using the -dir option) localhost -dir=<log-dir>/snappydata/locator If there are two locators running on different nodes, add the line twice to set the locator directories: node1 -dir=<log-dir>/snappydata/node1-locator node2 -dir=<log-dir>/snappydata/node2-locator To configure the servers: In /snappydata-1.0.0-bin/conf , make a copy of the servers.template file, and rename it to servers . Edit the servers file to set it to the previous location of the GemFire XD server (using the -dir option). You can use -server-group option to configure server groups. localhost -dir=<log-dir>/snappydata/server -server-groups=cache If there are two servers running on different nodes, add the line twice the set the server directories: node1 -dir=<log-dir>/snappydata/node1-server -server-groups=cache node2 -dir=<log-dir>/snappydata/node2-server -server-groups=cache Note The -heap-size property replaces GemFire XD -initial-heap and/or -max-heap properties. SnappyData no longer supports the -initial-heap and -max-heap properties . In GemFire XD, if you are using a properties file ( gemfirexd.properties ) for configuring various properties such as authentication, please rename the file to snappydata.properties . Also, the property prefix has been changed to snappydata instead of gemfirexd . Alternatively, you may add these properties to above-mentioned conf/locators and conf/servers files in front of each hostname.","title":"Creating configuration files for SnappyData servers and locators"},{"location":"migration/upgrade_gemfirexd_standalone/#starting-the-snappydata-rowstore-cluster","text":"After creating the configuration files start SnappyData RowStore using the following command: snappy-start-all rowstore The above command starts the SnappyData servers and locators with properties as mentioned in the conf/servers and conf/locators files. The rowstore option instructs SnappyData not to start the lead node, which is used when SnappyData is used for SnappyData specific extensions (like column store, running spark jobs etc.) Note Ensure that the distributed system is running by executing the command snappy-status-all . For example: $snappy-status-all SnappyData Locator pid: 13750 status: running SnappyData Server pid: 13897 status: running SnappyData Server pid: 13905 status: running Distributed system now has 2 members. Other members: localhost(13750:locator)<v0>:10811 SnappyData Leader pid: 0 status: stopped","title":"Starting the SnappyData RowStore Cluster"},{"location":"migration/upgrade_gemfirexd_standalone/#step-4-verifying-upgrade-is-successful","text":"The system is started and you may fire queries on the snappy-shell to check the status of the upgrade. The gfxd shell is replaced by the snappy-sql command in SnappyData. For example: snappy-sql rowstore >gfxd version 1.6.0 gfxd> connect client '127.0.0.1:1527'; Using CONNECTION0 snappy> show tables; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE |REMARKS ------------------------------------------------------------------------------------- SYS |ASYNCEVENTLISTENERS |SYSTEM TABLE| SYS |GATEWAYRECEIVERS |SYSTEM TABLE| SYS |GATEWAYSENDERS |SYSTEM TABLE| SYS |SYSALIASES |SYSTEM TABLE| SYS |SYSCHECKS |SYSTEM TABLE| SYS |SYSCOLPERMS |SYSTEM TABLE| SYS |SYSCOLUMNS |SYSTEM TABLE| SYS |SYSCONGLOMERATES |SYSTEM TABLE| SYS |SYSCONSTRAINTS |SYSTEM TABLE| SYS |SYSDEPENDS |SYSTEM TABLE| SYS |SYSDISKSTORES |SYSTEM TABLE| SYS |SYSFILES |SYSTEM TABLE| SYS |SYSFOREIGNKEYS |SYSTEM TABLE| SYS |SYSHDFSSTORES |SYSTEM TABLE| SYS |SYSKEYS |SYSTEM TABLE| SYS |SYSROLES |SYSTEM TABLE| SYS |SYSROUTINEPERMS |SYSTEM TABLE| SYS |SYSSCHEMAS |SYSTEM TABLE| SYS |SYSSTATEMENTS |SYSTEM TABLE| SYS |SYSSTATISTICS |SYSTEM TABLE| SYS |SYSTABLEPERMS |SYSTEM TABLE| SYS |SYSTABLES |SYSTEM TABLE| SYS |SYSTRIGGERS |SYSTEM TABLE| SYS |SYSVIEWS |SYSTEM TABLE| SYSIBM |SYSDUMMY1 |SYSTEM TABLE| You can execute show members query to see the running members: snappy> show members; The show members command displays all members of the cluster (such as locators, datastores) in its output. You can execute queries on the tables (such as 'select count(*) from table') to verify the contents.","title":"Step 4: Verifying Upgrade is Successful"},{"location":"migration/upgrade_gemfirexd_standalone/#post-upgrade-tasks","text":"After you have upgraded, perform these tasks: The JDBC connection string prefix has changed from jdbc:gemfirexd to jdbc:snappydata (the old jdbc:gemfirexd prefix works for backward compatibility reasons). Update any custom or third-party JDBC clients to use the correct connection string format. Run all required applications to test that your development system is functioning correctly with the upgraded version of SnappyData.","title":"Post-Upgrade Tasks"},{"location":"migration/upgrade_sqlfire_standalone/","text":"Manually Upgrading from SQLFire 1.1.x to SnappyData RowStore 1.6.0 \u00b6 In this document, you can find information on how to manually upgrade SQLFire 1.1.x to SnappyData 1.0.0/RowStore 1.6. It is a step-by-step guide for configuring and customizing your system. This guide assumes you have a basic understanding of your Linux system. Prerequisites Upgrade Process Prerequisites \u00b6 Download the distribution file from the SnappyData Release page (snappydata-1.0.0) Ensure that before moving into production, you have tested your development systems thoroughly with the new version Upgrade Process \u00b6 Pre-Upgrade Tasks Downloading and Extracting Product Distribution Starting the Upgrade Additional Step for Table CHAR Primary Key Recreating Objects Verifying Upgrade is Successful Step1: Pre-Upgrade Tasks \u00b6 Before you stop the SQLFire cluster, you must drop the procedures, remove the loaders for all tables, stop, detach and drop the listeners. Stopping and Removing Objects Shutting the System Backing Up all Data Directories Stopping and Removing Objects \u00b6 Stop and remove the objects that depend upon the SQLFire packages (listeners, procedures and loaders etc.) If your system uses any stored procedures, row loaders, writers, async event listeners, or functions that reference the SQLFire package names (either com.vmware.sqlfire or com.pivotal.sqlfire ), then you must stop and remove these objects from the system before you begin the upgrade process. For example, if you are upgrading from SQLFire and you configured DBSynchronizer using the built-in com.vmware.sqlfire.callbacks.DBSynchronizer implementation, then you must remove this listener implementation. Perform the following steps for any database objects that uses an older SQLFire ( com.vmware.sqlfire or com.pivotal.sqlfire ) package name: Ensure that you have the DDL commands necessary to recreate the procedures, async event listeners, row loaders and writers. Typically these commands are stored in a SQL script, used to create your database schema. For example, an existing DBSynchronizer might have been created using the statement: create asynceventlistener testlistener ( listenerclass 'com.vmware.sqlfire.callbacks.DBSynchronizer' initparams 'com.mysql.jdbc.Driver,jdbc:mysql://localhost:3306/gfxddb,SkipIdentityColumns=true,user=sqlfuser,secret=25325ffc3345be8888eda8156bd1c313' ) server groups (dbsync); Similarly a row loader may have been created using a command: call sys.attach_loader(\u2018schema\u2019, \u2018table\u2019, \u2018com.company.RowLoader\u2019, null); If you do not have the DDL commands readily available, use the sqlf write-schema-to-sql command to write the existing DDL commands to the SQL file, and verify that the necessary commands are present. Use SYS.STOP_ASYNC_EVENT_LISTENER to stop any listener or DBSynchronizer implementation that uses the older API. For example: call sys.stop_async_event_listener('TESTLISTENER'); Alter any tables that use a stopped listener or DBSynchronizer to remove the listener from the table. For example: alter table mytable set asynceventlistener(); Use the appropriate DROP command ( DROP PROCEDURE , DROP ASYNCEVENTLISTENER , or DROP FUNCTION ) or system procedure ( SYS.REMOVE_LISTENER , SYS.REMOVE_LOADER , SYS.REMOVE_WRITER ) to remove procedures and listener configurations that use the older API. For example: drop asynceventlistener testlistener; Similarly, for row loaders remove them using sys.remove_loader system procedure. For example: call sys.remove_loader(\u2018schema\u2019, \u2018table\u2019) Modify your procedure or listener code to use the newer com.pivotal.gemfirexd package prefixes, and recompile as necessary. If you are using the built-in DBSynchronizer implementation, you only need to modify the DDL SQL script to specify the newer implementation in the CREATE ASYNCEVENTLISTENER command. For example a new DDL can be: -- Upgraded DBSynchronizer DDL create asynceventlistener testlistener ( listenerclass 'com.pivotal.gemfirexd.callbacks.DBSynchronizer' Initparams 'com.mysql.jdbc.Driver,jdbc:mysql://localhost:3306/gfxddb,SkipIdentityColumns=true,user=sqlfuser,secret=25325ffc3345be8888eda8156bd1c313' ) server groups (dbsync); The above DDL is executed after the upgrade is done to create the DBSynchronizer. Shutting the System \u00b6 Use the shut-down-all and locator stop commands to stop all members. For example: $ sqlf shut-down-all -locators=localhost[10101] $ sqlf locator stop -dir=$HOME/locator Backing Up all Data Directories \u00b6 In this step, you create a backup of all the data directories. For example: $cp -R node1-server /backup/snappydata/ $cp -R node2-server /backup/snappydata/ Here node1-server and node2-server are the data directories of SQLFire servers. Step 2: Downloading and Extracting Product Distribution \u00b6 Download the distribution binary file (snappydata-1.0.0/RowStore1.6.0) from the SnappyData Release page (if not done already), and extract the contents of the file to a suitable location on your computer. For example for Linux: $ unzip snappydata-1.0.0-bin.zip -d <path_to_product> Here the is the directory where you want to install the product. Repeat this step to install RowStore on each different computer where you want to run a SnappyData RowStore member. Alternatively, SnappyData RowStore can also be installed on an NFS location that is accessible to all members. If the PATH variable is set to the path of SQLFire, ensure that you update it to snappydata-1.0.0-bin . Add the SnappyData RowStore bin and sbin directories to your path, as described below: $ export PATH=$PATH:/path_to_product/snappydata-1.0.0-bin/bin:/path_to_product/snappydata-1.0.0-bin/sbin Step 3: Starting the Upgrade \u00b6 Setting up Passwordless SSH Creating configuration files for SnappyData servers and locators Starting the SnappyData RowStore cluster Setting up Passwordless SSH \u00b6 SnappyData provides utility scripts to start/stop cluster (located in the sbin directory of the product distribution). These scripts use SSH to execute commands on different machines on which SnappyData servers are to be started/stopped. It is recommended that you setup passwordless SSH on the system where these scripts are executed. For more information see Configuring SSH Login without Password . Creating configuration files for SnappyData servers and locators \u00b6 In this step, you create configuration files for servers and locators, and then use those to start the SnappyData RowStore cluster (locators and servers) using the backed up data directories. If the NFS location is not accessible to all members, you need to follow the process of each machine. If passwordless SSH is set, the cluster can be started from one host. These configuration files contain hostname of the node where a locator/server is to be started along with startup properties for it. To configure the locators: In the /snappydata-1.0.0-bin/conf directory, make a copy of the locators.template , and rename it to locators . Edit the locator file to set it to the previous location of the SQLFire locator (using the -dir option) localhost -dir=<log-dir>/snappydata/locator If there are two locators running on different nodes, add the line twice to set the locator directories: node1 -dir=<log-dir>/snappydata/node1-locator node2 -dir=<log-dir>/snappydata/node2-locator To configure the servers: In /snappydata-1.0.0-bin/conf , make a copy of the servers.template file, and rename it to servers . Edit the servers file to set it to the previous location of the SQLFire server (using the -dir option). You can use -server-group option to configure server groups. localhost -dir=<log-dir>/snappydata/server -server-groups=cache If there are two servers running on different nodes, add the line twice the set the server directories: node1 -dir=<log-dir>/snappydata/node1-server -server-groups=cache node2 -dir=<log-dir>/snappydata/node2-server -server-groups=cache Note The -heap-size property replaces SQLFire -initial-heap and/or -max-heap properties. SnappyData no longer supports the -initial-heap and -max-heap properties . In SQLFire, if you are using a properties file ( sqlfire.properties ) for configuring various properties such as authentication, rename the file to snappydata.properties . Also, the property prefix has been changed to snappydata instead of sqlfire . Alternatively, you may add these properties to the above-mentioned conf/locators and conf/servers files in front of each hostname. Starting the SnappyData RowStore cluster \u00b6 After creating the configuration files, start SnappyData RowStore using the following command: snappy-start-all rowstore The above command starts the SnappyData servers and locators with properties as mentioned in the conf/servers and conf/locators files. The rowstore option instructs SnappyData not to start the lead node, which is used when SnappyData is used for SnappyData specific extensions (like column store, running spark jobs etc.) Note Ensure that the distributed system is running by executing the command snappy-status-all For example: $snappy-status-all SnappyData Locator pid: 13750 status: running SnappyData Server pid: 13897 status: running SnappyData Server pid: 13905 status: running Distributed system now has 2 members. Other members: localhost(13750:locator)<v0>:10811 SnappyData Leader pid: 0 status: stopped Step 4: Additional Step for Table CHAR Primary Key \u00b6 Note You must do this step only if you have tables with CHAR type in the primary key. This step is necessary as the handling of blank padding of the CHAR keys has changed. If you have tables with CHAR type in primary key (single or composite), do the following for each table. Create a temporary table with the same schema as the original source table: snappy>create table app.member_tmp as select * from app.member with no data; Insert data from the original source table into new the temporary table: snappy>insert into app.member_tmp select * from app.member; Truncate the original source table: snappy>truncate table app.member; Insert data from the temporary table to original source table: snappy>insert into app.member_tmp select * from app.member; Drop the temporary table: snappy>drop table app.member_tmp; Step 5: Recreating objects \u00b6 In the initial steps of the upgrade, you had removed the objects that depended on com.vmware.sqlfire or com.pivotal.sqlfire , as described in - Stopping and Removing Objects . In this step, you use the modified DDL SQL scripts to recreate stored procedures, async event listener implementations, row loaders/writers using the new com.pivotal.gemfirexd package names. Replace the old jar files: After the distributed system is running, replace the older jar containing the listener/rowloader/ writer/procedure implementations using the sqlj.replace_jar system procedure. For example: call sqlj.replace_jar('/home/user1/lib/tours.jar', 'app.sample1') Recreate Objects: For example, to restore the sample DBSynchronizer configuration: gfxd> create asynceventlistener testlistener ( listenerclass 'com.pivotal.gemfirexd.callbacks.DBSynchronizer' initparams 'com.mysql.jdbc.Driver,jdbc:mysql://localhost:3306/gfxddb,SkipIdentityColumns=true,user=gfxduser,secret=25325ffc3345be8888eda8156bd1c313' ) server groups (dbsync); gfxd> alter table mytable set asynceventlistener (testlistener); gfxd> call sys.start_async_event_listener('TESTLISTENER'); Similarly attach the row loader: call sys.attach_loader(\u2018schema\u2019, \u2018table\u2019, \u2018com.company.RowLoader\u2019, null\u2019); Step 6: Verifying Upgrade is Successful \u00b6 The system is started and you can fire queries on the snappy-shell to check the status of the upgrade. The sql shell is replaced by the snappy-sql command in SnappyData. For example: snappy-sql >SnappyData version 1.0.0 snappy> connect client '127.0.0.1:1527'; Using CONNECTION0 snappy> show tables; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE |REMARKS ------------------------------------------------------------------------------------- SYS |ASYNCEVENTLISTENERS |SYSTEM TABLE| SYS |GATEWAYRECEIVERS |SYSTEM TABLE| SYS |GATEWAYSENDERS |SYSTEM TABLE| SYS |SYSALIASES |SYSTEM TABLE| SYS |SYSCHECKS |SYSTEM TABLE| SYS |SYSCOLPERMS |SYSTEM TABLE| SYS |SYSCOLUMNS |SYSTEM TABLE| SYS |SYSCONGLOMERATES |SYSTEM TABLE| SYS |SYSCONSTRAINTS |SYSTEM TABLE| SYS |SYSDEPENDS |SYSTEM TABLE| SYS |SYSDISKSTORES |SYSTEM TABLE| SYS |SYSFILES |SYSTEM TABLE| SYS |SYSFOREIGNKEYS |SYSTEM TABLE| SYS |SYSHDFSSTORES |SYSTEM TABLE| SYS |SYSKEYS |SYSTEM TABLE| SYS |SYSROLES |SYSTEM TABLE| SYS |SYSROUTINEPERMS |SYSTEM TABLE| SYS |SYSSCHEMAS |SYSTEM TABLE| SYS |SYSSTATEMENTS |SYSTEM TABLE| SYS |SYSSTATISTICS |SYSTEM TABLE| SYS |SYSTABLEPERMS |SYSTEM TABLE| SYS |SYSTABLES |SYSTEM TABLE| SYS |SYSTRIGGERS |SYSTEM TABLE| SYS |SYSVIEWS |SYSTEM TABLE| SYSIBM |SYSDUMMY1 |SYSTEM TABLE| You can execute the show members query to see the running members: snappy> show members; The show members command displays all members of the cluster (such as locators, datastores) in its output. You can execute queries on the tables (such as 'select count(*) from table') to verify the contents. Post-Upgrade Tasks \u00b6 After you have upgraded, perform these tasks: The JDBC connection string prefix has changed from jdbc:sqlfire to jdbc:snappydata . Update any custom or third-party JDBC clients to use the correct connection string format. Run all required applications to test that your development system is functioning correctly with the upgraded version of SnappyData.","title":"Manually Upgrading from SQLFire 1.1.x to SnappyData RowStore 1.6.0"},{"location":"migration/upgrade_sqlfire_standalone/#manually-upgrading-from-sqlfire-11x-to-snappydata-rowstore-160","text":"In this document, you can find information on how to manually upgrade SQLFire 1.1.x to SnappyData 1.0.0/RowStore 1.6. It is a step-by-step guide for configuring and customizing your system. This guide assumes you have a basic understanding of your Linux system. Prerequisites Upgrade Process","title":"Manually Upgrading from SQLFire 1.1.x to SnappyData RowStore 1.6.0"},{"location":"migration/upgrade_sqlfire_standalone/#prerequisites","text":"Download the distribution file from the SnappyData Release page (snappydata-1.0.0) Ensure that before moving into production, you have tested your development systems thoroughly with the new version","title":"Prerequisites"},{"location":"migration/upgrade_sqlfire_standalone/#upgrade-process","text":"Pre-Upgrade Tasks Downloading and Extracting Product Distribution Starting the Upgrade Additional Step for Table CHAR Primary Key Recreating Objects Verifying Upgrade is Successful","title":"Upgrade Process"},{"location":"migration/upgrade_sqlfire_standalone/#step1-pre-upgrade-tasks","text":"Before you stop the SQLFire cluster, you must drop the procedures, remove the loaders for all tables, stop, detach and drop the listeners. Stopping and Removing Objects Shutting the System Backing Up all Data Directories","title":"Step1: Pre-Upgrade Tasks"},{"location":"migration/upgrade_sqlfire_standalone/#stopping-and-removing-objects","text":"Stop and remove the objects that depend upon the SQLFire packages (listeners, procedures and loaders etc.) If your system uses any stored procedures, row loaders, writers, async event listeners, or functions that reference the SQLFire package names (either com.vmware.sqlfire or com.pivotal.sqlfire ), then you must stop and remove these objects from the system before you begin the upgrade process. For example, if you are upgrading from SQLFire and you configured DBSynchronizer using the built-in com.vmware.sqlfire.callbacks.DBSynchronizer implementation, then you must remove this listener implementation. Perform the following steps for any database objects that uses an older SQLFire ( com.vmware.sqlfire or com.pivotal.sqlfire ) package name: Ensure that you have the DDL commands necessary to recreate the procedures, async event listeners, row loaders and writers. Typically these commands are stored in a SQL script, used to create your database schema. For example, an existing DBSynchronizer might have been created using the statement: create asynceventlistener testlistener ( listenerclass 'com.vmware.sqlfire.callbacks.DBSynchronizer' initparams 'com.mysql.jdbc.Driver,jdbc:mysql://localhost:3306/gfxddb,SkipIdentityColumns=true,user=sqlfuser,secret=25325ffc3345be8888eda8156bd1c313' ) server groups (dbsync); Similarly a row loader may have been created using a command: call sys.attach_loader(\u2018schema\u2019, \u2018table\u2019, \u2018com.company.RowLoader\u2019, null); If you do not have the DDL commands readily available, use the sqlf write-schema-to-sql command to write the existing DDL commands to the SQL file, and verify that the necessary commands are present. Use SYS.STOP_ASYNC_EVENT_LISTENER to stop any listener or DBSynchronizer implementation that uses the older API. For example: call sys.stop_async_event_listener('TESTLISTENER'); Alter any tables that use a stopped listener or DBSynchronizer to remove the listener from the table. For example: alter table mytable set asynceventlistener(); Use the appropriate DROP command ( DROP PROCEDURE , DROP ASYNCEVENTLISTENER , or DROP FUNCTION ) or system procedure ( SYS.REMOVE_LISTENER , SYS.REMOVE_LOADER , SYS.REMOVE_WRITER ) to remove procedures and listener configurations that use the older API. For example: drop asynceventlistener testlistener; Similarly, for row loaders remove them using sys.remove_loader system procedure. For example: call sys.remove_loader(\u2018schema\u2019, \u2018table\u2019) Modify your procedure or listener code to use the newer com.pivotal.gemfirexd package prefixes, and recompile as necessary. If you are using the built-in DBSynchronizer implementation, you only need to modify the DDL SQL script to specify the newer implementation in the CREATE ASYNCEVENTLISTENER command. For example a new DDL can be: -- Upgraded DBSynchronizer DDL create asynceventlistener testlistener ( listenerclass 'com.pivotal.gemfirexd.callbacks.DBSynchronizer' Initparams 'com.mysql.jdbc.Driver,jdbc:mysql://localhost:3306/gfxddb,SkipIdentityColumns=true,user=sqlfuser,secret=25325ffc3345be8888eda8156bd1c313' ) server groups (dbsync); The above DDL is executed after the upgrade is done to create the DBSynchronizer.","title":"Stopping and Removing Objects"},{"location":"migration/upgrade_sqlfire_standalone/#shutting-the-system","text":"Use the shut-down-all and locator stop commands to stop all members. For example: $ sqlf shut-down-all -locators=localhost[10101] $ sqlf locator stop -dir=$HOME/locator","title":"Shutting the System"},{"location":"migration/upgrade_sqlfire_standalone/#backing-up-all-data-directories","text":"In this step, you create a backup of all the data directories. For example: $cp -R node1-server /backup/snappydata/ $cp -R node2-server /backup/snappydata/ Here node1-server and node2-server are the data directories of SQLFire servers.","title":"Backing Up all Data Directories"},{"location":"migration/upgrade_sqlfire_standalone/#step-2-downloading-and-extracting-product-distribution","text":"Download the distribution binary file (snappydata-1.0.0/RowStore1.6.0) from the SnappyData Release page (if not done already), and extract the contents of the file to a suitable location on your computer. For example for Linux: $ unzip snappydata-1.0.0-bin.zip -d <path_to_product> Here the is the directory where you want to install the product. Repeat this step to install RowStore on each different computer where you want to run a SnappyData RowStore member. Alternatively, SnappyData RowStore can also be installed on an NFS location that is accessible to all members. If the PATH variable is set to the path of SQLFire, ensure that you update it to snappydata-1.0.0-bin . Add the SnappyData RowStore bin and sbin directories to your path, as described below: $ export PATH=$PATH:/path_to_product/snappydata-1.0.0-bin/bin:/path_to_product/snappydata-1.0.0-bin/sbin","title":"Step 2: Downloading and Extracting Product Distribution"},{"location":"migration/upgrade_sqlfire_standalone/#step-3-starting-the-upgrade","text":"Setting up Passwordless SSH Creating configuration files for SnappyData servers and locators Starting the SnappyData RowStore cluster","title":"Step 3: Starting the Upgrade"},{"location":"migration/upgrade_sqlfire_standalone/#setting-up-passwordless-ssh","text":"SnappyData provides utility scripts to start/stop cluster (located in the sbin directory of the product distribution). These scripts use SSH to execute commands on different machines on which SnappyData servers are to be started/stopped. It is recommended that you setup passwordless SSH on the system where these scripts are executed. For more information see Configuring SSH Login without Password .","title":"Setting up Passwordless SSH"},{"location":"migration/upgrade_sqlfire_standalone/#creating-configuration-files-for-snappydata-servers-and-locators","text":"In this step, you create configuration files for servers and locators, and then use those to start the SnappyData RowStore cluster (locators and servers) using the backed up data directories. If the NFS location is not accessible to all members, you need to follow the process of each machine. If passwordless SSH is set, the cluster can be started from one host. These configuration files contain hostname of the node where a locator/server is to be started along with startup properties for it. To configure the locators: In the /snappydata-1.0.0-bin/conf directory, make a copy of the locators.template , and rename it to locators . Edit the locator file to set it to the previous location of the SQLFire locator (using the -dir option) localhost -dir=<log-dir>/snappydata/locator If there are two locators running on different nodes, add the line twice to set the locator directories: node1 -dir=<log-dir>/snappydata/node1-locator node2 -dir=<log-dir>/snappydata/node2-locator To configure the servers: In /snappydata-1.0.0-bin/conf , make a copy of the servers.template file, and rename it to servers . Edit the servers file to set it to the previous location of the SQLFire server (using the -dir option). You can use -server-group option to configure server groups. localhost -dir=<log-dir>/snappydata/server -server-groups=cache If there are two servers running on different nodes, add the line twice the set the server directories: node1 -dir=<log-dir>/snappydata/node1-server -server-groups=cache node2 -dir=<log-dir>/snappydata/node2-server -server-groups=cache Note The -heap-size property replaces SQLFire -initial-heap and/or -max-heap properties. SnappyData no longer supports the -initial-heap and -max-heap properties . In SQLFire, if you are using a properties file ( sqlfire.properties ) for configuring various properties such as authentication, rename the file to snappydata.properties . Also, the property prefix has been changed to snappydata instead of sqlfire . Alternatively, you may add these properties to the above-mentioned conf/locators and conf/servers files in front of each hostname.","title":"Creating configuration files for SnappyData servers and locators"},{"location":"migration/upgrade_sqlfire_standalone/#starting-the-snappydata-rowstore-cluster","text":"After creating the configuration files, start SnappyData RowStore using the following command: snappy-start-all rowstore The above command starts the SnappyData servers and locators with properties as mentioned in the conf/servers and conf/locators files. The rowstore option instructs SnappyData not to start the lead node, which is used when SnappyData is used for SnappyData specific extensions (like column store, running spark jobs etc.) Note Ensure that the distributed system is running by executing the command snappy-status-all For example: $snappy-status-all SnappyData Locator pid: 13750 status: running SnappyData Server pid: 13897 status: running SnappyData Server pid: 13905 status: running Distributed system now has 2 members. Other members: localhost(13750:locator)<v0>:10811 SnappyData Leader pid: 0 status: stopped","title":"Starting the SnappyData RowStore cluster"},{"location":"migration/upgrade_sqlfire_standalone/#step-4-additional-step-for-table-char-primary-key","text":"Note You must do this step only if you have tables with CHAR type in the primary key. This step is necessary as the handling of blank padding of the CHAR keys has changed. If you have tables with CHAR type in primary key (single or composite), do the following for each table. Create a temporary table with the same schema as the original source table: snappy>create table app.member_tmp as select * from app.member with no data; Insert data from the original source table into new the temporary table: snappy>insert into app.member_tmp select * from app.member; Truncate the original source table: snappy>truncate table app.member; Insert data from the temporary table to original source table: snappy>insert into app.member_tmp select * from app.member; Drop the temporary table: snappy>drop table app.member_tmp;","title":"Step 4: Additional Step for Table CHAR Primary Key"},{"location":"migration/upgrade_sqlfire_standalone/#step-5-recreating-objects","text":"In the initial steps of the upgrade, you had removed the objects that depended on com.vmware.sqlfire or com.pivotal.sqlfire , as described in - Stopping and Removing Objects . In this step, you use the modified DDL SQL scripts to recreate stored procedures, async event listener implementations, row loaders/writers using the new com.pivotal.gemfirexd package names. Replace the old jar files: After the distributed system is running, replace the older jar containing the listener/rowloader/ writer/procedure implementations using the sqlj.replace_jar system procedure. For example: call sqlj.replace_jar('/home/user1/lib/tours.jar', 'app.sample1') Recreate Objects: For example, to restore the sample DBSynchronizer configuration: gfxd> create asynceventlistener testlistener ( listenerclass 'com.pivotal.gemfirexd.callbacks.DBSynchronizer' initparams 'com.mysql.jdbc.Driver,jdbc:mysql://localhost:3306/gfxddb,SkipIdentityColumns=true,user=gfxduser,secret=25325ffc3345be8888eda8156bd1c313' ) server groups (dbsync); gfxd> alter table mytable set asynceventlistener (testlistener); gfxd> call sys.start_async_event_listener('TESTLISTENER'); Similarly attach the row loader: call sys.attach_loader(\u2018schema\u2019, \u2018table\u2019, \u2018com.company.RowLoader\u2019, null\u2019);","title":"Step 5: Recreating objects"},{"location":"migration/upgrade_sqlfire_standalone/#step-6-verifying-upgrade-is-successful","text":"The system is started and you can fire queries on the snappy-shell to check the status of the upgrade. The sql shell is replaced by the snappy-sql command in SnappyData. For example: snappy-sql >SnappyData version 1.0.0 snappy> connect client '127.0.0.1:1527'; Using CONNECTION0 snappy> show tables; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE |REMARKS ------------------------------------------------------------------------------------- SYS |ASYNCEVENTLISTENERS |SYSTEM TABLE| SYS |GATEWAYRECEIVERS |SYSTEM TABLE| SYS |GATEWAYSENDERS |SYSTEM TABLE| SYS |SYSALIASES |SYSTEM TABLE| SYS |SYSCHECKS |SYSTEM TABLE| SYS |SYSCOLPERMS |SYSTEM TABLE| SYS |SYSCOLUMNS |SYSTEM TABLE| SYS |SYSCONGLOMERATES |SYSTEM TABLE| SYS |SYSCONSTRAINTS |SYSTEM TABLE| SYS |SYSDEPENDS |SYSTEM TABLE| SYS |SYSDISKSTORES |SYSTEM TABLE| SYS |SYSFILES |SYSTEM TABLE| SYS |SYSFOREIGNKEYS |SYSTEM TABLE| SYS |SYSHDFSSTORES |SYSTEM TABLE| SYS |SYSKEYS |SYSTEM TABLE| SYS |SYSROLES |SYSTEM TABLE| SYS |SYSROUTINEPERMS |SYSTEM TABLE| SYS |SYSSCHEMAS |SYSTEM TABLE| SYS |SYSSTATEMENTS |SYSTEM TABLE| SYS |SYSSTATISTICS |SYSTEM TABLE| SYS |SYSTABLEPERMS |SYSTEM TABLE| SYS |SYSTABLES |SYSTEM TABLE| SYS |SYSTRIGGERS |SYSTEM TABLE| SYS |SYSVIEWS |SYSTEM TABLE| SYSIBM |SYSDUMMY1 |SYSTEM TABLE| You can execute the show members query to see the running members: snappy> show members; The show members command displays all members of the cluster (such as locators, datastores) in its output. You can execute queries on the tables (such as 'select count(*) from table') to verify the contents.","title":"Step 6: Verifying Upgrade is Successful"},{"location":"migration/upgrade_sqlfire_standalone/#post-upgrade-tasks","text":"After you have upgraded, perform these tasks: The JDBC connection string prefix has changed from jdbc:sqlfire to jdbc:snappydata . Update any custom or third-party JDBC clients to use the correct connection string format. Run all required applications to test that your development system is functioning correctly with the upgraded version of SnappyData.","title":"Post-Upgrade Tasks"},{"location":"monitoring/","text":"Managing and Monitoring \u00b6 Managing and Monitoring SnappyData describes how to use log files, system tables, and statistical data to understand the behavior of SnappyData deployment. This guide also provides general guidelines for tuning the performance of SnappyData members, applications, and individual queries. The following topics are covered in this section: SnappyData Monitoring Console Configuring High Availability for a Partitioned Table Configuring Logging Securing SnappyData Monitoring Console Connections Getting Information from SnappyData System Tables Recovering Data During Cluster Failures","title":"Managing and Monitoring"},{"location":"monitoring/#managing-and-monitoring","text":"Managing and Monitoring SnappyData describes how to use log files, system tables, and statistical data to understand the behavior of SnappyData deployment. This guide also provides general guidelines for tuning the performance of SnappyData members, applications, and individual queries. The following topics are covered in this section: SnappyData Monitoring Console Configuring High Availability for a Partitioned Table Configuring Logging Securing SnappyData Monitoring Console Connections Getting Information from SnappyData System Tables Recovering Data During Cluster Failures","title":"Managing and Monitoring"},{"location":"monitoring/collect-debug-artifacts/","text":"Collecting logs, stats and dumps using the collect-debug-artifacts script \u00b6 This section uses the term 'node' frequently. A node denotes a server or a locator member when a purely SnappyData system is there. In a SnappyData distributed system a node can mean server, locator or lead member. SnappyData uses a script for collecting the debug information like logs and stats. It also has an option to dump stacks of the running system. Details of all the options and capabilities of the script can be found below. The main purpose of this is to ease the collection of these information. The script collects all the artifacts node wise and outputs a tar file which contains member wise information Pre-requisites for running the script: The script assumes certain conditions to be fulfilled before it is invoked. Please ensure that these requirements are fulfilled because the script does not validate these. The conditions are: This script is expected to be run by a user who has read and write permissions on the output directories of all the SnappyData nodes. The user should have a passwordless ssh setup between all the machines where the SnappyData nodes are running. Below is the usage of the script <linux-shell> ./sbin/collect-debug-artifacts -h Usage: collect-debug-artifacts [ -c conffile|--conf=conffile|--config=conffile ] [ -o resultdir|--out=resultdir|--outdir=resultdir ] [ -h|--help ] [ -a|--all ] [ -d|--dump ] [ -v|--verbose ] [ -s starttimestamp|--start=starttimestamp ] [ -e endtimestamp|--end=endtimestamp ] Timestamp format: YYYY-MM-DD HH:MM[:SS] Options: All the options of the script are optional. By default the script tries to get the current logs. All the logs starting from the last restart and the last file before that. It also brings all the stat file in the output directory. However if you want to change this behavior of the script you can use the following options to collect the debug information as per your requirements. Please note that no stack dumps are collected by default. You need to use the '-d, --dump' option to get the stack dumps. -h, --help Prints a usage message summary briefly summarizing the command line options -c, --conf The script uses a configuration file which has three configuration elements. 1. MEMBERS_FILE -- This is a text file which has member information. Each line has the host machine name followed by the full path to the run directory of the member. This file is generated automatically when the sbin/start-all-scripts.sh is used. 2. NO_OF_STACK_DUMPS -- This parameter tells the script that how many stack dumps will be attempted per member/node of the running system. 3. INTERVAL_BETWEEN_DUMPS -- The amount of time in seconds the script waits between registering stack dumps. -o, --out, --outdir The directory where the output file in the form of tar, will be created. -a, --all With '-a or --all' option all the logs and stats file are collected from each members output directory. -d, --dump Stack dumps are not collected by default or with -a, --all option. The user need to explicitly provide this argument if the stack dumps need to be collected. -v, --verbose verbose mode is on. -s, --start The script can also be asked to collect log files for specified time interval. The time interval can be specified using the start time and an end time parameter. Both the parameter needs to be specified. The format in which the time stamp can be specified is 'YYYY-MM-DD HH:MM[:SS]'","title":"Collecting logs, stats and dumps using the collect-debug-artifacts script"},{"location":"monitoring/collect-debug-artifacts/#collecting-logs-stats-and-dumps-using-the-collect-debug-artifacts-script","text":"This section uses the term 'node' frequently. A node denotes a server or a locator member when a purely SnappyData system is there. In a SnappyData distributed system a node can mean server, locator or lead member. SnappyData uses a script for collecting the debug information like logs and stats. It also has an option to dump stacks of the running system. Details of all the options and capabilities of the script can be found below. The main purpose of this is to ease the collection of these information. The script collects all the artifacts node wise and outputs a tar file which contains member wise information Pre-requisites for running the script: The script assumes certain conditions to be fulfilled before it is invoked. Please ensure that these requirements are fulfilled because the script does not validate these. The conditions are: This script is expected to be run by a user who has read and write permissions on the output directories of all the SnappyData nodes. The user should have a passwordless ssh setup between all the machines where the SnappyData nodes are running. Below is the usage of the script <linux-shell> ./sbin/collect-debug-artifacts -h Usage: collect-debug-artifacts [ -c conffile|--conf=conffile|--config=conffile ] [ -o resultdir|--out=resultdir|--outdir=resultdir ] [ -h|--help ] [ -a|--all ] [ -d|--dump ] [ -v|--verbose ] [ -s starttimestamp|--start=starttimestamp ] [ -e endtimestamp|--end=endtimestamp ] Timestamp format: YYYY-MM-DD HH:MM[:SS] Options: All the options of the script are optional. By default the script tries to get the current logs. All the logs starting from the last restart and the last file before that. It also brings all the stat file in the output directory. However if you want to change this behavior of the script you can use the following options to collect the debug information as per your requirements. Please note that no stack dumps are collected by default. You need to use the '-d, --dump' option to get the stack dumps. -h, --help Prints a usage message summary briefly summarizing the command line options -c, --conf The script uses a configuration file which has three configuration elements. 1. MEMBERS_FILE -- This is a text file which has member information. Each line has the host machine name followed by the full path to the run directory of the member. This file is generated automatically when the sbin/start-all-scripts.sh is used. 2. NO_OF_STACK_DUMPS -- This parameter tells the script that how many stack dumps will be attempted per member/node of the running system. 3. INTERVAL_BETWEEN_DUMPS -- The amount of time in seconds the script waits between registering stack dumps. -o, --out, --outdir The directory where the output file in the form of tar, will be created. -a, --all With '-a or --all' option all the logs and stats file are collected from each members output directory. -d, --dump Stack dumps are not collected by default or with -a, --all option. The user need to explicitly provide this argument if the stack dumps need to be collected. -v, --verbose verbose mode is on. -s, --start The script can also be asked to collect log files for specified time interval. The time interval can be specified using the start time and an end time parameter. Both the parameter needs to be specified. The format in which the time stamp can be specified is 'YYYY-MM-DD HH:MM[:SS]'","title":"Collecting logs, stats and dumps using the collect-debug-artifacts script"},{"location":"monitoring/configure_high_availability/","text":"Configuring High Availability for a Partitioned Table \u00b6 Configure in-memory high availability for your partitioned table. Set other high-availability options, like redundancy zones and redundancy recovery strategies. Set the Number of Redundant Copies \u00b6 Configure in-memory high availability for your partitioned table by specifying the number of secondary copies you want to maintain REDUNDANCY clause of CREATE TABLE . Configure Redundancy Zones for Members \u00b6 Group members into redundancy zones so that SnappyData places redundant data copies in different zones. Understand how to set a member's gemfirexd.properties settings. See Configuration Properties . Group the datastore members that host partitioned tables into redundancy zones by using the setting redundancy-zone . For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. To do this, you set this zone in the gemfirexd.properties file for all members that run on one rack: redundancy-zone=rack1 You would set this zone in gemfirexd.properties for all members on the other rack: redundancy-zone=rack2 Each secondary copy would be hosted on the rack opposite the rack where its primary copy is hosted. Set Enforce Unique Host \u00b6 Configure SnappyData to use only unique physical machines for redundant copies of partitioned table data. Understand how to set a member's gemfirexd.properties settings. See Configuration Properties . Configure your members so SnappyData always uses different physical machines for redundant copies of partitioned table data using the setting enforce-unique-host . The default for this setting is false . Example: enforce-unique-host=true","title":"Configuring High Availability for a Partitioned Table"},{"location":"monitoring/configure_high_availability/#configuring-high-availability-for-a-partitioned-table","text":"Configure in-memory high availability for your partitioned table. Set other high-availability options, like redundancy zones and redundancy recovery strategies.","title":"Configuring High Availability for a Partitioned Table"},{"location":"monitoring/configure_high_availability/#set-the-number-of-redundant-copies","text":"Configure in-memory high availability for your partitioned table by specifying the number of secondary copies you want to maintain REDUNDANCY clause of CREATE TABLE .","title":"Set the Number of Redundant Copies"},{"location":"monitoring/configure_high_availability/#configure-redundancy-zones-for-members","text":"Group members into redundancy zones so that SnappyData places redundant data copies in different zones. Understand how to set a member's gemfirexd.properties settings. See Configuration Properties . Group the datastore members that host partitioned tables into redundancy zones by using the setting redundancy-zone . For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. To do this, you set this zone in the gemfirexd.properties file for all members that run on one rack: redundancy-zone=rack1 You would set this zone in gemfirexd.properties for all members on the other rack: redundancy-zone=rack2 Each secondary copy would be hosted on the rack opposite the rack where its primary copy is hosted.","title":"Configure Redundancy Zones for Members"},{"location":"monitoring/configure_high_availability/#set-enforce-unique-host","text":"Configure SnappyData to use only unique physical machines for redundant copies of partitioned table data. Understand how to set a member's gemfirexd.properties settings. See Configuration Properties . Configure your members so SnappyData always uses different physical machines for redundant copies of partitioned table data using the setting enforce-unique-host . The default for this setting is false . Example: enforce-unique-host=true","title":"Set Enforce Unique Host"},{"location":"monitoring/configure_logging/","text":"Configuring Logging \u00b6 By default, log files for SnappyData members are created inside the working directory of the member. To change the log file directory, you can specify a property -log-file as the path of the directory, while starting a member. SnappyData uses log4j 2 for logging. You can configure logging by copying the existing template file log4j2.properties.template to the conf directory and renaming it to log4j2.properties . For example, the following can be added to the log4j2.properties file to change the logging level of the classes of Spark scheduler. $ cat conf/log4j2.properties logger.scheduler.name = org.apache.spark.scheduler.TaskSchedulerImpl logger.scheduler.level = debug The default template uses a layout that includes both the thread name and ID. For example, the default pattern: appender.rolling.layout.pattern = %d{yy/MM/dd HH:mm:ss.SSS zzz} %t<tid=%T> %p %c{1}: %m%n produces 17/11/07 16:42:05.115 IST serverConnector<tid=9> INFO snappystore: GemFire P2P Listener started on tcp:///192.168.1.6:53116 This is the recommended pattern to use for SnappyData logging. Setting Log Level at Runtime \u00b6 The inbuilt procedure set_log_level can be used to set the log level of SnappyData classes at runtime. You must execute the procedure as a system user. Following is the usage of the procedure: call sys.set_log_level (loggerName, logLevel); The logLevel can be a log4j level, that is, ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF, TRACE. loggerName can be a class name or a package name. If it is left empty, the root logger's level is set. For example: // sets the root logger's level as WARN snappy> call sys.set_log_level ('', 'WARN' ); // sets the WholeStageCodegenExec class level as DEBUG snappy> call sys.set_log_level ('org.apache.spark.sql.execution.WholeStageCodegenExec', 'DEBUG'); // sets the apache spark package's log level as INFO snappy> call sys.set_log_level ('org.apache.spark', 'INFO'); SnappyData Store logging \u00b6 The fine-grained log settings are applicable for classes other than the SnappyData store classes. SnappyData store does not honor fine-grained log settings. That is, you can only set the log level for the root category. However, log level of specific features of SnappyData store can be controlled both during the start and during runtime. Using Trace Flags for Advanced Logging For SnappyData Store \u00b6 SnappyData Store provides the following trace flags that you can use with the gemfirexd.debug.true system property to log additional details about specific features: Trace flag Enables QueryDistribution Detailed logging for distributed queries and DML statements, including information about message distribution to SnappyData members and scan types that were opened. StatementMatching Logging for optimizations that are related to unprepared statements. TraceAuthentication Additional logging for authentication. TraceDBSynchronizer DBSynchronizer and WAN distribution logging. TraceClientConn Client-side connection open and close stack traces. TraceClientStatement Client-side, basic timing logging. TraceClientStatementMillis Client-side wall clock timing. TraceIndex Detailed index logging. TraceJars Logging for JAR installation, replace, and remove events. TraceTran Detailed logging for transaction events and operations, including commit and rollback. TraceLock_* Locking and unlocking information for all internal locks. TraceLock_DD Logging for all DataDictionary and table locks that are acquired or released. To enable logging of specific features of SnappyData, set the required trace flag in the gemfirexd.debug.true system property. For example, you can add the following setting inside the configuration file of the SnappyData member to enable logging for query distribution and indexing: localhost -J-Dgemfirexd.debug.true=QueryDistribution,TraceIndex If you need to set a trace flag in a running system, use the SYS.SET_TRACE_FLAG system procedure. The procedure sets the trace flag in all members of the distributed system, including locators. You must execute the procedure as a system user. For example: snappy> call sys.set_trace_flag('traceindex', 'true'); Statement executed. Note Trace flags work only for snappy and jdbc and not for snappy-sql .","title":"Configuring Logging"},{"location":"monitoring/configure_logging/#configuring-logging","text":"By default, log files for SnappyData members are created inside the working directory of the member. To change the log file directory, you can specify a property -log-file as the path of the directory, while starting a member. SnappyData uses log4j 2 for logging. You can configure logging by copying the existing template file log4j2.properties.template to the conf directory and renaming it to log4j2.properties . For example, the following can be added to the log4j2.properties file to change the logging level of the classes of Spark scheduler. $ cat conf/log4j2.properties logger.scheduler.name = org.apache.spark.scheduler.TaskSchedulerImpl logger.scheduler.level = debug The default template uses a layout that includes both the thread name and ID. For example, the default pattern: appender.rolling.layout.pattern = %d{yy/MM/dd HH:mm:ss.SSS zzz} %t<tid=%T> %p %c{1}: %m%n produces 17/11/07 16:42:05.115 IST serverConnector<tid=9> INFO snappystore: GemFire P2P Listener started on tcp:///192.168.1.6:53116 This is the recommended pattern to use for SnappyData logging.","title":"Configuring Logging"},{"location":"monitoring/configure_logging/#setting-log-level-at-runtime","text":"The inbuilt procedure set_log_level can be used to set the log level of SnappyData classes at runtime. You must execute the procedure as a system user. Following is the usage of the procedure: call sys.set_log_level (loggerName, logLevel); The logLevel can be a log4j level, that is, ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF, TRACE. loggerName can be a class name or a package name. If it is left empty, the root logger's level is set. For example: // sets the root logger's level as WARN snappy> call sys.set_log_level ('', 'WARN' ); // sets the WholeStageCodegenExec class level as DEBUG snappy> call sys.set_log_level ('org.apache.spark.sql.execution.WholeStageCodegenExec', 'DEBUG'); // sets the apache spark package's log level as INFO snappy> call sys.set_log_level ('org.apache.spark', 'INFO');","title":"Setting Log Level at Runtime"},{"location":"monitoring/configure_logging/#snappydata-store-logging","text":"The fine-grained log settings are applicable for classes other than the SnappyData store classes. SnappyData store does not honor fine-grained log settings. That is, you can only set the log level for the root category. However, log level of specific features of SnappyData store can be controlled both during the start and during runtime.","title":"SnappyData Store logging"},{"location":"monitoring/configure_logging/#using-trace-flags-for-advanced-logging-for-snappydata-store","text":"SnappyData Store provides the following trace flags that you can use with the gemfirexd.debug.true system property to log additional details about specific features: Trace flag Enables QueryDistribution Detailed logging for distributed queries and DML statements, including information about message distribution to SnappyData members and scan types that were opened. StatementMatching Logging for optimizations that are related to unprepared statements. TraceAuthentication Additional logging for authentication. TraceDBSynchronizer DBSynchronizer and WAN distribution logging. TraceClientConn Client-side connection open and close stack traces. TraceClientStatement Client-side, basic timing logging. TraceClientStatementMillis Client-side wall clock timing. TraceIndex Detailed index logging. TraceJars Logging for JAR installation, replace, and remove events. TraceTran Detailed logging for transaction events and operations, including commit and rollback. TraceLock_* Locking and unlocking information for all internal locks. TraceLock_DD Logging for all DataDictionary and table locks that are acquired or released. To enable logging of specific features of SnappyData, set the required trace flag in the gemfirexd.debug.true system property. For example, you can add the following setting inside the configuration file of the SnappyData member to enable logging for query distribution and indexing: localhost -J-Dgemfirexd.debug.true=QueryDistribution,TraceIndex If you need to set a trace flag in a running system, use the SYS.SET_TRACE_FLAG system procedure. The procedure sets the trace flag in all members of the distributed system, including locators. You must execute the procedure as a system user. For example: snappy> call sys.set_trace_flag('traceindex', 'true'); Statement executed. Note Trace flags work only for snappy and jdbc and not for snappy-sql .","title":"Using Trace Flags for Advanced Logging For SnappyData Store"},{"location":"monitoring/log-application/","text":"Using java.util.logging.Logger for Application Log Messages \u00b6 Applications that use the SnappyData JDBC peer driver can log messages through the java.util.Logger logging API. You can obtain a handle to the java.util.Logger logger object by entering com.pivotal.gemfirexd after the application connects to the SnappyData cluster with the peer driver. For example: import java.util.logging.Logger; Logger logger = Logger.getLogger(\"com.pivotal.gemfirexd\"); logger.info(\"Connected to a SnappyData system\");","title":"Using java.util.logging.Logger for Application Log Messages"},{"location":"monitoring/log-application/#using-javautillogginglogger-for-application-log-messages","text":"Applications that use the SnappyData JDBC peer driver can log messages through the java.util.Logger logging API. You can obtain a handle to the java.util.Logger logger object by entering com.pivotal.gemfirexd after the application connects to the SnappyData cluster with the peer driver. For example: import java.util.logging.Logger; Logger logger = Logger.getLogger(\"com.pivotal.gemfirexd\"); logger.info(\"Connected to a SnappyData system\");","title":"Using java.util.logging.Logger for Application Log Messages"},{"location":"monitoring/log-format/","text":"Log Message Format \u00b6 Each message in a server or locator log file contains the severity level, timestamp, and other important information. A SnappyData log message contains: Severity level of the message. Time that the message was logged. ID of the thread that logged the message. Body of the log message, which can be a string and/or an exception that includes the exception stack trace. The following shows an example log entry. [config 2011/05/24 17:19:45.705 IST <main> tid=0x1] This VM is setup with SnappyData datastore role.","title":"Log Message Format"},{"location":"monitoring/log-format/#log-message-format","text":"Each message in a server or locator log file contains the severity level, timestamp, and other important information. A SnappyData log message contains: Severity level of the message. Time that the message was logged. ID of the thread that logged the message. Body of the log message, which can be a string and/or an exception that includes the exception stack trace. The following shows an example log entry. [config 2011/05/24 17:19:45.705 IST <main> tid=0x1] This VM is setup with SnappyData datastore role.","title":"Log Message Format"},{"location":"monitoring/log-severity/","text":"Severity Levels \u00b6 You can configure the logging system to record only those messages that are at or above a specified logging level. By default, the logging level is set to \"config\", which means that the system logs messages at config, info, warning, error, and severe severity levels. If you are having problems with your system, first lower the log-level (recording more of detailed messages to the log file) and recreate the problem. The additional log messages often help uncover the source. To specify the logging level, use the log-level property when you start a SnappyData server or locator. For example, to record all messages at log level \"warning,\" \"error,\" or \"severe:\" snappy-shell snappydata server start -log-file=/home/user1/log/mygfxdlog.log -log-level=warning The table shows log levels from highest to lowest severity. Log Level Indication severe Highest severity level, indicating a serious failure that usually prevents normal program execution. You may need to shut down or restart part of your cluster to correct the problem. error The operation indicated in the error message has failed. The server or locator should continue to run. warning Potential problem with the system. In general, warning messages describe events that are of interest to end users or system managers. info Informational messages for end users and system administrators. config Default severity level for logging messages. This log level provides static configuration messages that you can use to debug configuration problems. fine Tracing information that is generally of interest only to application developers. This logging level may generate lots of \"noise\" that might not indicate a problem in your application. It creates very verbose logs that may require significantly more disk space than logs that record only higher severity levels. Do not use this log setting unless you are asked to do so by Pivotal Support. finer, finest, and all Reserved for internal use. These log levels produce a large amount of data and consume large amounts of disk space and system resources. Do not use these settings unless you are asked to do so by Pivotal Support.","title":"Severity Levels"},{"location":"monitoring/log-severity/#severity-levels","text":"You can configure the logging system to record only those messages that are at or above a specified logging level. By default, the logging level is set to \"config\", which means that the system logs messages at config, info, warning, error, and severe severity levels. If you are having problems with your system, first lower the log-level (recording more of detailed messages to the log file) and recreate the problem. The additional log messages often help uncover the source. To specify the logging level, use the log-level property when you start a SnappyData server or locator. For example, to record all messages at log level \"warning,\" \"error,\" or \"severe:\" snappy-shell snappydata server start -log-file=/home/user1/log/mygfxdlog.log -log-level=warning The table shows log levels from highest to lowest severity. Log Level Indication severe Highest severity level, indicating a serious failure that usually prevents normal program execution. You may need to shut down or restart part of your cluster to correct the problem. error The operation indicated in the error message has failed. The server or locator should continue to run. warning Potential problem with the system. In general, warning messages describe events that are of interest to end users or system managers. info Informational messages for end users and system administrators. config Default severity level for logging messages. This log level provides static configuration messages that you can use to debug configuration problems. fine Tracing information that is generally of interest only to application developers. This logging level may generate lots of \"noise\" that might not indicate a problem in your application. It creates very verbose logs that may require significantly more disk space than logs that record only higher severity levels. Do not use this log setting unless you are asked to do so by Pivotal Support. finer, finest, and all Reserved for internal use. These log levels produce a large amount of data and consume large amounts of disk space and system resources. Do not use these settings unless you are asked to do so by Pivotal Support.","title":"Severity Levels"},{"location":"monitoring/logging/","text":"Configuring and Using SnappyData Log Files \u00b6 By default SnappyData creates a log file named gfxdserver.log in the current directory when you start a server programmatically or using the snappy-shell utility. SnappyData creates a log file named gfxdlocator.log in the current working directory when you start a locator. You can specify the name and location of the log file by using the JDBC boot property log-file when you start a server or locator. For example: snappy-shell snappydata server start -log-file=/home/user1/log/mysnappystorelog.log Product Usage Logging Each SnappyData locator creates a log file that record the different membership views of the SnappyData distributed system. You can use the contents of these log files to verify that your product usage is compliance with your SnappyData license. Log Message Format Each message in a server or locator log file contains the severity level, timestamp, and other important information. - Severity Levels You can configure the logging system to record only those messages that are at or above a specified logging level. By default, the logging level is set to \"config\", which means that the system logs messages at config, info, warning, error, and severe severity levels. Using java.util.logging.Logger for Application Log Messages Applications that use the SnappyData JDBC peer driver can log messages through the java.util.Logger logging API. You can obtain a handle to the java.util.Logger logger object by entering com.pivotal.gemfirexd after the application connects to the SnappyData cluster with the peer driver. Using Trace Flags for Advanced Debugging SnappyData provides debug trace flags to record additional information about SnappyData features in the log file.","title":"Configuring and Using SnappyData Log Files"},{"location":"monitoring/logging/#configuring-and-using-snappydata-log-files","text":"By default SnappyData creates a log file named gfxdserver.log in the current directory when you start a server programmatically or using the snappy-shell utility. SnappyData creates a log file named gfxdlocator.log in the current working directory when you start a locator. You can specify the name and location of the log file by using the JDBC boot property log-file when you start a server or locator. For example: snappy-shell snappydata server start -log-file=/home/user1/log/mysnappystorelog.log Product Usage Logging Each SnappyData locator creates a log file that record the different membership views of the SnappyData distributed system. You can use the contents of these log files to verify that your product usage is compliance with your SnappyData license. Log Message Format Each message in a server or locator log file contains the severity level, timestamp, and other important information. - Severity Levels You can configure the logging system to record only those messages that are at or above a specified logging level. By default, the logging level is set to \"config\", which means that the system logs messages at config, info, warning, error, and severe severity levels. Using java.util.logging.Logger for Application Log Messages Applications that use the SnappyData JDBC peer driver can log messages through the java.util.Logger logging API. You can obtain a handle to the java.util.Logger logger object by entering com.pivotal.gemfirexd after the application connects to the SnappyData cluster with the peer driver. Using Trace Flags for Advanced Debugging SnappyData provides debug trace flags to record additional information about SnappyData features in the log file.","title":"Configuring and Using SnappyData Log Files"},{"location":"monitoring/membership-logging/","text":"Product Usage Logging \u00b6 Each SnappyData locator creates a log file that record the different membership views of the SnappyData distributed system. You can use the contents of these log files to verify that your product usage is compliance with your SnappyData license. The locator membership view log file is stored in the locator member directory using the file format locator XXXX views.log , where XXXX is the port number that the locator uses for peer connections. A locator records a membership view log entry when the it starts, and adds a new entry each time a member joins the system. Each time a new data store joins the system, a log entry records the total number of servers and clients in the system at that time. For example: [info 2014/01/29 16:43:34.493 PST <CacheServerLauncher#serverConnector> tid=0xe] Log opened with new distributed system connection. View(creator=ward(14487)<v0>:4720, viewId=0, [ward(14487)<v0>:4720]) [info 2014/01/29 16:44:35.421 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14501)<v1>:21541. View(creator=ward(14487)<v0>:4720, viewId=1, [ward(14487)<v0>:4720, ward(14501)<v1>:21541]) [info 2014/01/29 16:44:52.986 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14512)<v3>:52370. View(creator=ward(14487)<v0>:4720, viewId=3, [ward(14487)<v0>:4720, ward(14512)<v3>:52370]) [info 2014/01/29 16:44:57.765 PST <Pooled High Priority Message Processor 2> tid=0x4c] server summary: 1 cache servers with 0 client connection load current cache servers : ward(14512)<v3>:52370 [info 2014/01/29 17:06:40.752 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14578)<v4>:5088. View(creator=ward(14487)<v0>:4720, viewId=4, [ward(14487)<v0>:4720, ward(14512)<v3>:52370, ward(14578)<v4>:5088]) [info 2014/01/29 17:06:45.549 PST <Pooled High Priority Message Processor 1> tid=0x47] server summary: 2 cache servers with 0 client connection load current cache servers : ward(14512)<v3>:52370 ward(14578)<v4>:5088 [info 2014/01/29 17:09:38.378 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14604)<v6>:8818. View(creator=ward(14487)<v0>:4720, viewId=6, [ward(14487)<v0>:4720, ward(14578)<v4>:5088, ward(14604)<v6>:8818]) [info 2014/01/29 17:09:43.367 PST <Pooled High Priority Message Processor 2> tid=0x4c] server summary: 2 cache servers with 0 client connection load current cache servers : ward(14578)<v4>:5088 ward(14604)<v6>:8818 The above output shows that some data stores left the distributed system and others joined the system, because the last membership detail messages both show 2 servers in the system. Note that no log entries are created when a member leaves the system. By default, the size of the membership view log file is limited to 5MB. You can change the maximum file size using the max_view_log_size property. After the maximum file size is reached, the locator deletes the log file and begins recording messages in a new file.","title":"Product Usage Logging"},{"location":"monitoring/membership-logging/#product-usage-logging","text":"Each SnappyData locator creates a log file that record the different membership views of the SnappyData distributed system. You can use the contents of these log files to verify that your product usage is compliance with your SnappyData license. The locator membership view log file is stored in the locator member directory using the file format locator XXXX views.log , where XXXX is the port number that the locator uses for peer connections. A locator records a membership view log entry when the it starts, and adds a new entry each time a member joins the system. Each time a new data store joins the system, a log entry records the total number of servers and clients in the system at that time. For example: [info 2014/01/29 16:43:34.493 PST <CacheServerLauncher#serverConnector> tid=0xe] Log opened with new distributed system connection. View(creator=ward(14487)<v0>:4720, viewId=0, [ward(14487)<v0>:4720]) [info 2014/01/29 16:44:35.421 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14501)<v1>:21541. View(creator=ward(14487)<v0>:4720, viewId=1, [ward(14487)<v0>:4720, ward(14501)<v1>:21541]) [info 2014/01/29 16:44:52.986 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14512)<v3>:52370. View(creator=ward(14487)<v0>:4720, viewId=3, [ward(14487)<v0>:4720, ward(14512)<v3>:52370]) [info 2014/01/29 16:44:57.765 PST <Pooled High Priority Message Processor 2> tid=0x4c] server summary: 1 cache servers with 0 client connection load current cache servers : ward(14512)<v3>:52370 [info 2014/01/29 17:06:40.752 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14578)<v4>:5088. View(creator=ward(14487)<v0>:4720, viewId=4, [ward(14487)<v0>:4720, ward(14512)<v3>:52370, ward(14578)<v4>:5088]) [info 2014/01/29 17:06:45.549 PST <Pooled High Priority Message Processor 1> tid=0x47] server summary: 2 cache servers with 0 client connection load current cache servers : ward(14512)<v3>:52370 ward(14578)<v4>:5088 [info 2014/01/29 17:09:38.378 PST <DM-MemberEventInvoker> tid=0x1c] A new member joined: ward(14604)<v6>:8818. View(creator=ward(14487)<v0>:4720, viewId=6, [ward(14487)<v0>:4720, ward(14578)<v4>:5088, ward(14604)<v6>:8818]) [info 2014/01/29 17:09:43.367 PST <Pooled High Priority Message Processor 2> tid=0x4c] server summary: 2 cache servers with 0 client connection load current cache servers : ward(14578)<v4>:5088 ward(14604)<v6>:8818 The above output shows that some data stores left the distributed system and others joined the system, because the last membership detail messages both show 2 servers in the system. Note that no log entries are created when a member leaves the system. By default, the size of the membership view log file is limited to 5MB. You can change the maximum file size using the max_view_log_size property. After the maximum file size is reached, the locator deletes the log file and begins recording messages in a new file.","title":"Product Usage Logging"},{"location":"monitoring/metrics/","text":"Monitoring with Metrics \u00b6 Metrics constitutes of the measurements of resource usage or behavior that can be observed and collected all over SnappyData clusters. Using the Metrics, you can monitor the cluster health and statistics. Monitoring the clusters allows you to do the following: Increase availability by quickly detecting downtime/degradation. Facilitate performance monitoring by external tools. These external tools can handle functions such as metrics aggregation, alerting, and visualization. SnappyData uses the Spark\u2019s Metrics Subsystem for metrics collection. This system allows you to publish the metrics to a variety of sinks that you can enable for metrics collection. Spark supports the following sinks for Metrics. SnappyData can send metrics to all these sinks. However, MetricsServlet sink is enabled by default and all the metrics get published here. Sink Description MetricsServlet Adds a servlet within the existing Spark UI to serve metrics data as JSON data. The metrics are published here by default and can be easily accessible via a web page. The servlet URL is http:// :5050/metrics/json. JmxSink Register metrics for viewing in a JMX console. Monitoring tools/ agents that understand the JMX protocol, such as JMX exporter from Prometheus, can export this information to external monitoring systems for storage and visualization. CsvSink Exports metrics data to CSV files at regular intervals. ConsoleSink Logs metrics information to the console. GraphiteSink Sends metrics to a Graphite node. Slf4jSink Sends metrics to slf4j as log entries. Note You cannot store metrics for long term retention as well as for archival purposes. Detailed instructions for integration with external monitoring systems to build monitoring dashboards by sourcing metrics data from the sinks will be published in a future release. Whenever you start the cluster, the metrics are published and made available through commonly used sinks, which can be consumed by monitoring tools. Note Metrics are not published for Smart Connector mode. The following type of metrics are made available for SnappyData: Availability Metrics This type of metrics alerts about the status and availability of the cluster. Examples: Cluster and node status Cluster and node uptime/downtime Member Statistics Table Statistics Performance Metrics This type of metric provides insights into system performance. Examples: Resource usage for system-level resources such as CPU, memory, disk, and network. Resource usage for SnappyData application/component resources such as heap or off-heap memory and their percentage utilization etc. Enabling the Sinks for Metrics collection \u00b6 If you want to publish the SnappyData metrics using any of the sinks, you must enable these sinks in the metrics.properties file, which is located in the conf folder of the SnappyData installation directory. To enable a sink, do the following: Open the conf folder and change the name of metrics.properties.template to metrics.properties file. cp metrics.properties.template metrics.properties Uncomment the properties (remove the #) of the sink that you want to enable and provide the necessary property values. For example, for enabling CsvSink, you must uncomment the following properties: # Enable CsvSink for all instances by class name *.sink.csv.class=org.apache.spark.metrics.sink.CsvSink # Polling period for the CsvSink *.sink.csv.period=1 # Unit of the polling period for the CsvSink *.sink.csv.unit=minutes # Polling directory for CsvSink *.sink.csv.directory=/tmp/ # Polling period for the CsvSink specific for the worker instance worker.sink.csv.period=10 # Unit of the polling period for the CsvSink specific for the worker instance worker.sink.csv.unit=minutes Start the cluster for the configurations to take effect by executing this command from the SnappyData installation folder: ./sbin/snappy-start-all.sh Note In case you make changes to metrics.properties file when the cluster is running, you must always restart the cluster for the configuration changes to reflect. Accessing Metrics \u00b6 You can check the SnappyData metrics collected through MetricsServlet , which is enabled by default, at the following URL: <LeadNode-hostname>:<5050>/metrics/json Accessing Metrics from JmxSink \u00b6 Do the following to access Metrics from JmxSink: If org.apache.spark.metrics.sink.JmxSink is enabled in the metrics.properties file, you can use JConsole to access Metrics captured through JmxSink. To enable JmxSink, uncomment the properties for the JmxSink in the metrics.properties file. For example: *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink Launch JConsole and connect to the host on which primary lead node is running, select the process ID of SnappyData primary lead process. Go to MBeans > metrics , to access the SnappyData Metrics. Note To access metrics for remote processes, you need to add the following JMX remote properties in the node configuration file: -jmx-manager=true -jmx-manager-start=true -jmx-manager-port=<port_value> Accessing Metrics from CsvSink \u00b6 Do the following to access Metrics from CsvSink: If org.apache.spark.metrics.sink.CsvSink is enabled in the metrics.properties file, you can access the metrics captured in the CSV files at the location specified in the *.sink.csv.directory property. To enable CsvSink, uncomment the properties for the CsvSink in the metrics.properties file. For example, *.sink.csv.class=org.apache.spark.metrics.sink.CsvSink Note You must ensure that the location mentioned in the *.sink.csv.directory property already exists and has write permissions. Disabling Metrics Collection \u00b6 To disable the metrics collection for a specific Sink, edit the metrics.properties file and comment the corresponding entries by adding the prefix # to the required lines. Save the file and then restart the cluster. Statistics \u00b6 The following statistics in SnappyData are collected when you Metrics monitoring is enabled. Note All memory statistics are published in MB units. TableCountStatistics \u00b6 Source Description Metric Type Probable Values embeddedTablesCount Count of the row and column tables. Gauge rowTablesCount Count of the row tables. Gauge columnTablesCount Count of the column tables. Gauge externalTablesCount Count of external tables. Gauge Table Statistics \u00b6 Source Description Metric Type Probable Values tableName For each table, the tableName is provided as the fully qualified table name. Gauge isColumnTable Specifies if it is a column table. Gauge Boolean value(True or False). rowCount Number of rows. Gauge Specifies the number of rows inserted which is otherwise 0. sizeInMemory Table size in memory. Gauge sizeSpillToDisk Table size spilled to disk. Gauge totalSize Total size of the table. Gauge isReplicatedTable Specifies if it is a replicated table. Gauge Boolean Values(True or False) bucketCount Number of buckets. Gauge redundancy Specifies if the redundancy is enabled. Gauge Specifies the redundancy value provided while creating a table which is otherwise 0. isRedundancyImpaired Specifies if the redundancy is impaired. (since one or more replicas are unavailable) Gauge Boolean Values(True or False) isAnyBucketLost Specifies if any buckets are lost. (UI shows bucket count in red color.) Gauge Boolean Values(True or False) ExternalTableStatistics \u00b6 Source Description Metric Type Probable Values dataSourcePath Data source path. Gauge Path of the file from which data to be loaded provider Data source provider. Gauge csv, parquet, orc, json, etc tableName Table name Gauge tableType Table type Gauge EXTERNAL MemberStatistics \u00b6 Source Description Metric Type Probable Values totalMembersCount Count of total members. Gauge leadCount Count of leads. Gauge locatorCount Count of locators. Gauge dataServerCount Count of data servers. Gauge connectorCount Count of connectors. Gauge MemberStatistics \u00b6 Source Description Metric Type Probable Values memberId Contains IP address, port and process ID. Gauge nameOrId Contains IP address, port, and pid or the name Gauge host IP address or name of the machine. Gauge shortDirName Relative path of the log directory. Gauge fullDirName Absolute path of log directory. Gauge logFile Name of the log file. Gauge processId Member's process ID. Gauge diskStoreUUID Member's unique disk store UUID. Gauge diskStoreName Member's disk store name. Gauge status Current status of the member. Gauge Running / Stopped memberType Type (Lead/ Server/ Locator/ Accessor). Gauge Lead / Locator/ Data Server isLocator Flag returns true if the member is locator or false otherwise. Gauge Boolean value(True or false) isDataServer Flag returns true if the member is data server or false otherwise. Gauge Boolean value(True or false) isLead Flag returns true if the member is lead or false otherwise. Gauge Boolean value(True or false) isActiveLead Flag returns true if the member is primary lead or false otherwise. Gauge Boolean value(True or false) cores Total number of cores. Gauge cpuActive Number of active CPUs. Gauge clients Number of clients connected. Gauge jvmHeapMax Max JVM heap size. Gauge jvmHeapUsed Used JVM heap size. Gauge jvmHeapTotal Total JVM heap size. Gauge jvmHeapFree Free JVM heap size. Gauge heapStoragePoolUsed Used heap storage pool. Gauge heapStoragePoolSize Heap storage pool size. Gauge heapExecutionPoolUsed Used heap execution pool. Gauge heapExecutionPoolSize Heap execution pool size. Gauge heapMemorySize Heap memory size. Gauge heapMemoryUsed Used heap memory. Gauge offHeapStoragePoolUsed Used off-heap storage pool. Gauge offHeapStoragePoolSize Off-heap storage pool size. Gauge offHeapExecutionPoolUsed Used off-heap execution pool. Gauge offHeapExecutionPoolSize Off-heap execution pool size. Gauge offHeapMemorySize Off-heap memory size. Gauge offHeapMemoryUsed Used off-heap memory. Gauge diskStoreDiskSpace Disk store disk space. Gauge cpuUsage CPU usage. Gauge jvmUsage JVM usage. Gauge heapUsage Heap usage. Gauge heapStorageUsage Heap storage usage. Gauge heapExecutionUsage Heap execution usage. Gauge offHeapUsage Off-heap usage. Gauge offHeapStorageUsage Off-heap storage usage. Gauge offHeapExecutionUsage Off-heap execution size Gauge aggrMemoryUsage Aggregate memory usage. Gauge cpuUsageTrends CPU usage trends. Histogram jvmUsageTrends JVM usage trends. Histogram heapUsageTrends Heap usage trends. Histogram heapStorageUsageTrends Heap storage usage trends. Histogram heapExecutionUsageTrends Heap execution usage trends. Histogram offHeapUsageTrends Off-heap usage trends. Histogram offHeapStorageUsageTrends Off-heap storage usage trends. Histogram offHeapExecutionUsageTrends Off-heap execution usage trends. Histogram aggrMemoryUsageTrends Aggregate memory usage trends. Histogram diskStoreDiskSpaceTrend Disk store and space trends. Histogram ClusterStatistics \u00b6 Source Description Metric Type Probable Values totalCores Totals number of cores in the cluster. Gauge jvmUsageTrends JVM usage trends in the cluster. Histogram heapUsageTrends Heap usage trends in the cluster. Histogram heapStorageUsageTrends JVM usage trends in the cluster. Histogram heapExecutionUsageTrends Heap execution usage trends in the cluster. Histogram offHeapUsageTrends Off-heap usage trends in the cluster. Histogram offHeapStorageUsageTrends Off-heap storage usage trends in the cluster. Histogram offHeapExecutionUsageTrends Off-heap execution usage trends in the cluster. Histogram aggrMemoryUsageTrends Aggregate memory usage trends in the cluster. Histogram diskStoreDiskSpaceTrend Disk store and space trends in the cluster. Histogram","title":"Monitoring with Metrics"},{"location":"monitoring/metrics/#monitoring-with-metrics","text":"Metrics constitutes of the measurements of resource usage or behavior that can be observed and collected all over SnappyData clusters. Using the Metrics, you can monitor the cluster health and statistics. Monitoring the clusters allows you to do the following: Increase availability by quickly detecting downtime/degradation. Facilitate performance monitoring by external tools. These external tools can handle functions such as metrics aggregation, alerting, and visualization. SnappyData uses the Spark\u2019s Metrics Subsystem for metrics collection. This system allows you to publish the metrics to a variety of sinks that you can enable for metrics collection. Spark supports the following sinks for Metrics. SnappyData can send metrics to all these sinks. However, MetricsServlet sink is enabled by default and all the metrics get published here. Sink Description MetricsServlet Adds a servlet within the existing Spark UI to serve metrics data as JSON data. The metrics are published here by default and can be easily accessible via a web page. The servlet URL is http:// :5050/metrics/json. JmxSink Register metrics for viewing in a JMX console. Monitoring tools/ agents that understand the JMX protocol, such as JMX exporter from Prometheus, can export this information to external monitoring systems for storage and visualization. CsvSink Exports metrics data to CSV files at regular intervals. ConsoleSink Logs metrics information to the console. GraphiteSink Sends metrics to a Graphite node. Slf4jSink Sends metrics to slf4j as log entries. Note You cannot store metrics for long term retention as well as for archival purposes. Detailed instructions for integration with external monitoring systems to build monitoring dashboards by sourcing metrics data from the sinks will be published in a future release. Whenever you start the cluster, the metrics are published and made available through commonly used sinks, which can be consumed by monitoring tools. Note Metrics are not published for Smart Connector mode. The following type of metrics are made available for SnappyData: Availability Metrics This type of metrics alerts about the status and availability of the cluster. Examples: Cluster and node status Cluster and node uptime/downtime Member Statistics Table Statistics Performance Metrics This type of metric provides insights into system performance. Examples: Resource usage for system-level resources such as CPU, memory, disk, and network. Resource usage for SnappyData application/component resources such as heap or off-heap memory and their percentage utilization etc.","title":"Monitoring with Metrics"},{"location":"monitoring/metrics/#enabling-the-sinks-for-metrics-collection","text":"If you want to publish the SnappyData metrics using any of the sinks, you must enable these sinks in the metrics.properties file, which is located in the conf folder of the SnappyData installation directory. To enable a sink, do the following: Open the conf folder and change the name of metrics.properties.template to metrics.properties file. cp metrics.properties.template metrics.properties Uncomment the properties (remove the #) of the sink that you want to enable and provide the necessary property values. For example, for enabling CsvSink, you must uncomment the following properties: # Enable CsvSink for all instances by class name *.sink.csv.class=org.apache.spark.metrics.sink.CsvSink # Polling period for the CsvSink *.sink.csv.period=1 # Unit of the polling period for the CsvSink *.sink.csv.unit=minutes # Polling directory for CsvSink *.sink.csv.directory=/tmp/ # Polling period for the CsvSink specific for the worker instance worker.sink.csv.period=10 # Unit of the polling period for the CsvSink specific for the worker instance worker.sink.csv.unit=minutes Start the cluster for the configurations to take effect by executing this command from the SnappyData installation folder: ./sbin/snappy-start-all.sh Note In case you make changes to metrics.properties file when the cluster is running, you must always restart the cluster for the configuration changes to reflect.","title":"Enabling the Sinks for Metrics collection"},{"location":"monitoring/metrics/#accessing-metrics","text":"You can check the SnappyData metrics collected through MetricsServlet , which is enabled by default, at the following URL: <LeadNode-hostname>:<5050>/metrics/json","title":"Accessing Metrics"},{"location":"monitoring/metrics/#accessing-metrics-from-jmxsink","text":"Do the following to access Metrics from JmxSink: If org.apache.spark.metrics.sink.JmxSink is enabled in the metrics.properties file, you can use JConsole to access Metrics captured through JmxSink. To enable JmxSink, uncomment the properties for the JmxSink in the metrics.properties file. For example: *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink Launch JConsole and connect to the host on which primary lead node is running, select the process ID of SnappyData primary lead process. Go to MBeans > metrics , to access the SnappyData Metrics. Note To access metrics for remote processes, you need to add the following JMX remote properties in the node configuration file: -jmx-manager=true -jmx-manager-start=true -jmx-manager-port=<port_value>","title":"Accessing Metrics from JmxSink"},{"location":"monitoring/metrics/#accessing-metrics-from-csvsink","text":"Do the following to access Metrics from CsvSink: If org.apache.spark.metrics.sink.CsvSink is enabled in the metrics.properties file, you can access the metrics captured in the CSV files at the location specified in the *.sink.csv.directory property. To enable CsvSink, uncomment the properties for the CsvSink in the metrics.properties file. For example, *.sink.csv.class=org.apache.spark.metrics.sink.CsvSink Note You must ensure that the location mentioned in the *.sink.csv.directory property already exists and has write permissions.","title":"Accessing Metrics from CsvSink"},{"location":"monitoring/metrics/#disabling-metrics-collection","text":"To disable the metrics collection for a specific Sink, edit the metrics.properties file and comment the corresponding entries by adding the prefix # to the required lines. Save the file and then restart the cluster.","title":"Disabling Metrics Collection"},{"location":"monitoring/metrics/#statistics","text":"The following statistics in SnappyData are collected when you Metrics monitoring is enabled. Note All memory statistics are published in MB units.","title":"Statistics"},{"location":"monitoring/metrics/#tablecountstatistics","text":"Source Description Metric Type Probable Values embeddedTablesCount Count of the row and column tables. Gauge rowTablesCount Count of the row tables. Gauge columnTablesCount Count of the column tables. Gauge externalTablesCount Count of external tables. Gauge","title":"TableCountStatistics"},{"location":"monitoring/metrics/#table-statistics","text":"Source Description Metric Type Probable Values tableName For each table, the tableName is provided as the fully qualified table name. Gauge isColumnTable Specifies if it is a column table. Gauge Boolean value(True or False). rowCount Number of rows. Gauge Specifies the number of rows inserted which is otherwise 0. sizeInMemory Table size in memory. Gauge sizeSpillToDisk Table size spilled to disk. Gauge totalSize Total size of the table. Gauge isReplicatedTable Specifies if it is a replicated table. Gauge Boolean Values(True or False) bucketCount Number of buckets. Gauge redundancy Specifies if the redundancy is enabled. Gauge Specifies the redundancy value provided while creating a table which is otherwise 0. isRedundancyImpaired Specifies if the redundancy is impaired. (since one or more replicas are unavailable) Gauge Boolean Values(True or False) isAnyBucketLost Specifies if any buckets are lost. (UI shows bucket count in red color.) Gauge Boolean Values(True or False)","title":"Table Statistics"},{"location":"monitoring/metrics/#externaltablestatistics","text":"Source Description Metric Type Probable Values dataSourcePath Data source path. Gauge Path of the file from which data to be loaded provider Data source provider. Gauge csv, parquet, orc, json, etc tableName Table name Gauge tableType Table type Gauge EXTERNAL","title":"ExternalTableStatistics"},{"location":"monitoring/metrics/#memberstatistics","text":"Source Description Metric Type Probable Values totalMembersCount Count of total members. Gauge leadCount Count of leads. Gauge locatorCount Count of locators. Gauge dataServerCount Count of data servers. Gauge connectorCount Count of connectors. Gauge","title":"MemberStatistics"},{"location":"monitoring/metrics/#memberstatistics_1","text":"Source Description Metric Type Probable Values memberId Contains IP address, port and process ID. Gauge nameOrId Contains IP address, port, and pid or the name Gauge host IP address or name of the machine. Gauge shortDirName Relative path of the log directory. Gauge fullDirName Absolute path of log directory. Gauge logFile Name of the log file. Gauge processId Member's process ID. Gauge diskStoreUUID Member's unique disk store UUID. Gauge diskStoreName Member's disk store name. Gauge status Current status of the member. Gauge Running / Stopped memberType Type (Lead/ Server/ Locator/ Accessor). Gauge Lead / Locator/ Data Server isLocator Flag returns true if the member is locator or false otherwise. Gauge Boolean value(True or false) isDataServer Flag returns true if the member is data server or false otherwise. Gauge Boolean value(True or false) isLead Flag returns true if the member is lead or false otherwise. Gauge Boolean value(True or false) isActiveLead Flag returns true if the member is primary lead or false otherwise. Gauge Boolean value(True or false) cores Total number of cores. Gauge cpuActive Number of active CPUs. Gauge clients Number of clients connected. Gauge jvmHeapMax Max JVM heap size. Gauge jvmHeapUsed Used JVM heap size. Gauge jvmHeapTotal Total JVM heap size. Gauge jvmHeapFree Free JVM heap size. Gauge heapStoragePoolUsed Used heap storage pool. Gauge heapStoragePoolSize Heap storage pool size. Gauge heapExecutionPoolUsed Used heap execution pool. Gauge heapExecutionPoolSize Heap execution pool size. Gauge heapMemorySize Heap memory size. Gauge heapMemoryUsed Used heap memory. Gauge offHeapStoragePoolUsed Used off-heap storage pool. Gauge offHeapStoragePoolSize Off-heap storage pool size. Gauge offHeapExecutionPoolUsed Used off-heap execution pool. Gauge offHeapExecutionPoolSize Off-heap execution pool size. Gauge offHeapMemorySize Off-heap memory size. Gauge offHeapMemoryUsed Used off-heap memory. Gauge diskStoreDiskSpace Disk store disk space. Gauge cpuUsage CPU usage. Gauge jvmUsage JVM usage. Gauge heapUsage Heap usage. Gauge heapStorageUsage Heap storage usage. Gauge heapExecutionUsage Heap execution usage. Gauge offHeapUsage Off-heap usage. Gauge offHeapStorageUsage Off-heap storage usage. Gauge offHeapExecutionUsage Off-heap execution size Gauge aggrMemoryUsage Aggregate memory usage. Gauge cpuUsageTrends CPU usage trends. Histogram jvmUsageTrends JVM usage trends. Histogram heapUsageTrends Heap usage trends. Histogram heapStorageUsageTrends Heap storage usage trends. Histogram heapExecutionUsageTrends Heap execution usage trends. Histogram offHeapUsageTrends Off-heap usage trends. Histogram offHeapStorageUsageTrends Off-heap storage usage trends. Histogram offHeapExecutionUsageTrends Off-heap execution usage trends. Histogram aggrMemoryUsageTrends Aggregate memory usage trends. Histogram diskStoreDiskSpaceTrend Disk store and space trends. Histogram","title":"MemberStatistics"},{"location":"monitoring/metrics/#clusterstatistics","text":"Source Description Metric Type Probable Values totalCores Totals number of cores in the cluster. Gauge jvmUsageTrends JVM usage trends in the cluster. Histogram heapUsageTrends Heap usage trends in the cluster. Histogram heapStorageUsageTrends JVM usage trends in the cluster. Histogram heapExecutionUsageTrends Heap execution usage trends in the cluster. Histogram offHeapUsageTrends Off-heap usage trends in the cluster. Histogram offHeapStorageUsageTrends Off-heap storage usage trends in the cluster. Histogram offHeapExecutionUsageTrends Off-heap execution usage trends in the cluster. Histogram aggrMemoryUsageTrends Aggregate memory usage trends in the cluster. Histogram diskStoreDiskSpaceTrend Disk store and space trends in the cluster. Histogram","title":"ClusterStatistics"},{"location":"monitoring/monitor-manage/","text":"Getting Information from SnappyData System Tables \u00b6 You can monitor many common aspects of SnappyData by using SQL commands (system procedures and simple queries) to collect and analyze data in SnappyData system tables. Distributed System Membership Information \u00b6 The SYS.MEMBERS table provides information about all peers and servers that make up the SnappyData system. You can use different queries to obtain details about individual members and their role in the cluster. Determining Cluster Membership \u00b6 To display a list of all members that participate in a given cluster, simply query all ID entries in sys.members. For example: snappy> select ID from SYS.MEMBERS; ID ----------------------------------- 127.0.0.1(10889)<v1>:43634 127.0.0.1(11045)<v2>:19283 127.0.0.1(10749)<ec><v0>:51430 3 rows selected The number of rows returned corresponds to the total number of peers, servers, and locators in the cluster. To determine each member's role in the system, include the KIND column in the query: snappy> select ID, KIND from SYS.MEMBERS; ID |KIND ----------------------------------------------------------------------- 127.0.0.1(10889)<v1>:43634 |datastore(normal) 127.0.0.1(11045)<v2>:19283 |accessor(normal) 127.0.0.1(10749)<ec><v0>:51430 |locator(normal) 3 rows selected To view the members of cluster, query: snappy> show members; ID |HOST |KIND |STATUS |THRIFTSERVERS |SERVERGROUPS --------------------------------------------------------------------------------------------------------- 127.0.0.1(10749)<ec><v0>:51430|localhost|locator |RUNNING|localhost/127.0.0.1[1527]| 127.0.0.1(10889)<v1>:43634 |localhost|datastore |RUNNING|localhost/127.0.0.1[1528]| 127.0.0.1(11045)<v2>:19283 |localhost|primary lead |RUNNING| | 3 rows selected Data store members host data in the cluster, while accessor members do not host data. This role is determined by the host-data boot property. If a cluster contains only a single data store, its KIND is listed as \"loner\". Table and Data Storage Information \u00b6 The SYS.SYSTABLES table provides information about all tables that are created in the SnappyData system. You can use different queries to obtain details about tables and the server groups that host data for those tables. Displaying a List of Tables Determining Whether a Table Is Replicated or Partitioned Determining How Persistent Data Is Stored Displaying Eviction Settings Displaying Indexes Displaying a List of Tables \u00b6 To display a list of all tables in the cluster: snappy> select TABLESCHEMANAME, TABLENAME from SYS.SYSTABLES order by TABLESCHEMANAME; TABLESCHEMANAME |TABLENAME ------------------------------------------------------------------------------------------------------------------------ APP |SUPPLIER_1 APP |SUPPLIER APP |AIRLINEREF APP |AIRLINE APP |SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_ APP |EMPLOYEE SNAPPY_HIVE_METASTORE |TBLS SNAPPY_HIVE_METASTORE |PARTITION_PARAMS SNAPPY_HIVE_METASTORE |SKEWED_COL_VALUE_LOC_MAP SNAPPY_HIVE_METASTORE |FUNCS SNAPPY_HIVE_METASTORE |SDS SNAPPY_HIVE_METASTORE |SERDE_PARAMS SNAPPY_HIVE_METASTORE |PART_COL_STATS SNAPPY_HIVE_METASTORE |SKEWED_STRING_LIST SNAPPY_HIVE_METASTORE |DBS SNAPPY_HIVE_METASTORE |PARTITIONS SNAPPY_HIVE_METASTORE |BUCKETING_COLS SNAPPY_HIVE_METASTORE |FUNC_RU SNAPPY_HIVE_METASTORE |SKEWED_VALUES SNAPPY_HIVE_METASTORE |ROLES SNAPPY_HIVE_METASTORE |SORT_COLS SNAPPY_HIVE_METASTORE |SD_PARAMS SNAPPY_HIVE_METASTORE |TAB_COL_STATS SNAPPY_HIVE_METASTORE |GLOBAL_PRIVS SNAPPY_HIVE_METASTORE |SKEWED_COL_NAMES SNAPPY_HIVE_METASTORE |SKEWED_STRING_LIST_VALUES SNAPPY_HIVE_METASTORE |VERSION SNAPPY_HIVE_METASTORE |CDS SNAPPY_HIVE_METASTORE |SEQUENCE_TABLE SNAPPY_HIVE_METASTORE |PARTITION_KEYS SNAPPY_HIVE_METASTORE |TABLE_PARAMS SNAPPY_HIVE_METASTORE |DATABASE_PARAMS SNAPPY_HIVE_METASTORE |COLUMNS_V2 SNAPPY_HIVE_METASTORE |SERDES SNAPPY_HIVE_METASTORE |PARTITION_KEY_VALS SYS |GATEWAYSENDERS SYS |SYSSTATEMENTS SYS |SYSKEYS SYS |SYSROLES SYS |SYSFILES SYS |SYSROUTINEPERMS SYS |SYSCONSTRAINTS SYS |SYSCOLPERMS SYS |SYSHDFSSTORES SYS |SYSDEPENDS SYS |SYSALIASES SYS |SYSTABLEPERMS SYS |SYSTABLES SYS |SYSVIEWS SYS |ASYNCEVENTLISTENERS SYS |SYSCHECKS SYS |SYSSTATISTICS SYS |SYSCONGLOMERATES SYS |GATEWAYRECEIVERS SYS |SYSTRIGGERS SYS |SYSDISKSTORES SYS |SYSSCHEMAS SYS |SYSFOREIGNKEYS SYS |SYSCOLUMNS SYSIBM |SYSDUMMY1 SYSSTAT |SYSXPLAIN_RESULTSETS SYSSTAT |SYSXPLAIN_STATEMENTS 60 rows selected Determining Whether a Table Is Replicated or Partitioned \u00b6 The DATAPOLICY column specifies whether a table is replicated or partitioned, and whether a table is persisted to a disk store. For example: snappy> select TABLENAME, DATAPOLICY from SYS.SYSTABLES where TABLESCHEMANAME = 'APP'; TABLENAME |DATAPOLICY -------------------------------------------------------------------------- SUPPLIER_1 |PERSISTENT_PARTITION SUPPLIER |PERSISTENT_PARTITION AIRLINEREF |PERSISTENT_REPLICATE AIRLINE |PERSISTENT_REPLICATE SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_ |PERSISTENT_PARTITION EMPLOYEE |PERSISTENT_PARTITION 6 rows selected Determining How Persistent Data Is Stored \u00b6 For persistent tables, you can also display the disk store that persists the table's data, and whether the table uses synchronous or asynchronous persistence: snappy> select TABLENAME, DISKATTRS from SYS.SYSTABLES where TABLESCHEMANAME = 'APP'; TABLENAME |DISKATTRS ------------------------------------------------------------------------------------------------------------------------ SUPPLIER_1 |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk SUPPLIER |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk AIRLINEREF |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk AIRLINE |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_ |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk EMPLOYEE |DiskStore is SNAPPY-INTERNAL-DELTA; Synchronous writes to disk 6 rows selected Displaying Eviction Settings \u00b6 Use the EVICTIONATTRS column to determine if a table uses eviction settings and whether a table is configured to overflow to disk. For example: snappy> select TABLENAME, EVICTIONATTRS from SYS.SYSTABLES where TABLESCHEMANAME = 'APP'; TABLENAME |EVICTIONATTRS --------------------------------------------------------------------------------------------------------------------------- SUPPLIER_1 | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer SUPPLIER | algorithm=lru-entry-count; action=overflow-to-disk; maximum=3 AIRLINEREF | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer AIRLINE | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_| algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer EMPLOYEE | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer 6 rows selected","title":"Getting Information from SnappyData System Tables"},{"location":"monitoring/monitor-manage/#getting-information-from-snappydata-system-tables","text":"You can monitor many common aspects of SnappyData by using SQL commands (system procedures and simple queries) to collect and analyze data in SnappyData system tables.","title":"Getting Information from SnappyData System Tables"},{"location":"monitoring/monitor-manage/#distributed-system-membership-information","text":"The SYS.MEMBERS table provides information about all peers and servers that make up the SnappyData system. You can use different queries to obtain details about individual members and their role in the cluster.","title":"Distributed System Membership Information"},{"location":"monitoring/monitor-manage/#determining-cluster-membership","text":"To display a list of all members that participate in a given cluster, simply query all ID entries in sys.members. For example: snappy> select ID from SYS.MEMBERS; ID ----------------------------------- 127.0.0.1(10889)<v1>:43634 127.0.0.1(11045)<v2>:19283 127.0.0.1(10749)<ec><v0>:51430 3 rows selected The number of rows returned corresponds to the total number of peers, servers, and locators in the cluster. To determine each member's role in the system, include the KIND column in the query: snappy> select ID, KIND from SYS.MEMBERS; ID |KIND ----------------------------------------------------------------------- 127.0.0.1(10889)<v1>:43634 |datastore(normal) 127.0.0.1(11045)<v2>:19283 |accessor(normal) 127.0.0.1(10749)<ec><v0>:51430 |locator(normal) 3 rows selected To view the members of cluster, query: snappy> show members; ID |HOST |KIND |STATUS |THRIFTSERVERS |SERVERGROUPS --------------------------------------------------------------------------------------------------------- 127.0.0.1(10749)<ec><v0>:51430|localhost|locator |RUNNING|localhost/127.0.0.1[1527]| 127.0.0.1(10889)<v1>:43634 |localhost|datastore |RUNNING|localhost/127.0.0.1[1528]| 127.0.0.1(11045)<v2>:19283 |localhost|primary lead |RUNNING| | 3 rows selected Data store members host data in the cluster, while accessor members do not host data. This role is determined by the host-data boot property. If a cluster contains only a single data store, its KIND is listed as \"loner\".","title":"Determining Cluster Membership"},{"location":"monitoring/monitor-manage/#table-and-data-storage-information","text":"The SYS.SYSTABLES table provides information about all tables that are created in the SnappyData system. You can use different queries to obtain details about tables and the server groups that host data for those tables. Displaying a List of Tables Determining Whether a Table Is Replicated or Partitioned Determining How Persistent Data Is Stored Displaying Eviction Settings Displaying Indexes","title":"Table and Data Storage Information"},{"location":"monitoring/monitor-manage/#displaying-a-list-of-tables","text":"To display a list of all tables in the cluster: snappy> select TABLESCHEMANAME, TABLENAME from SYS.SYSTABLES order by TABLESCHEMANAME; TABLESCHEMANAME |TABLENAME ------------------------------------------------------------------------------------------------------------------------ APP |SUPPLIER_1 APP |SUPPLIER APP |AIRLINEREF APP |AIRLINE APP |SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_ APP |EMPLOYEE SNAPPY_HIVE_METASTORE |TBLS SNAPPY_HIVE_METASTORE |PARTITION_PARAMS SNAPPY_HIVE_METASTORE |SKEWED_COL_VALUE_LOC_MAP SNAPPY_HIVE_METASTORE |FUNCS SNAPPY_HIVE_METASTORE |SDS SNAPPY_HIVE_METASTORE |SERDE_PARAMS SNAPPY_HIVE_METASTORE |PART_COL_STATS SNAPPY_HIVE_METASTORE |SKEWED_STRING_LIST SNAPPY_HIVE_METASTORE |DBS SNAPPY_HIVE_METASTORE |PARTITIONS SNAPPY_HIVE_METASTORE |BUCKETING_COLS SNAPPY_HIVE_METASTORE |FUNC_RU SNAPPY_HIVE_METASTORE |SKEWED_VALUES SNAPPY_HIVE_METASTORE |ROLES SNAPPY_HIVE_METASTORE |SORT_COLS SNAPPY_HIVE_METASTORE |SD_PARAMS SNAPPY_HIVE_METASTORE |TAB_COL_STATS SNAPPY_HIVE_METASTORE |GLOBAL_PRIVS SNAPPY_HIVE_METASTORE |SKEWED_COL_NAMES SNAPPY_HIVE_METASTORE |SKEWED_STRING_LIST_VALUES SNAPPY_HIVE_METASTORE |VERSION SNAPPY_HIVE_METASTORE |CDS SNAPPY_HIVE_METASTORE |SEQUENCE_TABLE SNAPPY_HIVE_METASTORE |PARTITION_KEYS SNAPPY_HIVE_METASTORE |TABLE_PARAMS SNAPPY_HIVE_METASTORE |DATABASE_PARAMS SNAPPY_HIVE_METASTORE |COLUMNS_V2 SNAPPY_HIVE_METASTORE |SERDES SNAPPY_HIVE_METASTORE |PARTITION_KEY_VALS SYS |GATEWAYSENDERS SYS |SYSSTATEMENTS SYS |SYSKEYS SYS |SYSROLES SYS |SYSFILES SYS |SYSROUTINEPERMS SYS |SYSCONSTRAINTS SYS |SYSCOLPERMS SYS |SYSHDFSSTORES SYS |SYSDEPENDS SYS |SYSALIASES SYS |SYSTABLEPERMS SYS |SYSTABLES SYS |SYSVIEWS SYS |ASYNCEVENTLISTENERS SYS |SYSCHECKS SYS |SYSSTATISTICS SYS |SYSCONGLOMERATES SYS |GATEWAYRECEIVERS SYS |SYSTRIGGERS SYS |SYSDISKSTORES SYS |SYSSCHEMAS SYS |SYSFOREIGNKEYS SYS |SYSCOLUMNS SYSIBM |SYSDUMMY1 SYSSTAT |SYSXPLAIN_RESULTSETS SYSSTAT |SYSXPLAIN_STATEMENTS 60 rows selected","title":"Displaying a List of Tables"},{"location":"monitoring/monitor-manage/#determining-whether-a-table-is-replicated-or-partitioned","text":"The DATAPOLICY column specifies whether a table is replicated or partitioned, and whether a table is persisted to a disk store. For example: snappy> select TABLENAME, DATAPOLICY from SYS.SYSTABLES where TABLESCHEMANAME = 'APP'; TABLENAME |DATAPOLICY -------------------------------------------------------------------------- SUPPLIER_1 |PERSISTENT_PARTITION SUPPLIER |PERSISTENT_PARTITION AIRLINEREF |PERSISTENT_REPLICATE AIRLINE |PERSISTENT_REPLICATE SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_ |PERSISTENT_PARTITION EMPLOYEE |PERSISTENT_PARTITION 6 rows selected","title":"Determining Whether a Table Is Replicated or Partitioned"},{"location":"monitoring/monitor-manage/#determining-how-persistent-data-is-stored","text":"For persistent tables, you can also display the disk store that persists the table's data, and whether the table uses synchronous or asynchronous persistence: snappy> select TABLENAME, DISKATTRS from SYS.SYSTABLES where TABLESCHEMANAME = 'APP'; TABLENAME |DISKATTRS ------------------------------------------------------------------------------------------------------------------------ SUPPLIER_1 |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk SUPPLIER |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk AIRLINEREF |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk AIRLINE |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_ |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk EMPLOYEE |DiskStore is SNAPPY-INTERNAL-DELTA; Synchronous writes to disk 6 rows selected","title":"Determining How Persistent Data Is Stored"},{"location":"monitoring/monitor-manage/#displaying-eviction-settings","text":"Use the EVICTIONATTRS column to determine if a table uses eviction settings and whether a table is configured to overflow to disk. For example: snappy> select TABLENAME, EVICTIONATTRS from SYS.SYSTABLES where TABLESCHEMANAME = 'APP'; TABLENAME |EVICTIONATTRS --------------------------------------------------------------------------------------------------------------------------- SUPPLIER_1 | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer SUPPLIER | algorithm=lru-entry-count; action=overflow-to-disk; maximum=3 AIRLINEREF | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer AIRLINE | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_| algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer EMPLOYEE | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer 6 rows selected","title":"Displaying Eviction Settings"},{"location":"monitoring/monitoring/","text":"SnappyData Monitoring Console \u00b6 SnappyData Monitoring Console is a dashboard that provides a real-time view into cluster members, member logs, resource usage, running Jobs, SQL queries along with performance data. This simple widget based view allows you to navigate easily, visualize, and monitor your cluster. You can monitor the overall status of the cluster as well as the status of each member in the cluster. All the usage details are automatically refreshed after every five seconds. Note SnappyData Monitoring Console is not yet tested and supported on Internet Explorer. To access SnappyData Monitoring Console, start your cluster and open http: <leadhost> :5050/dashboard/ in the web browser. Note <leadhost> is the hostname or IP of the lead node in your cluster which is provided in the conf/leads file. The following topics are covered in this section: Dashboard Member Details Jobs Structured Streaming Stages Spark Cache Environment Executors SQL Note When connecting to a SnappyData cluster using Smart Connector, the information related to SQL , Jobs , and Stages are NOT displayed, as the Jobs and queries are primarily executed in your client Spark cluster. You can find this information on the Spark UI console of your client cluster. Read more about SnappyData Smart Connector Mode here . In cases where you cannot access the SnappyData Monitoring Console to analyse Jobs and tasks, you must turn on the Spark History server . On the top-right side of the SnappyData Monitoring Console page, you can view the version details of SnappyData Snapshot. When you click this, the name and version of the product, the build details, the source revision details and the version number of the underlying spark are displayed. On the top of the dashboard, the date and time details about when the cluster is started are displayed. The period till when the cluster is running is also shown. Dashboard \u00b6 The Dashboard page graphically presents various cluster-level statistics that can be used to monitor the current health status of a cluster. The statistics on the dashboard page can be set to update automatically after every five seconds. If you want to turn off the auto-refresh, use the Auto Refresh switch that is provided on the upper-right corner. The Dashboard page displays the following sections: Cluster Members Tables External Tables You can use the search and sort functionalities in any of the sections, except for the Cluster section. Sorting is enabled to sort items in an ascending and descending order. Further, you can also set the number of items that must be listed in each of these sections. Cluster \u00b6 In the Cluster section, you can view the following graphs which are automatically refreshed: On the Dashboard page, the Cluster section displays the date and time when the cluster was launched. It also displays the cluster's up-time since cluster was launched. You can view the total number of physical CPU cores present in your cluster on the top-right side of the page. Graphs Description CPU Usage Graphically presents the trend of CPU utilization by all the nodes in the cluster for the last 15 minutes. The utilization is represented in percentage value. Heap Usage Graphically presents the collective utilization of Heap Memory by all the nodes in the cluster. This graph displays three trend lines which corresponds to the utilization of Heap Memory for the following: Storage Execution JVM Off-Heap Usage Graphically presents the collective utilization of Off-Heap Memory by all the nodes in the cluster. This graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following: Storage Execution Disk Space Graphically presents the collective utilization of disk space memory by all the nodes in the cluster. Members \u00b6 In the Members section, you can view, in a tabular format, the details of each locator, data server, and lead member within a cluster. The details are automatically refreshed after every five seconds. This table provides member details in the following columns: Column Description Status Displays the status of the members, which can be either Running or Stopped . Member Displays a brief description of the member. Click the link in the column to view the Member Details where the usage trends and statistics of the members are shown along with the Member Logs . Click the drop-down arrow to find information such as the IP address of the host, the current working directory, and the Process ID number. Type Displays the type of the member. The type can be LEAD, LOCATOR, or DATA SERVER. The name of the active lead member is displayed in bold letters. CPU Usage Displays the CPU utilized by the member's host and it's trend over last 15 mins. Memory Usage Displays the collective Heap and Off-Heap memory utilization of a cluster member and it's trend over last 15 mins. Heap Memory Displays the member's utilized Heap memory versus total Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Heap Memory for storage, execution, and JVM. Off-Heap Memory Displays the member's used Off-Heap memory and total Off-Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Off-Heap memory for storage and execution. Status Description Member is running. Member has stopped or is unavailable. Tables \u00b6 The Tables section lists all the tables in the cluster along with their corresponding statistical details. All these details are automatically refreshed after every five seconds. The following columns are displayed in this section: Column Description Name Displays the name of the data table. Storage Model Displays the data storage model of the data table. Possible models are ROW and COLUMN . Distribution Type Displays the data distribution type for the table. Possible values are: PARTITION REPLICATE Row Count Displays the row count, which is the number of records present in the data table. In-Memory Size Displays the heap memory used by data table to store its data. If less than Total Size then the data is overflowing to disk. Spill-to-Disk Size Displays size of data overflown to disk Total Size Displays the collective physical memory and disk overflow space used by the data table to store its data. Buckets Displays the total number of buckets in the data table. If a number displayed in red here, it indicates that some of the buckets are offline. Redundancy Displays the number of redundant copies. Redundancy value 0 indicates that redundant copies are not configured. Redundancy value 1 indicates that one redundant copy is configured. Redundancy value NA indicates that redundancy is not applicable. Redundancy Status Displays whether redundancy criteria is satisfied or broken. Redundancy status Satisfied indicates that all the configured redundant copies are available. Redundancy status Broken indicates that some of the redundant copies are not available. Redundancy status NA indicates that redundancy is not applicable. External Tables \u00b6 The External Tables section lists all the external tables present in the cluster along with their various statistical details. The displayed details are automatically refreshed after every five seconds. The following columns are displayed in this section: Column Description Name Displays the name of the external table. Provider Displays the data store provider that is used when the external table was created. For example, Parquet, CSV, JDBC, etc. Source For Parquet and CSV format, the path of the data file used to create the external table is displayed. For JDBC, the name of the client driver is displayed. Member Details \u00b6 The Member Details view shows the usage trend and statistics of a specific cluster member. To check the Member Details view, go to the Members section and click the link in the Member column. Here you can also view the Member Logs generated for a cluster member. The usage trends and the statistics of a specific member are auto updated periodically after every five seconds. If you want to turn off the auto-refresh, use the Auto Refresh switch that is provided on the upper-right corner. You can view, on demand, the latest logs by clicking on the Load New button provided at the bottom of the logs. You can also click the Load More button to view the older logs. Member Statistics \u00b6 The following member specific statistics are displayed: Item Description Member Name/ID Displays the name or ID of the member. Type Displays the type of member, which can be LEAD, LOCATOR or DATA SERVER. Process ID Displays the process ID of the member. Status Displays the status of the member. This can be either Running or Unavailable Heap Memory Displays the total available heap memory, used heap memory, their distribution into heap storage, heap execution memory and their utilization. Off-Heap Memory Usage Displays the members total off-heap memory, used off-heap memory, their distribution into off-heap storage and off-heap execution memory, and their utilization. The usage trends of the member are represented in the following graphs: Graphs Description CPU Usage Graphically presents the trend of CPU utilization by the member host for the last 15 minutes. The utilization is represented in percentage value. Heap Usage Graphically presents the utilization of Heap Memory by the member host. This graph displays three trend lines which corresponds to the utilization of Heap Memory for the following: Storage Execution JVM Off-Heap Usage Graphically presents the utilization of Off-Heap Memory by the member host. This graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following: Storage Execution Disk Space Graphically presents the utilization of disk space memory by the member host. Member Logs \u00b6 In the Member Details page, you can view the logs generated for a single member in the cluster. The following details are included: Item Description Log File Location Displays the absolute path of the member's primary log file, which is on the host where the current member's processes are running. Log Details Displays details of the loaded logs such as Loaded Bytes, Start and End Indexes of Loaded Bytes, and Total Bytes of logs content. Logs Displays the actual log entries from the log files. It also displays the following buttons: Load New - Loads the latest log entries from the log file, if generated, after logs were last loaded or updated. Load More - Loads older log entries from log files, if available. SQL \u00b6 The SQL section shows all the queries and their corresponding details along with their execution plans and stagewise breakups. Item Description Colocated When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. This improves the performance of the query significantly instead of broadcasting the data across all the data partitions. Whole-Stage Code Generation A whole stage code generation node compiles a sub-tree of plans that support code generation together into a single Java function, which helps improve execution performance. Per node execution timing Displays the time required for the execution of each node. If there are too many rows that are not getting filtered or exchanged. Pool Name Default/Low Latency. Applications can explicitly configure the use of this pool using a SQL command set snappydata.scheduler.pool=lowlatency . Query Node Details Hover over a component to view its details. Filter Displays the number of rows that are filtered for each node. Joins If HashJoin puts pressure on memory, you can change the HashJoin size to use SortMergeJoin to avoid on-heap memory pressure. Spark Cache \u00b6 Spark Cache is the inbuilt storage mechanism of Spark. When you do a dataSet.cache() , it uses this storage to store the dataset's data in a columnar format. This storage can be configured to be one of the following: MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2 For more details, see RDD Persistence section . Environment \u00b6 The Environment page provides detailed configurations for Spark environment including JVM, SparkContext, and SparkSession. Executors \u00b6 Executors are the entities that perform the tasks within a Spark job. Each Spark job is divided into multiple stages which can have one or more tasks depending on the number of partitions to be processed. All these tasks are scheduled on executor nodes which actually run them. Jobs \u00b6 The Jobs page lists all the Spark jobs. Each Spark action is translated as a Spark job. A job encapsulates the whole execution of an API or SQL. For example, dataSet.count() triggers a job. Status : Displays the status of the job. Stages : Click on the stage to view its details. The table displays the time taken for the completion of each stage. Tip You can cancel a long running job, using the Kill option. Stages \u00b6 The Stages page displays the stage details of a Spark Job. Each Spark job is segregated into one or more stages. Each stage is an execution boundary where data exchange between nodes is required. On this page, you can view the total time required for all the tasks in a job to complete. You can also view if any of the tasks got delayed for completion. This may occur in case of uneven data distribution. Scheduler Delay indicates the waiting period for the task. Delays can be caused if there are too many concurrent jobs. Shuffle reads and writes : Shuffles are written to disk and can take a lot of time to write and read. This can be avoided by using colocated and replicated tables. You can use high-performance SSD drives for temporary storage (spark.local.dir) to improve shuffle time. Number of parallel tasks : Due to concurrency, multiple queries may take cores and a specific query can take longer. To fix this, you can create a new scheduler and assign appropriate cores to it . GC time : Occasionally, on-heap object creation can slow down a query because of garbage collection. In these cases, it is recommended that you increase the on-heap memory, especially when you have row tables. Structured Streaming \u00b6 Structured Streaming tab provides a real-time view into all the running structured streaming queries in the cluster. You can navigate, visualize, and monitor all the structured streaming queries from a single location. Further, you can also track various statistics of these queries as listed here . All these statistics are automatically refreshed after every five seconds. Attention The Structured Streaming tab is available in the Embedded mode as well as in the Smart Connector mode. However, in the Smart Connector mode, this tab is visible only if you use the Snappy-Spark distribution. In case you use the upstream or stock spark distribution, then the Structured Streaming tab will not be visible. Only those streaming queries that are started using SnappySession will be available for monitoring on the UI. Queries that are started using SparkSession won't be available for monitoring on the UI. You can view the Structured Streaming tab only for the monitoring console of the corresponding cluster. For example, if you submit a streaming query on the embedded cluster using snappy-job , then the Structured Streaming tab is available on the monitoring console of the embedded cluster. If you submit a streaming query as a Smart Connector job, on a separate spark cluster, then the Structured Streaming tab is available on the monitoring console of the Spark cluster where the Smart Connector job is submitted. The following details are shown on the Structured Streaming tab: Query Statistics Information \u00b6 Item Description Query Names A list of the active and inactive streaming queries in the cluster, which also acts as the query navigation panel. You can select a query and monitor the status of that query. The relevant statistics of the query are displayed on right side. You can configure the maximum number of queries to be displayed on the UI by using the property spark.sql.streaming.uiRunningQueriesDisplayLimit . The default value is 20 . When the maximum limit is reached, then the newly created streaming queries gets added only after replacing the oldest inactive query. If there are no inactive queries present in the existing query list then the newly created streaming queries are not displayed. Click the list header to sort the list. Query Name A unique name given to structured streaming query. Queries are distinguished based on the query ID that is generated by Spark internally. Therefore, if multiple streaming queries have different query IDs but the same name, then all the queries get listed under the same name. To avoid this, you must ensure to maintain unique query names. Start Date & Time Calendar date and clock time when the structured streaming query was started or restarted. Uptime Total time that has passed since the streaming has started. Trigger Interval Time interval on which successive batch is expected to start it\u2019s execution. Each event is fired on this interval but each event may or may not have batches to process. Batches Processed Total number of batches processed since streaming query has been started. Status Status can be Active or Inactive . * Active : Query is running. * Inactive : Query has stopped or failed. Total Input Records Total count of records received from the sources and processed since the streaming query has been started. Current Input Rate Number of input records per second in a batch, which has been received in the current event trigger. This value is the number of records received between the start time of previous event and start time of current event and divided by the number of seconds between these two events. Current Processing Rate Average input records processed per second. This value is number of input records processed per second between the start time and the end time of the current event. Total Batch Processing Time Total time taken to process all the batches received from the start time of the streaming query until now. Avg. Batch Processing Time Average processing time per batch. This value is the total batch processing time, which is divided by the number of batches processed since the streaming query has started. Structured Streaming Queries Sources \u00b6 Item Description Type Source Type can be one of the following: * FileStream * Kafka * Memory * Socket * JDBC (only available with enterprise version) Description Description is a string representation of the source object. In general, it is a concise but informative representation. For example: KafkaSource[Subscribe[adImpressionTopic]] Input Records Number of input records received in current trigger event. Input Rate Input records per second for current trigger. Processing Rate Records processed per second for current trigger. Structured Streaming Queries Sink \u00b6 Item Description Type Any of the following sink type for the structured streaming queries: * Snappy Store * File Stream * Foreach * Kafka Description Description is a string representation of the sink object. In general, it is a concise but informative representation. For example: SnappyStoreSink[queryname -> log_stream, tablename -> aggrAdImpressions, checkpointlocation ->/home/xyz/tmp/io.snappydata.adanalytics.SnappyAPILogAggreg ator1$, sinkcallback -> org.apache.spark.sql.streaming.DefaultSnappySinkCallback] Graphical Charts \u00b6 The Structured Streaming tab presents four types of graphical charts. All these charts display historical trends or behavior of respective parameters for a given number of data points or sample size. You can customize the data points sample size by setting the configuration parameter spark.sql.streaming.uiTrendsMaxSampleSize . The default value is 60 data points, which indicates that all the graphical charts display data of the last 60 events. Input Records \u00b6 This line chart displays the number of input records received for each of the batches in the last n trigger events. Input Rate Vs. Processing Rate \u00b6 This line chart displays the behavior of two parameters, for the last n triggers, in two curve lines. n refers to value configured for spark.sql.streaming.uiTrendsMaxSampleSize , which defaults to 60 . One line represents the input rate that is the number of input records per second in the given triggers. The other line represents the processing rate, which is the number of records processed per second in the given triggers. Processing rate should ideally remain higher than Input rate. If it is below the input rate, then that indicates that the streaming query is falling behind the input data and you may need to add more resources to the cluster to keep up the processing speed. Processing Time (in Milliseconds) \u00b6 This line chart displays the time taken to process all the input records received in each of the batches for the last n triggers. This chart also displays the processing threshold line, which is the expected maximum time limit in milliseconds for processing a batch. This chart allows you to supervise whether the processing is happening within the time limit specified for Event Trigger Interval. In case the trend line for the Processing Time trend line is continuously below the Processing Threshold, then that indicates that there are enough resources to handle the streaming query operations. However, if the Processing Time trend line is often above the Processing Threshold, then that indicates that resources are insufficient to handle the streaming query operations, and you must add resources to manage the streaming input load. The Processing Threshold line will be always at zero if the trigger interval is not specified while defining the query. Aggregation State \u00b6 This line chart is displayed only if the streaming query is using one or more aggregate functions. SQL \u00b6 The SQL section shows all the queries and their corresponding details along with their execution plans and stagewise breakups. Item Description Colocated When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. This improves the performance of the query significantly instead of broadcasting the data across all the data partitions. Whole-Stage Code Generation A whole stage code generation node compiles a sub-tree of plans that support code generation together into a single Java function, which helps improve execution performance. Per node execution timing Displays the time required for the execution of each node. If there are too many rows that are not getting filtered or exchanged. Pool Name Default/Low Latency. Applications can explicitly configure the use of this pool using a SQL command set snappydata.scheduler.pool=lowlatency . Query Node Details Hover over a component to view its details. Filter Displays the number of rows that are filtered for each node. Joins If HashJoin puts pressure on memory, you can change the HashJoin size to use SortMergeJoin to avoid on-heap memory pressure. Spark History Server \u00b6 The Spark History server is a Spark UI extension. It is a web application that lets you analyze the running as well as completed SQL queries and the associated Spark jobs. The metadata collected in the form of event logs by the Spark History server can be shipped from a SnappyData cluster to another SnappyData cluster for further analysis. The first step in tuning query performance in SnappyData is to understand the query physical plan that is available through the SQL tab on the SnappyData Monitoring console. The detailed execution plan requires one to understand the jobs and tasks associated with the query. This is available in the Jobs/Tasks tab. However, if the SnappyData Monitoring console is not accessible to the investigator, it becomes a difficult exercise. To overcome this, it is recommended to turn on the History server for production applications. To turn on the History server, do the following: Ensure to provide a shared disk that can be accessed from all the SnappyData nodes. If you do not have the NFS access, use HDFS. Provide the necessary permissions to access a shared folder when you start SnappyData. Enable event logging for the Spark jobs. For example, specify the following properties in conf/lead : -spark.eventLog.enabled=true -spark.eventLog.dir=hdfs://namenode/shared/spark-logs Additionally, set the following property in conf/spark-defaults.conf file (it defaults to /tmp/spark-event s): spark.history.fs.logDirectory hdfs://namenode/shared/spark-logs If this property is not set and the directory /tmp/spark-events does not exist, Spark History server fails to start. By default, the history server loads only 1000 most recent events (SQL, Jobs, Stages). Older events will not be visible. To retain more number of events, set additional properties using SPARK_HISTORY_OPTS , and also increase the heap-size of the history server as follows in the file conf/SPARK_ENV.sh . SPARK_HISTORY_OPTS=\"-Dspark.history.fs.numReplayThreads=16 -Dspark.ui.retainedJobs=10000 -Dspark.ui.retainedStages=20000 -Dspark.sql.ui.retainedExecutions=10000\" SPARK_DAEMON_MEMORY=12g Note When you retain a high number of events, it increases the disk space requirement. Therefore, ensure that the disk store, where the events are stored, has adequate disk space. Start the History server. ./sbin/start-history-server.sh This creates a web interface at http:// :18080 by default, listing incomplete and completed instances of SQL queries and the associated Spark jobs and tasks. For more details about History server, refer to Configuring History Server .","title":"SnappyData Monitoring UI"},{"location":"monitoring/monitoring/#snappydata-monitoring-console","text":"SnappyData Monitoring Console is a dashboard that provides a real-time view into cluster members, member logs, resource usage, running Jobs, SQL queries along with performance data. This simple widget based view allows you to navigate easily, visualize, and monitor your cluster. You can monitor the overall status of the cluster as well as the status of each member in the cluster. All the usage details are automatically refreshed after every five seconds. Note SnappyData Monitoring Console is not yet tested and supported on Internet Explorer. To access SnappyData Monitoring Console, start your cluster and open http: <leadhost> :5050/dashboard/ in the web browser. Note <leadhost> is the hostname or IP of the lead node in your cluster which is provided in the conf/leads file. The following topics are covered in this section: Dashboard Member Details Jobs Structured Streaming Stages Spark Cache Environment Executors SQL Note When connecting to a SnappyData cluster using Smart Connector, the information related to SQL , Jobs , and Stages are NOT displayed, as the Jobs and queries are primarily executed in your client Spark cluster. You can find this information on the Spark UI console of your client cluster. Read more about SnappyData Smart Connector Mode here . In cases where you cannot access the SnappyData Monitoring Console to analyse Jobs and tasks, you must turn on the Spark History server . On the top-right side of the SnappyData Monitoring Console page, you can view the version details of SnappyData Snapshot. When you click this, the name and version of the product, the build details, the source revision details and the version number of the underlying spark are displayed. On the top of the dashboard, the date and time details about when the cluster is started are displayed. The period till when the cluster is running is also shown.","title":"SnappyData Monitoring Console"},{"location":"monitoring/monitoring/#dashboard","text":"The Dashboard page graphically presents various cluster-level statistics that can be used to monitor the current health status of a cluster. The statistics on the dashboard page can be set to update automatically after every five seconds. If you want to turn off the auto-refresh, use the Auto Refresh switch that is provided on the upper-right corner. The Dashboard page displays the following sections: Cluster Members Tables External Tables You can use the search and sort functionalities in any of the sections, except for the Cluster section. Sorting is enabled to sort items in an ascending and descending order. Further, you can also set the number of items that must be listed in each of these sections.","title":"Dashboard"},{"location":"monitoring/monitoring/#cluster","text":"In the Cluster section, you can view the following graphs which are automatically refreshed: On the Dashboard page, the Cluster section displays the date and time when the cluster was launched. It also displays the cluster's up-time since cluster was launched. You can view the total number of physical CPU cores present in your cluster on the top-right side of the page. Graphs Description CPU Usage Graphically presents the trend of CPU utilization by all the nodes in the cluster for the last 15 minutes. The utilization is represented in percentage value. Heap Usage Graphically presents the collective utilization of Heap Memory by all the nodes in the cluster. This graph displays three trend lines which corresponds to the utilization of Heap Memory for the following: Storage Execution JVM Off-Heap Usage Graphically presents the collective utilization of Off-Heap Memory by all the nodes in the cluster. This graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following: Storage Execution Disk Space Graphically presents the collective utilization of disk space memory by all the nodes in the cluster.","title":"Cluster"},{"location":"monitoring/monitoring/#members","text":"In the Members section, you can view, in a tabular format, the details of each locator, data server, and lead member within a cluster. The details are automatically refreshed after every five seconds. This table provides member details in the following columns: Column Description Status Displays the status of the members, which can be either Running or Stopped . Member Displays a brief description of the member. Click the link in the column to view the Member Details where the usage trends and statistics of the members are shown along with the Member Logs . Click the drop-down arrow to find information such as the IP address of the host, the current working directory, and the Process ID number. Type Displays the type of the member. The type can be LEAD, LOCATOR, or DATA SERVER. The name of the active lead member is displayed in bold letters. CPU Usage Displays the CPU utilized by the member's host and it's trend over last 15 mins. Memory Usage Displays the collective Heap and Off-Heap memory utilization of a cluster member and it's trend over last 15 mins. Heap Memory Displays the member's utilized Heap memory versus total Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Heap Memory for storage, execution, and JVM. Off-Heap Memory Displays the member's used Off-Heap memory and total Off-Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Off-Heap memory for storage and execution. Status Description Member is running. Member has stopped or is unavailable.","title":"Members"},{"location":"monitoring/monitoring/#tables","text":"The Tables section lists all the tables in the cluster along with their corresponding statistical details. All these details are automatically refreshed after every five seconds. The following columns are displayed in this section: Column Description Name Displays the name of the data table. Storage Model Displays the data storage model of the data table. Possible models are ROW and COLUMN . Distribution Type Displays the data distribution type for the table. Possible values are: PARTITION REPLICATE Row Count Displays the row count, which is the number of records present in the data table. In-Memory Size Displays the heap memory used by data table to store its data. If less than Total Size then the data is overflowing to disk. Spill-to-Disk Size Displays size of data overflown to disk Total Size Displays the collective physical memory and disk overflow space used by the data table to store its data. Buckets Displays the total number of buckets in the data table. If a number displayed in red here, it indicates that some of the buckets are offline. Redundancy Displays the number of redundant copies. Redundancy value 0 indicates that redundant copies are not configured. Redundancy value 1 indicates that one redundant copy is configured. Redundancy value NA indicates that redundancy is not applicable. Redundancy Status Displays whether redundancy criteria is satisfied or broken. Redundancy status Satisfied indicates that all the configured redundant copies are available. Redundancy status Broken indicates that some of the redundant copies are not available. Redundancy status NA indicates that redundancy is not applicable.","title":"Tables"},{"location":"monitoring/monitoring/#external-tables","text":"The External Tables section lists all the external tables present in the cluster along with their various statistical details. The displayed details are automatically refreshed after every five seconds. The following columns are displayed in this section: Column Description Name Displays the name of the external table. Provider Displays the data store provider that is used when the external table was created. For example, Parquet, CSV, JDBC, etc. Source For Parquet and CSV format, the path of the data file used to create the external table is displayed. For JDBC, the name of the client driver is displayed.","title":"External Tables"},{"location":"monitoring/monitoring/#member-details","text":"The Member Details view shows the usage trend and statistics of a specific cluster member. To check the Member Details view, go to the Members section and click the link in the Member column. Here you can also view the Member Logs generated for a cluster member. The usage trends and the statistics of a specific member are auto updated periodically after every five seconds. If you want to turn off the auto-refresh, use the Auto Refresh switch that is provided on the upper-right corner. You can view, on demand, the latest logs by clicking on the Load New button provided at the bottom of the logs. You can also click the Load More button to view the older logs.","title":"Member Details"},{"location":"monitoring/monitoring/#member-statistics","text":"The following member specific statistics are displayed: Item Description Member Name/ID Displays the name or ID of the member. Type Displays the type of member, which can be LEAD, LOCATOR or DATA SERVER. Process ID Displays the process ID of the member. Status Displays the status of the member. This can be either Running or Unavailable Heap Memory Displays the total available heap memory, used heap memory, their distribution into heap storage, heap execution memory and their utilization. Off-Heap Memory Usage Displays the members total off-heap memory, used off-heap memory, their distribution into off-heap storage and off-heap execution memory, and their utilization. The usage trends of the member are represented in the following graphs: Graphs Description CPU Usage Graphically presents the trend of CPU utilization by the member host for the last 15 minutes. The utilization is represented in percentage value. Heap Usage Graphically presents the utilization of Heap Memory by the member host. This graph displays three trend lines which corresponds to the utilization of Heap Memory for the following: Storage Execution JVM Off-Heap Usage Graphically presents the utilization of Off-Heap Memory by the member host. This graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following: Storage Execution Disk Space Graphically presents the utilization of disk space memory by the member host.","title":"Member Statistics"},{"location":"monitoring/monitoring/#member-logs","text":"In the Member Details page, you can view the logs generated for a single member in the cluster. The following details are included: Item Description Log File Location Displays the absolute path of the member's primary log file, which is on the host where the current member's processes are running. Log Details Displays details of the loaded logs such as Loaded Bytes, Start and End Indexes of Loaded Bytes, and Total Bytes of logs content. Logs Displays the actual log entries from the log files. It also displays the following buttons: Load New - Loads the latest log entries from the log file, if generated, after logs were last loaded or updated. Load More - Loads older log entries from log files, if available.","title":"Member Logs"},{"location":"monitoring/monitoring/#sql","text":"The SQL section shows all the queries and their corresponding details along with their execution plans and stagewise breakups. Item Description Colocated When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. This improves the performance of the query significantly instead of broadcasting the data across all the data partitions. Whole-Stage Code Generation A whole stage code generation node compiles a sub-tree of plans that support code generation together into a single Java function, which helps improve execution performance. Per node execution timing Displays the time required for the execution of each node. If there are too many rows that are not getting filtered or exchanged. Pool Name Default/Low Latency. Applications can explicitly configure the use of this pool using a SQL command set snappydata.scheduler.pool=lowlatency . Query Node Details Hover over a component to view its details. Filter Displays the number of rows that are filtered for each node. Joins If HashJoin puts pressure on memory, you can change the HashJoin size to use SortMergeJoin to avoid on-heap memory pressure.","title":"SQL"},{"location":"monitoring/monitoring/#spark-cache","text":"Spark Cache is the inbuilt storage mechanism of Spark. When you do a dataSet.cache() , it uses this storage to store the dataset's data in a columnar format. This storage can be configured to be one of the following: MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2 For more details, see RDD Persistence section .","title":"Spark Cache"},{"location":"monitoring/monitoring/#environment","text":"The Environment page provides detailed configurations for Spark environment including JVM, SparkContext, and SparkSession.","title":"Environment"},{"location":"monitoring/monitoring/#executors","text":"Executors are the entities that perform the tasks within a Spark job. Each Spark job is divided into multiple stages which can have one or more tasks depending on the number of partitions to be processed. All these tasks are scheduled on executor nodes which actually run them.","title":"Executors"},{"location":"monitoring/monitoring/#jobs","text":"The Jobs page lists all the Spark jobs. Each Spark action is translated as a Spark job. A job encapsulates the whole execution of an API or SQL. For example, dataSet.count() triggers a job. Status : Displays the status of the job. Stages : Click on the stage to view its details. The table displays the time taken for the completion of each stage. Tip You can cancel a long running job, using the Kill option.","title":"Jobs"},{"location":"monitoring/monitoring/#stages","text":"The Stages page displays the stage details of a Spark Job. Each Spark job is segregated into one or more stages. Each stage is an execution boundary where data exchange between nodes is required. On this page, you can view the total time required for all the tasks in a job to complete. You can also view if any of the tasks got delayed for completion. This may occur in case of uneven data distribution. Scheduler Delay indicates the waiting period for the task. Delays can be caused if there are too many concurrent jobs. Shuffle reads and writes : Shuffles are written to disk and can take a lot of time to write and read. This can be avoided by using colocated and replicated tables. You can use high-performance SSD drives for temporary storage (spark.local.dir) to improve shuffle time. Number of parallel tasks : Due to concurrency, multiple queries may take cores and a specific query can take longer. To fix this, you can create a new scheduler and assign appropriate cores to it . GC time : Occasionally, on-heap object creation can slow down a query because of garbage collection. In these cases, it is recommended that you increase the on-heap memory, especially when you have row tables.","title":"Stages"},{"location":"monitoring/monitoring/#structured-streaming","text":"Structured Streaming tab provides a real-time view into all the running structured streaming queries in the cluster. You can navigate, visualize, and monitor all the structured streaming queries from a single location. Further, you can also track various statistics of these queries as listed here . All these statistics are automatically refreshed after every five seconds. Attention The Structured Streaming tab is available in the Embedded mode as well as in the Smart Connector mode. However, in the Smart Connector mode, this tab is visible only if you use the Snappy-Spark distribution. In case you use the upstream or stock spark distribution, then the Structured Streaming tab will not be visible. Only those streaming queries that are started using SnappySession will be available for monitoring on the UI. Queries that are started using SparkSession won't be available for monitoring on the UI. You can view the Structured Streaming tab only for the monitoring console of the corresponding cluster. For example, if you submit a streaming query on the embedded cluster using snappy-job , then the Structured Streaming tab is available on the monitoring console of the embedded cluster. If you submit a streaming query as a Smart Connector job, on a separate spark cluster, then the Structured Streaming tab is available on the monitoring console of the Spark cluster where the Smart Connector job is submitted. The following details are shown on the Structured Streaming tab:","title":"Structured Streaming"},{"location":"monitoring/monitoring/#query-statistics-information","text":"Item Description Query Names A list of the active and inactive streaming queries in the cluster, which also acts as the query navigation panel. You can select a query and monitor the status of that query. The relevant statistics of the query are displayed on right side. You can configure the maximum number of queries to be displayed on the UI by using the property spark.sql.streaming.uiRunningQueriesDisplayLimit . The default value is 20 . When the maximum limit is reached, then the newly created streaming queries gets added only after replacing the oldest inactive query. If there are no inactive queries present in the existing query list then the newly created streaming queries are not displayed. Click the list header to sort the list. Query Name A unique name given to structured streaming query. Queries are distinguished based on the query ID that is generated by Spark internally. Therefore, if multiple streaming queries have different query IDs but the same name, then all the queries get listed under the same name. To avoid this, you must ensure to maintain unique query names. Start Date & Time Calendar date and clock time when the structured streaming query was started or restarted. Uptime Total time that has passed since the streaming has started. Trigger Interval Time interval on which successive batch is expected to start it\u2019s execution. Each event is fired on this interval but each event may or may not have batches to process. Batches Processed Total number of batches processed since streaming query has been started. Status Status can be Active or Inactive . * Active : Query is running. * Inactive : Query has stopped or failed. Total Input Records Total count of records received from the sources and processed since the streaming query has been started. Current Input Rate Number of input records per second in a batch, which has been received in the current event trigger. This value is the number of records received between the start time of previous event and start time of current event and divided by the number of seconds between these two events. Current Processing Rate Average input records processed per second. This value is number of input records processed per second between the start time and the end time of the current event. Total Batch Processing Time Total time taken to process all the batches received from the start time of the streaming query until now. Avg. Batch Processing Time Average processing time per batch. This value is the total batch processing time, which is divided by the number of batches processed since the streaming query has started.","title":"Query Statistics Information"},{"location":"monitoring/monitoring/#structured-streaming-queries-sources","text":"Item Description Type Source Type can be one of the following: * FileStream * Kafka * Memory * Socket * JDBC (only available with enterprise version) Description Description is a string representation of the source object. In general, it is a concise but informative representation. For example: KafkaSource[Subscribe[adImpressionTopic]] Input Records Number of input records received in current trigger event. Input Rate Input records per second for current trigger. Processing Rate Records processed per second for current trigger.","title":"Structured Streaming Queries Sources"},{"location":"monitoring/monitoring/#structured-streaming-queries-sink","text":"Item Description Type Any of the following sink type for the structured streaming queries: * Snappy Store * File Stream * Foreach * Kafka Description Description is a string representation of the sink object. In general, it is a concise but informative representation. For example: SnappyStoreSink[queryname -> log_stream, tablename -> aggrAdImpressions, checkpointlocation ->/home/xyz/tmp/io.snappydata.adanalytics.SnappyAPILogAggreg ator1$, sinkcallback -> org.apache.spark.sql.streaming.DefaultSnappySinkCallback]","title":"Structured Streaming Queries Sink"},{"location":"monitoring/monitoring/#graphical-charts","text":"The Structured Streaming tab presents four types of graphical charts. All these charts display historical trends or behavior of respective parameters for a given number of data points or sample size. You can customize the data points sample size by setting the configuration parameter spark.sql.streaming.uiTrendsMaxSampleSize . The default value is 60 data points, which indicates that all the graphical charts display data of the last 60 events.","title":"Graphical Charts"},{"location":"monitoring/monitoring/#input-records","text":"This line chart displays the number of input records received for each of the batches in the last n trigger events.","title":"Input Records"},{"location":"monitoring/monitoring/#input-rate-vs-processing-rate","text":"This line chart displays the behavior of two parameters, for the last n triggers, in two curve lines. n refers to value configured for spark.sql.streaming.uiTrendsMaxSampleSize , which defaults to 60 . One line represents the input rate that is the number of input records per second in the given triggers. The other line represents the processing rate, which is the number of records processed per second in the given triggers. Processing rate should ideally remain higher than Input rate. If it is below the input rate, then that indicates that the streaming query is falling behind the input data and you may need to add more resources to the cluster to keep up the processing speed.","title":"Input Rate Vs. Processing Rate"},{"location":"monitoring/monitoring/#processing-time-in-milliseconds","text":"This line chart displays the time taken to process all the input records received in each of the batches for the last n triggers. This chart also displays the processing threshold line, which is the expected maximum time limit in milliseconds for processing a batch. This chart allows you to supervise whether the processing is happening within the time limit specified for Event Trigger Interval. In case the trend line for the Processing Time trend line is continuously below the Processing Threshold, then that indicates that there are enough resources to handle the streaming query operations. However, if the Processing Time trend line is often above the Processing Threshold, then that indicates that resources are insufficient to handle the streaming query operations, and you must add resources to manage the streaming input load. The Processing Threshold line will be always at zero if the trigger interval is not specified while defining the query.","title":"Processing Time (in Milliseconds)"},{"location":"monitoring/monitoring/#aggregation-state","text":"This line chart is displayed only if the streaming query is using one or more aggregate functions.","title":"Aggregation State"},{"location":"monitoring/monitoring/#sql_1","text":"The SQL section shows all the queries and their corresponding details along with their execution plans and stagewise breakups. Item Description Colocated When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. This improves the performance of the query significantly instead of broadcasting the data across all the data partitions. Whole-Stage Code Generation A whole stage code generation node compiles a sub-tree of plans that support code generation together into a single Java function, which helps improve execution performance. Per node execution timing Displays the time required for the execution of each node. If there are too many rows that are not getting filtered or exchanged. Pool Name Default/Low Latency. Applications can explicitly configure the use of this pool using a SQL command set snappydata.scheduler.pool=lowlatency . Query Node Details Hover over a component to view its details. Filter Displays the number of rows that are filtered for each node. Joins If HashJoin puts pressure on memory, you can change the HashJoin size to use SortMergeJoin to avoid on-heap memory pressure.","title":"SQL"},{"location":"monitoring/monitoring/#spark-history-server","text":"The Spark History server is a Spark UI extension. It is a web application that lets you analyze the running as well as completed SQL queries and the associated Spark jobs. The metadata collected in the form of event logs by the Spark History server can be shipped from a SnappyData cluster to another SnappyData cluster for further analysis. The first step in tuning query performance in SnappyData is to understand the query physical plan that is available through the SQL tab on the SnappyData Monitoring console. The detailed execution plan requires one to understand the jobs and tasks associated with the query. This is available in the Jobs/Tasks tab. However, if the SnappyData Monitoring console is not accessible to the investigator, it becomes a difficult exercise. To overcome this, it is recommended to turn on the History server for production applications. To turn on the History server, do the following: Ensure to provide a shared disk that can be accessed from all the SnappyData nodes. If you do not have the NFS access, use HDFS. Provide the necessary permissions to access a shared folder when you start SnappyData. Enable event logging for the Spark jobs. For example, specify the following properties in conf/lead : -spark.eventLog.enabled=true -spark.eventLog.dir=hdfs://namenode/shared/spark-logs Additionally, set the following property in conf/spark-defaults.conf file (it defaults to /tmp/spark-event s): spark.history.fs.logDirectory hdfs://namenode/shared/spark-logs If this property is not set and the directory /tmp/spark-events does not exist, Spark History server fails to start. By default, the history server loads only 1000 most recent events (SQL, Jobs, Stages). Older events will not be visible. To retain more number of events, set additional properties using SPARK_HISTORY_OPTS , and also increase the heap-size of the history server as follows in the file conf/SPARK_ENV.sh . SPARK_HISTORY_OPTS=\"-Dspark.history.fs.numReplayThreads=16 -Dspark.ui.retainedJobs=10000 -Dspark.ui.retainedStages=20000 -Dspark.sql.ui.retainedExecutions=10000\" SPARK_DAEMON_MEMORY=12g Note When you retain a high number of events, it increases the disk space requirement. Therefore, ensure that the disk store, where the events are stored, has adequate disk space. Start the History server. ./sbin/start-history-server.sh This creates a web interface at http:// :18080 by default, listing incomplete and completed instances of SQL queries and the associated Spark jobs and tasks. For more details about History server, refer to Configuring History Server .","title":"Spark History Server"},{"location":"monitoring/recoveringdata/","text":"Recovering Data During Cluster Failures \u00b6 In scenarios where the SnappyData cluster fails to come up due to some issues, the Data Extractor utility can be used to retrieve the data in a standard format along with the schema definitions. Typically, the SnappyData cluster starts when all the instances of the servers, leads, and locators within the cluster are started. However, sometimes, the cluster does not come up. In such situations, there is a possibility that the data inside the cluster remains either entirely or partially unavailable. In such situations, you must first refer to the Troubleshooting Common Problems section in the SnappyData product documentation, fix the corresponding issues, and bring up the cluster. Even after this, if the cluster cannot be started successfully, due to unforeseen circumstances, you can use the Data Extractor utility to start the cluster in Recovery mode and salvage the data. Data Extractor utility is a read-only mode of the cluster. In this mode, you cannot make any changes to the data such as INSERT, UPDATE, DELETE, and CREATE/ALTER etc. Moreover, in this mode, the inter-dependencies between the nodes during the startup process is minimized. Therefore, reducing failures during startup. In the Recovery mode: You cannot perform operations with Data Definition Language (DDL) and Data Manipulation Language (DML). You are provided with procedures to extract data, DDLs, etc., and generate load-scripts. You can launch the Snappy shell and run SELECT/SHOW/DESCRIBE queries. Extracting Data in Recovery Mode \u00b6 To bring up the cluster and salvage the data, do the following: Launch the cluster in a Recovery mode Retrieve table definitions and data Load a new cluster with data extracted from Recovery mode Launching a Cluster in Recovery Mode \u00b6 Launching a cluster in recovery mode is similar to launching it in the regular mode. To specify this mode, all one has to do is pass an extra argument -r or --recover to the cluster start script as shown in the following example: snappy-start-all.sh -r Caution DDL or DML cannot be executed in a recovery Mode. Recovery mode does not repair the existing cluster. Retrieving Metadata and Table Data \u00b6 After you bring the cluster into recovery mode, you can retrieve the metadata and the table data in the cluster. The following system procedures are provided for this purpose: EXPORT_DDLS EXPORT_DATA Thus the table definitions and tables in a specific format can be exported and used later to launch a new cluster. Caution Ensure there is enough disk space, preferably double the existing cluster size, to store the recovered data. If the DDL statements that are used on the cluster have credentials, it will appear masked in the output of the procedure EXPORT_DDLS. You must replace it before using the file. For example: JDBC URL in external tables, Location URIs in table options. Note In the recovery mode, you must check the console, the logs of the server, and the lead for errors. Also, check for any tables that are skipped during the export process. In case of any failures, visit the Known issues in or Troubleshooting Common Problems section in the product documentation to resolve the failures. Loading a New Cluster with Extracted Data \u00b6 After you extract the DDLs and the data from the cluster, do the following to load a new cluster with the extracted data: Verify that the exportURI that is provided to EXPORT_DATA contains the table DDLs and the data by listing the output directories. Also, ensure that the directory contains one sub-directory each for the tables in the cluster and that these sub-directories are not empty. If a sub-directory corresponding to a table is empty, stop proceeding further, go through the logs to find if there are any skipped or failed tables. In such a case, you must refer to the troubleshooting section for any known fixes or workarounds. Clear or move the work directory of the old cluster only after you have verified that the logs do not report any failures for any tables and that you have ensured to complete step 1. Start a new cluster. Connect to Snappy shell. Use the exported DDLs and helper load-scripts to load the extracted data into the new cluster. For example snappy-sql> run \u2018/home/xyz/extracted/ddls_1571059691610/part-00000\u2019; snappy-sql> run \u2018/home/xyz/extracted/data_1571059786952_load_scripts/part-00000\u2019; Example of Using Data Extractor Utililty \u00b6 Here is an example of using the DataExtractor utility to salvage data from a faulty cluster or a healthy cluster. The following are the sequence of steps to follow: Start a fresh cluster and create a table that you can attempt to recover using this example. Start the Snappy shell and run the following DDL and INSERT statements: snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> create table customers(cid integer, name string, phone varchar(20)); snappy-sql> insert into customers values (0, 'customer1', '9988776655'), (0, 'customer1', '9988776654'); snappy-sql> exit; Run the snappy-stop-all.sh and ensure that all the instances of cluster services are stopped. You can confirm this by checking the output of jps to verify that the services (ServerLauncher,LeaderLauncher,LocatorLauncher) are not present. snappy-stop-all.sh The SnappyData Leader on user1-dell(localhost-lead-1) has stopped. The SnappyData Server on user1-dell(localhost-server-1) has stopped. The SnappyData Locator on user1-dell(localhost-locator-1) has stopped. jps 11684 Main 14295 Jps Run the command snappy-start-all.sh -r to launch the cluster in recovery mode. Check the console output and take a note of all the instances that could come up and those that fail to come up. snappy-start-all.sh -r Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 14500 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-server-1/snappyserver.log SnappyData Server pid: 14726 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 14940 status: running Distributed system now has 3 members. Starting hive thrift server (session=snappy) Starting job server on: 0.0.0.0[8090] If any of the members fail to come up, and if redundancy is not used, there could be a partial loss of data that is hosted on those particular members. If the cluster comes up successfully, launch Snappy shell, then you can run the SELECT queries or the provided procedures to export DDLs, Data of the cluster. Run the procedure EXPORT_DDLS to export the definitions of tables, views, UDFs etc. snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> call sys.EXPORT_DDLS('/tmp/recovered/ddls'); snappy-sql> exit; ls /tmp/recovered/ddls_1576074336371/ part-00000 _SUCCESS cat /tmp/recovered/ddls_1576074336371/part-00000 create table customers(cid integer, name string, phone varchar(20)); ## Next, run the procedure \u201cEXPORT_DATA\u201d to export the data of selected tables to selected location. snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> call sys.EXPORT_DATA('/tmp/recovered/data', 'csv', 'all', true); snappy-sql> exit; ls /tmp/recovered/data_1576074561789 APP.CUSTOMERS ls /tmp/recovered/data_1576074561789/APP.CUSTOMERS/ part-00000-5da7511d-e2ca-4fe5-9c97-23c8d1f52204.csv _SUCCESS cat /tmp/recovered/data_1576074561789/APP.CUSTOMERS/part-00000-5da7511d-e2ca-4fe5-9c97-23c8d1f52204.csv cid,name,phone 0,customer1,9988776654 0,customer1,9988776655 ls /tmp/recovered/data_1576074561789_load_scripts/ part-00000 _SUCCESS cat /tmp/recovered/data_1576074561789_load_scripts/part-00000 CREATE EXTERNAL TABLE temp_app_customers USING csv OPTIONS (PATH '/tmp/recovered/data_1576074561789//APP.CUSTOMERS',header 'true'); INSERT OVERWRITE app.customers SELECT * FROM temp_app_customers; With both the DDLs and data exported you can use the helper scripts that are already generated by the procedure EXPORT_DATA to load data into a fresh cluster. For the purpose of the example, the work directory is stopped and cleared of the existing cluster and the cluster is launched in the normal mode to simulate a fresh cluster. After this, run the DDL script, and the helper load script to load the extracted data back into the fresh cluster. snappy-stop-all.sh The SnappyData Leader on user1-dell(localhost-lead-1) has stopped. The SnappyData Server on user1-dell(localhost-server-1) has stopped. The SnappyData Locator on user1-dell(localhost-locator-1) has stopped. Rm -rf /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/ snappy-start-all.sh Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 19887 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-server-1/snappyserver.log SnappyData Server pid: 20062 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 20259 status: running Distributed system now has 3 members. Starting hive thrift server (session=snappy) Starting job server on: 0.0.0.0[8090] snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> run '/tmp/recovered/ddls_1576074336371/part-00000'; snappy-sql> create table customers(cid integer, name string, phone varchar(20)); snappy-sql> run '/tmp/recovered/data_1576074561789_load_scripts/part-00000'; snappy-sql> CREATE EXTERNAL TABLE temp_app_customers USING csv OPTIONS (PATH '/tmp/recovered/data_1576074561789//APP.CUSTOMERS',header 'true'); snappy-sql> INSERT OVERWRITE app.customers SELECT * FROM temp_app_customers; 2 rows inserted/updated/deleted show tables; schemaName |tableName |isTemporary ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- app |customers |false app |temp_app_customers |false 2 rows selected snappy-sql> select * from customers; cid |name |phone ------------------------------------------------ 0 |customer1 |9988776655 0 |customer1 |9988776654 2 rows selected Viewing the User Interface in Recovery Mode \u00b6 In the recovery mode, by default, the table counts and sizes do not appear on the UI. To view the table counts, you should set the property snappydata.recovery.enableTableCountInUI to true in the lead's conf file. By default, the property is set to false , and the table count is shown as -1 . The cluster that you have started in the recovery mode with the flag, can get busy fetching the table counts for some time based on the data size. If the table counts are enabled and there is an error while reading a particular table, the count is shown as -1 . Known Issues \u00b6 If one of the replicas of the table partitions is corrupt, the Data Extractor utility attempts to recover data from redundant replicas. However, if the redundancy is not available, and if the data files are corrupt, the utility fails to recover data. The Leader log contains an error message: Table/View 'DBS' does not exist . You can ignore this message. Troubleshooting \u00b6 Your SnappyData cluster has tables with big schema or a large number of buckets and this cluster has stopped without any exceptions in the log. Workaround : Add the property, -recovery-state-chunk-size into the conf files of each server, and set the value lower than the current(default 30). This property is responsible for chunking the table information while moving across the network in recovery mode. For example: localhost -recovery-state-chunk-size=10 An error message, Expected compute to launch at x but was launched at y is shown. Workaround : Increase the value of the property spark.locality.wait.process to higher than the current value (default 1800s). The error could be due to any of the following reasons: The data distribution is skewed, which causes more tasks to be assigned to server x which further leads to time-out for some tasks, and are re-launched on server y . The expected host is the best option to get the data; others are not in good shape. It is imperative to use a specific host for a particular partition( of a table) to get data in recovery mode reliably. You can also face this error when there was a failure on server x hence the task scheduler re-launches the task on server y , without waiting for the time-out. In this case the log for server x should be analysed for errors. You are facing memory-related issues such as LowMemoryException or if the server gets killed and a jvmkill_<pid>.log is generated in the server directory. In such a case enough memory may not be available to the servers. Workaround : Decrease the number of CPUs available for each server. This action ensures that at a time, less number of tasks are launched simultaneously. You can also decrease the cores by individually setting the -spark.executor.cores property to a lower value, in the server's conf file. After this, restart the cluster in recovery mode and again export the failed tables.","title":"Recovering Data During Cluster Failures"},{"location":"monitoring/recoveringdata/#recovering-data-during-cluster-failures","text":"In scenarios where the SnappyData cluster fails to come up due to some issues, the Data Extractor utility can be used to retrieve the data in a standard format along with the schema definitions. Typically, the SnappyData cluster starts when all the instances of the servers, leads, and locators within the cluster are started. However, sometimes, the cluster does not come up. In such situations, there is a possibility that the data inside the cluster remains either entirely or partially unavailable. In such situations, you must first refer to the Troubleshooting Common Problems section in the SnappyData product documentation, fix the corresponding issues, and bring up the cluster. Even after this, if the cluster cannot be started successfully, due to unforeseen circumstances, you can use the Data Extractor utility to start the cluster in Recovery mode and salvage the data. Data Extractor utility is a read-only mode of the cluster. In this mode, you cannot make any changes to the data such as INSERT, UPDATE, DELETE, and CREATE/ALTER etc. Moreover, in this mode, the inter-dependencies between the nodes during the startup process is minimized. Therefore, reducing failures during startup. In the Recovery mode: You cannot perform operations with Data Definition Language (DDL) and Data Manipulation Language (DML). You are provided with procedures to extract data, DDLs, etc., and generate load-scripts. You can launch the Snappy shell and run SELECT/SHOW/DESCRIBE queries.","title":"Recovering Data During Cluster Failures"},{"location":"monitoring/recoveringdata/#extracting-data-in-recovery-mode","text":"To bring up the cluster and salvage the data, do the following: Launch the cluster in a Recovery mode Retrieve table definitions and data Load a new cluster with data extracted from Recovery mode","title":"Extracting Data in Recovery Mode"},{"location":"monitoring/recoveringdata/#launching-a-cluster-in-recovery-mode","text":"Launching a cluster in recovery mode is similar to launching it in the regular mode. To specify this mode, all one has to do is pass an extra argument -r or --recover to the cluster start script as shown in the following example: snappy-start-all.sh -r Caution DDL or DML cannot be executed in a recovery Mode. Recovery mode does not repair the existing cluster.","title":"Launching a Cluster in Recovery Mode"},{"location":"monitoring/recoveringdata/#retrieving-metadata-and-table-data","text":"After you bring the cluster into recovery mode, you can retrieve the metadata and the table data in the cluster. The following system procedures are provided for this purpose: EXPORT_DDLS EXPORT_DATA Thus the table definitions and tables in a specific format can be exported and used later to launch a new cluster. Caution Ensure there is enough disk space, preferably double the existing cluster size, to store the recovered data. If the DDL statements that are used on the cluster have credentials, it will appear masked in the output of the procedure EXPORT_DDLS. You must replace it before using the file. For example: JDBC URL in external tables, Location URIs in table options. Note In the recovery mode, you must check the console, the logs of the server, and the lead for errors. Also, check for any tables that are skipped during the export process. In case of any failures, visit the Known issues in or Troubleshooting Common Problems section in the product documentation to resolve the failures.","title":"Retrieving Metadata and Table Data"},{"location":"monitoring/recoveringdata/#loading-a-new-cluster-with-extracted-data","text":"After you extract the DDLs and the data from the cluster, do the following to load a new cluster with the extracted data: Verify that the exportURI that is provided to EXPORT_DATA contains the table DDLs and the data by listing the output directories. Also, ensure that the directory contains one sub-directory each for the tables in the cluster and that these sub-directories are not empty. If a sub-directory corresponding to a table is empty, stop proceeding further, go through the logs to find if there are any skipped or failed tables. In such a case, you must refer to the troubleshooting section for any known fixes or workarounds. Clear or move the work directory of the old cluster only after you have verified that the logs do not report any failures for any tables and that you have ensured to complete step 1. Start a new cluster. Connect to Snappy shell. Use the exported DDLs and helper load-scripts to load the extracted data into the new cluster. For example snappy-sql> run \u2018/home/xyz/extracted/ddls_1571059691610/part-00000\u2019; snappy-sql> run \u2018/home/xyz/extracted/data_1571059786952_load_scripts/part-00000\u2019;","title":"Loading a New Cluster with Extracted Data"},{"location":"monitoring/recoveringdata/#example-of-using-data-extractor-utililty","text":"Here is an example of using the DataExtractor utility to salvage data from a faulty cluster or a healthy cluster. The following are the sequence of steps to follow: Start a fresh cluster and create a table that you can attempt to recover using this example. Start the Snappy shell and run the following DDL and INSERT statements: snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> create table customers(cid integer, name string, phone varchar(20)); snappy-sql> insert into customers values (0, 'customer1', '9988776655'), (0, 'customer1', '9988776654'); snappy-sql> exit; Run the snappy-stop-all.sh and ensure that all the instances of cluster services are stopped. You can confirm this by checking the output of jps to verify that the services (ServerLauncher,LeaderLauncher,LocatorLauncher) are not present. snappy-stop-all.sh The SnappyData Leader on user1-dell(localhost-lead-1) has stopped. The SnappyData Server on user1-dell(localhost-server-1) has stopped. The SnappyData Locator on user1-dell(localhost-locator-1) has stopped. jps 11684 Main 14295 Jps Run the command snappy-start-all.sh -r to launch the cluster in recovery mode. Check the console output and take a note of all the instances that could come up and those that fail to come up. snappy-start-all.sh -r Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 14500 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-server-1/snappyserver.log SnappyData Server pid: 14726 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 14940 status: running Distributed system now has 3 members. Starting hive thrift server (session=snappy) Starting job server on: 0.0.0.0[8090] If any of the members fail to come up, and if redundancy is not used, there could be a partial loss of data that is hosted on those particular members. If the cluster comes up successfully, launch Snappy shell, then you can run the SELECT queries or the provided procedures to export DDLs, Data of the cluster. Run the procedure EXPORT_DDLS to export the definitions of tables, views, UDFs etc. snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> call sys.EXPORT_DDLS('/tmp/recovered/ddls'); snappy-sql> exit; ls /tmp/recovered/ddls_1576074336371/ part-00000 _SUCCESS cat /tmp/recovered/ddls_1576074336371/part-00000 create table customers(cid integer, name string, phone varchar(20)); ## Next, run the procedure \u201cEXPORT_DATA\u201d to export the data of selected tables to selected location. snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> call sys.EXPORT_DATA('/tmp/recovered/data', 'csv', 'all', true); snappy-sql> exit; ls /tmp/recovered/data_1576074561789 APP.CUSTOMERS ls /tmp/recovered/data_1576074561789/APP.CUSTOMERS/ part-00000-5da7511d-e2ca-4fe5-9c97-23c8d1f52204.csv _SUCCESS cat /tmp/recovered/data_1576074561789/APP.CUSTOMERS/part-00000-5da7511d-e2ca-4fe5-9c97-23c8d1f52204.csv cid,name,phone 0,customer1,9988776654 0,customer1,9988776655 ls /tmp/recovered/data_1576074561789_load_scripts/ part-00000 _SUCCESS cat /tmp/recovered/data_1576074561789_load_scripts/part-00000 CREATE EXTERNAL TABLE temp_app_customers USING csv OPTIONS (PATH '/tmp/recovered/data_1576074561789//APP.CUSTOMERS',header 'true'); INSERT OVERWRITE app.customers SELECT * FROM temp_app_customers; With both the DDLs and data exported you can use the helper scripts that are already generated by the procedure EXPORT_DATA to load data into a fresh cluster. For the purpose of the example, the work directory is stopped and cleared of the existing cluster and the cluster is launched in the normal mode to simulate a fresh cluster. After this, run the DDL script, and the helper load script to load the extracted data back into the fresh cluster. snappy-stop-all.sh The SnappyData Leader on user1-dell(localhost-lead-1) has stopped. The SnappyData Server on user1-dell(localhost-server-1) has stopped. The SnappyData Locator on user1-dell(localhost-locator-1) has stopped. Rm -rf /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/ snappy-start-all.sh Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 19887 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-server-1/snappyserver.log SnappyData Server pid: 20062 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/user1/workspace/snappydata/build-artifacts/scala-2.11/snappy/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 20259 status: running Distributed system now has 3 members. Starting hive thrift server (session=snappy) Starting job server on: 0.0.0.0[8090] snappy-sql SnappyData version 1.2-SNAPSHOT snappy-sql> connect client 'localhost:1527'; snappy-sql> run '/tmp/recovered/ddls_1576074336371/part-00000'; snappy-sql> create table customers(cid integer, name string, phone varchar(20)); snappy-sql> run '/tmp/recovered/data_1576074561789_load_scripts/part-00000'; snappy-sql> CREATE EXTERNAL TABLE temp_app_customers USING csv OPTIONS (PATH '/tmp/recovered/data_1576074561789//APP.CUSTOMERS',header 'true'); snappy-sql> INSERT OVERWRITE app.customers SELECT * FROM temp_app_customers; 2 rows inserted/updated/deleted show tables; schemaName |tableName |isTemporary ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- app |customers |false app |temp_app_customers |false 2 rows selected snappy-sql> select * from customers; cid |name |phone ------------------------------------------------ 0 |customer1 |9988776655 0 |customer1 |9988776654 2 rows selected","title":"Example of Using Data Extractor Utililty"},{"location":"monitoring/recoveringdata/#viewing-the-user-interface-in-recovery-mode","text":"In the recovery mode, by default, the table counts and sizes do not appear on the UI. To view the table counts, you should set the property snappydata.recovery.enableTableCountInUI to true in the lead's conf file. By default, the property is set to false , and the table count is shown as -1 . The cluster that you have started in the recovery mode with the flag, can get busy fetching the table counts for some time based on the data size. If the table counts are enabled and there is an error while reading a particular table, the count is shown as -1 .","title":"Viewing the User Interface in Recovery Mode"},{"location":"monitoring/recoveringdata/#known-issues","text":"If one of the replicas of the table partitions is corrupt, the Data Extractor utility attempts to recover data from redundant replicas. However, if the redundancy is not available, and if the data files are corrupt, the utility fails to recover data. The Leader log contains an error message: Table/View 'DBS' does not exist . You can ignore this message.","title":"Known Issues"},{"location":"monitoring/recoveringdata/#troubleshooting","text":"Your SnappyData cluster has tables with big schema or a large number of buckets and this cluster has stopped without any exceptions in the log. Workaround : Add the property, -recovery-state-chunk-size into the conf files of each server, and set the value lower than the current(default 30). This property is responsible for chunking the table information while moving across the network in recovery mode. For example: localhost -recovery-state-chunk-size=10 An error message, Expected compute to launch at x but was launched at y is shown. Workaround : Increase the value of the property spark.locality.wait.process to higher than the current value (default 1800s). The error could be due to any of the following reasons: The data distribution is skewed, which causes more tasks to be assigned to server x which further leads to time-out for some tasks, and are re-launched on server y . The expected host is the best option to get the data; others are not in good shape. It is imperative to use a specific host for a particular partition( of a table) to get data in recovery mode reliably. You can also face this error when there was a failure on server x hence the task scheduler re-launches the task on server y , without waiting for the time-out. In this case the log for server x should be analysed for errors. You are facing memory-related issues such as LowMemoryException or if the server gets killed and a jvmkill_<pid>.log is generated in the server directory. In such a case enough memory may not be available to the servers. Workaround : Decrease the number of CPUs available for each server. This action ensures that at a time, less number of tasks are launched simultaneously. You can also decrease the cores by individually setting the -spark.executor.cores property to a lower value, in the server's conf file. After this, restart the cluster in recovery mode and again export the failed tables.","title":"Troubleshooting"},{"location":"old_files/binary/","text":"Heading \u00b6 Text Heading \u00b6 Text Heading \u00b6 Text Table: First Header Second Header Third Header Left Center Right Left Center Right Image Link: Hyperlink: Refer to the handbook . Formatting: Bold , ++Underline++, Italics Bullets and Numbering: * Item * Item * Item - sub-item - sub-item - sub-item Item 1 Item 2 Item 3 1.1 1.2 1.3 Code Example: CREATE EXTERNAL TABLE TAXIFARE USING parquet options (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE); Code Text: The values are do_nothing , local_omit , strict , run_on_full_table , partial_run_on_base_table . The default value is run_on_full_table Note: Note: The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.","title":"Binary"},{"location":"old_files/binary/#heading","text":"Text","title":"Heading"},{"location":"old_files/binary/#heading_1","text":"Text","title":"Heading"},{"location":"old_files/binary/#heading_2","text":"Text Table: First Header Second Header Third Header Left Center Right Left Center Right Image Link: Hyperlink: Refer to the handbook . Formatting: Bold , ++Underline++, Italics Bullets and Numbering: * Item * Item * Item - sub-item - sub-item - sub-item Item 1 Item 2 Item 3 1.1 1.2 1.3 Code Example: CREATE EXTERNAL TABLE TAXIFARE USING parquet options (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE); Code Text: The values are do_nothing , local_omit , strict , run_on_full_table , partial_run_on_base_table . The default value is run_on_full_table Note: Note: The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.","title":"Heading"},{"location":"old_files/build-instructions/","text":"Build Quickstart \u00b6 Building SnappyData requires JDK 7+ installation ( Oracle Java SE ). Quickstart to build all components of snappydata: Latest release branch > git clone https://github.com/TIBCOSoftware/snappydata.git -b branch-0.9 --recursive > cd snappydata > ./gradlew product Master > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew product The product will be in build-artifacts/scala-2.11/snappy If you want to build only the top-level snappydata project but pull in jars for other projects ( spark , store , spark-jobserver ): Latest release branch > git clone https://github.com/TIBCOSoftware/snappydata.git -b branch-0.9 > cd snappydata > ./gradlew product Master > git clone https://github.com/TIBCOSoftware/snappydata.git > cd snappydata > ./gradlew product Repository layout \u00b6 core - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between spark and store (GemFireXD). For example: SnappyContext, row and column store, streaming additions etc. cluster - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc. This component depends on core and store . Code in cluster depends on core but not the other way round. spark - Apache Spark code with SnappyData enhancements. store - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch. spark-jobserver - Fork of spark-jobserver project with some additions to integrate with SnappyData. The spark , store and spark-jobserver directories are required to be clones of the respective SnappyData repositories, and are integrated in the top-level snappydata project as git submodules. When working with submodules, updating the repositories follows the normal git submodules . One can add some aliases in gitconfig to aid pull/push like: [alias] spull = !git pull && git submodule sync --recursive && git submodule update --init --recursive spush = push --recurse-submodules=on-demand The above aliases can serve as useful shortcuts to pull and push all projects from top-level snappydata repository. Building \u00b6 Gradle is the build tool used for all the SnappyData projects. Changes to Apache Spark and spark-jobserver forks include addition of gradle build scripts to allow building them independently as well as a subproject of snappydata. The only requirement for the build is a JDK 7+ installation. Currently most of the testing has been with JDK 7. The gradlew wrapper script will download all the other build dependencies as required. If a user does not want to deal with submodules and only work on snappydata project, then can clone only the snappydata repository (without the --recursive option) and the build will pull those SnappyData project jar dependencies from maven central. If working on all the separate projects integrated inside the top-level snappydata clone, the gradle build will recognize the same and build those projects too and include the same in the top-level product distribution jar. The spark and store submodules can also be built and published independently. Useful build and test targets: ./gradlew assemble - build all the sources ./gradlew testClasses - build all the tests ./gradlew product - build and place the product distribution (in build-artifacts/scala_2.11/snappy) ./gradlew distTar - create a tar.gz archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew distZip - create a zip archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew buildAll - build all sources, tests, product, packages (all targets above) ./gradlew checkAll - run testsuites of snappydata components ./gradlew cleanAll - clean all build and test output ./gradlew runQuickstart - run the quickstart suite (the \"Getting Started\" section of docs) ./gradlew precheckin - cleanAll, buildAll, scalaStyle, build docs, and run full snappydata testsuite including quickstart ./gradlew precheckin -Pstore - cleanAll, buildAll, scalaStyle, build docs, run full snappydata testsuite including quickstart and also full SnappyData store testsuite The default build directory is build-artifacts/scala-2.11 for projects. Exception is store project, where the default build directory is build-artifacts/;os; where ;os; is linux on Linux systems, osx on Mac, windows on Windows. The usual gradle test run targets ( test , check ) work as expected for junit tests. Separate targets have been provided for running scala tests ( scalaTest ) while the check target will run both the junit and scalatests. One can run a single scala test suite class with singleSuite option while running a single test within some suite works with the --tests option: > ./gradlew core:scalaTest -PsingleSuite = **.ColumnTableTest # run all tests in the class > ./gradlew core:scalaTest \\ > --tests \"Test the creation/dropping of table using SQL\" # run a single test (use full name) Running individual tests within some suite works using the --tests argument. Setting up Intellij with gradle \u00b6 Intellij is the IDE commonly used by the snappydata developers. Those who really prefer Eclipse can try the scala-IDE and gradle support, but has been seen to not work as well (e.g. gradle support is not integrated with scala plugin etc). To import into Intellij: Update Intellij to the latest version, including the latest Scala plugin. Older versions have trouble dealing with scala code particularly some of the code in spark . Increase the Xmx to 2g or more (4g if possible) in the IDEA global vmoptions (in product bin directory, files named idea64.vmoptions for 64-bit and idea.vmoptions for 32-bit). If using Java 8 release 144 or later, also add -Djdk.util.zip.ensureTrailingSlash=false to the global vmoptions file to fix an IDEA issue (https://intellij-support.jetbrains.com/hc/en-us/community/posts/115000754864--SOLVED-No-default-file-and-code-templates). Not required for the latest IDEA releases. Select import project, then point to the snappydata directory. Use external Gradle import. Unselect \"Create separate module per source set\" option. When using JDK 7, add -XX:MaxPermSize=350m to VM options in global Gradle settings. Select defaults, next, next ... finish. Ignore \"Gradle location is unknown warning\" . Ensure that a JDK 7/8 installation has been selected. Ignore and dismiss the \"Unindexed remote maven repositories found\" warning message, if seen. Once import finishes, go to File->Settings->Editor->Code Style->Scala . Set the scheme as Project . Check that the same has been set in Java Code Style too. Then OK to close it. Next copy codeStyleSettings.xml in snappydata top-level directory to .idea directory created by Intellij. Check that settings are now applied in File->Settings->Editor->Code Style->Java which should show Indent as 2 and continuation indent as 4 (same for Scala). If the Gradle tab is not visible immediately, then select it from window list popup at the left-bottom corner of IDE. If you click on that window list icon, then the tabs will appear permanently. Generate avro and GemFireXD required sources by expanding: snappydata_2.11->Tasks->other . Right click on generateSources and run it. The Run item may not be available if indexing is still in progress, so wait for it to finish. The first run may take a while as it downloads jars etc. This step has to be done the first time, or if ./gradlew clean has been run, or you have made changes to javacc/avro/messages.xml source files. If you get unexpected \"Database not found\" or NullPointerException errors in GemFireXD layer, then first thing to try is to run the generateSources target again. Increase the compiler heap sizes or else the build can take quite long especially with integrated spark and store . In File->Settings->Build, Execution, Deployment->Compiler increase , Build process heap size to say 1536 or 2048. Similarly increase JVM maximum heap size in Languages & Frameworks->Scala Compiler Server to 1536 or 2048. Test the full build. For JDK 7: Open Run->Edit Configurations . Expand Defaults, and select Application. Add -XX:MaxPermSize=350m in VM options. Similarly add it to VM parameters for ScalaTest and JUnit. Most of unit tests will have trouble without this option. For JUnit configuration also append /build-artifacts to the working directory i.e. the directory should be \\$MODULE_DIR\\$/build-artifacts . Likewise change working directory for ScalaTest to be inside build-artifacts otherwise all intermediate log and other files (especially created by GemFireXD) will pollute the source tree and may need to cleaned manually. If you see below error during building the project, open module settings, select the module snappy-cluster_2.11, go to its Dependencies tab and ensure that snappy-spark-unsafe_2.11 comes before spark-unsafe (or just find snappy-spark-unsafe_2.11 and move it to top). Error:(236, 18) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String if (source.getByte(i) == first && matchAt(source, target, i)) return true Error:(233, 24) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String val first = target.getByte(0) Running a scalatest/junit \u00b6 Running scala/junit tests from Intellij should be straightforward -- just ensure that MaxPermSize has been increased when using JDK 7 as mentioned above especially for Spark/Snappy tests. - When selecting a run configuration for junit/scalatest, avoid selecting the gradle one (green round icon) otherwise that will launch an external gradle process that can start building the project again and won't be cleanly integrated with Intellij. Use the normal junit (red+green arrows icon) or scalatest (junit like with red overlay). - For JUnit tests, ensure that working directory is \\$MODULE_DIR\\$/build-artifacts as mentioned before. Otherwise many GemFireXD tests will fail to find the resource files required in tests. They will also pollute the checkouts with log files etc, so this will allow those to go into build-artifacts that is easier to clean. For that reason is may be preferable to do the same for scalatests. - Some of the tests use data files from the tests-common directory. For such tests, run the gradle task snappydata_2.11->Tasks->other->copyResourcesAll to copy the resources in build area where Intellij runs can find it.","title":"Build instructions"},{"location":"old_files/build-instructions/#build-quickstart","text":"Building SnappyData requires JDK 7+ installation ( Oracle Java SE ). Quickstart to build all components of snappydata: Latest release branch > git clone https://github.com/TIBCOSoftware/snappydata.git -b branch-0.9 --recursive > cd snappydata > ./gradlew product Master > git clone https://github.com/TIBCOSoftware/snappydata.git --recursive > cd snappydata > ./gradlew product The product will be in build-artifacts/scala-2.11/snappy If you want to build only the top-level snappydata project but pull in jars for other projects ( spark , store , spark-jobserver ): Latest release branch > git clone https://github.com/TIBCOSoftware/snappydata.git -b branch-0.9 > cd snappydata > ./gradlew product Master > git clone https://github.com/TIBCOSoftware/snappydata.git > cd snappydata > ./gradlew product","title":"Build Quickstart"},{"location":"old_files/build-instructions/#repository-layout","text":"core - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between spark and store (GemFireXD). For example: SnappyContext, row and column store, streaming additions etc. cluster - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc. This component depends on core and store . Code in cluster depends on core but not the other way round. spark - Apache Spark code with SnappyData enhancements. store - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch. spark-jobserver - Fork of spark-jobserver project with some additions to integrate with SnappyData. The spark , store and spark-jobserver directories are required to be clones of the respective SnappyData repositories, and are integrated in the top-level snappydata project as git submodules. When working with submodules, updating the repositories follows the normal git submodules . One can add some aliases in gitconfig to aid pull/push like: [alias] spull = !git pull && git submodule sync --recursive && git submodule update --init --recursive spush = push --recurse-submodules=on-demand The above aliases can serve as useful shortcuts to pull and push all projects from top-level snappydata repository.","title":"Repository layout"},{"location":"old_files/build-instructions/#building","text":"Gradle is the build tool used for all the SnappyData projects. Changes to Apache Spark and spark-jobserver forks include addition of gradle build scripts to allow building them independently as well as a subproject of snappydata. The only requirement for the build is a JDK 7+ installation. Currently most of the testing has been with JDK 7. The gradlew wrapper script will download all the other build dependencies as required. If a user does not want to deal with submodules and only work on snappydata project, then can clone only the snappydata repository (without the --recursive option) and the build will pull those SnappyData project jar dependencies from maven central. If working on all the separate projects integrated inside the top-level snappydata clone, the gradle build will recognize the same and build those projects too and include the same in the top-level product distribution jar. The spark and store submodules can also be built and published independently. Useful build and test targets: ./gradlew assemble - build all the sources ./gradlew testClasses - build all the tests ./gradlew product - build and place the product distribution (in build-artifacts/scala_2.11/snappy) ./gradlew distTar - create a tar.gz archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew distZip - create a zip archive of product distribution (in build-artifacts/scala_2.11/distributions) ./gradlew buildAll - build all sources, tests, product, packages (all targets above) ./gradlew checkAll - run testsuites of snappydata components ./gradlew cleanAll - clean all build and test output ./gradlew runQuickstart - run the quickstart suite (the \"Getting Started\" section of docs) ./gradlew precheckin - cleanAll, buildAll, scalaStyle, build docs, and run full snappydata testsuite including quickstart ./gradlew precheckin -Pstore - cleanAll, buildAll, scalaStyle, build docs, run full snappydata testsuite including quickstart and also full SnappyData store testsuite The default build directory is build-artifacts/scala-2.11 for projects. Exception is store project, where the default build directory is build-artifacts/;os; where ;os; is linux on Linux systems, osx on Mac, windows on Windows. The usual gradle test run targets ( test , check ) work as expected for junit tests. Separate targets have been provided for running scala tests ( scalaTest ) while the check target will run both the junit and scalatests. One can run a single scala test suite class with singleSuite option while running a single test within some suite works with the --tests option: > ./gradlew core:scalaTest -PsingleSuite = **.ColumnTableTest # run all tests in the class > ./gradlew core:scalaTest \\ > --tests \"Test the creation/dropping of table using SQL\" # run a single test (use full name) Running individual tests within some suite works using the --tests argument.","title":"Building"},{"location":"old_files/build-instructions/#setting-up-intellij-with-gradle","text":"Intellij is the IDE commonly used by the snappydata developers. Those who really prefer Eclipse can try the scala-IDE and gradle support, but has been seen to not work as well (e.g. gradle support is not integrated with scala plugin etc). To import into Intellij: Update Intellij to the latest version, including the latest Scala plugin. Older versions have trouble dealing with scala code particularly some of the code in spark . Increase the Xmx to 2g or more (4g if possible) in the IDEA global vmoptions (in product bin directory, files named idea64.vmoptions for 64-bit and idea.vmoptions for 32-bit). If using Java 8 release 144 or later, also add -Djdk.util.zip.ensureTrailingSlash=false to the global vmoptions file to fix an IDEA issue (https://intellij-support.jetbrains.com/hc/en-us/community/posts/115000754864--SOLVED-No-default-file-and-code-templates). Not required for the latest IDEA releases. Select import project, then point to the snappydata directory. Use external Gradle import. Unselect \"Create separate module per source set\" option. When using JDK 7, add -XX:MaxPermSize=350m to VM options in global Gradle settings. Select defaults, next, next ... finish. Ignore \"Gradle location is unknown warning\" . Ensure that a JDK 7/8 installation has been selected. Ignore and dismiss the \"Unindexed remote maven repositories found\" warning message, if seen. Once import finishes, go to File->Settings->Editor->Code Style->Scala . Set the scheme as Project . Check that the same has been set in Java Code Style too. Then OK to close it. Next copy codeStyleSettings.xml in snappydata top-level directory to .idea directory created by Intellij. Check that settings are now applied in File->Settings->Editor->Code Style->Java which should show Indent as 2 and continuation indent as 4 (same for Scala). If the Gradle tab is not visible immediately, then select it from window list popup at the left-bottom corner of IDE. If you click on that window list icon, then the tabs will appear permanently. Generate avro and GemFireXD required sources by expanding: snappydata_2.11->Tasks->other . Right click on generateSources and run it. The Run item may not be available if indexing is still in progress, so wait for it to finish. The first run may take a while as it downloads jars etc. This step has to be done the first time, or if ./gradlew clean has been run, or you have made changes to javacc/avro/messages.xml source files. If you get unexpected \"Database not found\" or NullPointerException errors in GemFireXD layer, then first thing to try is to run the generateSources target again. Increase the compiler heap sizes or else the build can take quite long especially with integrated spark and store . In File->Settings->Build, Execution, Deployment->Compiler increase , Build process heap size to say 1536 or 2048. Similarly increase JVM maximum heap size in Languages & Frameworks->Scala Compiler Server to 1536 or 2048. Test the full build. For JDK 7: Open Run->Edit Configurations . Expand Defaults, and select Application. Add -XX:MaxPermSize=350m in VM options. Similarly add it to VM parameters for ScalaTest and JUnit. Most of unit tests will have trouble without this option. For JUnit configuration also append /build-artifacts to the working directory i.e. the directory should be \\$MODULE_DIR\\$/build-artifacts . Likewise change working directory for ScalaTest to be inside build-artifacts otherwise all intermediate log and other files (especially created by GemFireXD) will pollute the source tree and may need to cleaned manually. If you see below error during building the project, open module settings, select the module snappy-cluster_2.11, go to its Dependencies tab and ensure that snappy-spark-unsafe_2.11 comes before spark-unsafe (or just find snappy-spark-unsafe_2.11 and move it to top). Error:(236, 18) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String if (source.getByte(i) == first && matchAt(source, target, i)) return true Error:(233, 24) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String val first = target.getByte(0)","title":"Setting up Intellij with gradle"},{"location":"old_files/build-instructions/#running-a-scalatestjunit","text":"Running scala/junit tests from Intellij should be straightforward -- just ensure that MaxPermSize has been increased when using JDK 7 as mentioned above especially for Spark/Snappy tests. - When selecting a run configuration for junit/scalatest, avoid selecting the gradle one (green round icon) otherwise that will launch an external gradle process that can start building the project again and won't be cleanly integrated with Intellij. Use the normal junit (red+green arrows icon) or scalatest (junit like with red overlay). - For JUnit tests, ensure that working directory is \\$MODULE_DIR\\$/build-artifacts as mentioned before. Otherwise many GemFireXD tests will fail to find the resource files required in tests. They will also pollute the checkouts with log files etc, so this will allow those to go into build-artifacts that is easier to clean. For that reason is may be preferable to do the same for scalatests. - Some of the tests use data files from the tests-common directory. For such tests, run the gradle task snappydata_2.11->Tasks->other->copyResourcesAll to copy the resources in build area where Intellij runs can find it.","title":"Running a scalatest/junit"},{"location":"old_files/capacity_planning/","text":"Overview \u00b6 The following topics are covered in this section: Concurrency and Management of Cores Memory Management: Heap and Off-Heap HA Considerations Important Settings Operating System Settings Table Memory Requirements JVM Settings for SnappyData Smart Connector mode and Local mode Concurrency and Management of Cores \u00b6 Executing queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job has multiple tasks. The number of tasks is determined by the number of partitions of the underlying data. Concurrency in SnappyData is tightly bound with the capacity of the cluster, which means, the number of cores available in the cluster determines the number of concurrent tasks that can be run. The default setting is CORES = 2 X number of cores on a machine . It is recommended to use 2 X number of cores on a machine. If more than one server is running on a machine, the cores should be divided accordingly and specified using the spark.executor.cores property. spark.executor.cores is used to override the number of cores per server. For example, for a cluster with 2 servers running on two different machines with 4 CPU cores each, a maximum number of tasks that can run concurrently is 16. If a table has 16 partitions (buckets, for row or column tables), a scan query on this table creates 16 tasks. This means, 16 tasks runs concurrently and the last task will run when one of these 16 tasks has finished execution. SnappyData uses an optimization method which clubs multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions. In SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get fair distribution of resources. For example: In the image below, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently. Pending tasks have to wait for completion of the current tasks and are assigned to the core that is first available. When you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute. Memory Management: Heap and Off-Heap \u00b6 SnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (eg, columns stored as byte arrays). It is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The memory-size and heap-size properties control the off-heap and on-heap sizes of the SnappyData server process. The memory pool (off-heap and on-heap) available in SnappyData's cluster is divided into two parts \u2013 Execution and Storage memory pool. The storage memory pool as the name indicates is for the table storage. The amount of memory that is available for storage is 50% of the total memory but it can grow to 90% (or eviction-heap-percentage store property for heap memory if set) if the execution memory is unused. This can be altered by specifying the spark.memory.storageFraction property. But, it is recommend to not change this setting. A certain fraction of heap memory is reserved for JVM objects outside of SnappyData storage and eviction. If the critical-heap-percentage store property is set then SnappyData uses memory only until that limit is reached and the remaining memory is reserved. If no critical-heap-percentage has been specified then it defaults to 90%. There is no reserved memory for off-heap. SnappyData tables are by default configured for eviction which means, when there is memory pressure, the tables are evicted to disk. This impacts performance to some degree and hence it is recommended to size your VM before you begin. HA Considerations \u00b6 High availability options are available for all the SnappyData components. Lead HA SnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of the secondary lead nodes takes over immediately. Setting up the secondary lead node is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted. Locator SnappyData supports multiple locators in the cluster for high availability. It is recommended to set up multiple locators as, if a locator becomes unavailable, the cluster continues to be available. New members can however not join the cluster. With multiple locators, clients notice nothing and the failover recovery is completely transparent. DataServer SnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data. So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. However, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again. Important Settings \u00b6 Buckets \u00b6 Bucket is the unit of partitioning for SnappyData tables. The data is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, the buckets are moved around for rebalancing. The number of buckets should be set according to the table size. The default number of buckets in the SnappyData cluster mode is 128. In the local mode it is cores*2, subject to a maximum of 64 buckets and a minumum of 8 buckets. If there are more buckets in a table than required, it means there is fewer data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. Similarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently. Also, if the cluster is scaled at a later point of time rebalancing may not be optimal. For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data. member-timeout \u00b6 SnappyData efficiently uses CPU for running OLAP queries. In such cases, due to the amount of garbage generated, JVMs garbage collection can result in a system pause. These pauses are rare and can also be minimized by setting the off-heap memory. For such cases, it is recommended that the member-timeout should be increased to a higher value. This ensures that the members are not thrown out of the cluster in case of a system pause. The default value of member-timeout is: 5 sec. spark.local.dir \u00b6 SnappyData writes table data on disk. By default, the disk location that SnappyData uses is the directory specified using -dir option, while starting the member. SnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size. To achieve better performance, it is recommended to store temporary data on a different disk than the table data. This can be done by setting the spark.local.dir parameter. Operating System Settings \u00b6 For best performance, the following operating system settings are recommended on the lead and server nodes. Ulimit Spark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes. A minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K. To change the limits of these settings for a user, the /etc/security/limits.conf file needs to be updated. A typical limits.conf used for SnappyData servers and leads looks like: ec2-user hard nofile 163840 ec2-user soft nofile 16384 ec2-user hard nproc unlimited ec2-user soft nproc 524288 ec2-user hard sigpending unlimited ec2-user soft sigpending 524288 * ec2-user is the user running SnappyData. OS Cache Size When there is lot of disk activity especially during table joins and during eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages. The following are the typical configuration to be done on the machines that are running SnappyData processes. sudo sysctl -w vm.dirty_background_ratio=2 sudo sysctl -w vm.dirty_ratio=4 sudo sysctl -w vm.dirty_expire_centisecs=2000 sudo sysctl -w vm.dirty_writeback_centisecs=300 Swap File Since modern operating systems perform lazy allocation, it has been observed that despite setting -Xmx and -Xms settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in process going down. It is recommended to set swap space on your system using the following commands. # sets a swap space of 32 GB sudo dd if=/dev/zero of=/var/swapfile.1 bs=1M count=32768 sudo chmod 600 /var/swapfile.1 sudo mkswap /var/swapfile.1 sudo swapon /var/swapfile.1 Table Memory Requirements \u00b6 SnappyData column tables encodes data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (i.e. off-heap is enabled), the entire column table is stored in off-heap memory. SnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk. TABLE IS PERSISTED? OVERFLOW IS CONFIGURED? APPROXIMATE HEAP OVERHEAD No No 64 bytes Yes No 120 bytes Yes Yes 152 bytes Note For a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.) TYPE OF INDEX ENTRY APPROXIMATE HEAP OVERHEAD New index entry 80 bytes First non-unique index entry 24 bytes Subsequent non-unique index entry 8 bytes to 24 bytes* If there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes. JVM Settings for SnappyData Smart Connector mode and Local mode \u00b6 For SnappyData Smart Connector mode and local mode, we recommend the following JVM settings for optimal performance: -XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k","title":"Capacity planning"},{"location":"old_files/capacity_planning/#overview","text":"The following topics are covered in this section: Concurrency and Management of Cores Memory Management: Heap and Off-Heap HA Considerations Important Settings Operating System Settings Table Memory Requirements JVM Settings for SnappyData Smart Connector mode and Local mode","title":"Overview"},{"location":"old_files/capacity_planning/#concurrency-and-management-of-cores","text":"Executing queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job has multiple tasks. The number of tasks is determined by the number of partitions of the underlying data. Concurrency in SnappyData is tightly bound with the capacity of the cluster, which means, the number of cores available in the cluster determines the number of concurrent tasks that can be run. The default setting is CORES = 2 X number of cores on a machine . It is recommended to use 2 X number of cores on a machine. If more than one server is running on a machine, the cores should be divided accordingly and specified using the spark.executor.cores property. spark.executor.cores is used to override the number of cores per server. For example, for a cluster with 2 servers running on two different machines with 4 CPU cores each, a maximum number of tasks that can run concurrently is 16. If a table has 16 partitions (buckets, for row or column tables), a scan query on this table creates 16 tasks. This means, 16 tasks runs concurrently and the last task will run when one of these 16 tasks has finished execution. SnappyData uses an optimization method which clubs multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions. In SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get fair distribution of resources. For example: In the image below, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently. Pending tasks have to wait for completion of the current tasks and are assigned to the core that is first available. When you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute.","title":"Concurrency and Management of Cores"},{"location":"old_files/capacity_planning/#memory-management-heap-and-off-heap","text":"SnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (eg, columns stored as byte arrays). It is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The memory-size and heap-size properties control the off-heap and on-heap sizes of the SnappyData server process. The memory pool (off-heap and on-heap) available in SnappyData's cluster is divided into two parts \u2013 Execution and Storage memory pool. The storage memory pool as the name indicates is for the table storage. The amount of memory that is available for storage is 50% of the total memory but it can grow to 90% (or eviction-heap-percentage store property for heap memory if set) if the execution memory is unused. This can be altered by specifying the spark.memory.storageFraction property. But, it is recommend to not change this setting. A certain fraction of heap memory is reserved for JVM objects outside of SnappyData storage and eviction. If the critical-heap-percentage store property is set then SnappyData uses memory only until that limit is reached and the remaining memory is reserved. If no critical-heap-percentage has been specified then it defaults to 90%. There is no reserved memory for off-heap. SnappyData tables are by default configured for eviction which means, when there is memory pressure, the tables are evicted to disk. This impacts performance to some degree and hence it is recommended to size your VM before you begin.","title":"Memory Management: Heap and Off-Heap"},{"location":"old_files/capacity_planning/#ha-considerations","text":"High availability options are available for all the SnappyData components. Lead HA SnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of the secondary lead nodes takes over immediately. Setting up the secondary lead node is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted. Locator SnappyData supports multiple locators in the cluster for high availability. It is recommended to set up multiple locators as, if a locator becomes unavailable, the cluster continues to be available. New members can however not join the cluster. With multiple locators, clients notice nothing and the failover recovery is completely transparent. DataServer SnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data. So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. However, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again.","title":"HA Considerations"},{"location":"old_files/capacity_planning/#important-settings","text":"","title":"Important Settings"},{"location":"old_files/capacity_planning/#buckets","text":"Bucket is the unit of partitioning for SnappyData tables. The data is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, the buckets are moved around for rebalancing. The number of buckets should be set according to the table size. The default number of buckets in the SnappyData cluster mode is 128. In the local mode it is cores*2, subject to a maximum of 64 buckets and a minumum of 8 buckets. If there are more buckets in a table than required, it means there is fewer data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. Similarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently. Also, if the cluster is scaled at a later point of time rebalancing may not be optimal. For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data.","title":"Buckets"},{"location":"old_files/capacity_planning/#member-timeout","text":"SnappyData efficiently uses CPU for running OLAP queries. In such cases, due to the amount of garbage generated, JVMs garbage collection can result in a system pause. These pauses are rare and can also be minimized by setting the off-heap memory. For such cases, it is recommended that the member-timeout should be increased to a higher value. This ensures that the members are not thrown out of the cluster in case of a system pause. The default value of member-timeout is: 5 sec.","title":"member-timeout"},{"location":"old_files/capacity_planning/#sparklocaldir","text":"SnappyData writes table data on disk. By default, the disk location that SnappyData uses is the directory specified using -dir option, while starting the member. SnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size. To achieve better performance, it is recommended to store temporary data on a different disk than the table data. This can be done by setting the spark.local.dir parameter.","title":"spark.local.dir"},{"location":"old_files/capacity_planning/#operating-system-settings","text":"For best performance, the following operating system settings are recommended on the lead and server nodes. Ulimit Spark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes. A minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K. To change the limits of these settings for a user, the /etc/security/limits.conf file needs to be updated. A typical limits.conf used for SnappyData servers and leads looks like: ec2-user hard nofile 163840 ec2-user soft nofile 16384 ec2-user hard nproc unlimited ec2-user soft nproc 524288 ec2-user hard sigpending unlimited ec2-user soft sigpending 524288 * ec2-user is the user running SnappyData. OS Cache Size When there is lot of disk activity especially during table joins and during eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages. The following are the typical configuration to be done on the machines that are running SnappyData processes. sudo sysctl -w vm.dirty_background_ratio=2 sudo sysctl -w vm.dirty_ratio=4 sudo sysctl -w vm.dirty_expire_centisecs=2000 sudo sysctl -w vm.dirty_writeback_centisecs=300 Swap File Since modern operating systems perform lazy allocation, it has been observed that despite setting -Xmx and -Xms settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in process going down. It is recommended to set swap space on your system using the following commands. # sets a swap space of 32 GB sudo dd if=/dev/zero of=/var/swapfile.1 bs=1M count=32768 sudo chmod 600 /var/swapfile.1 sudo mkswap /var/swapfile.1 sudo swapon /var/swapfile.1","title":"Operating System Settings"},{"location":"old_files/capacity_planning/#table-memory-requirements","text":"SnappyData column tables encodes data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (i.e. off-heap is enabled), the entire column table is stored in off-heap memory. SnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk. TABLE IS PERSISTED? OVERFLOW IS CONFIGURED? APPROXIMATE HEAP OVERHEAD No No 64 bytes Yes No 120 bytes Yes Yes 152 bytes Note For a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.) TYPE OF INDEX ENTRY APPROXIMATE HEAP OVERHEAD New index entry 80 bytes First non-unique index entry 24 bytes Subsequent non-unique index entry 8 bytes to 24 bytes* If there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes.","title":"Table Memory Requirements"},{"location":"old_files/capacity_planning/#jvm-settings-for-snappydata-smart-connector-mode-and-local-mode","text":"For SnappyData Smart Connector mode and local mode, we recommend the following JVM settings for optimal performance: -XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k","title":"JVM Settings for SnappyData Smart Connector mode and Local mode"},{"location":"old_files/cluster/","text":"SnappyData, a database server cluster, has three main components - Locator, Server and Lead. Locator : Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons. Lead Node : Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members. Data Servers : Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL. For details of the architecture refer to Architecture SnappyData also has multiple deployment options. For more information refer to, Deployment Options . Interacting with SnappyData \u00b6 Note: For the section on the Spark API, we assume some familiarity with core Spark, Spark SQL and Spark Streaming concepts . And, you can try out the Spark Quick Start . All the commands and programs listed in the Spark guides work in SnappyData as well. For the section on SQL, no Spark knowledge is necessary. To interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self contained Spark application or can share state with other jobs using the SnappyData store. Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways. Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster. Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver. When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the Spark JobServer to manage Jobs and queries within a \"lead\" node. Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). Read our docs for details on the architecture. In this document, we showcase mostly the same set of features via the Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to the Spark API section .","title":"Cluster"},{"location":"old_files/cluster/#interacting-with-snappydata","text":"Note: For the section on the Spark API, we assume some familiarity with core Spark, Spark SQL and Spark Streaming concepts . And, you can try out the Spark Quick Start . All the commands and programs listed in the Spark guides work in SnappyData as well. For the section on SQL, no Spark knowledge is necessary. To interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self contained Spark application or can share state with other jobs using the SnappyData store. Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways. Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster. Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver. When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the Spark JobServer to manage Jobs and queries within a \"lead\" node. Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). Read our docs for details on the architecture. In this document, we showcase mostly the same set of features via the Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to the Spark API section .","title":"Interacting with SnappyData"},{"location":"old_files/cluster_aws/","text":"Cluster AWS \u00b6","title":"Cluster aws"},{"location":"old_files/cluster_aws/#cluster-aws","text":"","title":"Cluster AWS"},{"location":"old_files/cluster_azure/","text":"Cluster AWS \u00b6","title":"Cluster azure"},{"location":"old_files/cluster_azure/#cluster-aws","text":"","title":"Cluster AWS"},{"location":"old_files/cluster_docker/","text":"Cluster AWS \u00b6","title":"Cluster docker"},{"location":"old_files/cluster_docker/#cluster-aws","text":"","title":"Cluster AWS"},{"location":"old_files/clustersparkapi/","text":"SnappySession is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's SparkSession to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame. Similarly, SnappyStreamingContext is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's Streaming Context . Applications typically submit Jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using the Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. class SnappySampleJob extends SnappySQLJob { /** Snappy uses this as an entry point to execute Snappy jobs. **/ def runJob ( snSession : SnappySession , jobConfig : Config ): Any /** SnappyData calls this function to validate the job input and reject invalid job requests **/ def validate ( snSession : SnappySession , config : Config ): SparkJobValidation } The implementation of the runJob function from SnappySQLJob uses a SnappySession to interact with the SnappyData store to process and store tables. The implementation of runJob from SnappyStreamingJob uses a SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to the lead node of SnappyData over REST API using a spark-submit like utility. See more details about jobs here: SnappyData Jobs","title":"Clustersparkapi"},{"location":"old_files/clustersql/","text":"For SQL, the SnappyData SQL Shell ( snappy-sql ) provides a simple way to inspect the catalog, run admin operations, manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer (a JDBC connection to the cluster). From the SnappyData base directory, /snappy/, run: ./bin/snappy-sql Connect to the cluster with snappy> connect client 'localhost:1527'; You can view connections with snappy> show connections; And check member status with: snappy> show members;","title":"Clustersql"},{"location":"old_files/connectingToCluster/","text":"Using the SnappyData SQL Shell \u00b6 The SnappyData SQL Shell ( snappy-sql ) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. Internally it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData. // from the SnappyData base directory $ cd quickstart / scripts $ .. / .. / bin / snappy - sql Version 2.0 - BETA snappy > //Connect to the cluster as a client snappy > connect client 'localhost:1527' ; //Show active connections snappy > show connections ; //Display cluster members by querying a system table snappy > select id , kind , status , host , port from sys . members ; //or snappy > show members ; //Run a sql script. This particular script creates and loads a column table in the default schema snappy > run 'create_and_load_column_table.sql' ; //Run a sql script. This particular script creates and loads a row table in the default schema snappy > run 'create_and_load_row_table.sql' ; The complete list of commands available through snappy_shell can be found here Using the Spark Shell and spark-submit \u00b6 SnappyData, out-of-the-box, colocates Spark executors and the SnappyData store for efficient data intensive computations. But it may be desirable to isolate the computational cluster for other reasons, for instance, a computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store spark.snappydata.connection property needs to be provided while starting the spark-shell. To run all SnappyData functionalities you need to create a SnappySession . // from the SnappyData base directory # Start the spark shell in local mode. Pass SnappyData's locators host:port as a conf parameter. # Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. $ bin/spark-shell --master local [ * ] --conf spark.snappydata.connection = locatorhost:port --conf spark.ui.port = 4041 scala> #Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql scala> val snappy = new org.apache.spark.sql.SnappySession ( spark.sparkContext ) scala> val airlineDF = snappy.table ( \"airline\" ) .show scala> val resultset = snappy.sql ( \"select * from airline\" ) Any spark application can also use the SnappyData as store and spark as computational engine by providing an extra spark.snappydata.connection property in the conf. # Start the Spark standalone cluster from SnappyData base directory $ sbin/start-all.sh # Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port. $ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection = locatorhost:port --conf spark.ui.port = 4041 $SNAPPY_HOME /examples/jars/quickstart.jar # The results can be seen on the command line. Using JDBC with SnappyData \u00b6 SnappyData ships with a few JDBC drivers. The connection URL typically points to one of the Locators. Underneath the covers the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically swizzling underlying physical connections in case servers were to fail. // 1527 is the default port a Locator or Server uses to listen for thin client connections Connection c = DriverManager . getConnection ( \"jdbc:snappydata://locatorHostName:1527/\" ); // While, clients typically just point to a locator, you could also directly point the // connection at a server endpoint Accessing SnappyData Tables from Spark code \u00b6 Spark applications access the SnappyStore using the new Spark Data Source API . By default, SnappyData servers runs the Spark Executors colocated with the data store. And, the default store provider is SnappyData. When the spark program connects to the cluster using a SnappyContext (extends SQLContext), there is no need to configure the database URL and other options. // Here is an Scala example val sc = new org . apache . spark . SparkContext ( conf ) val snappy = new org . apache . spark . sql . SnappySession ( sc ) val props = Map [ String , String ]() // Save some application dataframe into a SnappyData row table myAppDataFrame . write . format ( \"row\" ). options ( props ). saveAsTable ( \"MutableTable\" ) When running a native spark program, you can access SnappyData purely as a DataSource ... // Access SnappyData as a storage cluster .. val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"local[4]\" ) . getOrCreate val props = Map ( \"url\" -> \"jdbc:snappydata://locatorHostName:1527/\" , \"poolImpl\" -> \"tomcat\" , \"user\" -> \"app\" , \"password\" -> \"app\" ) // Save some application dataframe into a JDBC DataSource myAppDataFrame . write . format ( \"jdbc\" ). options ( props ). saveAsTable ( \"MutableTable\" )","title":"connectingToCluster"},{"location":"old_files/connectingToCluster/#using-the-snappydata-sql-shell","text":"The SnappyData SQL Shell ( snappy-sql ) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. Internally it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData. // from the SnappyData base directory $ cd quickstart / scripts $ .. / .. / bin / snappy - sql Version 2.0 - BETA snappy > //Connect to the cluster as a client snappy > connect client 'localhost:1527' ; //Show active connections snappy > show connections ; //Display cluster members by querying a system table snappy > select id , kind , status , host , port from sys . members ; //or snappy > show members ; //Run a sql script. This particular script creates and loads a column table in the default schema snappy > run 'create_and_load_column_table.sql' ; //Run a sql script. This particular script creates and loads a row table in the default schema snappy > run 'create_and_load_row_table.sql' ; The complete list of commands available through snappy_shell can be found here","title":"Using the SnappyData SQL Shell"},{"location":"old_files/connectingToCluster/#using-the-spark-shell-and-spark-submit","text":"SnappyData, out-of-the-box, colocates Spark executors and the SnappyData store for efficient data intensive computations. But it may be desirable to isolate the computational cluster for other reasons, for instance, a computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store spark.snappydata.connection property needs to be provided while starting the spark-shell. To run all SnappyData functionalities you need to create a SnappySession . // from the SnappyData base directory # Start the spark shell in local mode. Pass SnappyData's locators host:port as a conf parameter. # Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. $ bin/spark-shell --master local [ * ] --conf spark.snappydata.connection = locatorhost:port --conf spark.ui.port = 4041 scala> #Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql scala> val snappy = new org.apache.spark.sql.SnappySession ( spark.sparkContext ) scala> val airlineDF = snappy.table ( \"airline\" ) .show scala> val resultset = snappy.sql ( \"select * from airline\" ) Any spark application can also use the SnappyData as store and spark as computational engine by providing an extra spark.snappydata.connection property in the conf. # Start the Spark standalone cluster from SnappyData base directory $ sbin/start-all.sh # Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port. $ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection = locatorhost:port --conf spark.ui.port = 4041 $SNAPPY_HOME /examples/jars/quickstart.jar # The results can be seen on the command line.","title":"Using the Spark Shell and spark-submit"},{"location":"old_files/connectingToCluster/#using-jdbc-with-snappydata","text":"SnappyData ships with a few JDBC drivers. The connection URL typically points to one of the Locators. Underneath the covers the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically swizzling underlying physical connections in case servers were to fail. // 1527 is the default port a Locator or Server uses to listen for thin client connections Connection c = DriverManager . getConnection ( \"jdbc:snappydata://locatorHostName:1527/\" ); // While, clients typically just point to a locator, you could also directly point the // connection at a server endpoint","title":"Using JDBC with SnappyData"},{"location":"old_files/connectingToCluster/#accessing-snappydata-tables-from-spark-code","text":"Spark applications access the SnappyStore using the new Spark Data Source API . By default, SnappyData servers runs the Spark Executors colocated with the data store. And, the default store provider is SnappyData. When the spark program connects to the cluster using a SnappyContext (extends SQLContext), there is no need to configure the database URL and other options. // Here is an Scala example val sc = new org . apache . spark . SparkContext ( conf ) val snappy = new org . apache . spark . sql . SnappySession ( sc ) val props = Map [ String , String ]() // Save some application dataframe into a SnappyData row table myAppDataFrame . write . format ( \"row\" ). options ( props ). saveAsTable ( \"MutableTable\" ) When running a native spark program, you can access SnappyData purely as a DataSource ... // Access SnappyData as a storage cluster .. val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"local[4]\" ) . getOrCreate val props = Map ( \"url\" -> \"jdbc:snappydata://locatorHostName:1527/\" , \"poolImpl\" -> \"tomcat\" , \"user\" -> \"app\" , \"password\" -> \"app\" ) // Save some application dataframe into a JDBC DataSource myAppDataFrame . write . format ( \"jdbc\" ). options ( props ). saveAsTable ( \"MutableTable\" )","title":"Accessing SnappyData Tables from Spark code"},{"location":"old_files/features/","text":"","title":"Features"},{"location":"old_files/rowAndColumnTables/","text":"Row and column tables \u00b6 Column tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model. Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates. Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc. DDL and DML Syntax for tables \u00b6 CREATE TABLE [IF NOT EXISTS] table_name ( COLUMN_DEFININTION ) USING 'row | column' OPTIONS ( COLOCATE_WITH 'table_name', // Default none PARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table. BUCKETS 'NumPartitions', // Default 128 REDUNDANCY '1' , RECOVER_DELAY '-1', MAX_PART_SIZE '50', EVICTION_BY \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT, PERSISTENT \u2018DISKSTORE_NAME ASYNCHRONOUS | SYNCHRONOUS\u2019, //empty string will map to default diskstore OFFHEAP \u2018true | false\u2019 , EXPIRE \u2018TIMETOLIVE in seconds', ) [AS select_statement]; DROP TABLE [IF EXISTS] table_name For row format tables column definition can take underlying GemFire XD syntax to create a table.e.g.note the PRIMARY KEY clause below. snc.sql(\"CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT) USING row options(BUCKETS '8')\" ) But for column table it's restricted to Spark syntax for column definition e.g. snc.sql(\"CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '8')\" ) Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition. Spark API for managing tables \u00b6 Get a reference to SnappyContext val snc: SnappyContext = SnappyContext.getOrCreate(sparkContext) Create a SnappyStore table using Spark APIs val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section case class Data(col1: Int, col2: Int, col3: Int) val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sc.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snc.createDataFrame(rdd) snc.createTable(\"column_table\", \"column\", dataDF.schema, props) //or create a row format table snc.createTable(\"row_table\", \"row\", dataDF.schema, props) Drop a SnappyStore table using Spark APIs snc.dropTable(tableName, ifExists = true) DDL extensions to SnappyStore tables \u00b6 The below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions. COLOCATE_WITH : The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist. PARTITION_BY : Use the PARTITION_BY {COLUMN} clause to provide a set of column names that will determine the partitioning. As a shortcut you can use PARTITION BY PRIMARY KEY to refer to the primary key columns defined for the table . If not specified, it will be a replicated table. BUCKETS : The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 128. REDUNDANCY : Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. RECOVER_DELAY : Use the RECOVERY_DELAY clause to specify the default time in milliseconds that existing members will wait before satisfying redundancy after a member crashes. The default is -1, which indicates that redundancy is not recovered after a member fails. MAX_PART_SIZE : The MAXPARTSIZE attribute specifies the maximum memory for any partition on a member in megabytes. Use it to load-balance partitions among available members. If you omit MAXPARTSIZE, then GemFire XD calculates a default value for the table based on available heap memory. You can view the MAXPARTSIZE setting by querying the EVICTIONATTRS column in SYSTABLES. EVICTION_BY : Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store PERSISTENT : When you specify the PERSISTENT keyword, GemFire XD persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member. OFFHEAP : SnappyStore enables you to store the data for selected tables outside of the JVM heap. Storing a table in off-heap memory can improve performance for the table by reducing the CPU resources required to manage the table's data in the heap (garbage collection) EXPIRE: You can use the EXPIRE clause with tables to control SnappyStore memory usage. It will expire the rows after configured TTL. Restrictions on column tables \u00b6 Column tables can not specify any primary key, unique key constraints. Index on column table is not supported. Option EXPIRE is not applicable for column tables. DML operations on tables \u00b6 INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ; UPDATE tablename SET column = value [, column = value ...] [WHERE expression] PUT INTO tableName (column, ...) VALUES (value, ...) DELETE FROM tablename1 [WHERE expression] TRUNCATE TABLE tablename1; API extensions provided in SnappyContext \u00b6 We have added several APIs in SnappyContext to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables. // Applicable for both row & column tables def insert(tableName: String, rows: Row*): Int . // Only for row tables def put(tableName: String, rows: Row*): Int def update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*): Int def delete(tableName: String, filterExpr: String): Int Usage SnappyConytext.insert(): Insert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snappyContext.insert(\"tableName\", Row.fromSeq(r)) } Usage SnappyConytext.put(): Upsert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snc.put(tableName, Row.fromSeq(r)) } Usage SnappyConytext.update(): Update all rows in table that match passed filter expression snc.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" ) Usage SnappyConytext.delete(): Delete all rows in table that match passed filter expression snc.delete(tableName, \"ITEMREF = 3\") Row Buffers for column tables \u00b6 Generally, the Column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored. In SnappyData, the column table consists of two components, delta row buffer and column store. We try to support individual insert of single row, we store them in a delta row buffer which is write optimized and highly available. Once the size of buffer reaches the COLUMN_BATCH_SIZE set by user, the delta row buffer is compressed column wise and stored in the column store. Any query on column table, also takes into account the row cached buffer. By doing this, we ensure that the query doesn't miss any data. Catalog in SnappyStore \u00b6 We use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us ability to query underlying GemFireXD to reconstruct the metastore incase of a driver failover. There are pending work towards unifying DRDA & Spark layer catalog, which will part of future releases. SQL Reference to the Syntax \u00b6 For detailed syntax for GemFire XD check http://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/sql-language-reference.html","title":"rowAndColumnTables"},{"location":"old_files/rowAndColumnTables/#row-and-column-tables","text":"Column tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model. Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates. Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.","title":"Row and column tables"},{"location":"old_files/rowAndColumnTables/#ddl-and-dml-syntax-for-tables","text":"CREATE TABLE [IF NOT EXISTS] table_name ( COLUMN_DEFININTION ) USING 'row | column' OPTIONS ( COLOCATE_WITH 'table_name', // Default none PARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table. BUCKETS 'NumPartitions', // Default 128 REDUNDANCY '1' , RECOVER_DELAY '-1', MAX_PART_SIZE '50', EVICTION_BY \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT, PERSISTENT \u2018DISKSTORE_NAME ASYNCHRONOUS | SYNCHRONOUS\u2019, //empty string will map to default diskstore OFFHEAP \u2018true | false\u2019 , EXPIRE \u2018TIMETOLIVE in seconds', ) [AS select_statement]; DROP TABLE [IF EXISTS] table_name For row format tables column definition can take underlying GemFire XD syntax to create a table.e.g.note the PRIMARY KEY clause below. snc.sql(\"CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT) USING row options(BUCKETS '8')\" ) But for column table it's restricted to Spark syntax for column definition e.g. snc.sql(\"CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '8')\" ) Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.","title":"DDL and DML Syntax for tables"},{"location":"old_files/rowAndColumnTables/#spark-api-for-managing-tables","text":"Get a reference to SnappyContext val snc: SnappyContext = SnappyContext.getOrCreate(sparkContext) Create a SnappyStore table using Spark APIs val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section case class Data(col1: Int, col2: Int, col3: Int) val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sc.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snc.createDataFrame(rdd) snc.createTable(\"column_table\", \"column\", dataDF.schema, props) //or create a row format table snc.createTable(\"row_table\", \"row\", dataDF.schema, props) Drop a SnappyStore table using Spark APIs snc.dropTable(tableName, ifExists = true)","title":"Spark API for managing tables"},{"location":"old_files/rowAndColumnTables/#ddl-extensions-to-snappystore-tables","text":"The below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions. COLOCATE_WITH : The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist. PARTITION_BY : Use the PARTITION_BY {COLUMN} clause to provide a set of column names that will determine the partitioning. As a shortcut you can use PARTITION BY PRIMARY KEY to refer to the primary key columns defined for the table . If not specified, it will be a replicated table. BUCKETS : The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 128. REDUNDANCY : Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. RECOVER_DELAY : Use the RECOVERY_DELAY clause to specify the default time in milliseconds that existing members will wait before satisfying redundancy after a member crashes. The default is -1, which indicates that redundancy is not recovered after a member fails. MAX_PART_SIZE : The MAXPARTSIZE attribute specifies the maximum memory for any partition on a member in megabytes. Use it to load-balance partitions among available members. If you omit MAXPARTSIZE, then GemFire XD calculates a default value for the table based on available heap memory. You can view the MAXPARTSIZE setting by querying the EVICTIONATTRS column in SYSTABLES. EVICTION_BY : Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store PERSISTENT : When you specify the PERSISTENT keyword, GemFire XD persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member. OFFHEAP : SnappyStore enables you to store the data for selected tables outside of the JVM heap. Storing a table in off-heap memory can improve performance for the table by reducing the CPU resources required to manage the table's data in the heap (garbage collection) EXPIRE: You can use the EXPIRE clause with tables to control SnappyStore memory usage. It will expire the rows after configured TTL.","title":"DDL extensions to SnappyStore tables"},{"location":"old_files/rowAndColumnTables/#restrictions-on-column-tables","text":"Column tables can not specify any primary key, unique key constraints. Index on column table is not supported. Option EXPIRE is not applicable for column tables.","title":"Restrictions on column tables"},{"location":"old_files/rowAndColumnTables/#dml-operations-on-tables","text":"INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ; UPDATE tablename SET column = value [, column = value ...] [WHERE expression] PUT INTO tableName (column, ...) VALUES (value, ...) DELETE FROM tablename1 [WHERE expression] TRUNCATE TABLE tablename1;","title":"DML operations on tables"},{"location":"old_files/rowAndColumnTables/#api-extensions-provided-in-snappycontext","text":"We have added several APIs in SnappyContext to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables. // Applicable for both row & column tables def insert(tableName: String, rows: Row*): Int . // Only for row tables def put(tableName: String, rows: Row*): Int def update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*): Int def delete(tableName: String, filterExpr: String): Int Usage SnappyConytext.insert(): Insert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snappyContext.insert(\"tableName\", Row.fromSeq(r)) } Usage SnappyConytext.put(): Upsert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snc.put(tableName, Row.fromSeq(r)) } Usage SnappyConytext.update(): Update all rows in table that match passed filter expression snc.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" ) Usage SnappyConytext.delete(): Delete all rows in table that match passed filter expression snc.delete(tableName, \"ITEMREF = 3\")","title":"API extensions provided in SnappyContext"},{"location":"old_files/rowAndColumnTables/#row-buffers-for-column-tables","text":"Generally, the Column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored. In SnappyData, the column table consists of two components, delta row buffer and column store. We try to support individual insert of single row, we store them in a delta row buffer which is write optimized and highly available. Once the size of buffer reaches the COLUMN_BATCH_SIZE set by user, the delta row buffer is compressed column wise and stored in the column store. Any query on column table, also takes into account the row cached buffer. By doing this, we ensure that the query doesn't miss any data.","title":"Row Buffers for column tables"},{"location":"old_files/rowAndColumnTables/#catalog-in-snappystore","text":"We use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us ability to query underlying GemFireXD to reconstruct the metastore incase of a driver failover. There are pending work towards unifying DRDA & Spark layer catalog, which will part of future releases.","title":"Catalog in SnappyStore"},{"location":"old_files/rowAndColumnTables/#sql-reference-to-the-syntax","text":"For detailed syntax for GemFire XD check http://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/sql-language-reference.html","title":"SQL Reference to the Syntax"},{"location":"old_files/snappyIntroduction/","text":"About SnappyData \u00b6 Key Features \u00b6 100% compatible with Spark - Use SnappyData as a database but also use any of the Spark APIs - ML, Graph, etc in-memory row and column stores : run the store colocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster) SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints. SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel. Interactive analytics using Synopsis Data Engine (SDE) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. Mutate, transact on data in Spark : You can use SQL to insert, update, delete data in tables as one would expect. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store. Optimizations - Indexing : You can index your row store and the GemFire SQL optimizer will automatically use in-memory indexes when available. Optimizations - colocation : SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be colocated using declarative custom partitioning strategies(e.g. common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent. High availability not just Fault tolerance : Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications continuous HA. Durability and recovery: Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled. Spark Challenges for Mixed Workloads (OLTP, OLAP) \u00b6 Spark is designed as a computational engine for processing batch jobs. Each Spark application (e.g., a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an ex- ternal storage tier, such as HDFS. We, on the other hand, target a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database on the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and de-coupled from individual applications. A second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces a single point of contention for all requests, and a barrier for achieving high availability (HA). Executors are shutdown if the driver fails, requiring a full refresh of any cached state. Spark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, we need to manage more complex data structures (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mu- tability characteristics and conformance to SQL. Finally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.","title":"snappyIntroduction"},{"location":"old_files/snappyIntroduction/#about-snappydata","text":"","title":"About SnappyData"},{"location":"old_files/snappyIntroduction/#key-features","text":"100% compatible with Spark - Use SnappyData as a database but also use any of the Spark APIs - ML, Graph, etc in-memory row and column stores : run the store colocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster) SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints. SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel. Interactive analytics using Synopsis Data Engine (SDE) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. Mutate, transact on data in Spark : You can use SQL to insert, update, delete data in tables as one would expect. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store. Optimizations - Indexing : You can index your row store and the GemFire SQL optimizer will automatically use in-memory indexes when available. Optimizations - colocation : SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be colocated using declarative custom partitioning strategies(e.g. common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent. High availability not just Fault tolerance : Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications continuous HA. Durability and recovery: Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.","title":"Key Features"},{"location":"old_files/snappyIntroduction/#spark-challenges-for-mixed-workloads-oltp-olap","text":"Spark is designed as a computational engine for processing batch jobs. Each Spark application (e.g., a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an ex- ternal storage tier, such as HDFS. We, on the other hand, target a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database on the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and de-coupled from individual applications. A second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces a single point of contention for all requests, and a barrier for achieving high availability (HA). Executors are shutdown if the driver fails, requiring a full refresh of any cached state. Spark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, we need to manage more complex data structures (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mu- tability characteristics and conformance to SQL. Finally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.","title":"Spark Challenges for Mixed Workloads (OLTP, OLAP)"},{"location":"old_files/snappyOnAWS/","text":"The ec2 script is still under development. Feel free to try it out and provide your feedback. Introduction \u00b6 The snappy-ec2 script, present in SnappyData's ec2 directory, enables users to quickly launch and manage SnappyData clusters on Amazon EC2. Users can also configure the individual nodes of the cluster by providing properties in specific conf files which the script would read before launching the cluster. The snappy-ec2 script has been derived from the spark-ec2 script available in Apache Spark 1.6 . Prerequisites You need to have an account with Amazon Web Service (AWS) with adequate credentials to launch EC2 resources. You also need to set two environment variables viz. AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID with your AWS secrete access key and ID. These can be obtained from the AWS homepage by clicking Account > Security Credentials > Access Credentials. export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112 export AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10 Alternatively, if you already have setup AWS Command Line Interface on your local machine, these credentials should be picked up from the aws credentials file by the script. Cluster management \u00b6 Launching a cluster You can launch and manage multiple clusters using this script and each of the clusters is identified by the unique cluster name you provide. The script internally ties members (locators, leads and stores) of the cluster with EC2 security groups whose names are derived from the provided cluster name. So if you launch a cluster named 'my-cluster', it would have its locator in security group named 'my-cluster-locator' and its stores in 'my-cluster-stores'. By default, the script starts one instance of locator, lead and server each. Below command will start a snappydata cluster named 'snappydata-cluster' with 4 stores (or servers) in N. Virginia region of AWS. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file --stores=4 --region=us-east-1 launch snappydata-cluster Specifying properties If you want to configure each of the locator, lead or servers with specific properties, you can do so by specifying them in files named locators, leads or servers, respectively and placing these under product_dir/ec2/deploy/home/ec2-user/snappydata/. Refer to this SnappyData documentation page for example on how to write these conf files. This is similar to how one would provide properties to snappydata cluster nodes while launching it via sbin/snappy-start-all.sh script. The only and important difference here is that, instead of the hostnames of the locator, lead or store, you will have to write {{LOCATOR_N}}, {{LEAD_N}} or {{SERVER_N}} in these files, respectively. N stands for Nth locator, lead or server. The script will replace these with the actual hostname of the members when they are launched. The sample conf files for a cluster with 2 locators, 1 lead and 2 stores are given below. locators {{LOCATOR_0}} -peer-discovery-port=9999 -heap-size=1024m {{LOCATOR_1}} -peer-discovery-port=9888 -heap-size=1024m leads {{LEAD_0}} -heap-size=4096m -spark.ui.port=3333 -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -spark.executor.cores=10 servers {{SERVER_0}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 {{SERVER_1}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -client-port=1530 When you run snappy-ec2, it'll look for these files under ec2/deploy/home/ec2-user/snappydata/ and, if present, read them while launching the cluster on Amazon EC2. You must ensure that the number of locators, leads or servers specified by options --locators, --leads or --stores must match to the number of entries in their respective conf files. The script will also read snappy-env.sh, if present in this location. Stopping a cluster When you stop a cluster, it will shut down the EC2 instances and you will loose any data saved on its local instance stores but the data saved on EBS volumes, if any, will be retained unless the spot-instances were used. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file stop cluster-name Starting a cluster When you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file start cluster-name Note that the start command (or launch command with --resume option) ignores --locators, --leads or --stores options and launches SnappyData cluster on existing instances. But the conf files will be read in any case, if they are present in the location mentioned above. So you need to ensure that every time you use start command, the number of entries in conf files are equal to the number of instances in their respective security group. Adding servers to a cluster This is not yet fully supported via the script. You may have to manually launch an instance with (cluster-name)-stores group and then use launch command with --resume option. Listing members of a cluster To get the first locator's hostname, ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file get-locator cluster-name To get the first lead's hostname, use get-lead command. Connecting to a cluster You can connect to any instance of a cluster via ssh using the login command. It'll log you into the first lead instance. From there, you can ssh to any other member of the cluster without password. The SnappyData product directory would be located under /home/ec2-user/snappydata/ on all the members. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file login cluster-name Destroying a cluster Destroying a cluster will destroy all the data on the local instance stores as well as on the attached EBS volumes permanently. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file destroy cluster-name This will retain the security groups created for this cluster. To delete them as well, use it with --delete-group option. Starting cluster with Apache Zeppelin Optionally, you can start an instance of Apache Zeppelin server with the cluster. Apache Zeppelin is a web-based notebook that enables interactive notebook. The Zeppelin server is launched on the same EC2 instance where the lead node is running. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file --with-zeppelin launch cluster-name More options For a complete list of options this script has, execute ./snappy-ec2 . These are pasted below for your quick reference. Usage: snappy-ec2 [options] <action> <cluster_name> <action> can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster Options: --version show program's version number and exit -h, --help show this help message and exit -s STORES, --stores=STORES Number of stores to launch (default: 1) --locators=LOCATORS Number of locator nodes to launch (default: 1) --leads=LEADS Number of lead nodes to launch (default: 1) -w WAIT, --wait=WAIT DEPRECATED (no longer necessary) - Seconds to wait for nodes to start -k KEY_PAIR, --key-pair=KEY_PAIR Key pair to use on instances -i IDENTITY_FILE, --identity-file=IDENTITY_FILE SSH private key file to use for logging into instances -p PROFILE, --profile=PROFILE If you have multiple profiles (AWS or boto config), you can configure additional, named profiles by using this option (default: none) -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE Type of instance to launch (default: m3.large). WARNING: must be 64-bit; small instances won't work --locator-instance-type=LOCATOR_INSTANCE_TYPE Locator instance type (leave empty for same as instance-type) -r REGION, --region=REGION EC2 region used to launch instances in, or to find them in (default: us-east-1) -z ZONE, --zone=ZONE Availability zone to launch instances in, or 'all' to spread stores across multiple (an additional $0.01/Gb for bandwidthbetween zones applies) (default: a single zone chosen at random) -a AMI, --ami=AMI Amazon Machine Image ID to use -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION Version of SnappyData to use: 'X.Y.Z' (default: LATEST) --with-zeppelin Launch Apache Zeppelin server with the cluster. It'll be launched on the same instance where lead node will be running. --deploy-root-dir=DEPLOY_ROOT_DIR A directory to copy into / on the first master. Must be absolute. Note that a trailing slash is handled as per rsync: If you omit it, the last directory of the --deploy-root-dir path will be created in / before copying its contents. If you append the trailing slash, the directory is not created and its contents are copied directly into /. (default: none). -D [ADDRESS:]PORT Use SSH dynamic port forwarding to create a SOCKS proxy at the given local address (for use with login) --resume Resume installation on a previously launched cluster (for debugging) --ebs-vol-size=SIZE Size (in GB) of each EBS volume. --ebs-vol-type=EBS_VOL_TYPE EBS volume type (e.g. 'gp2', 'standard'). --ebs-vol-num=EBS_VOL_NUM Number of EBS volumes to attach to each node as /vol[x]. The volumes will be deleted when the instances terminate. Only possible on EBS-backed AMIs. EBS volumes are only attached if --ebs-vol-size > 0. Only support up to 8 EBS volumes. --placement-group=PLACEMENT_GROUP Which placement group to try and launch instances into. Assumes placement group is already created. --spot-price=PRICE If specified, launch stores as spot instances with the given maximum price (in dollars) -u USER, --user=USER The SSH user you want to connect as (default: ec2-user) --delete-groups When destroying a cluster, delete the security groups that were created --use-existing-locator Launch fresh stores, but use an existing stopped locator if possible --user-data=USER_DATA Path to a user-data file (most AMIs interpret this as an initialization script) --authorized-address=AUTHORIZED_ADDRESS Address to authorize on created security groups (default: 0.0.0.0/0) --additional-security-group=ADDITIONAL_SECURITY_GROUP Additional security group to place the machines in --additional-tags=ADDITIONAL_TAGS Additional tags to set on the machines; tags are comma-separated, while name and value are colon separated; ex: \"Task:MySnappyProject,Env:production\" --subnet-id=SUBNET_ID VPC subnet to launch instances in --vpc-id=VPC_ID VPC to launch instances in --private-ips Use private IPs for instances rather than public if VPC/subnet requires that. --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR Whether instances should terminate when shut down or just stop --instance-profile-name=INSTANCE_PROFILE_NAME IAM profile name to launch instances under Limitations \u00b6 Launching cluster on custom AMI (specified via --ami option) will not work if it does not have the user 'ec2-user' with sudo permissions. Support for option --user is incomplete.","title":"snappyOnAWS"},{"location":"old_files/snappyOnAWS/#introduction","text":"The snappy-ec2 script, present in SnappyData's ec2 directory, enables users to quickly launch and manage SnappyData clusters on Amazon EC2. Users can also configure the individual nodes of the cluster by providing properties in specific conf files which the script would read before launching the cluster. The snappy-ec2 script has been derived from the spark-ec2 script available in Apache Spark 1.6 . Prerequisites You need to have an account with Amazon Web Service (AWS) with adequate credentials to launch EC2 resources. You also need to set two environment variables viz. AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID with your AWS secrete access key and ID. These can be obtained from the AWS homepage by clicking Account > Security Credentials > Access Credentials. export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112 export AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10 Alternatively, if you already have setup AWS Command Line Interface on your local machine, these credentials should be picked up from the aws credentials file by the script.","title":"Introduction"},{"location":"old_files/snappyOnAWS/#cluster-management","text":"Launching a cluster You can launch and manage multiple clusters using this script and each of the clusters is identified by the unique cluster name you provide. The script internally ties members (locators, leads and stores) of the cluster with EC2 security groups whose names are derived from the provided cluster name. So if you launch a cluster named 'my-cluster', it would have its locator in security group named 'my-cluster-locator' and its stores in 'my-cluster-stores'. By default, the script starts one instance of locator, lead and server each. Below command will start a snappydata cluster named 'snappydata-cluster' with 4 stores (or servers) in N. Virginia region of AWS. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file --stores=4 --region=us-east-1 launch snappydata-cluster Specifying properties If you want to configure each of the locator, lead or servers with specific properties, you can do so by specifying them in files named locators, leads or servers, respectively and placing these under product_dir/ec2/deploy/home/ec2-user/snappydata/. Refer to this SnappyData documentation page for example on how to write these conf files. This is similar to how one would provide properties to snappydata cluster nodes while launching it via sbin/snappy-start-all.sh script. The only and important difference here is that, instead of the hostnames of the locator, lead or store, you will have to write {{LOCATOR_N}}, {{LEAD_N}} or {{SERVER_N}} in these files, respectively. N stands for Nth locator, lead or server. The script will replace these with the actual hostname of the members when they are launched. The sample conf files for a cluster with 2 locators, 1 lead and 2 stores are given below. locators {{LOCATOR_0}} -peer-discovery-port=9999 -heap-size=1024m {{LOCATOR_1}} -peer-discovery-port=9888 -heap-size=1024m leads {{LEAD_0}} -heap-size=4096m -spark.ui.port=3333 -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -spark.executor.cores=10 servers {{SERVER_0}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 {{SERVER_1}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -client-port=1530 When you run snappy-ec2, it'll look for these files under ec2/deploy/home/ec2-user/snappydata/ and, if present, read them while launching the cluster on Amazon EC2. You must ensure that the number of locators, leads or servers specified by options --locators, --leads or --stores must match to the number of entries in their respective conf files. The script will also read snappy-env.sh, if present in this location. Stopping a cluster When you stop a cluster, it will shut down the EC2 instances and you will loose any data saved on its local instance stores but the data saved on EBS volumes, if any, will be retained unless the spot-instances were used. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file stop cluster-name Starting a cluster When you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file start cluster-name Note that the start command (or launch command with --resume option) ignores --locators, --leads or --stores options and launches SnappyData cluster on existing instances. But the conf files will be read in any case, if they are present in the location mentioned above. So you need to ensure that every time you use start command, the number of entries in conf files are equal to the number of instances in their respective security group. Adding servers to a cluster This is not yet fully supported via the script. You may have to manually launch an instance with (cluster-name)-stores group and then use launch command with --resume option. Listing members of a cluster To get the first locator's hostname, ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file get-locator cluster-name To get the first lead's hostname, use get-lead command. Connecting to a cluster You can connect to any instance of a cluster via ssh using the login command. It'll log you into the first lead instance. From there, you can ssh to any other member of the cluster without password. The SnappyData product directory would be located under /home/ec2-user/snappydata/ on all the members. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file login cluster-name Destroying a cluster Destroying a cluster will destroy all the data on the local instance stores as well as on the attached EBS volumes permanently. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file destroy cluster-name This will retain the security groups created for this cluster. To delete them as well, use it with --delete-group option. Starting cluster with Apache Zeppelin Optionally, you can start an instance of Apache Zeppelin server with the cluster. Apache Zeppelin is a web-based notebook that enables interactive notebook. The Zeppelin server is launched on the same EC2 instance where the lead node is running. ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file --with-zeppelin launch cluster-name More options For a complete list of options this script has, execute ./snappy-ec2 . These are pasted below for your quick reference. Usage: snappy-ec2 [options] <action> <cluster_name> <action> can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster Options: --version show program's version number and exit -h, --help show this help message and exit -s STORES, --stores=STORES Number of stores to launch (default: 1) --locators=LOCATORS Number of locator nodes to launch (default: 1) --leads=LEADS Number of lead nodes to launch (default: 1) -w WAIT, --wait=WAIT DEPRECATED (no longer necessary) - Seconds to wait for nodes to start -k KEY_PAIR, --key-pair=KEY_PAIR Key pair to use on instances -i IDENTITY_FILE, --identity-file=IDENTITY_FILE SSH private key file to use for logging into instances -p PROFILE, --profile=PROFILE If you have multiple profiles (AWS or boto config), you can configure additional, named profiles by using this option (default: none) -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE Type of instance to launch (default: m3.large). WARNING: must be 64-bit; small instances won't work --locator-instance-type=LOCATOR_INSTANCE_TYPE Locator instance type (leave empty for same as instance-type) -r REGION, --region=REGION EC2 region used to launch instances in, or to find them in (default: us-east-1) -z ZONE, --zone=ZONE Availability zone to launch instances in, or 'all' to spread stores across multiple (an additional $0.01/Gb for bandwidthbetween zones applies) (default: a single zone chosen at random) -a AMI, --ami=AMI Amazon Machine Image ID to use -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION Version of SnappyData to use: 'X.Y.Z' (default: LATEST) --with-zeppelin Launch Apache Zeppelin server with the cluster. It'll be launched on the same instance where lead node will be running. --deploy-root-dir=DEPLOY_ROOT_DIR A directory to copy into / on the first master. Must be absolute. Note that a trailing slash is handled as per rsync: If you omit it, the last directory of the --deploy-root-dir path will be created in / before copying its contents. If you append the trailing slash, the directory is not created and its contents are copied directly into /. (default: none). -D [ADDRESS:]PORT Use SSH dynamic port forwarding to create a SOCKS proxy at the given local address (for use with login) --resume Resume installation on a previously launched cluster (for debugging) --ebs-vol-size=SIZE Size (in GB) of each EBS volume. --ebs-vol-type=EBS_VOL_TYPE EBS volume type (e.g. 'gp2', 'standard'). --ebs-vol-num=EBS_VOL_NUM Number of EBS volumes to attach to each node as /vol[x]. The volumes will be deleted when the instances terminate. Only possible on EBS-backed AMIs. EBS volumes are only attached if --ebs-vol-size > 0. Only support up to 8 EBS volumes. --placement-group=PLACEMENT_GROUP Which placement group to try and launch instances into. Assumes placement group is already created. --spot-price=PRICE If specified, launch stores as spot instances with the given maximum price (in dollars) -u USER, --user=USER The SSH user you want to connect as (default: ec2-user) --delete-groups When destroying a cluster, delete the security groups that were created --use-existing-locator Launch fresh stores, but use an existing stopped locator if possible --user-data=USER_DATA Path to a user-data file (most AMIs interpret this as an initialization script) --authorized-address=AUTHORIZED_ADDRESS Address to authorize on created security groups (default: 0.0.0.0/0) --additional-security-group=ADDITIONAL_SECURITY_GROUP Additional security group to place the machines in --additional-tags=ADDITIONAL_TAGS Additional tags to set on the machines; tags are comma-separated, while name and value are colon separated; ex: \"Task:MySnappyProject,Env:production\" --subnet-id=SUBNET_ID VPC subnet to launch instances in --vpc-id=VPC_ID VPC to launch instances in --private-ips Use private IPs for instances rather than public if VPC/subnet requires that. --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR Whether instances should terminate when shut down or just stop --instance-profile-name=INSTANCE_PROFILE_NAME IAM profile name to launch instances under","title":"Cluster management"},{"location":"old_files/snappyOnAWS/#limitations","text":"Launching cluster on custom AMI (specified via --ami option) will not work if it does not have the user 'ec2-user' with sudo permissions. Support for option --user is incomplete.","title":"Limitations"},{"location":"old_files/streamingWithSQL/","text":"SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. Here is a brief overview of Spark streaming from the Spark Streaming guide. Spark Streaming Overview \u00b6 Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex\u2028algorithms expressed with high-level functions like map , reduce , join and window . Finally, processed data can be pushed out to filesystems, databases,\u2028and live dashboards. In fact, you can apply Spark's machine learning and graph processing algorithms on data streams.\u2028\u2028 Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. Spark Streaming provides a high-level abstraction called discretized stream or DStream , which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs . Additional details on the Spark Streaming concepts and programming is covered here . SnappyData Streaming extensions over Spark \u00b6 We offer the following enhancements over Spark Streaming : Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query. OLAP optimizations : By integrating and colocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store. Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays. Working with stream tables \u00b6 SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources. // DDL for creating a stream table CREATE STREAM TABLE [IF NOT EXISTS] table_name (COLUMN_DEFINITION) USING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream' OPTIONS ( // multiple stream source specific options storagelevel '', rowConverter '', topics '', kafkaParams '', consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', hostname '', port '', directory '' ) // DDL for dropping a stream table DROP TABLE [IF EXISTS] table_name // Initialize StreamingContext STREAMING INIT <batchInterval> [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE] // Start streaming STREAMING START // Stop streaming STREAMING STOP For example to create a stream table using kafka source : val sc = new SparkContext ( new SparkConf (). setAppName ( \"example\" ). setMaster ( \"local[*]\" )) val snc = SnappyContext . getOrCreate ( sc ) var snsc = SnappyStreamingContext ( snc , Seconds ( 1 )) snsc . sql ( \"create stream table streamTable (userId string, clickStreamLog string) \" + \"using kafka_stream options (\" + \"storagelevel 'MEMORY_AND_DISK_SER_2', \" + \"rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \" + \"kafkaParams 'zookeeper.connect->localhost:2181;auto.offset.reset->smallest;group.id->myGroupId', \" + \"topics 'streamTopic:01')\" ) // You can get a handle of underlying DStream of the table val dStream = snsc . getSchemaDStream ( \"streamTable\" ) // You can also save the DataFrames to an external table dStream . foreachDataFrame ( _ . write . insertInto ( tableName )) The streamTable created in above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries. Stream SQL through snappy-sql \u00b6 Start a SnappyData cluster and connect through snappy-sql : //create a connection snappy> connect client 'localhost:1527'; // Initialize streaming with batchInterval of 2 seconds snappy> streaming init 2secs; // Create a stream table snappy> create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter'); // Start the streaming snappy> streaming start; //Run ad-hoc queries on the streamTable on current batch of data snappy> select id, text, fullName from streamTable where text like '%snappy%' // Drop the streamTable snappy> drop table streamTable; // Stop the streaming snappy> streaming stop; SchemaDStream \u00b6 SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as a table. Some of these ideas (especially naming our abstractions) were borrowed from Intel's Streaming SQL project - https://github.com/Intel-bigdata/spark-streamingsql Registering Continuous queries \u00b6 //You can join two stream tables and produce a result stream. val resultStream = snsc . registerCQ ( \"SELECT s1.id, s1.text FROM stream1 window (duration '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\" ) // You can also save the DataFrames to an external table dStream . foreachDataFrame ( _ . write . insertInto ( \"yourTableName\" )) Dynamic(ad-hoc) Continuous queries \u00b6 Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The CQs can be registered even after the SnappyStreamingContext has started. Continuous Queries through command line (snappy-sql) \u00b6","title":"streamingWithSQL"},{"location":"old_files/streamingWithSQL/#spark-streaming-overview","text":"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex\u2028algorithms expressed with high-level functions like map , reduce , join and window . Finally, processed data can be pushed out to filesystems, databases,\u2028and live dashboards. In fact, you can apply Spark's machine learning and graph processing algorithms on data streams.\u2028\u2028 Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. Spark Streaming provides a high-level abstraction called discretized stream or DStream , which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs . Additional details on the Spark Streaming concepts and programming is covered here .","title":"Spark Streaming Overview"},{"location":"old_files/streamingWithSQL/#snappydata-streaming-extensions-over-spark","text":"We offer the following enhancements over Spark Streaming : Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query. OLAP optimizations : By integrating and colocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store. Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.","title":"SnappyData Streaming extensions over Spark"},{"location":"old_files/streamingWithSQL/#working-with-stream-tables","text":"SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources. // DDL for creating a stream table CREATE STREAM TABLE [IF NOT EXISTS] table_name (COLUMN_DEFINITION) USING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream' OPTIONS ( // multiple stream source specific options storagelevel '', rowConverter '', topics '', kafkaParams '', consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', hostname '', port '', directory '' ) // DDL for dropping a stream table DROP TABLE [IF EXISTS] table_name // Initialize StreamingContext STREAMING INIT <batchInterval> [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE] // Start streaming STREAMING START // Stop streaming STREAMING STOP For example to create a stream table using kafka source : val sc = new SparkContext ( new SparkConf (). setAppName ( \"example\" ). setMaster ( \"local[*]\" )) val snc = SnappyContext . getOrCreate ( sc ) var snsc = SnappyStreamingContext ( snc , Seconds ( 1 )) snsc . sql ( \"create stream table streamTable (userId string, clickStreamLog string) \" + \"using kafka_stream options (\" + \"storagelevel 'MEMORY_AND_DISK_SER_2', \" + \"rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \" + \"kafkaParams 'zookeeper.connect->localhost:2181;auto.offset.reset->smallest;group.id->myGroupId', \" + \"topics 'streamTopic:01')\" ) // You can get a handle of underlying DStream of the table val dStream = snsc . getSchemaDStream ( \"streamTable\" ) // You can also save the DataFrames to an external table dStream . foreachDataFrame ( _ . write . insertInto ( tableName )) The streamTable created in above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.","title":"Working with stream tables"},{"location":"old_files/streamingWithSQL/#stream-sql-through-snappy-sql","text":"Start a SnappyData cluster and connect through snappy-sql : //create a connection snappy> connect client 'localhost:1527'; // Initialize streaming with batchInterval of 2 seconds snappy> streaming init 2secs; // Create a stream table snappy> create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter'); // Start the streaming snappy> streaming start; //Run ad-hoc queries on the streamTable on current batch of data snappy> select id, text, fullName from streamTable where text like '%snappy%' // Drop the streamTable snappy> drop table streamTable; // Stop the streaming snappy> streaming stop;","title":"Stream SQL through snappy-sql"},{"location":"old_files/streamingWithSQL/#schemadstream","text":"SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as a table. Some of these ideas (especially naming our abstractions) were borrowed from Intel's Streaming SQL project - https://github.com/Intel-bigdata/spark-streamingsql","title":"SchemaDStream"},{"location":"old_files/streamingWithSQL/#registering-continuous-queries","text":"//You can join two stream tables and produce a result stream. val resultStream = snsc . registerCQ ( \"SELECT s1.id, s1.text FROM stream1 window (duration '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\" ) // You can also save the DataFrames to an external table dStream . foreachDataFrame ( _ . write . insertInto ( \"yourTableName\" ))","title":"Registering Continuous queries"},{"location":"old_files/streamingWithSQL/#dynamicad-hoc-continuous-queries","text":"Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The CQs can be registered even after the SnappyStreamingContext has started.","title":"Dynamic(ad-hoc) Continuous queries"},{"location":"old_files/streamingWithSQL/#continuous-queries-through-command-line-snappy-sql","text":"","title":"Continuous Queries through command line (snappy-sql)"},{"location":"programming_guide/","text":"Programming Guide \u00b6 SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames. It is therefore recommended that you understand the concepts in SparkSQL and the DataFrame API . You can also store and manage arbitrary RDDs (or even Spark DataSets) through the implicit or explicit transformation to a DataFrame. While the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced here . In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk. The following topics are covered in this section: SparkSession, SnappySession and SnappyStreamingContext SnappyData Jobs Managing JAR Files Using SnappyData Shell Using the Spark Shell and spark-submit Working with Hadoop YARN Cluster Manager Using JDBC with SnappyData Multiple Language Binding using Thrift Protocol Building SnappyData Applications using Spark API Tables in SnappyData Stream Processing using SQL User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)","title":"Programming Guide"},{"location":"programming_guide/#programming-guide","text":"SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames. It is therefore recommended that you understand the concepts in SparkSQL and the DataFrame API . You can also store and manage arbitrary RDDs (or even Spark DataSets) through the implicit or explicit transformation to a DataFrame. While the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced here . In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk. The following topics are covered in this section: SparkSession, SnappySession and SnappyStreamingContext SnappyData Jobs Managing JAR Files Using SnappyData Shell Using the Spark Shell and spark-submit Working with Hadoop YARN Cluster Manager Using JDBC with SnappyData Multiple Language Binding using Thrift Protocol Building SnappyData Applications using Spark API Tables in SnappyData Stream Processing using SQL User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)","title":"Programming Guide"},{"location":"programming_guide/building_snappydata_applications_using_spark_api/","text":"Building SnappyData Applications using Spark API \u00b6 SnappySession Usage \u00b6 Create Columnar Tables using API \u00b6 Other than create and drop table, rest are all based on the Spark SQL Data Source APIs. Scala val props = Map ( \"BUCKETS\" -> \"8\" ) // Number of partitions to use in the SnappyStore case class Data ( COL1 : Int , COL2 : Int , COL3 : Int ) val data = Seq ( Seq ( 1 , 2 , 3 ), Seq ( 7 , 8 , 9 ), Seq ( 9 , 2 , 3 ), Seq ( 4 , 2 , 3 ), Seq ( 5 , 6 , 7 )) val rdd = spark . sparkContext . parallelize ( data , data . length ). map ( s => new Data ( s ( 0 ), s ( 1 ), s ( 2 ))) val df = snappy . createDataFrame ( rdd ) // create a column table snappy . dropTable ( \"COLUMN_TABLE\" , ifExists = true ) // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snappy . createTable ( \"COLUMN_TABLE\" , \"column\" , df . schema , props ) // append dataDF into the table df . write . insertInto ( \"COLUMN_TABLE\" ) val results = snappy . sql ( \"SELECT * FROM COLUMN_TABLE\" ) println ( \"contents of column table are:\" ) results . foreach ( r => println ( r )) Java Map < String , String > props1 = new HashMap <> (); props1 . put ( \"buckets\" , \"16\" ); JavaRDD < Row > jrdd = jsc . parallelize ( Arrays . asList ( RowFactory . create ( 1 , 2 , 3 ), RowFactory . create ( 7 , 8 , 9 ), RowFactory . create ( 9 , 2 , 3 ), RowFactory . create ( 4 , 2 , 3 ), RowFactory . create ( 5 , 6 , 7 ) )); StructType schema = new StructType ( new StructField [] { new StructField ( \"col1\" , DataTypes . IntegerType , false , Metadata . empty ()), new StructField ( \"col2\" , DataTypes . IntegerType , false , Metadata . empty ()), new StructField ( \"col3\" , DataTypes . IntegerType , false , Metadata . empty ()), }); Dataset < Row > df = snappy . createDataFrame ( jrdd , schema ); // create a column table snappy . dropTable ( \"COLUMN_TABLE\" , true ); // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snappy . createTable ( \"COLUMN_TABLE\" , \"column\" , df . schema (), props1 , false ); // append dataDF into the table df . write (). insertInto ( \"COLUMN_TABLE\" ); Dataset < Row > results = snappy . sql ( \"SELECT * FROM COLUMN_TABLE\" ); System . out . println ( \"contents of column table are:\" ); for ( Row r : results . select ( \"col1\" , \"col2\" , \"col3\" ). collectAsList ()) { System . out . println ( r ); } Python from pyspark.sql.types import * data = [( 1 , 2 , 3 ),( 7 , 8 , 9 ),( 9 , 2 , 3 ),( 4 , 2 , 3 ),( 5 , 6 , 7 )] rdd = sc . parallelize ( data ) schema = StructType ([ StructField ( \"col1\" , IntegerType ()), StructField ( \"col2\" , IntegerType ()), StructField ( \"col3\" , IntegerType ())]) dataDF = snappy . createDataFrame ( rdd , schema ) # create a column table snappy . dropTable ( \"COLUMN_TABLE\" , True ) #\"column\" is the table format (that is row or column) #dataDF.schema provides the schema for table snappy . createTable ( \"COLUMN_TABLE\" , \"column\" , dataDF . schema , True , buckets = \"16\" ) #append dataDF into the table dataDF . write . insertInto ( \"COLUMN_TABLE\" ) results1 = snappy . sql ( \"SELECT * FROM COLUMN_TABLE\" ) print ( \"contents of column table are:\" ) results1 . select ( \"col1\" , \"col2\" , \"col3\" ) . show () The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to the documentation for row and column tables . Create Row Tables using API, Update the Contents of Row Table \u00b6 // create a row format table called ROW_TABLE snappy . dropTable ( \"ROW_TABLE\" , ifExists = true ) // \"row\" is the table format // dataDF.schema provides the schema for table val props2 = Map . empty [ String , String ] snappy . createTable ( \"ROW_TABLE\" , \"row\" , dataDF . schema , props2 ) // append dataDF into the data dataDF . write . insertInto ( \"ROW_TABLE\" ) val results2 = snappy . sql ( \"select * from ROW_TABLE\" ) println ( \"contents of row table are:\" ) results2 . foreach ( println ) // row tables can be mutated // for example update \"ROW_TABLE\" and set col3 to 99 where // criteria \"col3 = 3\" is true using update API snappy . update ( \"ROW_TABLE\" , \"COL3 = 3\" , org . apache . spark . sql . Row ( 99 ), \"COL3\" ) val results3 = snappy . sql ( \"SELECT * FROM ROW_TABLE\" ) println ( \"contents of row table are after setting col3 = 99 are:\" ) results3 . foreach ( println ) // update rows using sql update statement snappy . sql ( \"UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\" ) val results4 = snappy . sql ( \"SELECT * FROM ROW_TABLE\" ) println ( \"contents of row table are after setting col1 = 100 are:\" ) results4 . foreach ( println ) SnappyStreamingContext Usage \u00b6 SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL. Scala import org . apache . spark . sql . _ import org . apache . spark . streaming . _ import scala . collection . mutable import org . apache . spark . rdd . _ import org . apache . spark . sql . types . _ import scala . collection . immutable . Map val snsc = new SnappyStreamingContext ( spark . sparkContext , Duration ( 1 )) val schema = StructType ( List ( StructField ( \"id\" , IntegerType ) , StructField ( \"text\" , StringType ))) case class ShowCaseSchemaStream ( loc : Int , text : String ) snsc . snappyContext . dropTable ( \"streamingExample\" , ifExists = true ) snsc . snappyContext . createTable ( \"streamingExample\" , \"column\" , schema , Map . empty [ String , String ] , false ) def rddList ( start : Int , end : Int ) = sc . parallelize ( start to end ). map ( i => ShowCaseSchemaStream ( i , s\"Text $ i \" )) val dstream = snsc . queueStream [ ShowCaseSchemaStream ]( mutable . Queue ( rddList ( 1 , 10 ), rddList ( 10 , 20 ), rddList ( 20 , 30 ))) val schemaDStream = snsc . createSchemaDStream ( dstream ) schemaDStream . foreachDataFrame ( df => { df . write . format ( \"column\" ). mode ( SaveMode . Append ). options ( Map . empty [ String , String ]). saveAsTable ( \"streamingExample\" ) }) snsc . start () snsc . sql ( \"select count(*) from streamingExample\" ). show Java StructType schema = new StructType ( new StructField [] { new StructField ( \"id\" , DataTypes . IntegerType , false , Metadata . empty ()), new StructField ( \"text\" , DataTypes . StringType , false , Metadata . empty ()) }); Map < String , String > props = Collections . emptyMap (); jsnsc . snappySession (). dropTable ( \"streamingExample\" , true ); jsnsc . snappySession (). createTable ( \"streamingExample\" , \"column\" , schema , props , false ); Queue < JavaRDD < ShowCaseSchemaStream >> rddQueue = new LinkedList <> (); // Define a JavaBean named ShowCaseSchemaStream rddQueue . add ( rddList ( jsc , 1 , 10 )); rddQueue . add ( rddList ( jsc , 10 , 20 )); rddQueue . add ( rddList ( jsc , 20 , 30 )); //rddList methods is defined as /* private static JavaRDD<ShowCaseSchemaStream> rddList(JavaSparkContext jsc, int start, int end){ List<ShowCaseSchemaStream> objs = new ArrayList<>(); for(int i= start; i<=end; i++){ objs.add(new ShowCaseSchemaStream(i, String.format(\"Text %d\",i))); } return jsc.parallelize(objs); }*/ JavaDStream < ShowCaseSchemaStream > dStream = jsnsc . queueStream ( rddQueue ); SchemaDStream schemaDStream = jsnsc . createSchemaDStream ( dStream , ShowCaseSchemaStream . class ); schemaDStream . foreachDataFrame ( new VoidFunction < Dataset < Row >> () { @Override public void call ( Dataset < Row > df ) { df . write (). insertInto ( \"streamingExample\" ); } }); jsnsc . start (); jsnsc . sql ( \"select count(*) from streamingExample\" ). show (); Python from pyspark.streaming.snappy.context import SnappyStreamingContext from pyspark.sql.types import * def rddList ( start , end ): return sc . parallelize ( range ( start , end )) . map ( lambda i : ( i , \"Text\" + str ( i ))) def saveFunction ( df ): df . write . format ( \"column\" ) . mode ( \"append\" ) . saveAsTable ( \"streamingExample\" ) schema = StructType ([ StructField ( \"loc\" , IntegerType ()), StructField ( \"text\" , StringType ())]) snsc = SnappyStreamingContext ( sc , 1 ) dstream = snsc . queueStream ([ rddList ( 1 , 10 ) , rddList ( 10 , 20 ), rddList ( 20 , 30 )]) snsc . _snappycontext . dropTable ( \"streamingExample\" , True ) snsc . _snappycontext . createTable ( \"streamingExample\" , \"column\" , schema ) schemadstream = snsc . createSchemaDStream ( dstream , schema ) schemadstream . foreachDataFrame ( lambda df : saveFunction ( df )) snsc . start () time . sleep ( 1 ) snsc . sql ( \"select count(*) from streamingExample\" ) . show ()","title":"Building SnappyData Applications using Spark API"},{"location":"programming_guide/building_snappydata_applications_using_spark_api/#building-snappydata-applications-using-spark-api","text":"","title":"Building SnappyData Applications using Spark API"},{"location":"programming_guide/building_snappydata_applications_using_spark_api/#snappysession-usage","text":"","title":"SnappySession Usage"},{"location":"programming_guide/building_snappydata_applications_using_spark_api/#create-columnar-tables-using-api","text":"Other than create and drop table, rest are all based on the Spark SQL Data Source APIs. Scala val props = Map ( \"BUCKETS\" -> \"8\" ) // Number of partitions to use in the SnappyStore case class Data ( COL1 : Int , COL2 : Int , COL3 : Int ) val data = Seq ( Seq ( 1 , 2 , 3 ), Seq ( 7 , 8 , 9 ), Seq ( 9 , 2 , 3 ), Seq ( 4 , 2 , 3 ), Seq ( 5 , 6 , 7 )) val rdd = spark . sparkContext . parallelize ( data , data . length ). map ( s => new Data ( s ( 0 ), s ( 1 ), s ( 2 ))) val df = snappy . createDataFrame ( rdd ) // create a column table snappy . dropTable ( \"COLUMN_TABLE\" , ifExists = true ) // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snappy . createTable ( \"COLUMN_TABLE\" , \"column\" , df . schema , props ) // append dataDF into the table df . write . insertInto ( \"COLUMN_TABLE\" ) val results = snappy . sql ( \"SELECT * FROM COLUMN_TABLE\" ) println ( \"contents of column table are:\" ) results . foreach ( r => println ( r )) Java Map < String , String > props1 = new HashMap <> (); props1 . put ( \"buckets\" , \"16\" ); JavaRDD < Row > jrdd = jsc . parallelize ( Arrays . asList ( RowFactory . create ( 1 , 2 , 3 ), RowFactory . create ( 7 , 8 , 9 ), RowFactory . create ( 9 , 2 , 3 ), RowFactory . create ( 4 , 2 , 3 ), RowFactory . create ( 5 , 6 , 7 ) )); StructType schema = new StructType ( new StructField [] { new StructField ( \"col1\" , DataTypes . IntegerType , false , Metadata . empty ()), new StructField ( \"col2\" , DataTypes . IntegerType , false , Metadata . empty ()), new StructField ( \"col3\" , DataTypes . IntegerType , false , Metadata . empty ()), }); Dataset < Row > df = snappy . createDataFrame ( jrdd , schema ); // create a column table snappy . dropTable ( \"COLUMN_TABLE\" , true ); // \"column\" is the table format (that is row or column) // dataDF.schema provides the schema for table snappy . createTable ( \"COLUMN_TABLE\" , \"column\" , df . schema (), props1 , false ); // append dataDF into the table df . write (). insertInto ( \"COLUMN_TABLE\" ); Dataset < Row > results = snappy . sql ( \"SELECT * FROM COLUMN_TABLE\" ); System . out . println ( \"contents of column table are:\" ); for ( Row r : results . select ( \"col1\" , \"col2\" , \"col3\" ). collectAsList ()) { System . out . println ( r ); } Python from pyspark.sql.types import * data = [( 1 , 2 , 3 ),( 7 , 8 , 9 ),( 9 , 2 , 3 ),( 4 , 2 , 3 ),( 5 , 6 , 7 )] rdd = sc . parallelize ( data ) schema = StructType ([ StructField ( \"col1\" , IntegerType ()), StructField ( \"col2\" , IntegerType ()), StructField ( \"col3\" , IntegerType ())]) dataDF = snappy . createDataFrame ( rdd , schema ) # create a column table snappy . dropTable ( \"COLUMN_TABLE\" , True ) #\"column\" is the table format (that is row or column) #dataDF.schema provides the schema for table snappy . createTable ( \"COLUMN_TABLE\" , \"column\" , dataDF . schema , True , buckets = \"16\" ) #append dataDF into the table dataDF . write . insertInto ( \"COLUMN_TABLE\" ) results1 = snappy . sql ( \"SELECT * FROM COLUMN_TABLE\" ) print ( \"contents of column table are:\" ) results1 . select ( \"col1\" , \"col2\" , \"col3\" ) . show () The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to the documentation for row and column tables .","title":"Create Columnar Tables using API"},{"location":"programming_guide/building_snappydata_applications_using_spark_api/#create-row-tables-using-api-update-the-contents-of-row-table","text":"// create a row format table called ROW_TABLE snappy . dropTable ( \"ROW_TABLE\" , ifExists = true ) // \"row\" is the table format // dataDF.schema provides the schema for table val props2 = Map . empty [ String , String ] snappy . createTable ( \"ROW_TABLE\" , \"row\" , dataDF . schema , props2 ) // append dataDF into the data dataDF . write . insertInto ( \"ROW_TABLE\" ) val results2 = snappy . sql ( \"select * from ROW_TABLE\" ) println ( \"contents of row table are:\" ) results2 . foreach ( println ) // row tables can be mutated // for example update \"ROW_TABLE\" and set col3 to 99 where // criteria \"col3 = 3\" is true using update API snappy . update ( \"ROW_TABLE\" , \"COL3 = 3\" , org . apache . spark . sql . Row ( 99 ), \"COL3\" ) val results3 = snappy . sql ( \"SELECT * FROM ROW_TABLE\" ) println ( \"contents of row table are after setting col3 = 99 are:\" ) results3 . foreach ( println ) // update rows using sql update statement snappy . sql ( \"UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\" ) val results4 = snappy . sql ( \"SELECT * FROM ROW_TABLE\" ) println ( \"contents of row table are after setting col1 = 100 are:\" ) results4 . foreach ( println )","title":"Create Row Tables using API, Update the Contents of Row Table"},{"location":"programming_guide/building_snappydata_applications_using_spark_api/#snappystreamingcontext-usage","text":"SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL. Scala import org . apache . spark . sql . _ import org . apache . spark . streaming . _ import scala . collection . mutable import org . apache . spark . rdd . _ import org . apache . spark . sql . types . _ import scala . collection . immutable . Map val snsc = new SnappyStreamingContext ( spark . sparkContext , Duration ( 1 )) val schema = StructType ( List ( StructField ( \"id\" , IntegerType ) , StructField ( \"text\" , StringType ))) case class ShowCaseSchemaStream ( loc : Int , text : String ) snsc . snappyContext . dropTable ( \"streamingExample\" , ifExists = true ) snsc . snappyContext . createTable ( \"streamingExample\" , \"column\" , schema , Map . empty [ String , String ] , false ) def rddList ( start : Int , end : Int ) = sc . parallelize ( start to end ). map ( i => ShowCaseSchemaStream ( i , s\"Text $ i \" )) val dstream = snsc . queueStream [ ShowCaseSchemaStream ]( mutable . Queue ( rddList ( 1 , 10 ), rddList ( 10 , 20 ), rddList ( 20 , 30 ))) val schemaDStream = snsc . createSchemaDStream ( dstream ) schemaDStream . foreachDataFrame ( df => { df . write . format ( \"column\" ). mode ( SaveMode . Append ). options ( Map . empty [ String , String ]). saveAsTable ( \"streamingExample\" ) }) snsc . start () snsc . sql ( \"select count(*) from streamingExample\" ). show Java StructType schema = new StructType ( new StructField [] { new StructField ( \"id\" , DataTypes . IntegerType , false , Metadata . empty ()), new StructField ( \"text\" , DataTypes . StringType , false , Metadata . empty ()) }); Map < String , String > props = Collections . emptyMap (); jsnsc . snappySession (). dropTable ( \"streamingExample\" , true ); jsnsc . snappySession (). createTable ( \"streamingExample\" , \"column\" , schema , props , false ); Queue < JavaRDD < ShowCaseSchemaStream >> rddQueue = new LinkedList <> (); // Define a JavaBean named ShowCaseSchemaStream rddQueue . add ( rddList ( jsc , 1 , 10 )); rddQueue . add ( rddList ( jsc , 10 , 20 )); rddQueue . add ( rddList ( jsc , 20 , 30 )); //rddList methods is defined as /* private static JavaRDD<ShowCaseSchemaStream> rddList(JavaSparkContext jsc, int start, int end){ List<ShowCaseSchemaStream> objs = new ArrayList<>(); for(int i= start; i<=end; i++){ objs.add(new ShowCaseSchemaStream(i, String.format(\"Text %d\",i))); } return jsc.parallelize(objs); }*/ JavaDStream < ShowCaseSchemaStream > dStream = jsnsc . queueStream ( rddQueue ); SchemaDStream schemaDStream = jsnsc . createSchemaDStream ( dStream , ShowCaseSchemaStream . class ); schemaDStream . foreachDataFrame ( new VoidFunction < Dataset < Row >> () { @Override public void call ( Dataset < Row > df ) { df . write (). insertInto ( \"streamingExample\" ); } }); jsnsc . start (); jsnsc . sql ( \"select count(*) from streamingExample\" ). show (); Python from pyspark.streaming.snappy.context import SnappyStreamingContext from pyspark.sql.types import * def rddList ( start , end ): return sc . parallelize ( range ( start , end )) . map ( lambda i : ( i , \"Text\" + str ( i ))) def saveFunction ( df ): df . write . format ( \"column\" ) . mode ( \"append\" ) . saveAsTable ( \"streamingExample\" ) schema = StructType ([ StructField ( \"loc\" , IntegerType ()), StructField ( \"text\" , StringType ())]) snsc = SnappyStreamingContext ( sc , 1 ) dstream = snsc . queueStream ([ rddList ( 1 , 10 ) , rddList ( 10 , 20 ), rddList ( 20 , 30 )]) snsc . _snappycontext . dropTable ( \"streamingExample\" , True ) snsc . _snappycontext . createTable ( \"streamingExample\" , \"column\" , schema ) schemadstream = snsc . createSchemaDStream ( dstream , schema ) schemadstream . foreachDataFrame ( lambda df : saveFunction ( df )) snsc . start () time . sleep ( 1 ) snsc . sql ( \"select count(*) from streamingExample\" ) . show ()","title":"SnappyStreamingContext Usage"},{"location":"programming_guide/managing_jar_files/","text":"Managing JAR Files \u00b6 SnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster. Installing a JAR \u00b6 Related jobs may require some common libraries. These libraries can be made available to jobs by installing them. Use the SQLJ.INSTALL_JAR procedure to install a JAR file as mentioned below: Syntax: SQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER) JAR_FILE_PATH is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name. DEPLOY: This argument is currently ignored. Example: snappy> call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0); Replacing a JAR \u00b6 Use SQLJ.REPLACE_JAR procedure to replace an installed JAR file Syntax: SQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672)) * JAR_FILE_PATH is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using the locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers. QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name. Example: CALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs') Removing a JAR \u00b6 Use SQLJ.REMOVE_JAR procedure to remove a JAR file Syntax: SQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER) * QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name. UNDEPLOY: This argument is currently ignored. Example: CALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)","title":"Managing JAR Files"},{"location":"programming_guide/managing_jar_files/#managing-jar-files","text":"SnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster.","title":"Managing JAR Files"},{"location":"programming_guide/managing_jar_files/#installing-a-jar","text":"Related jobs may require some common libraries. These libraries can be made available to jobs by installing them. Use the SQLJ.INSTALL_JAR procedure to install a JAR file as mentioned below: Syntax: SQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER) JAR_FILE_PATH is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name. DEPLOY: This argument is currently ignored. Example: snappy> call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0);","title":"Installing a JAR"},{"location":"programming_guide/managing_jar_files/#replacing-a-jar","text":"Use SQLJ.REPLACE_JAR procedure to replace an installed JAR file Syntax: SQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672)) * JAR_FILE_PATH is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using the locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers. QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name. Example: CALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs')","title":"Replacing a JAR"},{"location":"programming_guide/managing_jar_files/#removing-a-jar","text":"Use SQLJ.REMOVE_JAR procedure to remove a JAR file Syntax: SQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER) * QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name. UNDEPLOY: This argument is currently ignored. Example: CALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)","title":"Removing a JAR"},{"location":"programming_guide/multiple_language_binding_using_thrift_protocol/","text":"Multiple Language Binding using Thrift Protocol \u00b6 SnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData. Thrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the Apache Thrift documentation . The JDBC driver for SnappyData that uses the jdbc:snappydata:// URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore jdbc:gemfirexd:// continues to use the deprecated DRDA protocol. Likewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode ( snappy-start-all.sh rowstore ) the DRDA servers are started as before. To explicitly start a DRDA server in SnappyData, you can use the -drda-server-address and -drda-server-port options for the bind address and port respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the -thrift-server-address and -thrift-server-port options. Refer to the following documents for information on support provided by SnappyData: About SnappyData Thrift : Contains detailed information about the feature and its capabilities. The Thrift Interface Definition Language (IDL) : This is a Thrift interface definition file for the SnappyData service. Example : Example of the Thrift definitions using the SnappyData Thrift IDL.","title":"Multiple Language Binding using Thrift Protocol"},{"location":"programming_guide/multiple_language_binding_using_thrift_protocol/#multiple-language-binding-using-thrift-protocol","text":"SnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData. Thrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the Apache Thrift documentation . The JDBC driver for SnappyData that uses the jdbc:snappydata:// URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore jdbc:gemfirexd:// continues to use the deprecated DRDA protocol. Likewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode ( snappy-start-all.sh rowstore ) the DRDA servers are started as before. To explicitly start a DRDA server in SnappyData, you can use the -drda-server-address and -drda-server-port options for the bind address and port respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the -thrift-server-address and -thrift-server-port options. Refer to the following documents for information on support provided by SnappyData: About SnappyData Thrift : Contains detailed information about the feature and its capabilities. The Thrift Interface Definition Language (IDL) : This is a Thrift interface definition file for the SnappyData service. Example : Example of the Thrift definitions using the SnappyData Thrift IDL.","title":"Multiple Language Binding using Thrift Protocol"},{"location":"programming_guide/scala_interpreter/","text":"Executing Spark Scala Code using SQL \u00b6 Prior to the 1.2.0 release, any execution of a Spark scala program required the user to compile his Spark program, comply to specific callback API required by SnappyData, package the classes into a JAR, and then submit the application using snappy-job tool. While, this procedure may still be the right option for a production application, it is quite cumbersome for the developer or data scientist wanting to quickly run some Spark code within the SnappyData store cluster and iterate. With the introduction of the exec scala SQL command , you can now get a JDBC or ODBC connection to the cluster and submit ad-hoc scala code for execution. The JDBC connection provides an ongoing session with the cluster so applications can maintain state and use/return this across multiple invocations. Beyond this developer productivity appeal, this feature also allows you to skip using the Smart Connector in several cases. Bespoke Spark applications using the smart connector are required to launch a client Spark application with its own executors and the store cluster is mostly providing parallel access to managed data. As the client is running queries this is additional capacity that you must budget. Moreover, the overall architecture is tough to understand. With exec scala , any Spark application can submit scala code as an SQL command now. Here is one use case that motivated us : TIBCO data scientists using Team Studio (TIBCO Data Science platform) can now build custom operators, target SnappyData for in-database computations, and run adhoc Spark Scala code. For instance, ETL feature engineering using Spark ML or running a training job in-memory and with parallelism. Following are some examples to demonstrate the usage through some examples: Example 1: \u00b6 // Note this is an SQL command... this is the text you will send using a JDBC or ODBC connection val prepDataCommand = \"\"\" Exec scala val dataDF = snapp.read.option(\"header\", \"true\").csv(\"../path/to/customers.csv\") // Variable 'snapp' is injected to your program automatically. Refers to a SnappySession instance. val newDF = dataDF.withColumn(\"promotion\", \"$20\") newDF.createOrReplaceTempView(\"customers\") //OR, store in in-memory table newDF.write.format(\"column\").saveAsTable(\"customers\") \"\"\" // Acquire JDBC connection and execute Spark program Class.forName(\"io.snappydata.jdbc.ClientDriver\") val conn = DriverManager.getConnection(\"jdbc:snappydata://localhost:1527/\") conn.createStatement().executeSQL(prepDataCommand) Example 2: \u00b6 // Return back some Dataframe ... you use keyword returnDF to indicate the variable name in scala to return val getDataFromCDB = \"\"\" exec scala options(returnDF 'someDF') val someDF = snapp.table(\"customers\").filter(..... ) \"\"\" ResultSet rs = conn.createStatement().executeSQL(prepDataCommand) //Use JDBC ResultSet API to fetch result. Data types will be mapped automatically Syntax \u00b6 exec scala [options (returnDF \u2018dfName\u2019)] `<Scala_code>`; exec and scala are the keywords to identify this SQL type. options is an optional part of the syntax. If it is present, then after the keyword options , you can specify the allowed options inside parentheses. Currently, only one optional parameter, that is returnDF , can be specified with the execution. For this option, you can provide the name of any actual symbol in the Scala code, which is of type DataFrame. Through the returnDF option, you can request the system to return the result of the specific dataframe, which got created as the result of the Scala code execution. By default, the exec scala just returns the output of each interpreted line, which the interpreter prints on the Console after executing each line. How does it work? \u00b6 All the code from exec scala is executed using the Scala REPL on the SnappyData Lead node. When Spark Dataframes are invoked this would automatically result in workload distribution across all the SnappyData servers. The Lead node manages a pool of REPL based interpreters. The user SQL activity is delegated to one of the interpreters from this pool. The pool is lazily created. Any connection (JDBC or ODBC) results in the creation of a SnappySession within the CDB cluster. Moreover, the session remains associated with the connection until it is closed or dereferenced. The first time exec scala is executed, an interpreter from the pool gets associated with the connection. This allows the user to manage any adhoc private state on the server side. For example, any variables, objects, or even classes created will be isolated from other users. The functioning of the interpreter is same as that of the interactive Spark-shell only with one difference. As commands are interpreted any output generated will be cached in a buffer. And, when the command is done, the cached output will be available in the client side ResultSet object. Securing the Usage of exec scala SQL \u00b6 The ability to run Scala code directly on a running cluster can be dangerous. This is because there are no checks on what code you can run. The submitted Scala code is executed on the lead node and has the potential to bring it down. It becomes essential to secure the use of this functionality. By default, in a secure cluster, only the database owner is allowed to run Scala code through exec scala SQL or even through snappy-scala shell. The database owner is the user who brings up the SnappyData cluster. If different credentials are used for different components of the SnappyData cluster, then the credentials with which the lead node is started becomes the database owner for this purpose. Ideally, every node should start with the same superuser credentials. The superuser or the database owner, in turn, can grant the privilege of executing Scala code to other users and even to entire LDAP groups. Similarly, it is only the superuser who can revoke this privilege from any user or LDAP group. You can use the following DDL for this purpose: GRANT grant privilege exec scala to <user(s) and or ldap_group(s)> Examples grant privilege exec scala to user1 revoke privilege exec scala from user1 grant privilege exec scala to user1,user2 grant privilege exec scala to LDAPGROUP:group1 revoke privilege exec scala from user2,LDAPGROUP:group1 Known Issues and Limitations \u00b6 An interpreter cannot serialize a dependent closure properly. A dependent closure is the closure that refers to some other class, function, or variable that is defined outside the closure. Thus, the following example fails with closure serialization error. exec scala def multiply(number: Int, factor: Int): Int = { number * factor } val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(multiply(_, 2)).collect() The execution of the last line fails as the closure cannot be serialized due to this issue. This is referring to the function multiply that is defined outside the closure. Similarly, even the following example fails: val x = 5 val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(_ * x).collect() This is because the closure is referring to x , which is defined outside the closure. There are no issues if the closure has no dependency on any external variables.","title":"Executing Spark Scala Code using SQL"},{"location":"programming_guide/scala_interpreter/#executing-spark-scala-code-using-sql","text":"Prior to the 1.2.0 release, any execution of a Spark scala program required the user to compile his Spark program, comply to specific callback API required by SnappyData, package the classes into a JAR, and then submit the application using snappy-job tool. While, this procedure may still be the right option for a production application, it is quite cumbersome for the developer or data scientist wanting to quickly run some Spark code within the SnappyData store cluster and iterate. With the introduction of the exec scala SQL command , you can now get a JDBC or ODBC connection to the cluster and submit ad-hoc scala code for execution. The JDBC connection provides an ongoing session with the cluster so applications can maintain state and use/return this across multiple invocations. Beyond this developer productivity appeal, this feature also allows you to skip using the Smart Connector in several cases. Bespoke Spark applications using the smart connector are required to launch a client Spark application with its own executors and the store cluster is mostly providing parallel access to managed data. As the client is running queries this is additional capacity that you must budget. Moreover, the overall architecture is tough to understand. With exec scala , any Spark application can submit scala code as an SQL command now. Here is one use case that motivated us : TIBCO data scientists using Team Studio (TIBCO Data Science platform) can now build custom operators, target SnappyData for in-database computations, and run adhoc Spark Scala code. For instance, ETL feature engineering using Spark ML or running a training job in-memory and with parallelism. Following are some examples to demonstrate the usage through some examples:","title":"Executing Spark Scala Code using SQL"},{"location":"programming_guide/scala_interpreter/#example-1","text":"// Note this is an SQL command... this is the text you will send using a JDBC or ODBC connection val prepDataCommand = \"\"\" Exec scala val dataDF = snapp.read.option(\"header\", \"true\").csv(\"../path/to/customers.csv\") // Variable 'snapp' is injected to your program automatically. Refers to a SnappySession instance. val newDF = dataDF.withColumn(\"promotion\", \"$20\") newDF.createOrReplaceTempView(\"customers\") //OR, store in in-memory table newDF.write.format(\"column\").saveAsTable(\"customers\") \"\"\" // Acquire JDBC connection and execute Spark program Class.forName(\"io.snappydata.jdbc.ClientDriver\") val conn = DriverManager.getConnection(\"jdbc:snappydata://localhost:1527/\") conn.createStatement().executeSQL(prepDataCommand)","title":"Example 1:"},{"location":"programming_guide/scala_interpreter/#example-2","text":"// Return back some Dataframe ... you use keyword returnDF to indicate the variable name in scala to return val getDataFromCDB = \"\"\" exec scala options(returnDF 'someDF') val someDF = snapp.table(\"customers\").filter(..... ) \"\"\" ResultSet rs = conn.createStatement().executeSQL(prepDataCommand) //Use JDBC ResultSet API to fetch result. Data types will be mapped automatically","title":"Example 2:"},{"location":"programming_guide/scala_interpreter/#syntax","text":"exec scala [options (returnDF \u2018dfName\u2019)] `<Scala_code>`; exec and scala are the keywords to identify this SQL type. options is an optional part of the syntax. If it is present, then after the keyword options , you can specify the allowed options inside parentheses. Currently, only one optional parameter, that is returnDF , can be specified with the execution. For this option, you can provide the name of any actual symbol in the Scala code, which is of type DataFrame. Through the returnDF option, you can request the system to return the result of the specific dataframe, which got created as the result of the Scala code execution. By default, the exec scala just returns the output of each interpreted line, which the interpreter prints on the Console after executing each line.","title":"Syntax"},{"location":"programming_guide/scala_interpreter/#how-does-it-work","text":"All the code from exec scala is executed using the Scala REPL on the SnappyData Lead node. When Spark Dataframes are invoked this would automatically result in workload distribution across all the SnappyData servers. The Lead node manages a pool of REPL based interpreters. The user SQL activity is delegated to one of the interpreters from this pool. The pool is lazily created. Any connection (JDBC or ODBC) results in the creation of a SnappySession within the CDB cluster. Moreover, the session remains associated with the connection until it is closed or dereferenced. The first time exec scala is executed, an interpreter from the pool gets associated with the connection. This allows the user to manage any adhoc private state on the server side. For example, any variables, objects, or even classes created will be isolated from other users. The functioning of the interpreter is same as that of the interactive Spark-shell only with one difference. As commands are interpreted any output generated will be cached in a buffer. And, when the command is done, the cached output will be available in the client side ResultSet object.","title":"How does it work?"},{"location":"programming_guide/scala_interpreter/#securing-the-usage-of-exec-scala-sql","text":"The ability to run Scala code directly on a running cluster can be dangerous. This is because there are no checks on what code you can run. The submitted Scala code is executed on the lead node and has the potential to bring it down. It becomes essential to secure the use of this functionality. By default, in a secure cluster, only the database owner is allowed to run Scala code through exec scala SQL or even through snappy-scala shell. The database owner is the user who brings up the SnappyData cluster. If different credentials are used for different components of the SnappyData cluster, then the credentials with which the lead node is started becomes the database owner for this purpose. Ideally, every node should start with the same superuser credentials. The superuser or the database owner, in turn, can grant the privilege of executing Scala code to other users and even to entire LDAP groups. Similarly, it is only the superuser who can revoke this privilege from any user or LDAP group. You can use the following DDL for this purpose: GRANT grant privilege exec scala to <user(s) and or ldap_group(s)> Examples grant privilege exec scala to user1 revoke privilege exec scala from user1 grant privilege exec scala to user1,user2 grant privilege exec scala to LDAPGROUP:group1 revoke privilege exec scala from user2,LDAPGROUP:group1","title":"Securing the Usage of  exec scala SQL"},{"location":"programming_guide/scala_interpreter/#known-issues-and-limitations","text":"An interpreter cannot serialize a dependent closure properly. A dependent closure is the closure that refers to some other class, function, or variable that is defined outside the closure. Thus, the following example fails with closure serialization error. exec scala def multiply(number: Int, factor: Int): Int = { number * factor } val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(multiply(_, 2)).collect() The execution of the last line fails as the closure cannot be serialized due to this issue. This is referring to the function multiply that is defined outside the closure. Similarly, even the following example fails: val x = 5 val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(_ * x).collect() This is because the closure is referring to x , which is defined outside the closure. There are no issues if the closure has no dependency on any external variables.","title":"Known Issues and Limitations"},{"location":"programming_guide/snappydata_jobs/","text":"SnappyData Jobs \u00b6 To create a job that can be submitted through the job server, the job must implement the SnappySQLJob or SnappyStreamingJob trait. The structure of a job looks as below: Scala object SnappySampleJob extends SnappySQLJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ override def runSnappyJob(snSession: SnappySession, jobConfig: Config): Any = { } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ override def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation = SnappyJobValid() } Java class SnappySampleJob extends JavaSnappySQLJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation} /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate} } Scala object SnappyStreamingSampleJob extends SnappyStreamingJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ override def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any = { } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ override def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation = SnappyJobValid() } Java class SnappyStreamingSampleJob extends JavaSnappyStreamingJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig) {//validate} } Note The Job traits are simply extensions of the SparkJob implemented by Spark JobServer . runSnappyJob contains the implementation of the Job. The SnappySession / SnappyStreamingContext is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from configuration management that comes with the creation of a Spark job and allows the Job Server to manage and reuse contexts. isValidJob allows for an initial validation of the context and any provided configuration. If the context and configuration can run the job, returning spark.jobserver.SnappyJobValid allows the job to execute, otherwise returning spark.jobserver.SnappyJobInvalid<reason> prevents the job from running and provides means to convey the reason for failure. In this case, the call immediately returns an \"HTTP/1.1 400 Bad Request\" status code. Validate helps you prevent running jobs that eventually fail due to a missing or wrong configuration, and saves both time and resources. See examples for Spark and Spark streaming jobs. SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SnappySession per job. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs. Submitting Jobs \u00b6 The following command submits CreateAndLoadAirlineDataJob . This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its runJob method. Note When submitting concurrent jobs user must ensure that the --app-name parameter is different for each concurrent job. If two applications with the same name are submitted concurrently, the job fails and an error is reported, as the job server maintains a map of the application names and jar files used for that application. The program must be compiled and bundled as a jar file and submitted to jobs server as shown below: $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.CreateAndLoadAirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The utility snappy-job.sh submits the job and returns a JSON that has a Job Id of this job. --lead : Specifies the host name of the lead node along with the port on which it accepts jobs (8090) --app-name : Specifies the name given to the submitted application --class : Specifies the name of the class that contains implementation of the Spark job to be run --app-jar : Specifies the jar file that packages the code for Spark job --packages : Specifies the packages names, which must be comma separated. These package names can be used to inform Spark about all the dependencies of a job. For more details, refer to Deploying Dependency Jars . The status returned by the utility is displayed below: { \"status\": \"STARTED\", \"result\": { \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\", \"context\": \"snappyContext1452598154529305363\" } } This Job ID can be used to query the status of the running job. $ ./bin/snappy-job.sh status \\ --lead localhost:8090 \\ --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 { \"duration\": \"17.53 secs\", \"classPath\": \"io.snappydata.examples.CreateAndLoadAirlineDataJob\", \"startTime\": \"2016-01-12T16:59:14.746+05:30\", \"context\": \"snappyContext1452598154529305363\", \"result\": \"See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\", \"status\": \"FINISHED\", \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\" } Once the tables are created, they can be queried by running another job. Please refer to AirlineDataJob for implementing the job. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.AirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results. Jar Dependencies for Jobs \u00b6 For writing jobs users need to include Maven/SBT dependencies for the latest released version of SnappyData to their project dependencies. In case the project already includes dependency on Apache Spark and the user does not want to include snappy-spark dependencies, then, it is possible to explicitly exclude the snappy-spark dependencies. For example, gradle can be configured as: compile('io.snappydata:snappydata-cluster_2.11:1.3.1') { exclude(group: 'io.snappydata', module: 'snappy-spark-unsafe_2.11') exclude(group: 'io.snappydata', module: 'snappy-spark-core_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-yarn_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-hive-thriftserver_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-streaming-kafka-0.10_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-repl_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-sql_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-mllib_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-streaming_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-catalyst_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-hive_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-graphx_2.11') } Running Python Applications \u00b6 Python users can submit a Python application using ./bin/spark-submit in the SnappyData Connector mode. Run the following command to submit a Python application: ./bin/spark-submit \\ --master local[*] \\ --conf snappydata.connection=localhost:1527 \\ --conf spark.ui.port=4042 ./quickstart/python/CreateTable.py snappydata.connection property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster. Note For running ML/MLlib applications you need to install appropriate python packages(if your application uses any). KMeans uses numpy hence you need to install numpy package before using Spark KMeans. For example sudo apt-get install python-numpy Streaming Jobs \u00b6 An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying --stream as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. Alternatively, you can specify the name of an existing/pre-created streaming context as --context <context-name> with the submit command. For example, TwitterPopularTagsJob can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.TwitterPopularTagsJob \\ --conf streaming.batch_interval=5000 \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\ --stream { \"status\": \"STARTED\", \"result\": { \"jobId\": \"982ac142-3550-41e1-aace-6987cb39fec8\", \"context\": \"snappyStreamingContext1463987084945028747\" } } To start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context. $ ./bin/snappy-job.sh stop \\ --lead localhost:8090 \\ --job-id 982ac142-3550-41e1-aace-6987cb39fec8 $ ./bin/snappy-job.sh listcontexts \\ --lead localhost:8090 [\"snappyContext1452598154529305363\", \"snappyStreamingContext1463987084945028747\", \"snappyStreamingContext\"] $ ./bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747 \\ --lead localhost:8090 You can import the examples into a separate independent gradle project as is and submit the jobs to the cluster. Refer to the instructions here . The link also contains instructions for importing and running examples from an IDE such as Intellij IDEA. Related Topic : How to use Python to Create Tables and Run Queries","title":"Snappy Jobs"},{"location":"programming_guide/snappydata_jobs/#snappydata-jobs","text":"To create a job that can be submitted through the job server, the job must implement the SnappySQLJob or SnappyStreamingJob trait. The structure of a job looks as below: Scala object SnappySampleJob extends SnappySQLJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ override def runSnappyJob(snSession: SnappySession, jobConfig: Config): Any = { } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ override def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation = SnappyJobValid() } Java class SnappySampleJob extends JavaSnappySQLJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation} /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate} } Scala object SnappyStreamingSampleJob extends SnappyStreamingJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ override def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any = { } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ override def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation = SnappyJobValid() } Java class SnappyStreamingSampleJob extends JavaSnappyStreamingJob { /** SnappyData uses this as an entry point to execute SnappyData jobs. **/ public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation } /** SnappyData calls this function to validate the job input and reject invalid job requests **/ public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig) {//validate} } Note The Job traits are simply extensions of the SparkJob implemented by Spark JobServer . runSnappyJob contains the implementation of the Job. The SnappySession / SnappyStreamingContext is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from configuration management that comes with the creation of a Spark job and allows the Job Server to manage and reuse contexts. isValidJob allows for an initial validation of the context and any provided configuration. If the context and configuration can run the job, returning spark.jobserver.SnappyJobValid allows the job to execute, otherwise returning spark.jobserver.SnappyJobInvalid<reason> prevents the job from running and provides means to convey the reason for failure. In this case, the call immediately returns an \"HTTP/1.1 400 Bad Request\" status code. Validate helps you prevent running jobs that eventually fail due to a missing or wrong configuration, and saves both time and resources. See examples for Spark and Spark streaming jobs. SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SnappySession per job. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs.","title":"SnappyData Jobs"},{"location":"programming_guide/snappydata_jobs/#submitting-jobs","text":"The following command submits CreateAndLoadAirlineDataJob . This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its runJob method. Note When submitting concurrent jobs user must ensure that the --app-name parameter is different for each concurrent job. If two applications with the same name are submitted concurrently, the job fails and an error is reported, as the job server maintains a map of the application names and jar files used for that application. The program must be compiled and bundled as a jar file and submitted to jobs server as shown below: $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.CreateAndLoadAirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The utility snappy-job.sh submits the job and returns a JSON that has a Job Id of this job. --lead : Specifies the host name of the lead node along with the port on which it accepts jobs (8090) --app-name : Specifies the name given to the submitted application --class : Specifies the name of the class that contains implementation of the Spark job to be run --app-jar : Specifies the jar file that packages the code for Spark job --packages : Specifies the packages names, which must be comma separated. These package names can be used to inform Spark about all the dependencies of a job. For more details, refer to Deploying Dependency Jars . The status returned by the utility is displayed below: { \"status\": \"STARTED\", \"result\": { \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\", \"context\": \"snappyContext1452598154529305363\" } } This Job ID can be used to query the status of the running job. $ ./bin/snappy-job.sh status \\ --lead localhost:8090 \\ --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 { \"duration\": \"17.53 secs\", \"classPath\": \"io.snappydata.examples.CreateAndLoadAirlineDataJob\", \"startTime\": \"2016-01-12T16:59:14.746+05:30\", \"context\": \"snappyContext1452598154529305363\", \"result\": \"See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\", \"status\": \"FINISHED\", \"jobId\": \"321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\" } Once the tables are created, they can be queried by running another job. Please refer to AirlineDataJob for implementing the job. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.AirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar The status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results.","title":"Submitting Jobs"},{"location":"programming_guide/snappydata_jobs/#jar-dependencies-for-jobs","text":"For writing jobs users need to include Maven/SBT dependencies for the latest released version of SnappyData to their project dependencies. In case the project already includes dependency on Apache Spark and the user does not want to include snappy-spark dependencies, then, it is possible to explicitly exclude the snappy-spark dependencies. For example, gradle can be configured as: compile('io.snappydata:snappydata-cluster_2.11:1.3.1') { exclude(group: 'io.snappydata', module: 'snappy-spark-unsafe_2.11') exclude(group: 'io.snappydata', module: 'snappy-spark-core_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-yarn_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-hive-thriftserver_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-streaming-kafka-0.10_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-repl_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-sql_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-mllib_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-streaming_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-catalyst_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-hive_2.11') exclude(group: 'io.snappydata',module: 'snappy-spark-graphx_2.11') }","title":"Jar Dependencies for Jobs"},{"location":"programming_guide/snappydata_jobs/#running-python-applications","text":"Python users can submit a Python application using ./bin/spark-submit in the SnappyData Connector mode. Run the following command to submit a Python application: ./bin/spark-submit \\ --master local[*] \\ --conf snappydata.connection=localhost:1527 \\ --conf spark.ui.port=4042 ./quickstart/python/CreateTable.py snappydata.connection property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster. Note For running ML/MLlib applications you need to install appropriate python packages(if your application uses any). KMeans uses numpy hence you need to install numpy package before using Spark KMeans. For example sudo apt-get install python-numpy","title":"Running Python Applications"},{"location":"programming_guide/snappydata_jobs/#streaming-jobs","text":"An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying --stream as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. Alternatively, you can specify the name of an existing/pre-created streaming context as --context <context-name> with the submit command. For example, TwitterPopularTagsJob can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets. $ ./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.TwitterPopularTagsJob \\ --conf streaming.batch_interval=5000 \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\ --stream { \"status\": \"STARTED\", \"result\": { \"jobId\": \"982ac142-3550-41e1-aace-6987cb39fec8\", \"context\": \"snappyStreamingContext1463987084945028747\" } } To start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context. $ ./bin/snappy-job.sh stop \\ --lead localhost:8090 \\ --job-id 982ac142-3550-41e1-aace-6987cb39fec8 $ ./bin/snappy-job.sh listcontexts \\ --lead localhost:8090 [\"snappyContext1452598154529305363\", \"snappyStreamingContext1463987084945028747\", \"snappyStreamingContext\"] $ ./bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747 \\ --lead localhost:8090 You can import the examples into a separate independent gradle project as is and submit the jobs to the cluster. Refer to the instructions here . The link also contains instructions for importing and running examples from an IDE such as Intellij IDEA. Related Topic : How to use Python to Create Tables and Run Queries","title":"Streaming Jobs"},{"location":"programming_guide/spark_jdbc_connector/","text":"Accessing SnappyData Tables from any Spark (2.1+) Cluster \u00b6 Spark applications can be run embedded inside the SnappyData cluster by submitting Jobs using Snappy-Job.sh or it can be run using the native Smart Connector . However, from SnappyData 1.0.2 release the connector can only be used from a Spark 2.1 compatible cluster. If you are using a Spark version or distribution that is based on a version higher than 2.1 then, you can use the SnappyData JDBC Extension Connector as described below. How can Spark Applications Connect to SnappyData using Spark JDBC? \u00b6 Spark SQL supports reading and writing to databases using a built-in JDBC data source . Applications can configure and use JDBC like any other Spark data source queries return data frames and can be efficiently processed in Spark SQL or joined with other data sources. The JDBC data source is also easy to use from Java or Python. All you need is a JDBC driver from the database vendor. Likewise, applications can use the Spark DataFrameWriter to insert, append, or replace a dataset in the database. Note The usage model for the Spark JDBC data source is described here . We strongly recommend you to go through this section in case you are not familiar with how Spark works with data sources. Pushing Entire Query into the Database \u00b6 When Spark queries are executed against external data sources, the current Spark model can only push down filters and projections in the query down to the database. If you are running an expensive aggregation on a large data set, then the entire data set is fetched into the Spark partitions, and the query is executed inside your Spark cluster. However, when you use a JDBC data source, you can pass entire queries or portions of the query entirely to the database such as shown in the following sample: val pushdownQuery = \"(select x, sum(y), max(z) from largeTable group by x order by x) t1\" ; spark . read . jdbc ( jdbcUrl , pushDownQuery , connectionProperties ); Deficiencies in the Spark JDBC Connector \u00b6 Unfortunately, there are following limitations with Spark JDBC Connector which we address in the SnappyData JDBC Extension Connector. Performance When an entire query is pushed down, Spark runs two queries: First it runs the query that is supplied to fetch the result set metadata so that it knows the structure of the data frame that is returned to the application. Secondly it runs the actual query. The SnappyData connector internally figures out the structure of the result set without having to run multiple queries. Lack of connection pooling With no built-in support for pooling connections, every time a query is executed against a JDBC database, each of the partition in Spark has to set up a new connection which can be expensive. SnappyData internally uses an efficient pooled implementation with sensible defaults. Data manipulation While the Spark DataFrameWriter API can be used to append/insert a full dataset (dataframe) into the database, it is not simple to run the ad-hoc updates on the database including mass updates. The SnappyData JDBC Extension Connector makes this much simpler. Usability With the SnappyData JDBC Extension Connector, it is easier to deal with all the connection properties. You need not impair your application with sensitive properties dispersed in your app code. Connecting to SnappyData using the JDBC Extension Connector \u00b6 Following is a sample of Spark JDBC Extension setup and usage: Include the snappydata-jdbc package in the Spark job with spark-submit or spark-shell. You can download the same from here : $SPARK_HOME /bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar Set the session properties. The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in spark-defaults.conf along with all the other Spark properties. You can also set any of these properties in your app code. Overloads of the above methods accepting user+password and host+port is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the DataFrameReader.jdbc method. Following is a sample code of configuring the properties in SparkConf : $SPARK_HOME /bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar --conf spark.snappydata.connection = localhost:1527 --conf spark.snappydata.user = <user> --conf spark.snappydata.password = <password> Import the required implicits in the job/shell code as follows: import io . snappydata . sql . implicits . _ Running Queries \u00b6 Once the required session properties are set (connection and user/password as shown above), then one can run the required queries/DMLs without any other configuration. Your application must import the SnappyData SQL implicits when using Scala while with Java, the wrapper has to be created explicitly as shown below. Query Example \u00b6 Scala Java import io . snappydata . sql . implicits . _ val spark = < create / access your spark session instance here > // query pushed down to SnappyData data cluster val dataset = spark . snappyQuery ( \"select x, sum(y), max(z) from largeTable group by x order by x\" ) import org.apache.spark.sql.* ; SparkSession spark = < create / access your spark session instance here > ; JdbcExecute exec = new JdbcExecute ( spark ); // query pushed down to SnappyData data cluster DataFrame df = exec . snappyQuery ( \"select x, sum(y), max(z) from largeTable group by x order by x\" ) Note Overloads of the above methods of accepting user+password and host+port is also provided in case those properties are not set in the session or need to be overridden. You can optionally pass additional connection properties such as in DataFrameReader.jdbc method. Updating/Writing Data in SnappyData Tables \u00b6 Your application can use the Spark DataFrameWriter API to either insert or append data. Also, for convenience, the connector provides an implicit in scala that is import io.snappydata.sql.implicits._ for the DataFrameWriter to simplify writing to SnappyData. Hence, there is no need to explicitly set the connection properties. After the required session properties are set (connection and user/password as shown above), then you can fire the required queries/DMLs without any other configuration. Inserting a dataset from the job can also use the snappy extension to avoid passing in the URL and credentials explicitly: // You can use all the Spark writer APIs when using the snappy implicit df . write . snappy ( \u201c testTable1 \u201d ) Or using explicit wrapper in Java: new JdbcWriter ( spark . write ). snappy ( \u201c testTable \u201d ) Using SQL DML to Execute Ad-hoc SQL \u00b6 You can also use the snappyExecute method (see below) to run the arbitrary SQL DML statements directly on the database. You need not acquire/manage explicit JDBC connections or set properties. Your application must import the SnappyData SQL implicits when using Scala while with Java, the wrapper has to be created explicitly as shown below. Scala Java import io . snappydata . sql . implicits . _ // execute DDL spark . snappyExecute ( \"create table testTable1 (id long, data string) using column\" ) // DML spark . snappyExecute ( \"insert into testTable1 values (1, \u2018data1\u2019)\" ) // bulk insert from external table in embedded mode spark . snappyExecute ( \"insert into testTable1 select * from externalTable1\" ) import org.apache.spark.sql.* ; JdbcExecute exec = new JdbcExecute ( spark ); // execute DDL exec . snappyExecute ( \u201c create table testTable1 ( id long , data string ) using column \u201d ); // DML exec . snappyExecute ( \"insert into testTable1 values (1, \u2018data1\u2019)\" ); // bulk insert from external table in embedded mode exec . snappyExecute ( \"insert into testTable1 select * from externalTable1\" ) Comparison with Current Spark APIs \u00b6 There is no equivalent of snappyExecute and one has to explicitly use JDBC API. For snappyQuery , if you were to use Spark\u2019s JDBC connector directly, then the equivalent code would appear as follows (assuming snappyExecute equivalent was done beforehand using JDBC API or otherwise): val jdbcUrl = \"jdbc:snappydata:pool://localhost:1527\" val connProps = new java . util . Properties () connProps . setProperty ( \"driver\" , \"io.snappydata.jdbc.ClientPoolDriver\" ) connProps . setProperty ( \"user\" , userName ) connProps . setProperty ( \"password\" , password ) val df = spark . read . jdbc ( jdbcUrl , \u201c ( select count ( * ) from testTable1 ) q \u201d , connProps ) Besides being more verbose, this suffers from the problem of double query execution (first query to fetch query result metadata, followed by the actual query). API Reference \u00b6 The following extensions are used to implement the Spark JDBC Connector: SparkSession.snappyQuery() This method creates a wrapper over SparkSession and is similar to SparkSession.sql() API . The only difference between the both is that the entire query is pushed down to the SnappyData cluster. Hence, a query cannot have any temporary or external tables/views that are not visible in the SnappyData cluster. SparkSession.snappyExecute() Similarly, with this method, the SQL (assuming it to be a DML) is pushed to SnappyData, using the JDBC API, and returns the update count. snappy An implicit for DataFrameWriter named snappy simplifies the bulk writes. Therefore, a write operation such as session.write.jdbc() becomes session.write.snappy() with the difference that JDBC URL, driver, and connection properties are auto-configured using session properties, if possible. Performance Considerations \u00b6 It should be noted that using the Spark Data Source Writer or the Snappy Implicit are both much slower compared to SnappyData embedded job or Smart Connector. If the DataFrame that is to be written is medium or large sized, then it is better to ingest directly in an embedded mode. In case writing an embedded job is not an option, the incoming DataFrame can be dumped to an external table in a location accessible to both Spark and SnappyData clusters. After this, it can be ingested in an embedded mode using snappyExecute .","title":"Accessing SnappyData Tables from any Spark (2.1+) Cluster"},{"location":"programming_guide/spark_jdbc_connector/#accessing-snappydata-tables-from-any-spark-21-cluster","text":"Spark applications can be run embedded inside the SnappyData cluster by submitting Jobs using Snappy-Job.sh or it can be run using the native Smart Connector . However, from SnappyData 1.0.2 release the connector can only be used from a Spark 2.1 compatible cluster. If you are using a Spark version or distribution that is based on a version higher than 2.1 then, you can use the SnappyData JDBC Extension Connector as described below.","title":"Accessing SnappyData Tables from any Spark (2.1+) Cluster"},{"location":"programming_guide/spark_jdbc_connector/#how-can-spark-applications-connect-to-snappydata-using-spark-jdbc","text":"Spark SQL supports reading and writing to databases using a built-in JDBC data source . Applications can configure and use JDBC like any other Spark data source queries return data frames and can be efficiently processed in Spark SQL or joined with other data sources. The JDBC data source is also easy to use from Java or Python. All you need is a JDBC driver from the database vendor. Likewise, applications can use the Spark DataFrameWriter to insert, append, or replace a dataset in the database. Note The usage model for the Spark JDBC data source is described here . We strongly recommend you to go through this section in case you are not familiar with how Spark works with data sources.","title":"How can Spark Applications Connect to SnappyData using Spark JDBC?"},{"location":"programming_guide/spark_jdbc_connector/#pushing-entire-query-into-the-database","text":"When Spark queries are executed against external data sources, the current Spark model can only push down filters and projections in the query down to the database. If you are running an expensive aggregation on a large data set, then the entire data set is fetched into the Spark partitions, and the query is executed inside your Spark cluster. However, when you use a JDBC data source, you can pass entire queries or portions of the query entirely to the database such as shown in the following sample: val pushdownQuery = \"(select x, sum(y), max(z) from largeTable group by x order by x) t1\" ; spark . read . jdbc ( jdbcUrl , pushDownQuery , connectionProperties );","title":"Pushing Entire Query into the Database"},{"location":"programming_guide/spark_jdbc_connector/#deficiencies-in-the-spark-jdbc-connector","text":"Unfortunately, there are following limitations with Spark JDBC Connector which we address in the SnappyData JDBC Extension Connector. Performance When an entire query is pushed down, Spark runs two queries: First it runs the query that is supplied to fetch the result set metadata so that it knows the structure of the data frame that is returned to the application. Secondly it runs the actual query. The SnappyData connector internally figures out the structure of the result set without having to run multiple queries. Lack of connection pooling With no built-in support for pooling connections, every time a query is executed against a JDBC database, each of the partition in Spark has to set up a new connection which can be expensive. SnappyData internally uses an efficient pooled implementation with sensible defaults. Data manipulation While the Spark DataFrameWriter API can be used to append/insert a full dataset (dataframe) into the database, it is not simple to run the ad-hoc updates on the database including mass updates. The SnappyData JDBC Extension Connector makes this much simpler. Usability With the SnappyData JDBC Extension Connector, it is easier to deal with all the connection properties. You need not impair your application with sensitive properties dispersed in your app code.","title":"Deficiencies in the Spark JDBC Connector"},{"location":"programming_guide/spark_jdbc_connector/#connecting-to-snappydata-using-the-jdbc-extension-connector","text":"Following is a sample of Spark JDBC Extension setup and usage: Include the snappydata-jdbc package in the Spark job with spark-submit or spark-shell. You can download the same from here : $SPARK_HOME /bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar Set the session properties. The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in spark-defaults.conf along with all the other Spark properties. You can also set any of these properties in your app code. Overloads of the above methods accepting user+password and host+port is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the DataFrameReader.jdbc method. Following is a sample code of configuring the properties in SparkConf : $SPARK_HOME /bin/spark-shell --jars snappydata-jdbc-2.11_1.3.1.jar --conf spark.snappydata.connection = localhost:1527 --conf spark.snappydata.user = <user> --conf spark.snappydata.password = <password> Import the required implicits in the job/shell code as follows: import io . snappydata . sql . implicits . _","title":"Connecting to SnappyData using the JDBC Extension Connector"},{"location":"programming_guide/spark_jdbc_connector/#running-queries","text":"Once the required session properties are set (connection and user/password as shown above), then one can run the required queries/DMLs without any other configuration. Your application must import the SnappyData SQL implicits when using Scala while with Java, the wrapper has to be created explicitly as shown below.","title":"Running Queries"},{"location":"programming_guide/spark_jdbc_connector/#query-example","text":"Scala Java import io . snappydata . sql . implicits . _ val spark = < create / access your spark session instance here > // query pushed down to SnappyData data cluster val dataset = spark . snappyQuery ( \"select x, sum(y), max(z) from largeTable group by x order by x\" ) import org.apache.spark.sql.* ; SparkSession spark = < create / access your spark session instance here > ; JdbcExecute exec = new JdbcExecute ( spark ); // query pushed down to SnappyData data cluster DataFrame df = exec . snappyQuery ( \"select x, sum(y), max(z) from largeTable group by x order by x\" ) Note Overloads of the above methods of accepting user+password and host+port is also provided in case those properties are not set in the session or need to be overridden. You can optionally pass additional connection properties such as in DataFrameReader.jdbc method.","title":"Query Example"},{"location":"programming_guide/spark_jdbc_connector/#updatingwriting-data-in-snappydata-tables","text":"Your application can use the Spark DataFrameWriter API to either insert or append data. Also, for convenience, the connector provides an implicit in scala that is import io.snappydata.sql.implicits._ for the DataFrameWriter to simplify writing to SnappyData. Hence, there is no need to explicitly set the connection properties. After the required session properties are set (connection and user/password as shown above), then you can fire the required queries/DMLs without any other configuration. Inserting a dataset from the job can also use the snappy extension to avoid passing in the URL and credentials explicitly: // You can use all the Spark writer APIs when using the snappy implicit df . write . snappy ( \u201c testTable1 \u201d ) Or using explicit wrapper in Java: new JdbcWriter ( spark . write ). snappy ( \u201c testTable \u201d )","title":"Updating/Writing Data in SnappyData Tables"},{"location":"programming_guide/spark_jdbc_connector/#using-sql-dml-to-execute-ad-hoc-sql","text":"You can also use the snappyExecute method (see below) to run the arbitrary SQL DML statements directly on the database. You need not acquire/manage explicit JDBC connections or set properties. Your application must import the SnappyData SQL implicits when using Scala while with Java, the wrapper has to be created explicitly as shown below. Scala Java import io . snappydata . sql . implicits . _ // execute DDL spark . snappyExecute ( \"create table testTable1 (id long, data string) using column\" ) // DML spark . snappyExecute ( \"insert into testTable1 values (1, \u2018data1\u2019)\" ) // bulk insert from external table in embedded mode spark . snappyExecute ( \"insert into testTable1 select * from externalTable1\" ) import org.apache.spark.sql.* ; JdbcExecute exec = new JdbcExecute ( spark ); // execute DDL exec . snappyExecute ( \u201c create table testTable1 ( id long , data string ) using column \u201d ); // DML exec . snappyExecute ( \"insert into testTable1 values (1, \u2018data1\u2019)\" ); // bulk insert from external table in embedded mode exec . snappyExecute ( \"insert into testTable1 select * from externalTable1\" )","title":"Using SQL DML to Execute Ad-hoc SQL"},{"location":"programming_guide/spark_jdbc_connector/#comparison-with-current-spark-apis","text":"There is no equivalent of snappyExecute and one has to explicitly use JDBC API. For snappyQuery , if you were to use Spark\u2019s JDBC connector directly, then the equivalent code would appear as follows (assuming snappyExecute equivalent was done beforehand using JDBC API or otherwise): val jdbcUrl = \"jdbc:snappydata:pool://localhost:1527\" val connProps = new java . util . Properties () connProps . setProperty ( \"driver\" , \"io.snappydata.jdbc.ClientPoolDriver\" ) connProps . setProperty ( \"user\" , userName ) connProps . setProperty ( \"password\" , password ) val df = spark . read . jdbc ( jdbcUrl , \u201c ( select count ( * ) from testTable1 ) q \u201d , connProps ) Besides being more verbose, this suffers from the problem of double query execution (first query to fetch query result metadata, followed by the actual query).","title":"Comparison with Current Spark APIs"},{"location":"programming_guide/spark_jdbc_connector/#api-reference","text":"The following extensions are used to implement the Spark JDBC Connector: SparkSession.snappyQuery() This method creates a wrapper over SparkSession and is similar to SparkSession.sql() API . The only difference between the both is that the entire query is pushed down to the SnappyData cluster. Hence, a query cannot have any temporary or external tables/views that are not visible in the SnappyData cluster. SparkSession.snappyExecute() Similarly, with this method, the SQL (assuming it to be a DML) is pushed to SnappyData, using the JDBC API, and returns the update count. snappy An implicit for DataFrameWriter named snappy simplifies the bulk writes. Therefore, a write operation such as session.write.jdbc() becomes session.write.snappy() with the difference that JDBC URL, driver, and connection properties are auto-configured using session properties, if possible.","title":"API Reference"},{"location":"programming_guide/spark_jdbc_connector/#performance-considerations","text":"It should be noted that using the Spark Data Source Writer or the Snappy Implicit are both much slower compared to SnappyData embedded job or Smart Connector. If the DataFrame that is to be written is medium or large sized, then it is better to ingest directly in an embedded mode. In case writing an embedded job is not an option, the incoming DataFrame can be dumped to an external table in a location accessible to both Spark and SnappyData clusters. After this, it can be ingested in an embedded mode using snappyExecute .","title":"Performance Considerations"},{"location":"programming_guide/sparksession_snappysession_and_snappystreamingcontext/","text":"SparkSession, SnappySession and SnappyStreamingContext \u00b6 Create a SparkSession \u00b6 Spark Context is the main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster and can be used to create RDDs, accumulators and broadcast variables on that cluster. Spark Session is the entry point to programming Spark with the Dataset and DataFrame API. SparkSession object can be created by using SparkSession.Builder used as below. SparkSession . builder () . master ( \"local\" ) . appName ( \"Word Count\" ) . config ( \"spark.some.config.option\" , \"some-value\" ) . getOrCreate () In environments where SparkSession has been created up front (e.g. REPL, notebooks), use the builder to get an existing session: SparkSession . builder (). getOrCreate () Create a SnappySession \u00b6 SnappySession is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's SparkSession to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame. To create a SnappySession: Scala Java Python val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate val snappy = new SnappySession ( spark . sparkContext ) SparkSession spark = SparkSession . builder () . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate (); JavaSparkContext jsc = new JavaSparkContext ( spark . sparkContext ()); SnappySession snappy = new SnappySession ( spark . sparkContext ()); from pyspark.sql.snappy import SnappySession from pyspark import SparkContext , SparkConf conf = SparkConf () . setAppName ( appName ) . setMaster ( master ) sc = SparkContext ( conf = conf ) snappy = SnappySession ( sc ) Note Dataframe created from Snappy session and Spark session cannot be used across interchangeably. Create a SnappyStreamingContext \u00b6 SnappyStreamingContext is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's Streaming Context . To create a SnappyStreamingContext: Scala Java Python val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate val snsc = new SnappyStreamingContext ( spark . sparkContext , Duration ( 1 )) SparkSession spark = SparkSession . builder () . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate (); JavaSparkContext jsc = new JavaSparkContext ( spark . sparkContext ()); Duration batchDuration = Milliseconds . apply ( 500 ); JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext ( jsc , batchDuration ); from pyspark.streaming.snappy.context import SnappyStreamingContext from pyspark import SparkContext , SparkConf conf = SparkConf () . setAppName ( appName ) . setMaster ( master ) sc = SparkContext ( conf = conf ) duration = .5 snsc = SnappyStreamingContext ( sc , duration ) Also, SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see Affinity modes . If you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession. If you are in the Embedded Mode, applications typically submit jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext. Jobs are the primary mechanism to interact with SnappyData using the Spark API in embedded mode. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.","title":"SparkSession, SnappySession and SnappyStreamingContext"},{"location":"programming_guide/sparksession_snappysession_and_snappystreamingcontext/#sparksession-snappysession-and-snappystreamingcontext","text":"","title":"SparkSession, SnappySession and SnappyStreamingContext"},{"location":"programming_guide/sparksession_snappysession_and_snappystreamingcontext/#create-a-sparksession","text":"Spark Context is the main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster and can be used to create RDDs, accumulators and broadcast variables on that cluster. Spark Session is the entry point to programming Spark with the Dataset and DataFrame API. SparkSession object can be created by using SparkSession.Builder used as below. SparkSession . builder () . master ( \"local\" ) . appName ( \"Word Count\" ) . config ( \"spark.some.config.option\" , \"some-value\" ) . getOrCreate () In environments where SparkSession has been created up front (e.g. REPL, notebooks), use the builder to get an existing session: SparkSession . builder (). getOrCreate ()","title":"Create a SparkSession"},{"location":"programming_guide/sparksession_snappysession_and_snappystreamingcontext/#create-a-snappysession","text":"SnappySession is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's SparkSession to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame. To create a SnappySession: Scala Java Python val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate val snappy = new SnappySession ( spark . sparkContext ) SparkSession spark = SparkSession . builder () . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate (); JavaSparkContext jsc = new JavaSparkContext ( spark . sparkContext ()); SnappySession snappy = new SnappySession ( spark . sparkContext ()); from pyspark.sql.snappy import SnappySession from pyspark import SparkContext , SparkConf conf = SparkConf () . setAppName ( appName ) . setMaster ( master ) sc = SparkContext ( conf = conf ) snappy = SnappySession ( sc ) Note Dataframe created from Snappy session and Spark session cannot be used across interchangeably.","title":"Create a SnappySession"},{"location":"programming_guide/sparksession_snappysession_and_snappystreamingcontext/#create-a-snappystreamingcontext","text":"SnappyStreamingContext is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's Streaming Context . To create a SnappyStreamingContext: Scala Java Python val spark : SparkSession = SparkSession . builder . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate val snsc = new SnappyStreamingContext ( spark . sparkContext , Duration ( 1 )) SparkSession spark = SparkSession . builder () . appName ( \"SparkApp\" ) . master ( \"master_url\" ) . getOrCreate (); JavaSparkContext jsc = new JavaSparkContext ( spark . sparkContext ()); Duration batchDuration = Milliseconds . apply ( 500 ); JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext ( jsc , batchDuration ); from pyspark.streaming.snappy.context import SnappyStreamingContext from pyspark import SparkContext , SparkConf conf = SparkConf () . setAppName ( appName ) . setMaster ( master ) sc = SparkContext ( conf = conf ) duration = .5 snsc = SnappyStreamingContext ( sc , duration ) Also, SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see Affinity modes . If you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession. If you are in the Embedded Mode, applications typically submit jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext. Jobs are the primary mechanism to interact with SnappyData using the Spark API in embedded mode. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.","title":"Create a SnappyStreamingContext"},{"location":"programming_guide/stream_processing_using_sql/","text":"Stream Processing using SQL \u00b6 SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. Here is a brief overview of Spark streaming from the Spark Streaming guide. Spark Streaming Overview \u00b6 Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map , reduce , join and window . Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's machine learning and graph processing algorithms on data streams. Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. Spark Streaming provides a high-level abstraction called discretized stream or DStream , which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs . Additional details on the Spark Streaming concepts and programming is covered here . SnappyData Streaming Extensions over Spark \u00b6 The following enhancements over Spark Streaming are provided: Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, the standard SQL is extended to be able to specify the time window for the query. OLAP optimizations : By integrating and colocating stream processing with the hybrid in-memory storage engine, the product leverages the optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with RowStore. Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays. Working with Stream Tables \u00b6 SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources. // DDL for creating a stream table CREATE STREAM TABLE [IF NOT EXISTS] table_name (COLUMN_DEFINITION) USING 'kafka_stream | file_stream | twitter_stream | socket_stream' OPTIONS ( // multiple stream source specific options storagelevel '', rowConverter '', subscribe '', kafkaParams '', consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', hostname '', port '', directory '' ) // DDL for dropping a stream table DROP TABLE [IF EXISTS] table_name // Initialize StreamingContext STREAMING INIT <batchInterval> [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE] // Start streaming STREAMING START // Stop streaming STREAMING STOP For example to create a stream table using kafka source : val spark: SparkSession = SparkSession .builder .appName(\"SparkApp\") .master(\"local[4]\") .getOrCreate val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1)) snsc.sql(\"create stream table streamTable (userId string, clickStreamLog string) \" + \"using kafka_stream options (\" + \"storagelevel 'MEMORY_AND_DISK_SER_2', \" + \"rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \" + \"kafkaParams 'zookeeper.connect->localhost:2181;auto.offset.reset->smallest;group.id->myGroupId', \" + \"subscribe 'streamTopic:01')\") // You can get a handle of underlying DStream of the table val dStream = snsc.getSchemaDStream(\"streamTable\") // You can also save the DataFrames to an external table dStream.foreachDataFrame(_.write.insertInto(tableName)) The streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries. Stream SQL through snappy-sql \u00b6 Start a SnappyData cluster and connect through snappy-sql : //create a connection snappy> connect client 'localhost:1527'; // Initialize streaming with batchInterval of 2 seconds snappy> streaming init 2secs; // Create a stream table snappy> create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter'); // Start the streaming snappy> streaming start; //Run ad-hoc queries on the streamTable on current batch of data snappy> select id, text, fullName from streamTable where text like '%snappy%' // Drop the streamTable snappy> drop table streamTable; // Stop the streaming snappy> streaming stop; SchemaDStream \u00b6 SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table. Some of these ideas (especially naming our abstractions) were borrowed from Intel's Streaming SQL project . Registering Continuous Queries \u00b6 //You can join two stream tables and produce a result stream. val resultStream = snsc.registerCQ(\"SELECT s1.id, s1.text FROM stream1 window (duration '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\") // You can also save the DataFrames to an external table dStream.foreachDataFrame(_.write.insertInto(\"yourTableName\")) Dynamic (ad-hoc) Continuous Queries \u00b6 Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the SnappyStreamingContext has started.","title":"Stream Processing using SQL"},{"location":"programming_guide/stream_processing_using_sql/#stream-processing-using-sql","text":"SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. Here is a brief overview of Spark streaming from the Spark Streaming guide.","title":"Stream Processing using SQL"},{"location":"programming_guide/stream_processing_using_sql/#spark-streaming-overview","text":"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map , reduce , join and window . Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's machine learning and graph processing algorithms on data streams. Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. Spark Streaming provides a high-level abstraction called discretized stream or DStream , which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs . Additional details on the Spark Streaming concepts and programming is covered here .","title":"Spark Streaming Overview"},{"location":"programming_guide/stream_processing_using_sql/#snappydata-streaming-extensions-over-spark","text":"The following enhancements over Spark Streaming are provided: Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, the standard SQL is extended to be able to specify the time window for the query. OLAP optimizations : By integrating and colocating stream processing with the hybrid in-memory storage engine, the product leverages the optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with RowStore. Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.","title":"SnappyData Streaming Extensions over Spark"},{"location":"programming_guide/stream_processing_using_sql/#working-with-stream-tables","text":"SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources. // DDL for creating a stream table CREATE STREAM TABLE [IF NOT EXISTS] table_name (COLUMN_DEFINITION) USING 'kafka_stream | file_stream | twitter_stream | socket_stream' OPTIONS ( // multiple stream source specific options storagelevel '', rowConverter '', subscribe '', kafkaParams '', consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', hostname '', port '', directory '' ) // DDL for dropping a stream table DROP TABLE [IF EXISTS] table_name // Initialize StreamingContext STREAMING INIT <batchInterval> [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE] // Start streaming STREAMING START // Stop streaming STREAMING STOP For example to create a stream table using kafka source : val spark: SparkSession = SparkSession .builder .appName(\"SparkApp\") .master(\"local[4]\") .getOrCreate val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1)) snsc.sql(\"create stream table streamTable (userId string, clickStreamLog string) \" + \"using kafka_stream options (\" + \"storagelevel 'MEMORY_AND_DISK_SER_2', \" + \"rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \" + \"kafkaParams 'zookeeper.connect->localhost:2181;auto.offset.reset->smallest;group.id->myGroupId', \" + \"subscribe 'streamTopic:01')\") // You can get a handle of underlying DStream of the table val dStream = snsc.getSchemaDStream(\"streamTable\") // You can also save the DataFrames to an external table dStream.foreachDataFrame(_.write.insertInto(tableName)) The streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.","title":"Working with Stream Tables"},{"location":"programming_guide/stream_processing_using_sql/#stream-sql-through-snappy-sql","text":"Start a SnappyData cluster and connect through snappy-sql : //create a connection snappy> connect client 'localhost:1527'; // Initialize streaming with batchInterval of 2 seconds snappy> streaming init 2secs; // Create a stream table snappy> create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter'); // Start the streaming snappy> streaming start; //Run ad-hoc queries on the streamTable on current batch of data snappy> select id, text, fullName from streamTable where text like '%snappy%' // Drop the streamTable snappy> drop table streamTable; // Stop the streaming snappy> streaming stop;","title":"Stream SQL through snappy-sql"},{"location":"programming_guide/stream_processing_using_sql/#schemadstream","text":"SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table. Some of these ideas (especially naming our abstractions) were borrowed from Intel's Streaming SQL project .","title":"SchemaDStream"},{"location":"programming_guide/stream_processing_using_sql/#registering-continuous-queries","text":"//You can join two stream tables and produce a result stream. val resultStream = snsc.registerCQ(\"SELECT s1.id, s1.text FROM stream1 window (duration '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\") // You can also save the DataFrames to an external table dStream.foreachDataFrame(_.write.insertInto(\"yourTableName\"))","title":"Registering Continuous Queries"},{"location":"programming_guide/stream_processing_using_sql/#dynamic-ad-hoc-continuous-queries","text":"Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the SnappyStreamingContext has started.","title":"Dynamic (ad-hoc) Continuous Queries"},{"location":"programming_guide/tables_in_snappydata/","text":"Tables in SnappyData \u00b6 Row and Column Tables \u00b6 Column tables organize and manage data in memory in a compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model. Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates. Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc. DDL and DML Syntax for Tables \u00b6 CREATE TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING [row | column] OPTIONS ( COLOCATE_WITH 'table-name', // Default none PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables. BUCKETS 'num-partitions', // Default 128. Must be an integer. REDUNDANCY 'num-of-copies' , // Must be an integer EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE 'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019, DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set. EXPIRE 'time_to_live_in_seconds', COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table. KEY_COLUMNS 'column_name,..', // Only for column table if putInto support is required COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer > 0 and < 2GB. Only for column table. ) [AS select_statement]; DROP TABLE [IF EXISTS] table_name Refer to the Best Practices section for more information on partitioning and colocating data and CREATE TABLE for information on creating a row/column table. DDL extensions are required to configure a table based on user requirements. You can also define complex types (Map, Array and StructType) as columns for column tables. snappy.sql(\"CREATE TABLE tableName ( col1 INT , col2 Array<Decimal>, col3 Map<Timestamp, Struct<x: Int, y: String, z: Decimal(10,5)>>, col6 Struct<a: Int, b: String, c: Decimal(10,5)> ) USING column options(BUCKETS '8')\" ) To access the complex data from JDBC you can see JDBCWithComplexTypes for examples. Note Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition. Spark API for Managing Tables \u00b6 Get a reference to SnappySession : val snappy: SnappySession = new SnappySession(spark.sparkContext) Create a SnappyStore table using Spark APIs val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section case class Data(col1: Int, col2: Int, col3: Int) val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sparkContext.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snappy.createDataFrame(rdd) snappy.createTable(\"column_table\", \"column\", dataDF.schema, props) //or create a row format table snappy.createTable(\"row_table\", \"row\", dataDF.schema, props) Drop a SnappyStore table using Spark APIs : snappy.dropTable(tableName, ifExists = true) Restrictions on Column Tables \u00b6 Column tables cannot specify any primary key, unique key constraints Index on column table is not supported Option EXPIRE is not applicable for column tables Option EVICTION_BY with value LRUCOUNT is supported for column tables but it may not work as expected because the count in this case does not refer to number of rows per node. The count instead is the number of column blocks that will be retained in memory before eviction kicks in, which may translate to an arbitrary number of rows (or none at all if only blocks having column statistics are retained). The recommended use case for this option is to have a tiny LRUCOUNT (should be at least 1) so that nearly all table data is on disk, that allows the column table to behave as a disk-only table with no memory caching which is useful for infrequently used historical data, for example. READ_COMMITTED and REPEATABLE_READ isolation levels are not supported for column tables. DML Operations on Tables \u00b6 INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ; UPDATE tablename SET column = value [, column = value ...] [WHERE expression] PUT INTO tableName (column, ...) VALUES (value, ...) DELETE FROM tablename1 [WHERE expression] TRUNCATE TABLE tablename1; API Extensions Provided in SnappyContext \u00b6 Several APIs have been added in SnappySession to manipulate data stored in row and column format. Apart from SQL, these APIs can be used to manipulate tables. // Applicable for both row and column tables def insert(tableName: String, rows: Row*): Int . // Only for row tables def put(tableName: String, rows: Row*): Int def update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*): Int def delete(tableName: String, filterExpr: String): Int Usage SnappySession.insert() : Insert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snappy.insert(\"tableName\", Row.fromSeq(r)) } Usage SnappySession.put() : Upsert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snappy.put(tableName, Row.fromSeq(r)) } Usage SnappySession.update(): Update all rows in table that match passed filter expression snappy.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" ) Usage SnappySession.delete() : Delete all rows in table that match passed filter expression snappy.delete(tableName, \"ITEMREF = 3\") Row Buffers for Column Tables \u00b6 Generally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored. In SnappyData, the column table consists of two components, delta row buffer and column store. SnappyData tries to support individual insert of a single row, as it is stored in a delta row buffer which is write optimized and highly available. Once the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store. Any query on column table also takes into account the row cached buffer. By doing this, it ensures that the query does not miss any data. SQL Reference to the Syntax \u00b6 Refer to the SQL Reference Guide for information on the syntax.","title":"Tables in SnappyData"},{"location":"programming_guide/tables_in_snappydata/#tables-in-snappydata","text":"","title":"Tables in SnappyData"},{"location":"programming_guide/tables_in_snappydata/#row-and-column-tables","text":"Column tables organize and manage data in memory in a compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model. Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates. Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.","title":"Row and Column Tables"},{"location":"programming_guide/tables_in_snappydata/#ddl-and-dml-syntax-for-tables","text":"CREATE TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING [row | column] OPTIONS ( COLOCATE_WITH 'table-name', // Default none PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables. BUCKETS 'num-partitions', // Default 128. Must be an integer. REDUNDANCY 'num-of-copies' , // Must be an integer EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE 'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019, DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set. EXPIRE 'time_to_live_in_seconds', COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table. KEY_COLUMNS 'column_name,..', // Only for column table if putInto support is required COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer > 0 and < 2GB. Only for column table. ) [AS select_statement]; DROP TABLE [IF EXISTS] table_name Refer to the Best Practices section for more information on partitioning and colocating data and CREATE TABLE for information on creating a row/column table. DDL extensions are required to configure a table based on user requirements. You can also define complex types (Map, Array and StructType) as columns for column tables. snappy.sql(\"CREATE TABLE tableName ( col1 INT , col2 Array<Decimal>, col3 Map<Timestamp, Struct<x: Int, y: String, z: Decimal(10,5)>>, col6 Struct<a: Int, b: String, c: Decimal(10,5)> ) USING column options(BUCKETS '8')\" ) To access the complex data from JDBC you can see JDBCWithComplexTypes for examples. Note Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.","title":"DDL and DML Syntax for Tables"},{"location":"programming_guide/tables_in_snappydata/#spark-api-for-managing-tables","text":"Get a reference to SnappySession : val snappy: SnappySession = new SnappySession(spark.sparkContext) Create a SnappyStore table using Spark APIs val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section case class Data(col1: Int, col2: Int, col3: Int) val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sparkContext.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snappy.createDataFrame(rdd) snappy.createTable(\"column_table\", \"column\", dataDF.schema, props) //or create a row format table snappy.createTable(\"row_table\", \"row\", dataDF.schema, props) Drop a SnappyStore table using Spark APIs : snappy.dropTable(tableName, ifExists = true)","title":"Spark API for Managing Tables"},{"location":"programming_guide/tables_in_snappydata/#restrictions-on-column-tables","text":"Column tables cannot specify any primary key, unique key constraints Index on column table is not supported Option EXPIRE is not applicable for column tables Option EVICTION_BY with value LRUCOUNT is supported for column tables but it may not work as expected because the count in this case does not refer to number of rows per node. The count instead is the number of column blocks that will be retained in memory before eviction kicks in, which may translate to an arbitrary number of rows (or none at all if only blocks having column statistics are retained). The recommended use case for this option is to have a tiny LRUCOUNT (should be at least 1) so that nearly all table data is on disk, that allows the column table to behave as a disk-only table with no memory caching which is useful for infrequently used historical data, for example. READ_COMMITTED and REPEATABLE_READ isolation levels are not supported for column tables.","title":"Restrictions on Column Tables"},{"location":"programming_guide/tables_in_snappydata/#dml-operations-on-tables","text":"INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ; UPDATE tablename SET column = value [, column = value ...] [WHERE expression] PUT INTO tableName (column, ...) VALUES (value, ...) DELETE FROM tablename1 [WHERE expression] TRUNCATE TABLE tablename1;","title":"DML Operations on Tables"},{"location":"programming_guide/tables_in_snappydata/#api-extensions-provided-in-snappycontext","text":"Several APIs have been added in SnappySession to manipulate data stored in row and column format. Apart from SQL, these APIs can be used to manipulate tables. // Applicable for both row and column tables def insert(tableName: String, rows: Row*): Int . // Only for row tables def put(tableName: String, rows: Row*): Int def update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*): Int def delete(tableName: String, filterExpr: String): Int Usage SnappySession.insert() : Insert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snappy.insert(\"tableName\", Row.fromSeq(r)) } Usage SnappySession.put() : Upsert one or more [[org.apache.spark.sql.Row]] into an existing table val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200)) data.map { r => snappy.put(tableName, Row.fromSeq(r)) } Usage SnappySession.update(): Update all rows in table that match passed filter expression snappy.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" ) Usage SnappySession.delete() : Delete all rows in table that match passed filter expression snappy.delete(tableName, \"ITEMREF = 3\")","title":"API Extensions Provided in SnappyContext"},{"location":"programming_guide/tables_in_snappydata/#row-buffers-for-column-tables","text":"Generally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored. In SnappyData, the column table consists of two components, delta row buffer and column store. SnappyData tries to support individual insert of a single row, as it is stored in a delta row buffer which is write optimized and highly available. Once the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store. Any query on column table also takes into account the row cached buffer. By doing this, it ensures that the query does not miss any data.","title":"Row Buffers for Column Tables"},{"location":"programming_guide/tables_in_snappydata/#sql-reference-to-the-syntax","text":"Refer to the SQL Reference Guide for information on the syntax.","title":"SQL Reference to the Syntax"},{"location":"programming_guide/udf_and_udaf/","text":"User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF) \u00b6 Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well. Note Support for UDF is available from SnappyData version release 0.8 onwards. Create User Defined Function \u00b6 You can simply extend any one of the interfaces in the package org.apache.spark.sql.api.java . These interfaces can be included in your client application by adding snappy-spark-sql_2.11-2.0.3-2.jar to your classpath. Define a User Defined Function class \u00b6 The number of the interfaces (UDF1 to UDF22) signifies the number of parameters a UDF can take. Note Currently, any UDF which can take more than 22 parameters is not supported. package some.package import org.apache.spark.sql.api.java.UDF1 class StringLengthUDF extends UDF1[String, Int] { override def call(t1: String): Int = t1.length } Create a User Defined Function \u00b6 Note Place the jars used for creating persistent UDFs in a shared location (NFS, HDFS etc.) if you are configuring multiple leads for high availability. The same jar is used for DDL replay while the standby lead becomes the active lead. After defining a UDF you can bundle the UDF class in a JAR file and create the function by using ./bin/snappy-sql of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF. CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar' For example: CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar' You can write a JAVA or SCALA class to write a UDF implementation. Note For input/output types: The framework always returns the Java types to the UDFs. So, if you are writing scala.math.BigDecimal as an input type or output type, an exception is reported. You can use java.math.BigDecimal in the SCALA code. Return Types to UDF program type mapping SnappyData Type UDF Type STRING java.lang.String INTEGER java.lang.Integer LONG java.lang.Long DOUBLE java.lang.Double DECIMAL java.math.BigDecimal DATE java.sql.Date TIMESTAMP java.sql.Timestamp FLOAT java.lang.Float BOOLEAN java.lang.Boolean SHORT java.lang.Short BYTE java.lang.Byte Use a User Defined Function \u00b6 select strnglen(string_column) from <table> If you try to use a UDF on a different type of column, for example, an Int column an exception is reported. Drop the Function \u00b6 DROP FUNCTION IF EXISTS udf_name For example: DROP FUNCTION IF EXISTS app.strnglen Modify an Existing User Defined Function \u00b6 1) Drop the existing UDF 2) Modify the UDF code and create a new UDF . You can create the UDF with the same name as that of the dropped UDF. Create User Defined Aggregate Functions \u00b6 SnappyData uses the same interface as that of Spark to define a User Defined Aggregate Function org.apache.spark.sql.expressions.UserDefinedAggregateFunction . For more information refer to this document . Known Limitations \u00b6 In the current version of the product, setting schema over a JDBC connection (using the set schema command) or SnappySession (using SnappySession.setSchema API) does not work in all scenarios. Even if the schema is set, the operations are occasionally performed in the default APP schema. As a workaround, you can qualify the schemaname with tablename. For example, to select all rows from table 't1' in schema 'schema1', use query select * from schema1.t1 In the current version of the product, user defined functions are not displayed when you run the SHOW FUNCTIONS command in SnappyData shell. This will be available in the future releases.","title":"User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)"},{"location":"programming_guide/udf_and_udaf/#user-defined-functions-udf-and-user-defined-aggregate-functions-udaf","text":"Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well. Note Support for UDF is available from SnappyData version release 0.8 onwards.","title":"User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)"},{"location":"programming_guide/udf_and_udaf/#create-user-defined-function","text":"You can simply extend any one of the interfaces in the package org.apache.spark.sql.api.java . These interfaces can be included in your client application by adding snappy-spark-sql_2.11-2.0.3-2.jar to your classpath.","title":"Create User Defined Function"},{"location":"programming_guide/udf_and_udaf/#define-a-user-defined-function-class","text":"The number of the interfaces (UDF1 to UDF22) signifies the number of parameters a UDF can take. Note Currently, any UDF which can take more than 22 parameters is not supported. package some.package import org.apache.spark.sql.api.java.UDF1 class StringLengthUDF extends UDF1[String, Int] { override def call(t1: String): Int = t1.length }","title":"Define a User Defined Function class"},{"location":"programming_guide/udf_and_udaf/#create-a-user-defined-function","text":"Note Place the jars used for creating persistent UDFs in a shared location (NFS, HDFS etc.) if you are configuring multiple leads for high availability. The same jar is used for DDL replay while the standby lead becomes the active lead. After defining a UDF you can bundle the UDF class in a JAR file and create the function by using ./bin/snappy-sql of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF. CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar' For example: CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar' You can write a JAVA or SCALA class to write a UDF implementation. Note For input/output types: The framework always returns the Java types to the UDFs. So, if you are writing scala.math.BigDecimal as an input type or output type, an exception is reported. You can use java.math.BigDecimal in the SCALA code. Return Types to UDF program type mapping SnappyData Type UDF Type STRING java.lang.String INTEGER java.lang.Integer LONG java.lang.Long DOUBLE java.lang.Double DECIMAL java.math.BigDecimal DATE java.sql.Date TIMESTAMP java.sql.Timestamp FLOAT java.lang.Float BOOLEAN java.lang.Boolean SHORT java.lang.Short BYTE java.lang.Byte","title":"Create a User Defined Function"},{"location":"programming_guide/udf_and_udaf/#use-a-user-defined-function","text":"select strnglen(string_column) from <table> If you try to use a UDF on a different type of column, for example, an Int column an exception is reported.","title":"Use a User Defined Function"},{"location":"programming_guide/udf_and_udaf/#drop-the-function","text":"DROP FUNCTION IF EXISTS udf_name For example: DROP FUNCTION IF EXISTS app.strnglen","title":"Drop the Function"},{"location":"programming_guide/udf_and_udaf/#modify-an-existing-user-defined-function","text":"1) Drop the existing UDF 2) Modify the UDF code and create a new UDF . You can create the UDF with the same name as that of the dropped UDF.","title":"Modify an Existing User Defined Function"},{"location":"programming_guide/udf_and_udaf/#create-user-defined-aggregate-functions","text":"SnappyData uses the same interface as that of Spark to define a User Defined Aggregate Function org.apache.spark.sql.expressions.UserDefinedAggregateFunction . For more information refer to this document .","title":"Create User Defined Aggregate Functions"},{"location":"programming_guide/udf_and_udaf/#known-limitations","text":"In the current version of the product, setting schema over a JDBC connection (using the set schema command) or SnappySession (using SnappySession.setSchema API) does not work in all scenarios. Even if the schema is set, the operations are occasionally performed in the default APP schema. As a workaround, you can qualify the schemaname with tablename. For example, to select all rows from table 't1' in schema 'schema1', use query select * from schema1.t1 In the current version of the product, user defined functions are not displayed when you run the SHOW FUNCTIONS command in SnappyData shell. This will be available in the future releases.","title":"Known Limitations"},{"location":"programming_guide/using_jdbc_with_snappydata/","text":"Using JDBC with SnappyData \u00b6 SnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail. // 1527 is the default port a Locator or Server uses to listen for thin client connections Connection c = DriverManager.getConnection (\"jdbc:snappydata://locatorHostName:1527/\"); // While, clients typically just point to a locator, you could also directly point the // connection at a server endpoint Note If you are using a third part tool that connects to the database using JDBC, and if the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the io.snappydata.jdbc.ClientDriver class. For more information, see How to connect using JDBC driver .","title":"Using JDBC with SnappyData"},{"location":"programming_guide/using_jdbc_with_snappydata/#using-jdbc-with-snappydata","text":"SnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail. // 1527 is the default port a Locator or Server uses to listen for thin client connections Connection c = DriverManager.getConnection (\"jdbc:snappydata://locatorHostName:1527/\"); // While, clients typically just point to a locator, you could also directly point the // connection at a server endpoint Note If you are using a third part tool that connects to the database using JDBC, and if the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the io.snappydata.jdbc.ClientDriver class. For more information, see How to connect using JDBC driver .","title":"Using JDBC with SnappyData"},{"location":"programming_guide/using_pyspark_shell/","text":"Using pyspark Shell \u00b6 By default pyspark shell does not support SnappySession. To use SnappySession with pyspark shell, the shell must be started with in-memory catalog implementation. Command to start pyspark shell with SnappySession: ./bin/pyspark --conf spark.sql.catalogImplementation=in-memory --conf spark.snappydata.connection=<locatorhost>:<clientPort> For example: ./bin/pyspark --conf spark.sql.catalogImplementation=in-memory --conf spark.snappydata.connection=localhost:1527 Note When started with in-memory catalog implementation, hive support is not enabled.","title":"Using pyspark Shell"},{"location":"programming_guide/using_pyspark_shell/#using-pyspark-shell","text":"By default pyspark shell does not support SnappySession. To use SnappySession with pyspark shell, the shell must be started with in-memory catalog implementation. Command to start pyspark shell with SnappySession: ./bin/pyspark --conf spark.sql.catalogImplementation=in-memory --conf spark.snappydata.connection=<locatorhost>:<clientPort> For example: ./bin/pyspark --conf spark.sql.catalogImplementation=in-memory --conf spark.snappydata.connection=localhost:1527 Note When started with in-memory catalog implementation, hive support is not enabled.","title":"Using pyspark Shell"},{"location":"programming_guide/using_snappydata_shell/","text":"Using SnappyData Shell \u00b6 The SnappyData SQL Shell ( snappy-sql ) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on the row and column stores, run administrative operations and run status commands on the cluster. Internally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData. Start the SnappyData cluster and enter the following: // From the SnappyData base directory $ ./bin/snappy-sql SnappyData version 1.3.1 snappy-sql> //Connect to the cluster as a client snappy-sql> connect client 'localhost:1527'; //It connects to the locator which is running in localhost with client port configured as 1527. //Show active connections snappy-sql> show connections; //Display cluster members by querying a system table snappy-sql> select id, kind, status, host, port from sys.members; //or snappy-sql> show members; //Run a sql script. This particular script creates and loads a column table in the default schema snappy-sql> run './quickstart/scripts/create_and_load_column_table.sql'; //Run a sql script. This particular script creates and loads a row table in the default schema snappy-sql> run './quickstart/scripts/create_and_load_row_table.sql'; The complete list of commands available through snappy_shell can be found here .","title":"Using Snappy Shell"},{"location":"programming_guide/using_snappydata_shell/#using-snappydata-shell","text":"The SnappyData SQL Shell ( snappy-sql ) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on the row and column stores, run administrative operations and run status commands on the cluster. Internally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData. Start the SnappyData cluster and enter the following: // From the SnappyData base directory $ ./bin/snappy-sql SnappyData version 1.3.1 snappy-sql> //Connect to the cluster as a client snappy-sql> connect client 'localhost:1527'; //It connects to the locator which is running in localhost with client port configured as 1527. //Show active connections snappy-sql> show connections; //Display cluster members by querying a system table snappy-sql> select id, kind, status, host, port from sys.members; //or snappy-sql> show members; //Run a sql script. This particular script creates and loads a column table in the default schema snappy-sql> run './quickstart/scripts/create_and_load_column_table.sql'; //Run a sql script. This particular script creates and loads a row table in the default schema snappy-sql> run './quickstart/scripts/create_and_load_row_table.sql'; The complete list of commands available through snappy_shell can be found here .","title":"Using SnappyData Shell"},{"location":"programming_guide/using_the_spark_shell_and_spark-submit/","text":"Using spark-shell and spark-submit \u00b6 SnappyData, out-of-the-box, colocates Spark executors and the SnappyData store for efficient data intensive computations. You, however, may need to isolate the computational cluster for other reasons. For instance, a computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. Refer to SnappyData Smart Connector Mode for examples. To support such cases it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the spark.snappydata.connection property should be provided while starting the Spark-shell. To run all SnappyData functionalities, you need to create a SnappySession . // from the SnappyData base directory // Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter. $ ./bin/spark-shell --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 scala> // Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql scala> val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext) scala> val airlineDF = snappy.table(\"airline\").show scala> val resultset = snappy.sql(\"select * from airline\") Any Spark application can also use the SnappyData as store and Spark as a computational engine by providing the spark.snappydata.connection property as mentioned below: // Start the Spark standalone cluster from SnappyData base directory $ ./sbin/start-all.sh // Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port. $ ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort $SNAPPY_HOME/examples/jars/quickstart.jar // The results can be seen on the command line.","title":"Using the Spark Shell and spark-submit"},{"location":"programming_guide/using_the_spark_shell_and_spark-submit/#using-spark-shell-and-spark-submit","text":"SnappyData, out-of-the-box, colocates Spark executors and the SnappyData store for efficient data intensive computations. You, however, may need to isolate the computational cluster for other reasons. For instance, a computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. Refer to SnappyData Smart Connector Mode for examples. To support such cases it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the spark.snappydata.connection property should be provided while starting the Spark-shell. To run all SnappyData functionalities, you need to create a SnappySession . // from the SnappyData base directory // Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter. $ ./bin/spark-shell --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 scala> // Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql scala> val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext) scala> val airlineDF = snappy.table(\"airline\").show scala> val resultset = snappy.sql(\"select * from airline\") Any Spark application can also use the SnappyData as store and Spark as a computational engine by providing the spark.snappydata.connection property as mentioned below: // Start the Spark standalone cluster from SnappyData base directory $ ./sbin/start-all.sh // Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port. $ ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort $SNAPPY_HOME/examples/jars/quickstart.jar // The results can be seen on the command line.","title":"Using spark-shell and spark-submit"},{"location":"programming_guide/working_with_hadoop_yarn_cluster_manager/","text":"Working with Hadoop YARN Cluster Manager \u00b6 The SnappyData embedded cluster uses its own cluster manager and as such cannot be managed using the YARN cluster manager. However, you can start the Spark cluster with the YARN cluster manager, which can interact with the SnappyData cluster in the Smart Connector Mode . Note We assume that Apache Hadoop and YARN are already installed, and you want to bring in SnappyData cluster to work with YARN. You need to set the following environment variables: export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop Launching spark-shell with YARN \u00b6 Start a SnappyData default cluster using the ./sbin/snappy-start-all.sh command To run SnappyData quickstart example using YARN, do the following: ./bin/spark-shell --master yarn --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 -i $SNAPPY_HOME/quickstart/scripts/Quickstart.scala Note YARN is mentioned as a master url. Submitting spark-jobs using YARN \u00b6 Create the required tables in the SnappyData cluster ./bin/snappy-job.sh submit --lead localhost:8090 --app-name CreateAndLoadAirlineDataJob --class io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar Run queries on the tables created from CreateAndLoadAirlineDataJob. ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master yarn --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar","title":"Working with Hadoop YARN cluster Manager"},{"location":"programming_guide/working_with_hadoop_yarn_cluster_manager/#working-with-hadoop-yarn-cluster-manager","text":"The SnappyData embedded cluster uses its own cluster manager and as such cannot be managed using the YARN cluster manager. However, you can start the Spark cluster with the YARN cluster manager, which can interact with the SnappyData cluster in the Smart Connector Mode . Note We assume that Apache Hadoop and YARN are already installed, and you want to bring in SnappyData cluster to work with YARN. You need to set the following environment variables: export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop","title":"Working with Hadoop YARN Cluster Manager"},{"location":"programming_guide/working_with_hadoop_yarn_cluster_manager/#launching-spark-shell-with-yarn","text":"Start a SnappyData default cluster using the ./sbin/snappy-start-all.sh command To run SnappyData quickstart example using YARN, do the following: ./bin/spark-shell --master yarn --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 -i $SNAPPY_HOME/quickstart/scripts/Quickstart.scala Note YARN is mentioned as a master url.","title":"Launching spark-shell with YARN"},{"location":"programming_guide/working_with_hadoop_yarn_cluster_manager/#submitting-spark-jobs-using-yarn","text":"Create the required tables in the SnappyData cluster ./bin/snappy-job.sh submit --lead localhost:8090 --app-name CreateAndLoadAirlineDataJob --class io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar Run queries on the tables created from CreateAndLoadAirlineDataJob. ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master yarn --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar","title":"Submitting spark-jobs using YARN"},{"location":"quickstart/","text":"Getting Started in 5 Minutes or less \u00b6 Welcome to the Getting Started section! In this section, multiple options are provided for getting started with SnappyData. Depending on your preference, you can try any of the following options: SnappyData with Spark distribution SnappyData On-Premise SnappyData with Docker image SnappyData on Kubernetes You can use the following instructions and examples to try out SnappyData in 5 minutes or less: Using Spark Scala APIs SnappyData performance: 16x-20x faster than Apache Spark Using SQL","title":"Getting Started in 5 Minutes or less"},{"location":"quickstart/#getting-started-in-5-minutes-or-less","text":"Welcome to the Getting Started section! In this section, multiple options are provided for getting started with SnappyData. Depending on your preference, you can try any of the following options: SnappyData with Spark distribution SnappyData On-Premise SnappyData with Docker image SnappyData on Kubernetes You can use the following instructions and examples to try out SnappyData in 5 minutes or less: Using Spark Scala APIs SnappyData performance: 16x-20x faster than Apache Spark Using SQL","title":"Getting Started in 5 Minutes or less"},{"location":"quickstart/getting_started_by_installing_snappydata_on-premise/","text":"Getting Started by Installing SnappyData On-Premise \u00b6 Download the latest version of SnappyData from the SnappyData Release Page , which lists the latest and previous releases of SnappyData. $ tar -xzf snappydata-1.3.1-bin.tar.gz $ cd snappydata-1.3.1-bin/ # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log It opens the Spark shell. All SnappyData metadata, as well as persistent data, is stored in the directory quickstartdatadir . The spark-shell can now be used to work with SnappyData using SQL and Scala APIs . Follow instructions here , to use the product from Apache Zeppelin.","title":"On-Premise"},{"location":"quickstart/getting_started_by_installing_snappydata_on-premise/#getting-started-by-installing-snappydata-on-premise","text":"Download the latest version of SnappyData from the SnappyData Release Page , which lists the latest and previous releases of SnappyData. $ tar -xzf snappydata-1.3.1-bin.tar.gz $ cd snappydata-1.3.1-bin/ # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log It opens the Spark shell. All SnappyData metadata, as well as persistent data, is stored in the directory quickstartdatadir . The spark-shell can now be used to work with SnappyData using SQL and Scala APIs . Follow instructions here , to use the product from Apache Zeppelin.","title":"Getting Started by Installing SnappyData On-Premise"},{"location":"quickstart/getting_started_on_aws/","text":"Getting Started on Amazon Web Services (AWS) \u00b6 You can quickly create a single host SnappyData cluster (that is, a lead node, a data node and a locator) in a single Amazon Elastic Compute Cloud (EC2) instance through the AWS CloudFormation. Prerequisites \u00b6 Before you begin: Ensure that you have an existing AWS account with the permissions required to launch the EC2 resources from CloudFormation Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins. Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster. Launching the cluster from AWS EC2 \u00b6 Launch the cluster from EC2 and follow the instructions below. The AWS Login Screen is displayed. Enter your AWS login credentials. The Select Template page is displayed. The URL for the template (JSON format) is pre-populated. Click Next to continue. Note You are placed in your default region. You can either continue in the selected region or change it in the console. On the Specify Details page, you can: Provide the stack name: Enter a name for the stack. This is a mandatory field, and must contain only letters, numbers, dashes and should start with an alpha character. Select Instance Type: By default, the c4.2xlarge instance (with 8 CPU core and 15 GB RAM) is selected. This is the recommended instance size for running this quick start. Select KeyPairName: Select a key pair from the list of key pairs available to you. This is a mandatory field. Search VPCID: Select the virtual private cloud (VPC) ID from the drop-down list. Your instance(s) are launched within this VPC. This is a mandatory field. Click Next . On the Options page, click Next to continue using the provided default values. On the Review page, verify the details and click Create to create a stack. The next page lists the existing stacks. Click Refresh to view the updated list. Select the stack to view its status. When the cluster has started, the status of the stack changes to CREATE_COMPLETE . This process may take 4-5 minutes to complete. Note If the status of the stack displays as ROLLBACK_IN_PROGRESS or DELETE_COMPLETE , the stack creation may have failed. Some common causes of the failure are: Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS. Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the SnappyData CloudBuilder creation steps. Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported. Your cluster is now running. You can explore it using Apache Zeppelin, which provides web-based notebooks for data exploration. The Apache Zeppelin server has been started on the instance for you. Simply follow its link (URL) from the Outputs tab. For more information, refer to the Apache Zeppelin section or refer to the Apache Zeppelin documentation . Note Multi-node cluster set up on AWS via CloudFormation will be supported in future releases. However, users can set it up using the EC2 scripts . To stop incurring charges for the instance, it is recommended that you either terminate the instance or delete the stack when not in use. However, you cannot connect to or restart an instance after you have terminated it.","title":"Getting started on aws"},{"location":"quickstart/getting_started_on_aws/#getting-started-on-amazon-web-services-aws","text":"You can quickly create a single host SnappyData cluster (that is, a lead node, a data node and a locator) in a single Amazon Elastic Compute Cloud (EC2) instance through the AWS CloudFormation.","title":"Getting Started on Amazon Web Services (AWS)"},{"location":"quickstart/getting_started_on_aws/#prerequisites","text":"Before you begin: Ensure that you have an existing AWS account with the permissions required to launch the EC2 resources from CloudFormation Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins. Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster.","title":"Prerequisites"},{"location":"quickstart/getting_started_on_aws/#launching-the-cluster-from-aws-ec2","text":"Launch the cluster from EC2 and follow the instructions below. The AWS Login Screen is displayed. Enter your AWS login credentials. The Select Template page is displayed. The URL for the template (JSON format) is pre-populated. Click Next to continue. Note You are placed in your default region. You can either continue in the selected region or change it in the console. On the Specify Details page, you can: Provide the stack name: Enter a name for the stack. This is a mandatory field, and must contain only letters, numbers, dashes and should start with an alpha character. Select Instance Type: By default, the c4.2xlarge instance (with 8 CPU core and 15 GB RAM) is selected. This is the recommended instance size for running this quick start. Select KeyPairName: Select a key pair from the list of key pairs available to you. This is a mandatory field. Search VPCID: Select the virtual private cloud (VPC) ID from the drop-down list. Your instance(s) are launched within this VPC. This is a mandatory field. Click Next . On the Options page, click Next to continue using the provided default values. On the Review page, verify the details and click Create to create a stack. The next page lists the existing stacks. Click Refresh to view the updated list. Select the stack to view its status. When the cluster has started, the status of the stack changes to CREATE_COMPLETE . This process may take 4-5 minutes to complete. Note If the status of the stack displays as ROLLBACK_IN_PROGRESS or DELETE_COMPLETE , the stack creation may have failed. Some common causes of the failure are: Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS. Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the SnappyData CloudBuilder creation steps. Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported. Your cluster is now running. You can explore it using Apache Zeppelin, which provides web-based notebooks for data exploration. The Apache Zeppelin server has been started on the instance for you. Simply follow its link (URL) from the Outputs tab. For more information, refer to the Apache Zeppelin section or refer to the Apache Zeppelin documentation . Note Multi-node cluster set up on AWS via CloudFormation will be supported in future releases. However, users can set it up using the EC2 scripts . To stop incurring charges for the instance, it is recommended that you either terminate the instance or delete the stack when not in use. However, you cannot connect to or restart an instance after you have terminated it.","title":"Launching the cluster from AWS EC2"},{"location":"quickstart/getting_started_on_kubernetes/","text":"Getting Started with SnappyData on Kubernetes \u00b6 Kubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes. This following sections are included in this topic: Prerequisites Deploying SnappyData Chart in Kubernetes Prerequisites \u00b6 The following prerequisites must be met to deploy SnappyData on Kubernetes: Kubernetes cluster A running Kubernetes cluster of version 1.9 or higher. Helm tool Helm tool must be deployed in the Kubernetes environment. Helm comprises of two parts, that is a client and a Tiller (Server portion of Helm) inside the kube-system namespace. Tiller runs inside the Kubernetes cluster and manages the deployment of charts or packages. You can follow the instructions here to deploy Helm in your Kubernetes enviroment. Docker image Helm charts use the Docker images to launch the container on Kubernetes. You can refer to these steps to build your Docker image for SnappyData, provided you have its tarball with you. TIBCO does not provide a Docker image for SnappyData. Deploying SnappyData on Kubernetes \u00b6 SnappyData Helm chart is used to deploy SnappyData on Kubernetes. It uses Kubernetes statefulsets to launch the locator, lead, and server members. To deploy SnappyData on Kubernetes: Clone the spark-on-k8s repository and change to charts directory. git clone https://github.com/TIBCOSoftware/spark-on-k8s cd spark-on-k8s/charts Optionally, go to snappydata > values.yaml file to edit the default configurations in SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer List of Configuration Parameters for SnappyData Chart Install the snappydata chart using the following command: helm install --name snappydata --namespace snappy ./snappydata/ The above command installs the SnappyData chart in a namespace called snappy and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console. By default, SnappyData helm chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints. You can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions here . SnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted. For more details on accessing and interacting with SnappyData cluster on Kubernetes refer to SnappyData Cluster on Kubernetes","title":"Kubernetes"},{"location":"quickstart/getting_started_on_kubernetes/#getting-started-with-snappydata-on-kubernetes","text":"Kubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes. This following sections are included in this topic: Prerequisites Deploying SnappyData Chart in Kubernetes","title":"Getting Started with SnappyData on Kubernetes"},{"location":"quickstart/getting_started_on_kubernetes/#prerequisites","text":"The following prerequisites must be met to deploy SnappyData on Kubernetes: Kubernetes cluster A running Kubernetes cluster of version 1.9 or higher. Helm tool Helm tool must be deployed in the Kubernetes environment. Helm comprises of two parts, that is a client and a Tiller (Server portion of Helm) inside the kube-system namespace. Tiller runs inside the Kubernetes cluster and manages the deployment of charts or packages. You can follow the instructions here to deploy Helm in your Kubernetes enviroment. Docker image Helm charts use the Docker images to launch the container on Kubernetes. You can refer to these steps to build your Docker image for SnappyData, provided you have its tarball with you. TIBCO does not provide a Docker image for SnappyData.","title":"Prerequisites"},{"location":"quickstart/getting_started_on_kubernetes/#deploying-snappydata-on-kubernetes","text":"SnappyData Helm chart is used to deploy SnappyData on Kubernetes. It uses Kubernetes statefulsets to launch the locator, lead, and server members. To deploy SnappyData on Kubernetes: Clone the spark-on-k8s repository and change to charts directory. git clone https://github.com/TIBCOSoftware/spark-on-k8s cd spark-on-k8s/charts Optionally, go to snappydata > values.yaml file to edit the default configurations in SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer List of Configuration Parameters for SnappyData Chart Install the snappydata chart using the following command: helm install --name snappydata --namespace snappy ./snappydata/ The above command installs the SnappyData chart in a namespace called snappy and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console. By default, SnappyData helm chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints. You can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions here . SnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted. For more details on accessing and interacting with SnappyData cluster on Kubernetes refer to SnappyData Cluster on Kubernetes","title":"Deploying SnappyData on Kubernetes"},{"location":"quickstart/getting_started_with_docker_image/","text":"Building a Docker Image with SnappyData Binaries \u00b6 The following instructions outline how to build a Docker image if you have the binaries of SnappyData. Note SnappyData does not provide a Docker image. You must build it explicitly. Before building the Docker image, ensure the following: You have Docker installed, configured, and it runs successfully on your machine. Refer to the Docker documentation for more information on installing Docker. The Docker containers have access to at least 4GB of RAM on your machine. Note To allow non-root users to run Docker commands, follow the instructions here Verifying Docker Installation \u00b6 In the command prompt, run the command: $ docker run hello-world Building Docker Image of SnappyData \u00b6 You can use the Dockerfile that is provided and create your own Docker image of SnappyData. Download the Dockerfile script and place it into a directory. The Dockerfile contains a link to the latest SnappyData OSS version to build the image. Note To download the Dockerfile on Linux or MAC, use the wget command. wget https://raw.githubusercontent.com/TIBCOSoftware/snappy-cloud-tools/master/docker/Dockerfile Move into the directory containing the downloaded Dockerfile and then run the Docker build command with the required details to build the Docker image. You can create an image using any one of the following options: Building Image from the Latest Version of SnappyData OSS Building Image from a URL Directing to SnappyData Binaries Building Image from Local Copy of SnappyData Product TAR file Building Image from the Latest Version of SnappyData OSS \u00b6 By default, the Dockerfile creates a Docker image from the latest version of SnappyData OSS. $ docker build -t <your-docker-repo-name>/<image_name>[:<image-tag>] . Note If you do not provide any argument to the Dockerfile, the latest version of the SnappyData OSS release is downloaded and a Docker image for the same is built. For example: The following command builds an image with tag latest : $ docker build -t myrepo/snappydata . The following command builds an image with tag 1.3.1 : $ docker build -t myrepo/snappydata:1.3.1 . Building Image from a URL Directing to SnappyData Binaries \u00b6 If you want to create a Docker image from any of the previous versions of SnappyData, you can specify the URL of the tarfile in the build command. $ docker build -t <your-docker-repo-name>/<image_name>[:<image-tag>] . --build-arg TARFILE_LOC=<public-url> For example: $ docker build -t myrepo/snappydata . --build-arg TARFILE_LOC=https://github.com/TIBCOSoftware/snappydata/releases/download/v1.3.1/snappydata-1.3.1-bin.tar.gz Building Image from Local Copy of SnappyData Product TAR file \u00b6 If you have already downloaded the SnappyData tarfile locally onto your machine, use the following steps to build an image from the downloaded binaries. To download SnappyData, refer to the Provisioning SnappyData section in the product documentation. Copy the downloaded tar.gz file to the Docker folder where you have placed the Dockerfile and run the following command: $ docker build -t <your-docker-repo-name>/<image_name>[:<image-tag>] . --build-arg TARFILE_LOC=<tarfile name> For example: $ docker build -t myrepo/snappydata . --build-arg TARFILE_LOC=snappydata-1.3.1-bin.tar.gz Verifying Details of Docker Images \u00b6 After the Docker build is successful, you can check the details for Docker images using the docker images command. For example: $ docker images Publishing Docker Image \u00b6 If you want to publish the Docker image onto the Docker Hub, login to the Docker account using docker login command, and provide your credentials. For more information on Docker login, visit here . After a successful login, you can publish the Docker image using the docker push command. $ docker push <your-docker-repo-name>/<image_name>[:<image-tag>] Ensure to use the same name in the docker push that is used in docker build . For example: $ docker push myrepo/snappydata Note This example only showcases how to push an image onto Docker Hub. You can also publish the image to other container registries such as gcr.io . For publishing on gcr.io, you can refer this document . Launching SnappyData Container \u00b6 The command to launch SnappyData container is different for Linux and macOS. Launching SnappyData Container on Linux \u00b6 In the command prompt, execute the following commands to launch the SnappyData cluster in a single container. $ docker run -itd --net=host --name <container-name> <your-docker-repo-name>/<image_name>[:<image-tag>] start all # -i: keep the STDIN open even if not attached. # -t: Allocate pseudo-TTY. # -d: Detach and run the container in background and print container ID. # --net=host: Use the Docker host network stack. If the image is not available locally, this fetches the Docker image from the Docker registry, launches a default cluster consisting of one data node, one lead, and one locator in a container. $ docker run -itd --net=host --name snappydata myrepo/snappydata start all Launching SnappyData Container on macOS \u00b6 If you are using macOS, you must redirect the ports manually using -p parameter. If you use --net=host , it may not work correctly on the macOS. You can use the following modified command for macOS: $ docker run -d --name=snappydata -p 5050:5050 -p 1527:1527 -p 1528:1528 myrepo/snappydata start all -hostname-for-clients=<Machine_IP/Public_IP> The -hostname-for-clients parameter sets the IP Address or Hostname that the server listens for client connections. The command may take few seconds to execute. Commonly used Docker Commands \u00b6 Description Docker Commands To check details of all the Docker containers. $ docker ps -a To check the container logs. $ docker logs <container-name> To launch Snappy Shell. $ docker exec -it <container-name> ./bin/snappy To launch Spark Shell. $ docker exec -it <container-name> ./bin/spark-shell To stop the cluster. $ docker exec -it <container-name> ./sbin/snappy-stop-all.sh To stop the container. $ docker stop <container-name> To open bash shell inside the container. $ docker exec -it <container-name> /bin/bash","title":"Building a Docker Image with SnappyData Binaries"},{"location":"quickstart/getting_started_with_docker_image/#building-a-docker-image-with-snappydata-binaries","text":"The following instructions outline how to build a Docker image if you have the binaries of SnappyData. Note SnappyData does not provide a Docker image. You must build it explicitly. Before building the Docker image, ensure the following: You have Docker installed, configured, and it runs successfully on your machine. Refer to the Docker documentation for more information on installing Docker. The Docker containers have access to at least 4GB of RAM on your machine. Note To allow non-root users to run Docker commands, follow the instructions here","title":"Building a Docker Image with SnappyData Binaries"},{"location":"quickstart/getting_started_with_docker_image/#verifying-docker-installation","text":"In the command prompt, run the command: $ docker run hello-world","title":"Verifying Docker Installation"},{"location":"quickstart/getting_started_with_docker_image/#building-docker-image-of-snappydata","text":"You can use the Dockerfile that is provided and create your own Docker image of SnappyData. Download the Dockerfile script and place it into a directory. The Dockerfile contains a link to the latest SnappyData OSS version to build the image. Note To download the Dockerfile on Linux or MAC, use the wget command. wget https://raw.githubusercontent.com/TIBCOSoftware/snappy-cloud-tools/master/docker/Dockerfile Move into the directory containing the downloaded Dockerfile and then run the Docker build command with the required details to build the Docker image. You can create an image using any one of the following options: Building Image from the Latest Version of SnappyData OSS Building Image from a URL Directing to SnappyData Binaries Building Image from Local Copy of SnappyData Product TAR file","title":"Building Docker Image of SnappyData"},{"location":"quickstart/getting_started_with_docker_image/#building-image-from-the-latest-version-of-snappydata-oss","text":"By default, the Dockerfile creates a Docker image from the latest version of SnappyData OSS. $ docker build -t <your-docker-repo-name>/<image_name>[:<image-tag>] . Note If you do not provide any argument to the Dockerfile, the latest version of the SnappyData OSS release is downloaded and a Docker image for the same is built. For example: The following command builds an image with tag latest : $ docker build -t myrepo/snappydata . The following command builds an image with tag 1.3.1 : $ docker build -t myrepo/snappydata:1.3.1 .","title":"Building Image from the Latest Version of SnappyData OSS"},{"location":"quickstart/getting_started_with_docker_image/#building-image-from-a-url-directing-to-snappydata-binaries","text":"If you want to create a Docker image from any of the previous versions of SnappyData, you can specify the URL of the tarfile in the build command. $ docker build -t <your-docker-repo-name>/<image_name>[:<image-tag>] . --build-arg TARFILE_LOC=<public-url> For example: $ docker build -t myrepo/snappydata . --build-arg TARFILE_LOC=https://github.com/TIBCOSoftware/snappydata/releases/download/v1.3.1/snappydata-1.3.1-bin.tar.gz","title":"Building Image from a URL Directing to SnappyData Binaries"},{"location":"quickstart/getting_started_with_docker_image/#building-image-from-local-copy-of-snappydata-product-tar-file","text":"If you have already downloaded the SnappyData tarfile locally onto your machine, use the following steps to build an image from the downloaded binaries. To download SnappyData, refer to the Provisioning SnappyData section in the product documentation. Copy the downloaded tar.gz file to the Docker folder where you have placed the Dockerfile and run the following command: $ docker build -t <your-docker-repo-name>/<image_name>[:<image-tag>] . --build-arg TARFILE_LOC=<tarfile name> For example: $ docker build -t myrepo/snappydata . --build-arg TARFILE_LOC=snappydata-1.3.1-bin.tar.gz","title":"Building Image from Local Copy of SnappyData Product TAR file"},{"location":"quickstart/getting_started_with_docker_image/#verifying-details-of-docker-images","text":"After the Docker build is successful, you can check the details for Docker images using the docker images command. For example: $ docker images","title":"Verifying Details of Docker Images"},{"location":"quickstart/getting_started_with_docker_image/#publishing-docker-image","text":"If you want to publish the Docker image onto the Docker Hub, login to the Docker account using docker login command, and provide your credentials. For more information on Docker login, visit here . After a successful login, you can publish the Docker image using the docker push command. $ docker push <your-docker-repo-name>/<image_name>[:<image-tag>] Ensure to use the same name in the docker push that is used in docker build . For example: $ docker push myrepo/snappydata Note This example only showcases how to push an image onto Docker Hub. You can also publish the image to other container registries such as gcr.io . For publishing on gcr.io, you can refer this document .","title":"Publishing Docker Image"},{"location":"quickstart/getting_started_with_docker_image/#launching-snappydata-container","text":"The command to launch SnappyData container is different for Linux and macOS.","title":"Launching SnappyData Container"},{"location":"quickstart/getting_started_with_docker_image/#launching-snappydata-container-on-linux","text":"In the command prompt, execute the following commands to launch the SnappyData cluster in a single container. $ docker run -itd --net=host --name <container-name> <your-docker-repo-name>/<image_name>[:<image-tag>] start all # -i: keep the STDIN open even if not attached. # -t: Allocate pseudo-TTY. # -d: Detach and run the container in background and print container ID. # --net=host: Use the Docker host network stack. If the image is not available locally, this fetches the Docker image from the Docker registry, launches a default cluster consisting of one data node, one lead, and one locator in a container. $ docker run -itd --net=host --name snappydata myrepo/snappydata start all","title":"Launching SnappyData Container on Linux"},{"location":"quickstart/getting_started_with_docker_image/#launching-snappydata-container-on-macos","text":"If you are using macOS, you must redirect the ports manually using -p parameter. If you use --net=host , it may not work correctly on the macOS. You can use the following modified command for macOS: $ docker run -d --name=snappydata -p 5050:5050 -p 1527:1527 -p 1528:1528 myrepo/snappydata start all -hostname-for-clients=<Machine_IP/Public_IP> The -hostname-for-clients parameter sets the IP Address or Hostname that the server listens for client connections. The command may take few seconds to execute.","title":"Launching SnappyData Container on macOS"},{"location":"quickstart/getting_started_with_docker_image/#commonly-used-docker-commands","text":"Description Docker Commands To check details of all the Docker containers. $ docker ps -a To check the container logs. $ docker logs <container-name> To launch Snappy Shell. $ docker exec -it <container-name> ./bin/snappy To launch Spark Shell. $ docker exec -it <container-name> ./bin/spark-shell To stop the cluster. $ docker exec -it <container-name> ./sbin/snappy-stop-all.sh To stop the container. $ docker stop <container-name> To open bash shell inside the container. $ docker exec -it <container-name> /bin/bash","title":"Commonly used Docker Commands"},{"location":"quickstart/getting_started_with_your_spark_distribution/","text":"Getting Started with your Spark Distribution \u00b6 If you are a Spark developer and already using Spark 2.1.1 to 2.1.3, the fastest way to work with SnappyData is to add SnappyData as a dependency. For instance, using the package option in the Spark shell. Open a command terminal, go to the location of the Spark installation directory, and enter the following: $ cd <Spark_Install_dir> # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $ ./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" This opens the Spark shell and downloads the relevant SnappyData files to your local machine. Depending on your network connection speed, it may take some time to download the files. All SnappyData metadata, as well as persistent data, is stored in the directory quickstartdatadir . The spark-shell can now be used to work with SnappyData using Scala APIs and SQL . For this exercise, it is assumed that you are either familiar with Spark or SQL (not necessarily both). Basic database capabilities like working with Columnar and Row-oriented tables, querying and updating these tables is showcased. Tables in SnappyData exhibit many operational capabilities like disk persistence, redundancy for HA, eviction, etc. For more information, you can refer to the detailed documentation . Next, you can try using the Scala APIs or SQL . We will add Java/Python examples in the future.","title":"Spark Distribution"},{"location":"quickstart/getting_started_with_your_spark_distribution/#getting-started-with-your-spark-distribution","text":"If you are a Spark developer and already using Spark 2.1.1 to 2.1.3, the fastest way to work with SnappyData is to add SnappyData as a dependency. For instance, using the package option in the Spark shell. Open a command terminal, go to the location of the Spark installation directory, and enter the following: $ cd <Spark_Install_dir> # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $ ./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" This opens the Spark shell and downloads the relevant SnappyData files to your local machine. Depending on your network connection speed, it may take some time to download the files. All SnappyData metadata, as well as persistent data, is stored in the directory quickstartdatadir . The spark-shell can now be used to work with SnappyData using Scala APIs and SQL . For this exercise, it is assumed that you are either familiar with Spark or SQL (not necessarily both). Basic database capabilities like working with Columnar and Row-oriented tables, querying and updating these tables is showcased. Tables in SnappyData exhibit many operational capabilities like disk persistence, redundancy for HA, eviction, etc. For more information, you can refer to the detailed documentation . Next, you can try using the Scala APIs or SQL . We will add Java/Python examples in the future.","title":"Getting Started with your Spark Distribution"},{"location":"quickstart/performance_apache_spark/","text":"Benchmark 16-20x Faster Performance than Apache Spark \u00b6 In this section, you are walked through a simple benchmark to compare SnappyData's performance to Spark 2.1.1. Millions of rows are loaded into a cached Spark DataFrame, some analytic queries measuring its performance are run, and then, the same using SnappyData's column table is repeated. A simple analytic query that scans a 100 million-row column table shows SnappyData outperforming Apache Spark by 16-20X when both products have all the data in memory. Note It is recommended that you should have at least 4GB of RAM reserved for this test. Start the Spark Shell \u00b6 Use any of the options mentioned below to start the Spark shell: If you are using your own Spark distribution that is compatible with version 2.1.1: # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\" If you have downloaded SnappyData : # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\" If you are using Docker : $ docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell --driver-memory=4g --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\" To get the Performance Numbers \u00b6 Ensure that you are in a Spark shell, and then follow the instructions below to get the performance numbers. Define a function \"benchmark\" , which tells us the average time to run queries after doing the initial warm-ups. scala> def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: => Unit) { for (i <- 1 to warmups) { f } val startTime = System.nanoTime for (i <- 1 to times) { f } val endTime = System.nanoTime println(s\"Average time taken in $name for $times runs: \" + (endTime - startTime).toDouble / (times * 1000000.0) + \" millis\") } Create a DataFrame and temporary table using Spark's range method : Cache it in Spark to get optimal performance. This creates a DataFrame of 100 million records.You can change the number of rows, based on your memory availability. scala> var testDF = spark.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as STRING)) as sym\") scala> testDF.cache scala> testDF.createOrReplaceTempView(\"sparkCacheTable\") Run a query and to check the performance : The queries use an average of a field, without any \"where\" clause. This ensures that it touches all records while scanning. scala> benchmark(\"Spark perf\") {spark.sql(\"select sym, avg(id) from sparkCacheTable group by sym\").collect()} Clean up the JVM : This ensures that all in-memory artifacts for Spark are cleaned up. scala> testDF.unpersist() scala> System.gc() scala> System.runFinalization() Create a SnappySession : scala> val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext) Create similar 100 million record DataFrame : scala> testDF = snappy.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as varchar(10))) as sym\") Create the table : scala> snappy.sql(\"drop table if exists snappyTable\") scala> snappy.sql(\"create table snappyTable (id bigint not null, sym varchar(10) not null) using column\") Insert the created DataFrame into the table and measure its performance : scala> benchmark(\"Snappy insert perf\", 1, 0) {testDF.write.insertInto(\"snappyTable\") } Now, let us run the same benchmark against Spark DataFrame : scala> benchmark(\"Snappy perf\") {snappy.sql(\"select sym, avg(id) from snappyTable group by sym\").collect()} scala> :q // Quit the Spark Shell Note This benchmark code is tested on a system with 4 CPUs (Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz) and 16GiB System Memory. Also, in an AWS t2.xlarge (Variable ECUs, 4 vCPUs, 2.4 GHz, Intel Xeon Family, 16 GiB memory, EBS only) instance SnappyData is approximately 16 to 18 times faster than Spark 2.1.1.","title":"Benchmark 16-20x Faster Performance than Apache Spark"},{"location":"quickstart/performance_apache_spark/#benchmark-16-20x-faster-performance-than-apache-spark","text":"In this section, you are walked through a simple benchmark to compare SnappyData's performance to Spark 2.1.1. Millions of rows are loaded into a cached Spark DataFrame, some analytic queries measuring its performance are run, and then, the same using SnappyData's column table is repeated. A simple analytic query that scans a 100 million-row column table shows SnappyData outperforming Apache Spark by 16-20X when both products have all the data in memory. Note It is recommended that you should have at least 4GB of RAM reserved for this test.","title":"Benchmark 16-20x Faster Performance than Apache Spark"},{"location":"quickstart/performance_apache_spark/#start-the-spark-shell","text":"Use any of the options mentioned below to start the Spark shell: If you are using your own Spark distribution that is compatible with version 2.1.1: # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \"io.snappydata:snappydata-spark-connector_2.11:1.3.1\" --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\" If you have downloaded SnappyData : # Create a directory for SnappyData artifacts $ mkdir quickstartdatadir $ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\" If you are using Docker : $ docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell --driver-memory=4g --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"","title":"Start the Spark Shell"},{"location":"quickstart/performance_apache_spark/#to-get-the-performance-numbers","text":"Ensure that you are in a Spark shell, and then follow the instructions below to get the performance numbers. Define a function \"benchmark\" , which tells us the average time to run queries after doing the initial warm-ups. scala> def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: => Unit) { for (i <- 1 to warmups) { f } val startTime = System.nanoTime for (i <- 1 to times) { f } val endTime = System.nanoTime println(s\"Average time taken in $name for $times runs: \" + (endTime - startTime).toDouble / (times * 1000000.0) + \" millis\") } Create a DataFrame and temporary table using Spark's range method : Cache it in Spark to get optimal performance. This creates a DataFrame of 100 million records.You can change the number of rows, based on your memory availability. scala> var testDF = spark.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as STRING)) as sym\") scala> testDF.cache scala> testDF.createOrReplaceTempView(\"sparkCacheTable\") Run a query and to check the performance : The queries use an average of a field, without any \"where\" clause. This ensures that it touches all records while scanning. scala> benchmark(\"Spark perf\") {spark.sql(\"select sym, avg(id) from sparkCacheTable group by sym\").collect()} Clean up the JVM : This ensures that all in-memory artifacts for Spark are cleaned up. scala> testDF.unpersist() scala> System.gc() scala> System.runFinalization() Create a SnappySession : scala> val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext) Create similar 100 million record DataFrame : scala> testDF = snappy.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as varchar(10))) as sym\") Create the table : scala> snappy.sql(\"drop table if exists snappyTable\") scala> snappy.sql(\"create table snappyTable (id bigint not null, sym varchar(10) not null) using column\") Insert the created DataFrame into the table and measure its performance : scala> benchmark(\"Snappy insert perf\", 1, 0) {testDF.write.insertInto(\"snappyTable\") } Now, let us run the same benchmark against Spark DataFrame : scala> benchmark(\"Snappy perf\") {snappy.sql(\"select sym, avg(id) from snappyTable group by sym\").collect()} scala> :q // Quit the Spark Shell Note This benchmark code is tested on a system with 4 CPUs (Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz) and 16GiB System Memory. Also, in an AWS t2.xlarge (Variable ECUs, 4 vCPUs, 2.4 GHz, Intel Xeon Family, 16 GiB memory, EBS only) instance SnappyData is approximately 16 to 18 times faster than Spark 2.1.1.","title":"To get the Performance Numbers"},{"location":"quickstart/snappydataquick_start/","text":"Create SnappyData Cluster \u00b6 In this section, you will get a quick tour to start a SnappyData cluster and try out the basic features and functionalities. The following items are covered in this guide: Start SnappyData Cluster Check SnappyData Cluster Status Connect/Disconnect SnappyData Shell Create Tables Create Tables and Import Data Using quickstart Scripts Create a Column Table using an External Table Run Queries Submit Snappy Jobs Add Servers to Cluster Rebalance Data on Servers Stop Cluster Start SnappyData Cluster \u00b6 Navigate to the SnappyData product root directory to start the cluster. Run the ./sbin/snappy-start-all.sh script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server. $./sbin/snappy-start-all.sh Logs generated in /home/xyz/<snappydata_install_dir>/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 9086 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/xyz/<snappydata_install_dir>/work/localhost-server-1/snappyserver.log SnappyData Server pid: 9220 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/xyz/<snappydata_install_dir>/snappy/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 9370 status: running Distributed system now has 3 members. Starting job server on: 0.0.0.0[8090] You can connect to Snappy SQL shell and run the select id, kind, netservers from sys.members; query to view the cluster members. ./bin/snappy connect client '127.0.0.1:1527'; select id, kind, netservers from sys.members; ID |KIND |NETSERVERS ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 127.0.0.1(3688)<v1>:32604 |datastore(normal) |localhost/127.0.0.1[1528] 127.0.0.1(3842)<v2>:25366 |accessor(normal) | 127.0.0.1(3431:locator)<ec><v0>:41406 |locator(normal) |localhost/127.0.0.1[1527] In the output, the details of the cluster members are displayed. Here in the KIND column, the member corresponding to accessor is the lead node. In a cluster, you can connect as a client to any member by specifying localhost with the unique port number of the member (the one specified in the NETSERVERS column corresponding to each member). However, connecting to the locator provides basic load balancing by routing the connection request to an available server member. Check SnappyData Cluster Status \u00b6 You can check the status of a running cluster using the following command: $ ./sbin/snappy-status-all.sh SnappyData Locator pid: 9748 status: running SnappyData Server pid: 9887 status: running SnappyData Leader pid: 10468 status: running Connect to Snappy SQL shell to perform various SQL operations. Alternatively , you can access the SnappyData Monitoring Console by entering http:// <leadhost> :5050/dashboard/ in the web browser. For example, http://localhost:5050/dashboard/. <leadhost> is the hostname or IP of the lead node in your cluster which is provided in the conf/leads file. On the SnappyData Monitoring Console dashboards, after starting a cluster, you can check the status of each of the cluster member. Connect/Disconnect to SnappyData Shell \u00b6 After starting the SnappyData cluster, run these commands together to start the SnappyData shell: ./bin/snappy connect client '127.0.0.1:1527'; Type exit; or press CTRL + C to disconnect the SnappyData Shell. Create Tables \u00b6 Create a simple table and insert a few rows. By default, if no options are provided, row replicated table is formed. However, you can create tables using the row or column option. # Create a table named quicktable. A replicated row table is formed without/empty options by default. CREATE TABLE quicktable (id int generated always as identity, item char(25)); # Create column table. A column table is created which is partitioned by default. CREATE TABLE quicktable_col (id int, item varchar(25)) using column options(); # Create partitioned Row table using the partition by options. A partitioned row table is created with partitioning scheme on the 'id' column. CREATE TABLE quicktable_row (id int generated always as identity, item char(25)) using row options(partition_by 'id'); # Insert one row into the table. INSERT into quicktable values (default, 'widget'); # Insert one more row into the table. INSERT into quicktable values (default, 'gadget'); # View the contents of the table. select * from quicktable; ID |ITEM ------------------------------------- 2 |gadget 1 |widget 2 rows selected # SnappyData replicates the row tables that are created by default onto the data store members. # You can validate this using the following query: select tablename, datapolicy from sys.systables where tablename='QUICKTABLE'; #The following output is displayed: TABLENAME |DATAPOLICY --------------------------------------------------------------------------------------------------------------------------------------------------------- QUICKTABLE |PERSISTENT_REPLICATE Create Tables and Import Data Using quickstart Scripts \u00b6 SnappyData contains various quickstart scripts that can be used to run some basic functionalities. For example, you can run the create_and_load_column_table.sql script. This script first drops the table if it exists and then creates an external table named STAGING AIRLINE to load the formatted data from a parquet file. Then a column table is created with only the specified columns from this external table. Connect to SnappyData shell before running these scripts. These script files must be run from within the quickstart directory if you are providing the relative path as shown: ./bin/snappy connect client '127.0.0.1:1527'; RUN 'quickstart/scripts/create_and_load_column_table.sql'; # Use the following command to view the details of the external table. describe staging_airline; # The following output is displayed: COLUMN_NAME |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE ----------------------------------------------------------------------------------------------------------------- Year |INTEGER |0 |10 |10 |NULL |NULL |YES Month |INTEGER |0 |10 |10 |NULL |NULL |YES DayOfMonth |INTEGER |0 |10 |10 |NULL |NULL |YES DayOfWeek |INTEGER |0 |10 |10 |NULL |NULL |YES DepTime |INTEGER |0 |10 |10 |NULL |NULL |YES CRSDepTime |INTEGER |0 |10 |10 |NULL |NULL |YES ArrTime |INTEGER |0 |10 |10 |NULL |NULL |YES CRSArrTime |INTEGER |0 |10 |10 |NULL |NULL |YES UniqueCarrier |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES FlightNum |INTEGER |0 |10 |10 |NULL |NULL |YES TailNum |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES ActualElapsedTime |INTEGER |0 |10 |10 |NULL |NULL |YES CRSElapsedTime |INTEGER |0 |10 |10 |NULL |NULL |YES AirTime |INTEGER |0 |10 |10 |NULL |NULL |YES ArrDelay |INTEGER |0 |10 |10 |NULL |NULL |YES DepDelay |INTEGER |0 |10 |10 |NULL |NULL |YES Origin |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES Dest |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES Distance |INTEGER |0 |10 |10 |NULL |NULL |YES TaxiIn |INTEGER |0 |10 |10 |NULL |NULL |YES TaxiOut |INTEGER |0 |10 |10 |NULL |NULL |YES Cancelled |INTEGER |0 |10 |10 |NULL |NULL |YES CancellationCode |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES Diverted |INTEGER |0 |10 |10 |NULL |NULL |YES CarrierDelay |INTEGER |0 |10 |10 |NULL |NULL |YES WeatherDelay |INTEGER |0 |10 |10 |NULL |NULL |YES NASDelay |INTEGER |0 |10 |10 |NULL |NULL |YES SecurityDelay |INTEGER |0 |10 |10 |NULL |NULL |YES LateAircraftDelay |INTEGER |0 |10 |10 |NULL |NULL |YES ArrDelaySlot |INTEGER |0 |10 |10 |NULL |NULL |YES # Use the following command to check the number of records in the staging_airline external table: select count(*) from staging_airline; count(1) -------------------- 1000000 1 row selected You can also try the following: Create and load a row table: RUN './quickstart/scripts/create_and_load_row_table.sql'; View the status of the system: RUN './quickstart/scripts/status_queries.sql'; Create a Column Table Using an External Table \u00b6 Similarly as the quickstart scripts, you can try to create an external table named staging_airline to load the formatted data from a airlineParquetData file with inferSchema option as true. Later, you can create a column table named airline and pull data from the external table into this table. After pulling in the data, you can check the number of records in the table. CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData', inferSchema 'true'); CREATE TABLE AIRLINE2 USING column AS (SELECT * FROM STAGING_AIRLINE); describe airline2; COLUMN_NAME |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE ----------------------------------------------------------------------------------------------------------------- YEAR |INTEGER |0 |10 |10 |NULL |NULL |YES MONTH |INTEGER |0 |10 |10 |NULL |NULL |YES DAYOFMONTH |INTEGER |0 |10 |10 |NULL |NULL |YES DAYOFWEEK |INTEGER |0 |10 |10 |NULL |NULL |YES DEPTIME |INTEGER |0 |10 |10 |NULL |NULL |YES CRSDEPTIME |INTEGER |0 |10 |10 |NULL |NULL |YES ARRTIME |INTEGER |0 |10 |10 |NULL |NULL |YES CRSARRTIME |INTEGER |0 |10 |10 |NULL |NULL |YES UNIQUECARRIER |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES FLIGHTNUM |INTEGER |0 |10 |10 |NULL |NULL |YES TAILNUM |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES ACTUALELAPSEDTIME |INTEGER |0 |10 |10 |NULL |NULL |YES CRSELAPSEDTIME |INTEGER |0 |10 |10 |NULL |NULL |YES AIRTIME |INTEGER |0 |10 |10 |NULL |NULL |YES ARRDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES DEPDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES ORIGIN |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES DEST |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES DISTANCE |INTEGER |0 |10 |10 |NULL |NULL |YES TAXIIN |INTEGER |0 |10 |10 |NULL |NULL |YES TAXIOUT |INTEGER |0 |10 |10 |NULL |NULL |YES CANCELLED |INTEGER |0 |10 |10 |NULL |NULL |YES CANCELLATIONCODE |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES DIVERTED |INTEGER |0 |10 |10 |NULL |NULL |YES CARRIERDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES WEATHERDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES NASDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES SECURITYDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES LATEAIRCRAFTDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES ARRDELAYSLOT |INTEGER |0 |10 |10 |NULL |NULL |YES 30 rows selected select count(*) > ; count(1) -------------------- 1 1 row selected After running these queries, you can check the table details on the SnappyData Monitoring Console Dashboards. The details of the newly created tables are displayed in the Tables section. Run Queries \u00b6 You can try a couple of analytical queries as shown: Query to find the average arrival delay. select avg(arrdelay) from airline; avg(ARRDELAY) ---------------------- 6.735443 1 row selected * Query for avg arrival delay of a specific airline. select max(arrdelay) from airline where DEST = ''; max(ARRDELAY) ------------- NULL 1 row selected Submit Snappy Jobs \u00b6 Instructions to submit Snappy jobs are available here . There are sample jobs available for reference. You can import the examples into a separate independent gradle project and submit the jobs to the cluster or run within an IDE. Refer to the instructions here . Add Servers into Cluster \u00b6 You can add more than one server to a cluster. To add a new server, do the following: Go to SnappyData home directory. cd <snappydata_install_dir> Create a configuration file named servers in the conf folder in the the SnappyData home directory. To do so, you can copy the existing template files servers.template and rename it to servers as shown: cp -f conf/servers.template conf/servers Open this file using a vi editor and add a hostname entry of the additional server, after the entry of the primary server, and save the file. For example, suppose there is an entry localhost in this file for the primary server. You can add an entry localhost below this entry for the additional server. The servers file should contain the hostnames of the nodes (one per line) where you intend to start the member. From the SnappyData home directory, start the cluster again using the ./sbin/snappy-start-all.sh command. The new server gets started. Ignore the error messages of the other nodes that are already running. You can check details of the newly added member from the SnappyData Monitoring Console. Rebalancing Data on Servers \u00b6 Further, you can distribute the data among the servers in the cluster. This ensures that each server carries almost equal data. To balance the data equally on the servers, do the following: Go to SnappyData home directory. cd <snappydata_install_dir> Connect to snappy shell and obtain the jdbc client connection. Run the rebalance command. call sys.rebalance_all_buckets(); On SnappyData Monitoring Console, check the Heap Memory Used/Total column for the servers. You will notice that before rebalancing the data, there was an unequal distribution of the memory usage and after running the rebalance command, the data is distributed equally among both the servers. Before Rebalance After Rebalance Stop the Cluster \u00b6 You can stop the cluster using the ./sbin/snappy-stop-all.sh command: ./sbin/snappy-stop-all.sh The SnappyData Leader has stopped. The SnappyData Server has stopped. The SnappyData Locator has stopped. For more details, refer to Stopping the Cluster","title":"SnappyData Quickstart Guide"},{"location":"quickstart/snappydataquick_start/#create-snappydata-cluster","text":"In this section, you will get a quick tour to start a SnappyData cluster and try out the basic features and functionalities. The following items are covered in this guide: Start SnappyData Cluster Check SnappyData Cluster Status Connect/Disconnect SnappyData Shell Create Tables Create Tables and Import Data Using quickstart Scripts Create a Column Table using an External Table Run Queries Submit Snappy Jobs Add Servers to Cluster Rebalance Data on Servers Stop Cluster","title":"Create SnappyData Cluster"},{"location":"quickstart/snappydataquick_start/#start-snappydata-cluster","text":"Navigate to the SnappyData product root directory to start the cluster. Run the ./sbin/snappy-start-all.sh script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server. $./sbin/snappy-start-all.sh Logs generated in /home/xyz/<snappydata_install_dir>/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 9086 status: running Distributed system now has 1 members. Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527] Logs generated in /home/xyz/<snappydata_install_dir>/work/localhost-server-1/snappyserver.log SnappyData Server pid: 9220 status: running Distributed system now has 2 members. Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528] Logs generated in /home/xyz/<snappydata_install_dir>/snappy/work/localhost-lead-1/snappyleader.log SnappyData Leader pid: 9370 status: running Distributed system now has 3 members. Starting job server on: 0.0.0.0[8090] You can connect to Snappy SQL shell and run the select id, kind, netservers from sys.members; query to view the cluster members. ./bin/snappy connect client '127.0.0.1:1527'; select id, kind, netservers from sys.members; ID |KIND |NETSERVERS ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 127.0.0.1(3688)<v1>:32604 |datastore(normal) |localhost/127.0.0.1[1528] 127.0.0.1(3842)<v2>:25366 |accessor(normal) | 127.0.0.1(3431:locator)<ec><v0>:41406 |locator(normal) |localhost/127.0.0.1[1527] In the output, the details of the cluster members are displayed. Here in the KIND column, the member corresponding to accessor is the lead node. In a cluster, you can connect as a client to any member by specifying localhost with the unique port number of the member (the one specified in the NETSERVERS column corresponding to each member). However, connecting to the locator provides basic load balancing by routing the connection request to an available server member.","title":"Start SnappyData Cluster"},{"location":"quickstart/snappydataquick_start/#check-snappydata-cluster-status","text":"You can check the status of a running cluster using the following command: $ ./sbin/snappy-status-all.sh SnappyData Locator pid: 9748 status: running SnappyData Server pid: 9887 status: running SnappyData Leader pid: 10468 status: running Connect to Snappy SQL shell to perform various SQL operations. Alternatively , you can access the SnappyData Monitoring Console by entering http:// <leadhost> :5050/dashboard/ in the web browser. For example, http://localhost:5050/dashboard/. <leadhost> is the hostname or IP of the lead node in your cluster which is provided in the conf/leads file. On the SnappyData Monitoring Console dashboards, after starting a cluster, you can check the status of each of the cluster member.","title":"Check SnappyData Cluster Status"},{"location":"quickstart/snappydataquick_start/#connectdisconnect-to-snappydata-shell","text":"After starting the SnappyData cluster, run these commands together to start the SnappyData shell: ./bin/snappy connect client '127.0.0.1:1527'; Type exit; or press CTRL + C to disconnect the SnappyData Shell.","title":"Connect/Disconnect to SnappyData Shell"},{"location":"quickstart/snappydataquick_start/#create-tables","text":"Create a simple table and insert a few rows. By default, if no options are provided, row replicated table is formed. However, you can create tables using the row or column option. # Create a table named quicktable. A replicated row table is formed without/empty options by default. CREATE TABLE quicktable (id int generated always as identity, item char(25)); # Create column table. A column table is created which is partitioned by default. CREATE TABLE quicktable_col (id int, item varchar(25)) using column options(); # Create partitioned Row table using the partition by options. A partitioned row table is created with partitioning scheme on the 'id' column. CREATE TABLE quicktable_row (id int generated always as identity, item char(25)) using row options(partition_by 'id'); # Insert one row into the table. INSERT into quicktable values (default, 'widget'); # Insert one more row into the table. INSERT into quicktable values (default, 'gadget'); # View the contents of the table. select * from quicktable; ID |ITEM ------------------------------------- 2 |gadget 1 |widget 2 rows selected # SnappyData replicates the row tables that are created by default onto the data store members. # You can validate this using the following query: select tablename, datapolicy from sys.systables where tablename='QUICKTABLE'; #The following output is displayed: TABLENAME |DATAPOLICY --------------------------------------------------------------------------------------------------------------------------------------------------------- QUICKTABLE |PERSISTENT_REPLICATE","title":"Create Tables"},{"location":"quickstart/snappydataquick_start/#create-tables-and-import-data-using-quickstart-scripts","text":"SnappyData contains various quickstart scripts that can be used to run some basic functionalities. For example, you can run the create_and_load_column_table.sql script. This script first drops the table if it exists and then creates an external table named STAGING AIRLINE to load the formatted data from a parquet file. Then a column table is created with only the specified columns from this external table. Connect to SnappyData shell before running these scripts. These script files must be run from within the quickstart directory if you are providing the relative path as shown: ./bin/snappy connect client '127.0.0.1:1527'; RUN 'quickstart/scripts/create_and_load_column_table.sql'; # Use the following command to view the details of the external table. describe staging_airline; # The following output is displayed: COLUMN_NAME |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE ----------------------------------------------------------------------------------------------------------------- Year |INTEGER |0 |10 |10 |NULL |NULL |YES Month |INTEGER |0 |10 |10 |NULL |NULL |YES DayOfMonth |INTEGER |0 |10 |10 |NULL |NULL |YES DayOfWeek |INTEGER |0 |10 |10 |NULL |NULL |YES DepTime |INTEGER |0 |10 |10 |NULL |NULL |YES CRSDepTime |INTEGER |0 |10 |10 |NULL |NULL |YES ArrTime |INTEGER |0 |10 |10 |NULL |NULL |YES CRSArrTime |INTEGER |0 |10 |10 |NULL |NULL |YES UniqueCarrier |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES FlightNum |INTEGER |0 |10 |10 |NULL |NULL |YES TailNum |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES ActualElapsedTime |INTEGER |0 |10 |10 |NULL |NULL |YES CRSElapsedTime |INTEGER |0 |10 |10 |NULL |NULL |YES AirTime |INTEGER |0 |10 |10 |NULL |NULL |YES ArrDelay |INTEGER |0 |10 |10 |NULL |NULL |YES DepDelay |INTEGER |0 |10 |10 |NULL |NULL |YES Origin |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES Dest |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES Distance |INTEGER |0 |10 |10 |NULL |NULL |YES TaxiIn |INTEGER |0 |10 |10 |NULL |NULL |YES TaxiOut |INTEGER |0 |10 |10 |NULL |NULL |YES Cancelled |INTEGER |0 |10 |10 |NULL |NULL |YES CancellationCode |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES Diverted |INTEGER |0 |10 |10 |NULL |NULL |YES CarrierDelay |INTEGER |0 |10 |10 |NULL |NULL |YES WeatherDelay |INTEGER |0 |10 |10 |NULL |NULL |YES NASDelay |INTEGER |0 |10 |10 |NULL |NULL |YES SecurityDelay |INTEGER |0 |10 |10 |NULL |NULL |YES LateAircraftDelay |INTEGER |0 |10 |10 |NULL |NULL |YES ArrDelaySlot |INTEGER |0 |10 |10 |NULL |NULL |YES # Use the following command to check the number of records in the staging_airline external table: select count(*) from staging_airline; count(1) -------------------- 1000000 1 row selected You can also try the following: Create and load a row table: RUN './quickstart/scripts/create_and_load_row_table.sql'; View the status of the system: RUN './quickstart/scripts/status_queries.sql';","title":"Create Tables and Import Data Using quickstart Scripts"},{"location":"quickstart/snappydataquick_start/#create-a-column-table-using-an-external-table","text":"Similarly as the quickstart scripts, you can try to create an external table named staging_airline to load the formatted data from a airlineParquetData file with inferSchema option as true. Later, you can create a column table named airline and pull data from the external table into this table. After pulling in the data, you can check the number of records in the table. CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData', inferSchema 'true'); CREATE TABLE AIRLINE2 USING column AS (SELECT * FROM STAGING_AIRLINE); describe airline2; COLUMN_NAME |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE ----------------------------------------------------------------------------------------------------------------- YEAR |INTEGER |0 |10 |10 |NULL |NULL |YES MONTH |INTEGER |0 |10 |10 |NULL |NULL |YES DAYOFMONTH |INTEGER |0 |10 |10 |NULL |NULL |YES DAYOFWEEK |INTEGER |0 |10 |10 |NULL |NULL |YES DEPTIME |INTEGER |0 |10 |10 |NULL |NULL |YES CRSDEPTIME |INTEGER |0 |10 |10 |NULL |NULL |YES ARRTIME |INTEGER |0 |10 |10 |NULL |NULL |YES CRSARRTIME |INTEGER |0 |10 |10 |NULL |NULL |YES UNIQUECARRIER |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES FLIGHTNUM |INTEGER |0 |10 |10 |NULL |NULL |YES TAILNUM |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES ACTUALELAPSEDTIME |INTEGER |0 |10 |10 |NULL |NULL |YES CRSELAPSEDTIME |INTEGER |0 |10 |10 |NULL |NULL |YES AIRTIME |INTEGER |0 |10 |10 |NULL |NULL |YES ARRDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES DEPDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES ORIGIN |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES DEST |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES DISTANCE |INTEGER |0 |10 |10 |NULL |NULL |YES TAXIIN |INTEGER |0 |10 |10 |NULL |NULL |YES TAXIOUT |INTEGER |0 |10 |10 |NULL |NULL |YES CANCELLED |INTEGER |0 |10 |10 |NULL |NULL |YES CANCELLATIONCODE |VARCHAR |NULL |NULL |32672 |NULL |65344 |YES DIVERTED |INTEGER |0 |10 |10 |NULL |NULL |YES CARRIERDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES WEATHERDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES NASDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES SECURITYDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES LATEAIRCRAFTDELAY |INTEGER |0 |10 |10 |NULL |NULL |YES ARRDELAYSLOT |INTEGER |0 |10 |10 |NULL |NULL |YES 30 rows selected select count(*) > ; count(1) -------------------- 1 1 row selected After running these queries, you can check the table details on the SnappyData Monitoring Console Dashboards. The details of the newly created tables are displayed in the Tables section.","title":"Create a Column Table Using an External Table"},{"location":"quickstart/snappydataquick_start/#run-queries","text":"You can try a couple of analytical queries as shown: Query to find the average arrival delay. select avg(arrdelay) from airline; avg(ARRDELAY) ---------------------- 6.735443 1 row selected * Query for avg arrival delay of a specific airline. select max(arrdelay) from airline where DEST = ''; max(ARRDELAY) ------------- NULL 1 row selected","title":"Run Queries"},{"location":"quickstart/snappydataquick_start/#submit-snappy-jobs","text":"Instructions to submit Snappy jobs are available here . There are sample jobs available for reference. You can import the examples into a separate independent gradle project and submit the jobs to the cluster or run within an IDE. Refer to the instructions here .","title":"Submit Snappy Jobs"},{"location":"quickstart/snappydataquick_start/#add-servers-into-cluster","text":"You can add more than one server to a cluster. To add a new server, do the following: Go to SnappyData home directory. cd <snappydata_install_dir> Create a configuration file named servers in the conf folder in the the SnappyData home directory. To do so, you can copy the existing template files servers.template and rename it to servers as shown: cp -f conf/servers.template conf/servers Open this file using a vi editor and add a hostname entry of the additional server, after the entry of the primary server, and save the file. For example, suppose there is an entry localhost in this file for the primary server. You can add an entry localhost below this entry for the additional server. The servers file should contain the hostnames of the nodes (one per line) where you intend to start the member. From the SnappyData home directory, start the cluster again using the ./sbin/snappy-start-all.sh command. The new server gets started. Ignore the error messages of the other nodes that are already running. You can check details of the newly added member from the SnappyData Monitoring Console.","title":"Add Servers into Cluster"},{"location":"quickstart/snappydataquick_start/#rebalancing-data-on-servers","text":"Further, you can distribute the data among the servers in the cluster. This ensures that each server carries almost equal data. To balance the data equally on the servers, do the following: Go to SnappyData home directory. cd <snappydata_install_dir> Connect to snappy shell and obtain the jdbc client connection. Run the rebalance command. call sys.rebalance_all_buckets(); On SnappyData Monitoring Console, check the Heap Memory Used/Total column for the servers. You will notice that before rebalancing the data, there was an unequal distribution of the memory usage and after running the rebalance command, the data is distributed equally among both the servers. Before Rebalance After Rebalance","title":"Rebalancing Data on Servers"},{"location":"quickstart/snappydataquick_start/#stop-the-cluster","text":"You can stop the cluster using the ./sbin/snappy-stop-all.sh command: ./sbin/snappy-stop-all.sh The SnappyData Leader has stopped. The SnappyData Server has stopped. The SnappyData Locator has stopped. For more details, refer to Stopping the Cluster","title":"Stop the Cluster"},{"location":"quickstart/structucture_streamingquickstart/","text":"Structured Streaming Quick Reference \u00b6 This quick start guide provides step-by-step instructions to perform structured streaming in SnappyData by using the Spark shell as well as through a Snappy job. For detailed information, refer to Structured Streaming . Structured Streaming using Spark Shell \u00b6 Following are the steps to perform structured streaming using Spark shell: Start Snappydata cluster using the following command. ./sbin/snappy-start-all Open a new terminal window and start Netcat connection listening to TCP port 9999: nc -lk 9999 Produce some input data in JSON format and keep the terminal running with Netcat connection. Example : {\"id\":\"device1\", \"signal\":10} {\"id\":\"device2\", \"signal\":20} {\"id\":\"device3\", \"signal\":30} Open a new terminal window, go to Snappydata distribution directory and start Spark shell using the following command : ./bin/spark-shell --master local[*] --conf spark.snappydata.connection=localhost:1527 Execute the following code to start a structure streaming query from Spark shell: import org.apache.spark.sql.SnappySession import org.apache.spark.sql.functions.from_json import org.apache.spark.sql.streaming.ProcessingTime val snappy = new SnappySession(sc) // Create target snappy table. Stream data will be ingested in this table snappy.sql(\"create table if not exists devices(id string, signal int) using column\") val schema = snappy.table(\"devices\").schema // Create streaming DataFrame representing the stream of input lines from socket connection val df = snappy. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load() // Start the execution of streaming query val streamingQuery = df. select(from_json(df.col(\"value\").cast(\"string\"), schema).alias(\"jsonObject\")). selectExpr(\"jsonObject.*\"). writeStream. format(\"snappysink\"). queryName(\"deviceStream\"). // must be unique across the Snappydata cluster trigger(ProcessingTime(\"1 seconds\")). option(\"tableName\", \"devices\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). start() Open a new terminal window, navigate to Snappydata distribution directory and start Snappy SQL: ./bin/snappy-sql Connect to running Snappydata cluster using the following command: connect client 'localhost:1527'; Check whether the data produced from Netcat connection is getting ingested in the target table: select * from devices; You can produce some more data on the Netcat connection in JSON format and check whether it is getting ingested in the devices table. To stop the streaming query, run the following command in the Spark shell terminal where you started the streaming query. streamingQuery.stop Structured Streaming with Kafka Source \u00b6 Assuming that your Kafka cluster is already setup and running, you can use the following steps to run a structured streaming query using Spark shell: Start Snappydata cluster using following command. ./sbin/snappy-start-all Create a topic named \"devices\": ./bin/kafka-topics --create --zookeeper zookeper_server:2181 --partitions 4 --replication-factor 1 --topic devices Start a console produce in new terminal window and produce some data in JSON format: ./bin/kafka-console-producer --broker-list kafka_broker:9092 --topic devices Example data: {\"id\":\"device1\", \"signal\":10} {\"id\":\"device2\", \"signal\":20} {\"id\":\"device3\", \"signal\":30} Open a new terminal window, go to Snappydata distribution directory and start Spark shell using the following command: ./bin/spark-shell --master local[*] --conf spark.snappydata.connection=localhost:1527 Execute the following code to start a structure streaming query from spark-shell: import org.apache.spark.sql.SnappySession import org.apache.spark.sql.functions.from_json import org.apache.spark.sql.streaming.ProcessingTime val snappy = new SnappySession(sc) // Create target snappy table. Stream data will be ingested in this table. snappy.sql(\"create table if not exists devices(id string, signal int) using column\") val schema = snappy.table(\"devices\").schema // Create DataFrame representing the stream of input lines from Kafka topic 'devices' val df = snappy. readStream. format(\"kafka\"). option(\"kafka.bootstrap.servers\", \"kafka_broker:9092\"). option(\"startingOffsets\", \"earliest\"). option(\"subscribe\", \"devices\"). option(\"maxOffsetsPerTrigger\", 100). // to restrict the batch size load() // Start the execution of streaming query val streamingQuery = df. select(from_json(df.col(\"value\").cast(\"string\"), schema).alias(\"jsonObject\")). selectExpr(\"jsonObject.*\"). writeStream. format(\"snappysink\"). queryName(\"deviceStream\"). // must be unique across the Snappydata cluster trigger(ProcessingTime(\"1 seconds\")). option(\"tableName\", \"devices\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). start() Open a new terminal window, navigate to Snappydata distribution directory and start Snappy SQL: ./bin/snappy-sql Connect to the running Snappydata cluster using the following command: connect client 'localhost:1527'; Check whether the data produced from netcat connection is getting ingested in the target table: select * from devices; You can produce some more data on the Netcat connection in JSON format and check whether it is getting ingested in the devices` table. To stop the streaming query, run the following command in the Spark shell terminal where you started the streaming query. streamingQuery.stop Structured Streaming using Snappy Job \u00b6 Refer to SnappyData Jobs for more information about Snappy Jobs. Following is a Snappy job code that contains similarly structured streaming query: package io.snappydata import com.typesafe.config.Config import org.apache.spark.sql.functions.from_json import org.apache.spark.sql.streaming.ProcessingTime import org.apache.spark.sql.{SnappyJobValid, SnappyJobValidation, SnappySQLJob, SnappySession} object Example extends SnappySQLJob { override def isValidJob(snappy: SnappySession, config: Config): SnappyJobValidation = SnappyJobValid() override def runSnappyJob(snappy: SnappySession, jobConfig: Config): Any = { // Create target snappy table. Stream data will be ingested in this table snappy.sql(\"create table if not exists devices(id string, signal int) using column\") val schema = snappy.table(\"devices\").schema // Create DataFrame representing the stream of input lines from socket connection val df = snappy. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load() // start the execution of streaming query val streamingQuery = df. select(from_json(df.col(\"value\").cast(\"string\"), schema).alias(\"jsonObject\")). selectExpr(\"jsonObject.*\"). writeStream. format(\"snappysink\"). queryName(\"deviceStream\"). // must be unique across the Snappydata cluster trigger(ProcessingTime(\"1 seconds\")). option(\"tableName\", \"devices\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). start() streamingQuery.awaitTermination() } } Package the above code as part of a jar and submit using the following command: ./bin/snappy-job.sh submit --app-name exampleApp --class io.snappydata.Example --app-jar /path/to/application.jar Output: OKOK{ \"status\": \"STARTED\", \"result\": { \"jobId\": \"6cdce50f-e86d-4da0-a34c-3804f7c6155b\", \"context\": \"snappyContext1561122043543655176\" } } Use the following command to stop the running job: ./bin/snappy-job.sh stop --job-id 6cdce50f-e86d-4da0-a34c-3804f7c6155b Note The job-id used for stopping the job is picked from the job submission response. Examples \u00b6 For more examples, refer to structured streaming examples . The following examples are shown: Example Description CDCExample.scala An example explaining CDC (change data capture) use case with SnappyData streaming Sink. CSVFileSourceExampleWithSnappySink.scala An example of structured streaming depicting CSV file processing with Snappy Sink. CSVKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from kafka source using snappy Sink. JSONFileSourceExampleWithSnappySink.scala An example of structured streaming depicting JSON file processing with Snappy Sink. JSONKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from Kafka source using Snappy Sink SocketSourceExample.scala An example showing usage of structured streaming with console Sink. SocketSourceExampleWithSnappySink.scala An example showing usage of structured streaming with SnappyData.","title":"Structured Streaming"},{"location":"quickstart/structucture_streamingquickstart/#structured-streaming-quick-reference","text":"This quick start guide provides step-by-step instructions to perform structured streaming in SnappyData by using the Spark shell as well as through a Snappy job. For detailed information, refer to Structured Streaming .","title":"Structured Streaming Quick Reference"},{"location":"quickstart/structucture_streamingquickstart/#structured-streaming-using-spark-shell","text":"Following are the steps to perform structured streaming using Spark shell: Start Snappydata cluster using the following command. ./sbin/snappy-start-all Open a new terminal window and start Netcat connection listening to TCP port 9999: nc -lk 9999 Produce some input data in JSON format and keep the terminal running with Netcat connection. Example : {\"id\":\"device1\", \"signal\":10} {\"id\":\"device2\", \"signal\":20} {\"id\":\"device3\", \"signal\":30} Open a new terminal window, go to Snappydata distribution directory and start Spark shell using the following command : ./bin/spark-shell --master local[*] --conf spark.snappydata.connection=localhost:1527 Execute the following code to start a structure streaming query from Spark shell: import org.apache.spark.sql.SnappySession import org.apache.spark.sql.functions.from_json import org.apache.spark.sql.streaming.ProcessingTime val snappy = new SnappySession(sc) // Create target snappy table. Stream data will be ingested in this table snappy.sql(\"create table if not exists devices(id string, signal int) using column\") val schema = snappy.table(\"devices\").schema // Create streaming DataFrame representing the stream of input lines from socket connection val df = snappy. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load() // Start the execution of streaming query val streamingQuery = df. select(from_json(df.col(\"value\").cast(\"string\"), schema).alias(\"jsonObject\")). selectExpr(\"jsonObject.*\"). writeStream. format(\"snappysink\"). queryName(\"deviceStream\"). // must be unique across the Snappydata cluster trigger(ProcessingTime(\"1 seconds\")). option(\"tableName\", \"devices\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). start() Open a new terminal window, navigate to Snappydata distribution directory and start Snappy SQL: ./bin/snappy-sql Connect to running Snappydata cluster using the following command: connect client 'localhost:1527'; Check whether the data produced from Netcat connection is getting ingested in the target table: select * from devices; You can produce some more data on the Netcat connection in JSON format and check whether it is getting ingested in the devices table. To stop the streaming query, run the following command in the Spark shell terminal where you started the streaming query. streamingQuery.stop","title":"Structured Streaming using Spark Shell"},{"location":"quickstart/structucture_streamingquickstart/#structured-streaming-with-kafka-source","text":"Assuming that your Kafka cluster is already setup and running, you can use the following steps to run a structured streaming query using Spark shell: Start Snappydata cluster using following command. ./sbin/snappy-start-all Create a topic named \"devices\": ./bin/kafka-topics --create --zookeeper zookeper_server:2181 --partitions 4 --replication-factor 1 --topic devices Start a console produce in new terminal window and produce some data in JSON format: ./bin/kafka-console-producer --broker-list kafka_broker:9092 --topic devices Example data: {\"id\":\"device1\", \"signal\":10} {\"id\":\"device2\", \"signal\":20} {\"id\":\"device3\", \"signal\":30} Open a new terminal window, go to Snappydata distribution directory and start Spark shell using the following command: ./bin/spark-shell --master local[*] --conf spark.snappydata.connection=localhost:1527 Execute the following code to start a structure streaming query from spark-shell: import org.apache.spark.sql.SnappySession import org.apache.spark.sql.functions.from_json import org.apache.spark.sql.streaming.ProcessingTime val snappy = new SnappySession(sc) // Create target snappy table. Stream data will be ingested in this table. snappy.sql(\"create table if not exists devices(id string, signal int) using column\") val schema = snappy.table(\"devices\").schema // Create DataFrame representing the stream of input lines from Kafka topic 'devices' val df = snappy. readStream. format(\"kafka\"). option(\"kafka.bootstrap.servers\", \"kafka_broker:9092\"). option(\"startingOffsets\", \"earliest\"). option(\"subscribe\", \"devices\"). option(\"maxOffsetsPerTrigger\", 100). // to restrict the batch size load() // Start the execution of streaming query val streamingQuery = df. select(from_json(df.col(\"value\").cast(\"string\"), schema).alias(\"jsonObject\")). selectExpr(\"jsonObject.*\"). writeStream. format(\"snappysink\"). queryName(\"deviceStream\"). // must be unique across the Snappydata cluster trigger(ProcessingTime(\"1 seconds\")). option(\"tableName\", \"devices\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). start() Open a new terminal window, navigate to Snappydata distribution directory and start Snappy SQL: ./bin/snappy-sql Connect to the running Snappydata cluster using the following command: connect client 'localhost:1527'; Check whether the data produced from netcat connection is getting ingested in the target table: select * from devices; You can produce some more data on the Netcat connection in JSON format and check whether it is getting ingested in the devices` table. To stop the streaming query, run the following command in the Spark shell terminal where you started the streaming query. streamingQuery.stop","title":"Structured Streaming  with Kafka Source"},{"location":"quickstart/structucture_streamingquickstart/#structured-streaming-using-snappy-job","text":"Refer to SnappyData Jobs for more information about Snappy Jobs. Following is a Snappy job code that contains similarly structured streaming query: package io.snappydata import com.typesafe.config.Config import org.apache.spark.sql.functions.from_json import org.apache.spark.sql.streaming.ProcessingTime import org.apache.spark.sql.{SnappyJobValid, SnappyJobValidation, SnappySQLJob, SnappySession} object Example extends SnappySQLJob { override def isValidJob(snappy: SnappySession, config: Config): SnappyJobValidation = SnappyJobValid() override def runSnappyJob(snappy: SnappySession, jobConfig: Config): Any = { // Create target snappy table. Stream data will be ingested in this table snappy.sql(\"create table if not exists devices(id string, signal int) using column\") val schema = snappy.table(\"devices\").schema // Create DataFrame representing the stream of input lines from socket connection val df = snappy. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load() // start the execution of streaming query val streamingQuery = df. select(from_json(df.col(\"value\").cast(\"string\"), schema).alias(\"jsonObject\")). selectExpr(\"jsonObject.*\"). writeStream. format(\"snappysink\"). queryName(\"deviceStream\"). // must be unique across the Snappydata cluster trigger(ProcessingTime(\"1 seconds\")). option(\"tableName\", \"devices\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). start() streamingQuery.awaitTermination() } } Package the above code as part of a jar and submit using the following command: ./bin/snappy-job.sh submit --app-name exampleApp --class io.snappydata.Example --app-jar /path/to/application.jar Output: OKOK{ \"status\": \"STARTED\", \"result\": { \"jobId\": \"6cdce50f-e86d-4da0-a34c-3804f7c6155b\", \"context\": \"snappyContext1561122043543655176\" } } Use the following command to stop the running job: ./bin/snappy-job.sh stop --job-id 6cdce50f-e86d-4da0-a34c-3804f7c6155b Note The job-id used for stopping the job is picked from the job submission response.","title":"Structured Streaming using Snappy Job"},{"location":"quickstart/structucture_streamingquickstart/#examples","text":"For more examples, refer to structured streaming examples . The following examples are shown: Example Description CDCExample.scala An example explaining CDC (change data capture) use case with SnappyData streaming Sink. CSVFileSourceExampleWithSnappySink.scala An example of structured streaming depicting CSV file processing with Snappy Sink. CSVKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from kafka source using snappy Sink. JSONFileSourceExampleWithSnappySink.scala An example of structured streaming depicting JSON file processing with Snappy Sink. JSONKafkaSourceExampleWithSnappySink.scala An example of structured streaming depicting processing of JSON coming from Kafka source using Snappy Sink SocketSourceExample.scala An example showing usage of structured streaming with console Sink. SocketSourceExampleWithSnappySink.scala An example showing usage of structured streaming with SnappyData.","title":"Examples"},{"location":"quickstart/using_spark_scala_apis/","text":"Using Spark Scala APIs \u00b6 Create a SnappySession SnappySession extends the SparkSession so you can mutate data, get much higher performance, etc. scala> val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext) // Import Snappy extensions scala> import snappy.implicits._ Create a dataset using the Spark APIs scala> val ds = Seq((1,\"a\"), (2, \"b\"), (3, \"c\")).toDS() Define a schema for the table scala> import org.apache.spark.sql.types._ scala> val tableSchema = StructType(Array(StructField(\"CustKey\", IntegerType, false), StructField(\"CustName\", StringType, false))) Create a \"column\" table with a simple schema [String, Int] and default options For detailed option refer to the Row and Column Tables section. // Column tables manage data is columnar form and offer superior performance for analytic class queries. scala> snappy.createTable(tableName = \"colTable\", provider = \"column\", // Create a SnappyData Column table schema = tableSchema, options = Map.empty[String, String], // Map for options allowExisting = false) SnappyData (SnappySession) extends SparkSession, so you can simply use all the Spark's APIs. Insert the created DataSet to the column table \"colTable\" scala> ds.write.insertInto(\"colTable\") // Check the total row count. scala> snappy.table(\"colTable\").count Create a row object using Spark's API and insert the row into the table Unlike Spark DataFrames SnappyData column tables are mutable. You can insert new rows to a column table. // Insert a new record scala> import org.apache.spark.sql.Row scala> snappy.insert(\"colTable\", Row(10, \"f\")) // Check the total row count after inserting the row scala> snappy.table(\"colTable\").count Create a \"row\" table with a simple schema [String, Int] and default options For detailed option refer to the Row and Column Tables section. // Row formatted tables are better when datasets constantly change or access is selective (like based on a key) scala> snappy.createTable(tableName = \"rowTable\", provider = \"row\", schema = tableSchema, options = Map.empty[String, String], allowExisting = false) Insert the created DataSet to the row table \"rowTable\" scala> ds.write.insertInto(\"rowTable\") // Check the row count scala> snappy.table(\"rowTable\").count Insert a new record scala> snappy.insert(\"rowTable\", Row(4, \"d\")) //Check the row count now scala> snappy.table(\"rowTable\").count Change some data in the row table // Updating a row for customer with custKey = 1 scala> snappy.update(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\", newColumnValues = Row(\"d\"), updateColumns = \"CUSTNAME\") scala> snappy.table(\"rowTable\").orderBy(\"CUSTKEY\").show // Delete the row for customer with custKey = 1 scala> snappy.delete(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\") // Drop the existing tables scala> snappy.dropTable(\"rowTable\", ifExists = true) scala> snappy.dropTable(\"colTable\", ifExists = true)","title":"Using Spark Scala APIs"},{"location":"quickstart/using_spark_scala_apis/#using-spark-scala-apis","text":"Create a SnappySession SnappySession extends the SparkSession so you can mutate data, get much higher performance, etc. scala> val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext) // Import Snappy extensions scala> import snappy.implicits._ Create a dataset using the Spark APIs scala> val ds = Seq((1,\"a\"), (2, \"b\"), (3, \"c\")).toDS() Define a schema for the table scala> import org.apache.spark.sql.types._ scala> val tableSchema = StructType(Array(StructField(\"CustKey\", IntegerType, false), StructField(\"CustName\", StringType, false))) Create a \"column\" table with a simple schema [String, Int] and default options For detailed option refer to the Row and Column Tables section. // Column tables manage data is columnar form and offer superior performance for analytic class queries. scala> snappy.createTable(tableName = \"colTable\", provider = \"column\", // Create a SnappyData Column table schema = tableSchema, options = Map.empty[String, String], // Map for options allowExisting = false) SnappyData (SnappySession) extends SparkSession, so you can simply use all the Spark's APIs. Insert the created DataSet to the column table \"colTable\" scala> ds.write.insertInto(\"colTable\") // Check the total row count. scala> snappy.table(\"colTable\").count Create a row object using Spark's API and insert the row into the table Unlike Spark DataFrames SnappyData column tables are mutable. You can insert new rows to a column table. // Insert a new record scala> import org.apache.spark.sql.Row scala> snappy.insert(\"colTable\", Row(10, \"f\")) // Check the total row count after inserting the row scala> snappy.table(\"colTable\").count Create a \"row\" table with a simple schema [String, Int] and default options For detailed option refer to the Row and Column Tables section. // Row formatted tables are better when datasets constantly change or access is selective (like based on a key) scala> snappy.createTable(tableName = \"rowTable\", provider = \"row\", schema = tableSchema, options = Map.empty[String, String], allowExisting = false) Insert the created DataSet to the row table \"rowTable\" scala> ds.write.insertInto(\"rowTable\") // Check the row count scala> snappy.table(\"rowTable\").count Insert a new record scala> snappy.insert(\"rowTable\", Row(4, \"d\")) //Check the row count now scala> snappy.table(\"rowTable\").count Change some data in the row table // Updating a row for customer with custKey = 1 scala> snappy.update(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\", newColumnValues = Row(\"d\"), updateColumns = \"CUSTNAME\") scala> snappy.table(\"rowTable\").orderBy(\"CUSTKEY\").show // Delete the row for customer with custKey = 1 scala> snappy.delete(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\") // Drop the existing tables scala> snappy.dropTable(\"rowTable\", ifExists = true) scala> snappy.dropTable(\"colTable\", ifExists = true)","title":"Using Spark Scala APIs"},{"location":"quickstart/using_sql/","text":"Using SQL \u00b6 In this section, you can also connect to SQL using Snappy Session API. You can use any SQL client tool (for example, Snappy shell). For an example, refer to the How-to section. Create a column table with a simple schema [Int, String] and default options For more information on the available options, refer to the Row and Column Tables section. scala> snappy.sql(\"create table colTable(CustKey Integer, CustName String) using column options()\") //Insert couple of records to the column table scala> snappy.sql(\"insert into colTable values(1, 'a')\") scala> snappy.sql(\"insert into colTable values(2, 'b')\") scala> snappy.sql(\"insert into colTable values(3, '3')\") // Check the total row count now scala> snappy.sql(\"select count(*) from colTable\").show Create a row table with a primary key : // Row formatted tables are better when data sets constantly change or access is selective (like based on a key). scala> snappy.sql(\"create table rowTable(CustKey Integer NOT NULL PRIMARY KEY, \" + \"CustName String) using row options()\") If you create a table using standard SQL (that is, no 'row options' clause) it creates a replicated row table. //Insert couple of records to the row table scala> snappy.sql(\"insert into rowTable values(1, 'a')\") scala> snappy.sql(\"insert into rowTable values(2, 'b')\") scala> snappy.sql(\"insert into rowTable values(3, '3')\") //Update some rows scala> snappy.sql(\"update rowTable set CustName='d' where custkey = 1\") scala> snappy.sql(\"select * from rowTable order by custkey\").show //Drop the existing tables scala> snappy.sql(\"drop table if exists rowTable \") scala> snappy.sql(\"drop table if exists colTable \") scala> :q //Quit the Spark Shell Now that you have seen the basic working of SnappyData tables, let us run the benchmark code to see the performance of SnappyData and compare it to Spark's native cache performance. More Information \u00b6 For more examples of the common operations, you can refer to the How-tos section. If you have questions or queries you can contact us through our community channels .","title":"Using SQL"},{"location":"quickstart/using_sql/#using-sql","text":"In this section, you can also connect to SQL using Snappy Session API. You can use any SQL client tool (for example, Snappy shell). For an example, refer to the How-to section. Create a column table with a simple schema [Int, String] and default options For more information on the available options, refer to the Row and Column Tables section. scala> snappy.sql(\"create table colTable(CustKey Integer, CustName String) using column options()\") //Insert couple of records to the column table scala> snappy.sql(\"insert into colTable values(1, 'a')\") scala> snappy.sql(\"insert into colTable values(2, 'b')\") scala> snappy.sql(\"insert into colTable values(3, '3')\") // Check the total row count now scala> snappy.sql(\"select count(*) from colTable\").show Create a row table with a primary key : // Row formatted tables are better when data sets constantly change or access is selective (like based on a key). scala> snappy.sql(\"create table rowTable(CustKey Integer NOT NULL PRIMARY KEY, \" + \"CustName String) using row options()\") If you create a table using standard SQL (that is, no 'row options' clause) it creates a replicated row table. //Insert couple of records to the row table scala> snappy.sql(\"insert into rowTable values(1, 'a')\") scala> snappy.sql(\"insert into rowTable values(2, 'b')\") scala> snappy.sql(\"insert into rowTable values(3, '3')\") //Update some rows scala> snappy.sql(\"update rowTable set CustName='d' where custkey = 1\") scala> snappy.sql(\"select * from rowTable order by custkey\").show //Drop the existing tables scala> snappy.sql(\"drop table if exists rowTable \") scala> snappy.sql(\"drop table if exists colTable \") scala> :q //Quit the Spark Shell Now that you have seen the basic working of SnappyData tables, let us run the benchmark code to see the performance of SnappyData and compare it to Spark's native cache performance.","title":"Using SQL"},{"location":"quickstart/using_sql/#more-information","text":"For more examples of the common operations, you can refer to the How-tos section. If you have questions or queries you can contact us through our community channels .","title":"More Information"},{"location":"reference/","text":"Reference Guides \u00b6 The SnappyData Reference provides a list and description of command-line utilities, APIs, and Standard Query Language (SQL) implementation. The following topics are covered in this section: SQL Reference Guide SQL Functions Built-in System Procedures System Tables Command Line Utilities Snappy-SQL Shell Interactive Commands Configuration Parameters SnappyData Spark extension API Reference Guide Full API Reference ODBC Supported API Supported Data Types Configuring SSH Login without Password Setting Up SnappyData ODBC Driver Setting Up SnappyData JDBC Client and QlikView Tip SnappyData supports all the built-in functions supported by Spark. For more information, refer to the SQL Functions page and Apache Spark Documentation .","title":"Index"},{"location":"reference/#reference-guides","text":"The SnappyData Reference provides a list and description of command-line utilities, APIs, and Standard Query Language (SQL) implementation. The following topics are covered in this section: SQL Reference Guide SQL Functions Built-in System Procedures System Tables Command Line Utilities Snappy-SQL Shell Interactive Commands Configuration Parameters SnappyData Spark extension API Reference Guide Full API Reference ODBC Supported API Supported Data Types Configuring SSH Login without Password Setting Up SnappyData ODBC Driver Setting Up SnappyData JDBC Client and QlikView Tip SnappyData supports all the built-in functions supported by Spark. For more information, refer to the SQL Functions page and Apache Spark Documentation .","title":"Reference Guides"},{"location":"reference/sql_general_limitations/","text":"Limitations \u00b6 The following SQL general limitations are observed in SnappyData: For row tables without primary key, DML operations that use Spark functions are not supported The syntax, INSERT INTO <table><(col1,...)> values (\u2026 ) , cannot contain Spark functions in the values clause . For complex data types (ARRAY, MAP, STRUCT), values to be inserted can not be directly used in the values clause of INSERT INTO <table> values (x, y, z \u2026 ) For row tables without primary key, DML operations that use Spark functions are not supported \u00b6 Note This limitation applies only to row tables. For column tables such DML operations are supported. In the current release of SnappyData, row tables must contain a primary key column, if a DML operation on the table uses Spark function. For example, functions such as spark_partition_id() , current_timestamp() . In the following example, table1 is a row table without primary key. As shown, in such a case UPDATE operations that use Spark functions produce an error: snappy> create table table1(c1 int, c2 timestamp, c3 string) using row options (partition_by 'c1'); snappy> insert into table1 values(1, '2019-07-22 14:29:22.432', 'value1'); 1 row inserted/updated/deleted snappy> insert into table1 values(2, '2019-07-22 14:29:22.432', 'value2'); 1 row inserted/updated/deleted snappy> update table1 set c3 = 'value3' where SPARK_PARTITION_ID() = 6; ERROR 42Y03: (SQLState=42Y03 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-0) 'SPARK_PARTITION_ID)' is not recognized as a function or procedure. snappy> update table1 set c2 = current_timestamp() where c1 = 2; ERROR 42X01: (SQLState=42X01 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-0) Syntax error: Encountered \"(\" at line 1, column 41. However, if table1 contains a primary key, then the DML operations are supported. In the following example, table1 is now created with column c1 as a primary key so the UPDATE operations succeed. snappy> create table table1(c1 int primary key, c2 timestamp, c3 string) using row options (partition_by 'c1'); snappy> insert into table1 values(1, '2019-07-22 14:29:22.432', 'value1'); 1 row inserted/updated/deleted snappy> insert into table1 values(2, '2019-07-22 14:29:22.432', 'value2'); 1 row inserted/updated/deleted snappy> update table1 set c3 = 'value3' where SPARK_PARTITION_ID() = 6; 1 row inserted/updated/deleted snappy> update table1 set c2 = current_timestamp() where c1 = 2; 1 row inserted/updated/deleted snappy> select * from table1; c1 |c2 |c3 ------------------------------------------------------ 1 |2019-07-22 14:29:22.432 |value1 2 |2019-07-22 14:36:47.879 |value3 2 rows selected The syntax, INSERT INTO <table><(col1,...)> values (\u2026 ) , cannot contain Spark functions in the values clause \u00b6 The value clause of INSERT INTO <table><(col1,...)> values (\u2026 ) operation can not contain Spark functions. In such a case, use syntax INSERT INTO <table> SELECT <> syntax. In the following example, insert operation fails as current_timestamp() function is used in the values: snappy> create table table1(c1 int, c2 timestamp, c3 string) using row options (partition_by 'c1'); snappy> insert into table1(c1, c2, c3) values(1, current_timestamp(), 'value1'); ERROR 38000: (SQLState=38000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-0) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(835)<v1>:23712, caused by java.lang.AssertionError: assertion failed: No plan for DMLExternalTable INSERT INTO \"TABLE1\"(\"C1\", \"C2\", \"C3\") values(1, current_timestamp(), 'value1') +- LocalRelation [col1#42, col2#43, col3#44] ' was thrown while evaluating an expression. However, the following syntax works: snappy> insert into table1 select 1, current_timestamp(), 'value1'; 1 row inserted/updated/deleted snappy> select * from table1; c1 |c2 |c3 ------------------------------------------------------ 1 |2019-07-22 14:49:20.022 |value1 1 row selected For complex data types (ARRAY, MAP, STRUCT), values to be inserted can not be directly used in the values clause of INSERT INTO <table> values (x, y, z \u2026 ) \u00b6 To insert values using Snappy shell or a SQL client use insert into <table> select syntax. For example: # create a table with column of type MAP and insert few records snappy> CREATE TABLE IF NOT EXISTS StudentGrades (rollno Integer, name String, Course Map<String, String>) USING column; snappy> INSERT INTO StudentGrades SELECT 1,'Jim', Map('English', 'A+'); 1 row inserted/updated/deleted # create a table with column of type ARRAY snappy> CREATE TABLE IF NOT EXISTS Student(rollno Int, name String, marks Array<Double>) USING column; snappy> INSERT INTO Student SELECT 1,'John', Array(97.8,85.2,63.9,45.2,75.2,96.5); 1 row inserted/updated/deleted # create a table with column of type STRUCT snappy> CREATE TABLE IF NOT EXISTS StocksInfo (SYMBOL STRING, INFO STRUCT<TRADING_YEAR: STRING, AVG_DAILY_VOLUME: LONG, HIGHEST_PRICE_IN_YEAR: INT, LOWEST_PRICE_IN_YEAR: INT>) USING COLUMN; snappy> INSERT INTO StocksInfo SELECT 'ORD', STRUCT('2018', '400000', '112', '52'); 1 row inserted/updated/deleted The following syntax will produce an error: snappy> insert into StudentGrades values (1, 'Jim',Map('English', 'A', 'Science', 'B')); ERROR 42Y03: (SQLState=42Y03 Severity=20000) (Server=localhost/127.0.0.1[1529] Thread=ThriftProcessor-0) 'MAP(java.lang.String,java.lang.String,java.lang.String,java.lang.String)' is not recognized as a function or procedure. For more details on complex datatypes, refer to Supported datatypes and how to store and retrieve complex data types using ComplexTypeSerializer class .","title":"SQL General Limitations"},{"location":"reference/sql_general_limitations/#limitations","text":"The following SQL general limitations are observed in SnappyData: For row tables without primary key, DML operations that use Spark functions are not supported The syntax, INSERT INTO <table><(col1,...)> values (\u2026 ) , cannot contain Spark functions in the values clause . For complex data types (ARRAY, MAP, STRUCT), values to be inserted can not be directly used in the values clause of INSERT INTO <table> values (x, y, z \u2026 )","title":"Limitations"},{"location":"reference/sql_general_limitations/#for-row-tables-without-primary-key-dml-operations-that-use-spark-functions-are-not-supported","text":"Note This limitation applies only to row tables. For column tables such DML operations are supported. In the current release of SnappyData, row tables must contain a primary key column, if a DML operation on the table uses Spark function. For example, functions such as spark_partition_id() , current_timestamp() . In the following example, table1 is a row table without primary key. As shown, in such a case UPDATE operations that use Spark functions produce an error: snappy> create table table1(c1 int, c2 timestamp, c3 string) using row options (partition_by 'c1'); snappy> insert into table1 values(1, '2019-07-22 14:29:22.432', 'value1'); 1 row inserted/updated/deleted snappy> insert into table1 values(2, '2019-07-22 14:29:22.432', 'value2'); 1 row inserted/updated/deleted snappy> update table1 set c3 = 'value3' where SPARK_PARTITION_ID() = 6; ERROR 42Y03: (SQLState=42Y03 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-0) 'SPARK_PARTITION_ID)' is not recognized as a function or procedure. snappy> update table1 set c2 = current_timestamp() where c1 = 2; ERROR 42X01: (SQLState=42X01 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-0) Syntax error: Encountered \"(\" at line 1, column 41. However, if table1 contains a primary key, then the DML operations are supported. In the following example, table1 is now created with column c1 as a primary key so the UPDATE operations succeed. snappy> create table table1(c1 int primary key, c2 timestamp, c3 string) using row options (partition_by 'c1'); snappy> insert into table1 values(1, '2019-07-22 14:29:22.432', 'value1'); 1 row inserted/updated/deleted snappy> insert into table1 values(2, '2019-07-22 14:29:22.432', 'value2'); 1 row inserted/updated/deleted snappy> update table1 set c3 = 'value3' where SPARK_PARTITION_ID() = 6; 1 row inserted/updated/deleted snappy> update table1 set c2 = current_timestamp() where c1 = 2; 1 row inserted/updated/deleted snappy> select * from table1; c1 |c2 |c3 ------------------------------------------------------ 1 |2019-07-22 14:29:22.432 |value1 2 |2019-07-22 14:36:47.879 |value3 2 rows selected","title":"For row tables without primary key, DML operations that use Spark functions are not supported"},{"location":"reference/sql_general_limitations/#the-syntax-insert-into-tablecol1-values-cannot-contain-spark-functions-in-the-values-clause","text":"The value clause of INSERT INTO <table><(col1,...)> values (\u2026 ) operation can not contain Spark functions. In such a case, use syntax INSERT INTO <table> SELECT <> syntax. In the following example, insert operation fails as current_timestamp() function is used in the values: snappy> create table table1(c1 int, c2 timestamp, c3 string) using row options (partition_by 'c1'); snappy> insert into table1(c1, c2, c3) values(1, current_timestamp(), 'value1'); ERROR 38000: (SQLState=38000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-0) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(835)<v1>:23712, caused by java.lang.AssertionError: assertion failed: No plan for DMLExternalTable INSERT INTO \"TABLE1\"(\"C1\", \"C2\", \"C3\") values(1, current_timestamp(), 'value1') +- LocalRelation [col1#42, col2#43, col3#44] ' was thrown while evaluating an expression. However, the following syntax works: snappy> insert into table1 select 1, current_timestamp(), 'value1'; 1 row inserted/updated/deleted snappy> select * from table1; c1 |c2 |c3 ------------------------------------------------------ 1 |2019-07-22 14:49:20.022 |value1 1 row selected","title":"The syntax, INSERT INTO &lt;table&gt;&lt;(col1,...)&gt; values (\u2026 ), cannot contain Spark functions in the values clause"},{"location":"reference/sql_general_limitations/#for-complex-data-types-array-map-struct-values-to-be-inserted-can-not-be-directly-used-in-the-values-clause-of-insert-into-table-values-x-y-z","text":"To insert values using Snappy shell or a SQL client use insert into <table> select syntax. For example: # create a table with column of type MAP and insert few records snappy> CREATE TABLE IF NOT EXISTS StudentGrades (rollno Integer, name String, Course Map<String, String>) USING column; snappy> INSERT INTO StudentGrades SELECT 1,'Jim', Map('English', 'A+'); 1 row inserted/updated/deleted # create a table with column of type ARRAY snappy> CREATE TABLE IF NOT EXISTS Student(rollno Int, name String, marks Array<Double>) USING column; snappy> INSERT INTO Student SELECT 1,'John', Array(97.8,85.2,63.9,45.2,75.2,96.5); 1 row inserted/updated/deleted # create a table with column of type STRUCT snappy> CREATE TABLE IF NOT EXISTS StocksInfo (SYMBOL STRING, INFO STRUCT<TRADING_YEAR: STRING, AVG_DAILY_VOLUME: LONG, HIGHEST_PRICE_IN_YEAR: INT, LOWEST_PRICE_IN_YEAR: INT>) USING COLUMN; snappy> INSERT INTO StocksInfo SELECT 'ORD', STRUCT('2018', '400000', '112', '52'); 1 row inserted/updated/deleted The following syntax will produce an error: snappy> insert into StudentGrades values (1, 'Jim',Map('English', 'A', 'Science', 'B')); ERROR 42Y03: (SQLState=42Y03 Severity=20000) (Server=localhost/127.0.0.1[1529] Thread=ThriftProcessor-0) 'MAP(java.lang.String,java.lang.String,java.lang.String,java.lang.String)' is not recognized as a function or procedure. For more details on complex datatypes, refer to Supported datatypes and how to store and retrieve complex data types using ComplexTypeSerializer class .","title":"For complex data types (ARRAY, MAP, STRUCT), values to be inserted can not be directly used in the values clause of INSERT INTO &lt;table&gt; values (x, y, z \u2026 )"},{"location":"reference/API_Reference/apireference_guide/","text":"SnappyData Spark Extension API Reference Guide \u00b6 This guide gives details of Spark extension APIs that are provided by SnappyData. The following APIs are included: SnappySession APIs DataFrameWriter APIs SnappySessionCatalog APIs sql Query Using Cached Plan putInto Put Dataframe Content into Table getKeyColumns Get Key Columns of SnappyData table sqlUncached Query Using Fresh Plan deleteFrom Delete DataFrame Content from Table getKeyColumnsAndPositions Gets primary key or key columns with their position in the table. createTable Create SnappyData Managed Table createTable Create SnappyData Managed JDBC Table truncateTable Empty Contents of Table dropTable Drop SnappyData Table createSampleTable Create Stratified Sample Table createApproxTSTopK Create Structure to Query Top-K setCurrentSchema Set Current Database/schema getCurrentSchema Get Current Schema of Session insert Insert Row into an Existing Table put Upsert Row into an Existing Table update Update all Rows in Table delete Delete all Rows in Table queryApproxTSTopK Fetch the TopK Entries SnappySession APIs \u00b6 The following APIs are available for SnappySession. sql sqlUncached createTable truncateTable dropTable createSampleTable createApproxTSTopK setSchema getCurrentSchema insert put delete queryApproxTSTopK sql \u00b6 You can use this API to run a query with a cached plan for a given SQL. Syntax sql(sqlText : String) Parameters Parameter Description sqlText The SQL string required to execute. Returns Dataframe Example snappySession.sql(\u201cselect * from t1\u201d) sqlUncached \u00b6 You can use this API to run a query using a fresh plan for a given SQL String. Syntax sqlUncached(sqlText : String) Parameters Parameter Description sqlText The SQL string required to execute. Returns Dataframe Example snappySession.sqlUncached(\u201cselect * from t1\u201d) createTable \u00b6 Creates a SnappyData managed table. Any relation providers, that is the row, column etc., which are supported by SnappyData can be created here. Syntax createTable( tableName: String, provider: String, schema: StructType, options: Map[String, String], allowExisting: Boolean) Parameters Parameter Description tableName Name of the table. provider Provider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc. schema The table schema. options Properties for table creation. For example, partition_by, buckets etc. allowExisting When set to true , tables with the same name are ignored, else an AnalysisException is thrown stating that the table already exists. Returns Dataframe Example case class Data(col1: Int, col2: Int, col3: Int) val props = Map.empty[String, String] val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sc.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snappySession.createDataFrame(rdd) snappySession.createTable(tableName, \"column\", dataDF.schema, props) createTable \u00b6 Creates a SnappyData managed JDBC table which takes a free format DDL string. The DDL string should adhere to the syntax of the underlying JDBC store. SnappyData ships with an inbuilt JDBC store, which can be accessed by the data store of Row format. The options parameter can take connection details. Syntax Syntax: createTable( tableName: String, provider: String, schemaDDL: String, options: Map[String, String], allowExisting: Boolean) Parameters Parameter Description tableName Name of the table. provider Provider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc. schemaDDL The table schema as a string interpreted by the provider. options Properties for table creation. For example, partition_by, buckets etc. allowExisting Example val props = Map( \"url\" -> s\"jdbc:derby:$path\", \"driver\" -> \"org.apache.derby.jdbc.EmbeddedDriver\", \"poolImpl\" -> \"tomcat\", \"user\" -> \"app\", \"password\" -> \"app\" ) val schemaDDL = \"(OrderId INT NOT NULL PRIMARY KEY,ItemId INT, ITEMREF INT)\" snappySession.createTable(\"jdbcTable\", \"jdbc\", schemaDDL, props) truncateTable \u00b6 Empties the contents of the table without deleting the catalog entry. Syntax truncateTable(tableName: String, ifExists: Boolean = false) Parameters Parameter Description tableName Name of the table. ifExists Attempt truncate only if the table exists. Returns Dataframe Example snappySession.truncateTable(\u201ct1\u201d, true) dropTable \u00b6 Drop a SnappyData table created by a call to SnappySession.createTable , Catalog.createExternalTable or Dataset.createOrReplaceTempView . Syntax dropTable(tableName: String, ifExists: Boolean = false) Parameters Parameter Description tableName Name of the table. ifExists Attempts drop only if the table exists. Returns Unit Example snappySession.dropTable(\u201ct1\u201d, true) createSampleTable \u00b6 Creates a stratified sample table. Note This API is not supported in the Smart Connector mode. Syntax createSampleTable(tableName: String, baseTable: Option[String], samplingOptions: Map[String, String], allowExisting: Boolean) Parameters Parameter Description tableName The qualified name of the table. baseTable The base table of the sample table, if any. samplingOptions sampling options such as QCS, reservoir size etc. allowExisting When set to true , tables with the same name are ignored, else a table exist exception is shown. Returns Dataframe Example snappySession.createSampleTable(\"airline_sample\", Some(\"airline\"), Map(\"qcs\" -> \"UniqueCarrier ,Year_ ,Month_\", \"fraction\" -> \"0.05\", \"strataReservoirSize\" -> \"25\", \"buckets\" -> \"57\"), allowExisting = false) createApproxTSTopK \u00b6 Creates an approximate structure to query top-K with time series support. Note This API is not supported in the Smart Connector mode. Syntax createApproxTSTopK(topKName: String, baseTable: Option[String], keyColumnName: String, inputDataSchema: StructType, topkOptions: Map[String, String], allowExisting: Boolean = false) Parameters Parameter Description topKName The qualified name of the top-K structure. baseTable The base table of the top-K structure, if any. keyColumnName Top-k key column for aggregation inputDataSchema Schema of input data topkOptions Extra options including the following: frequencyCol : column to use for top-k frequency count. epoch, timeInterval : start and interval for collecting samples. timeSeriesColumn : A column that accurately records timestamps for better handling of time range queries allowExisting When set to true , tables with the same name are ignored, else a table exist exception is shown. Returns Dataframe Example snappySession.createApproxTSTopK(\"topktable\", Some(\"hashtagTable\"), \"hashtag\", schema, topKOption) setCurrentSchema \u00b6 Sets the current database/schema. Syntax setCurrentSchema(schema: String) Parameters Parameter Description schema schema name which goes into the catalog. Returns Unit Example snappySession.setCurrentSchema(\"APP\") getCurrentSchema \u00b6 Gets the current schema of the session. Syntax getCurrentSchema Example snappySession.getCurrentSchema Returns String insert \u00b6 Inserts one or more row into an existing table. Syntax insert(tableName: String, rows: Row*) Parameters Parameter Description tableName Table name for the insert operation. Rows List of rows to be inserted into the table. Returns Int Example val row = Row(i, i, i) snappySession.insert(\"t1\", row) put \u00b6 Upserts one or more row into an existing table. Only works for row tables. Syntax put(tableName: String, rows: Row*) Parameters Parameter Description tableName Table name for the put operation rows List of rows to be put into the table. Returns Int Example snappySession.put(tableName, dataDF.collect(): _*) update \u00b6 Updates all the rows in the table that match passed filter expression. This works only for row tables. Syntax update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*) Parameters Parameter Description tableName Th table name which needs to be updated. filterExpr SQL WHERE criteria to select rows that will be updated. newColumnValues A single row containing all the updated column values. They MUST match the updateColumn: list passed . updateColumns List of all column names that are updated. Returns Int Example snappySession.update(\"t1\", \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" ) delete \u00b6 Deletes all the rows in the table that match passed filter expression. This works only for row tables. Syntax delete(tableName: String, filterExpr: String) Parameters Parameter Description tableName Name of the table. filterExpr SQL WHERE criteria to select rows that will be updated. Returns Int Example snappySession.delete(\u201ct1\u201d, s\"col1=$i\")) queryApproxTSTopK \u00b6 Fetches the topK entries in the Approx TopK synopsis for the specified time interval. The time interval specified here should not be less than the minimum time interval used when creating the TopK synopsis. Note This API is not supported in the Smart Connector mode. Syntax queryApproxTSTopK(topKName: String, startTime: String = null, endTime: String = null, k: Int = -1) Parameters Parameter Description topKName The topK structure that is to be queried. startTime Start time as string in the format yyyy-mm-dd hh:mm:ss . If passed as null , the oldest interval is considered as the start interval. endTime End time as string in the format yyyy-mm-dd hh:mm:ss . If passed as null , the newest interval is considered as the last interval. k Optional. The number of elements to be queried. This is to be passed only for stream summary Returns Dataframe Example snappySession.queryApproxTSTopK(\"topktable\") DataFrameWriter APIs \u00b6 The following APIs are available for DataFrameWriter: putInto deleteFrom putInto \u00b6 Puts the content of the DataFrame into the specified table. It requires that the schema of the DataFrame is the same as the schema of the table. Column names are ignored while matching the schemas and put into operation is performed using position based resolution. If some rows are already present in the table, then they are updated. Also, the table on which putInto is implemented should have defined key columns, if its a column table. If it is a row table, then it should have defined a primary key. Syntax putInto(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Unit Example import org.apache.spark.sql.snappy._ df.write.putInto(\u201csnappy_table\u201d) deleteFrom \u00b6 The deleteFrom API deletes all those records from given snappy table which exists in the input Dataframe. Existence of the record is checked by comparing the key columns (or the primary keys) values. To use this API, key columns(for column table) or primary keys(for row tables) must be defined in the SnappyData table. Also, the source DataFrame must contain all the key columns or primary keys (depending upon the type of snappy table). The column existence is checked using a case-insensitive match of column names. If the source DataFrame contains columns other than the key columns, it will be ignored by the deleteFrom API. Syntax deleteFrom(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Unit Example import org.apache.spark.sql.snappy._ df.write.deleteFrom(\u201csnappy_table\u201d) SnappySessionCatalog APIs \u00b6 The following APIs are available for SnappySessionCatalog: getKeyColumns getKeyColumnsAndPositions Note These are developer APIs and are subject to change in the future. getKeyColumns \u00b6 Gets primary key or key columns of a SnappyData table. Syntax getKeyColumns(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Sequence of key columns (for column tables) or sequence of primary keys (for row tables). Example snappySession.sessionCatalog.getKeyColumns(\"t1\") getKeyColumnsAndPositions \u00b6 Gets primary key or key columns of a SnappyData table along with their position in the table. Syntax getKeyColumnsAndPositions(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Sequence of scala.Tuple2 containing column and column's position in the table for each key columns (for column tables) or sequence of primary keys (for row tables). Example snappySession.sessionCatalog.getKeyColumnsAndPositions(\"t1\")","title":"SnappyData Spark Extension API Reference Guide"},{"location":"reference/API_Reference/apireference_guide/#snappydata-spark-extension-api-reference-guide","text":"This guide gives details of Spark extension APIs that are provided by SnappyData. The following APIs are included: SnappySession APIs DataFrameWriter APIs SnappySessionCatalog APIs sql Query Using Cached Plan putInto Put Dataframe Content into Table getKeyColumns Get Key Columns of SnappyData table sqlUncached Query Using Fresh Plan deleteFrom Delete DataFrame Content from Table getKeyColumnsAndPositions Gets primary key or key columns with their position in the table. createTable Create SnappyData Managed Table createTable Create SnappyData Managed JDBC Table truncateTable Empty Contents of Table dropTable Drop SnappyData Table createSampleTable Create Stratified Sample Table createApproxTSTopK Create Structure to Query Top-K setCurrentSchema Set Current Database/schema getCurrentSchema Get Current Schema of Session insert Insert Row into an Existing Table put Upsert Row into an Existing Table update Update all Rows in Table delete Delete all Rows in Table queryApproxTSTopK Fetch the TopK Entries","title":"SnappyData Spark Extension API Reference Guide"},{"location":"reference/API_Reference/apireference_guide/#snappysession-apis","text":"The following APIs are available for SnappySession. sql sqlUncached createTable truncateTable dropTable createSampleTable createApproxTSTopK setSchema getCurrentSchema insert put delete queryApproxTSTopK","title":"SnappySession APIs"},{"location":"reference/API_Reference/apireference_guide/#sql","text":"You can use this API to run a query with a cached plan for a given SQL. Syntax sql(sqlText : String) Parameters Parameter Description sqlText The SQL string required to execute. Returns Dataframe Example snappySession.sql(\u201cselect * from t1\u201d)","title":"sql"},{"location":"reference/API_Reference/apireference_guide/#sqluncached","text":"You can use this API to run a query using a fresh plan for a given SQL String. Syntax sqlUncached(sqlText : String) Parameters Parameter Description sqlText The SQL string required to execute. Returns Dataframe Example snappySession.sqlUncached(\u201cselect * from t1\u201d)","title":"sqlUncached"},{"location":"reference/API_Reference/apireference_guide/#createtable","text":"Creates a SnappyData managed table. Any relation providers, that is the row, column etc., which are supported by SnappyData can be created here. Syntax createTable( tableName: String, provider: String, schema: StructType, options: Map[String, String], allowExisting: Boolean) Parameters Parameter Description tableName Name of the table. provider Provider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc. schema The table schema. options Properties for table creation. For example, partition_by, buckets etc. allowExisting When set to true , tables with the same name are ignored, else an AnalysisException is thrown stating that the table already exists. Returns Dataframe Example case class Data(col1: Int, col2: Int, col3: Int) val props = Map.empty[String, String] val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7)) val rdd = sc.parallelize(data, data.length).map(s => new Data(s(0), s(1), s(2))) val dataDF = snappySession.createDataFrame(rdd) snappySession.createTable(tableName, \"column\", dataDF.schema, props)","title":"createTable"},{"location":"reference/API_Reference/apireference_guide/#createtable_1","text":"Creates a SnappyData managed JDBC table which takes a free format DDL string. The DDL string should adhere to the syntax of the underlying JDBC store. SnappyData ships with an inbuilt JDBC store, which can be accessed by the data store of Row format. The options parameter can take connection details. Syntax Syntax: createTable( tableName: String, provider: String, schemaDDL: String, options: Map[String, String], allowExisting: Boolean) Parameters Parameter Description tableName Name of the table. provider Provider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc. schemaDDL The table schema as a string interpreted by the provider. options Properties for table creation. For example, partition_by, buckets etc. allowExisting Example val props = Map( \"url\" -> s\"jdbc:derby:$path\", \"driver\" -> \"org.apache.derby.jdbc.EmbeddedDriver\", \"poolImpl\" -> \"tomcat\", \"user\" -> \"app\", \"password\" -> \"app\" ) val schemaDDL = \"(OrderId INT NOT NULL PRIMARY KEY,ItemId INT, ITEMREF INT)\" snappySession.createTable(\"jdbcTable\", \"jdbc\", schemaDDL, props)","title":"createTable"},{"location":"reference/API_Reference/apireference_guide/#truncatetable","text":"Empties the contents of the table without deleting the catalog entry. Syntax truncateTable(tableName: String, ifExists: Boolean = false) Parameters Parameter Description tableName Name of the table. ifExists Attempt truncate only if the table exists. Returns Dataframe Example snappySession.truncateTable(\u201ct1\u201d, true)","title":"truncateTable"},{"location":"reference/API_Reference/apireference_guide/#droptable","text":"Drop a SnappyData table created by a call to SnappySession.createTable , Catalog.createExternalTable or Dataset.createOrReplaceTempView . Syntax dropTable(tableName: String, ifExists: Boolean = false) Parameters Parameter Description tableName Name of the table. ifExists Attempts drop only if the table exists. Returns Unit Example snappySession.dropTable(\u201ct1\u201d, true)","title":"dropTable"},{"location":"reference/API_Reference/apireference_guide/#createsampletable","text":"Creates a stratified sample table. Note This API is not supported in the Smart Connector mode. Syntax createSampleTable(tableName: String, baseTable: Option[String], samplingOptions: Map[String, String], allowExisting: Boolean) Parameters Parameter Description tableName The qualified name of the table. baseTable The base table of the sample table, if any. samplingOptions sampling options such as QCS, reservoir size etc. allowExisting When set to true , tables with the same name are ignored, else a table exist exception is shown. Returns Dataframe Example snappySession.createSampleTable(\"airline_sample\", Some(\"airline\"), Map(\"qcs\" -> \"UniqueCarrier ,Year_ ,Month_\", \"fraction\" -> \"0.05\", \"strataReservoirSize\" -> \"25\", \"buckets\" -> \"57\"), allowExisting = false)","title":"createSampleTable"},{"location":"reference/API_Reference/apireference_guide/#createapproxtstopk","text":"Creates an approximate structure to query top-K with time series support. Note This API is not supported in the Smart Connector mode. Syntax createApproxTSTopK(topKName: String, baseTable: Option[String], keyColumnName: String, inputDataSchema: StructType, topkOptions: Map[String, String], allowExisting: Boolean = false) Parameters Parameter Description topKName The qualified name of the top-K structure. baseTable The base table of the top-K structure, if any. keyColumnName Top-k key column for aggregation inputDataSchema Schema of input data topkOptions Extra options including the following: frequencyCol : column to use for top-k frequency count. epoch, timeInterval : start and interval for collecting samples. timeSeriesColumn : A column that accurately records timestamps for better handling of time range queries allowExisting When set to true , tables with the same name are ignored, else a table exist exception is shown. Returns Dataframe Example snappySession.createApproxTSTopK(\"topktable\", Some(\"hashtagTable\"), \"hashtag\", schema, topKOption)","title":"createApproxTSTopK"},{"location":"reference/API_Reference/apireference_guide/#setcurrentschema","text":"Sets the current database/schema. Syntax setCurrentSchema(schema: String) Parameters Parameter Description schema schema name which goes into the catalog. Returns Unit Example snappySession.setCurrentSchema(\"APP\")","title":"setCurrentSchema"},{"location":"reference/API_Reference/apireference_guide/#getcurrentschema","text":"Gets the current schema of the session. Syntax getCurrentSchema Example snappySession.getCurrentSchema Returns String","title":"getCurrentSchema"},{"location":"reference/API_Reference/apireference_guide/#insert","text":"Inserts one or more row into an existing table. Syntax insert(tableName: String, rows: Row*) Parameters Parameter Description tableName Table name for the insert operation. Rows List of rows to be inserted into the table. Returns Int Example val row = Row(i, i, i) snappySession.insert(\"t1\", row)","title":"insert"},{"location":"reference/API_Reference/apireference_guide/#put","text":"Upserts one or more row into an existing table. Only works for row tables. Syntax put(tableName: String, rows: Row*) Parameters Parameter Description tableName Table name for the put operation rows List of rows to be put into the table. Returns Int Example snappySession.put(tableName, dataDF.collect(): _*)","title":"put"},{"location":"reference/API_Reference/apireference_guide/#update","text":"Updates all the rows in the table that match passed filter expression. This works only for row tables. Syntax update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*) Parameters Parameter Description tableName Th table name which needs to be updated. filterExpr SQL WHERE criteria to select rows that will be updated. newColumnValues A single row containing all the updated column values. They MUST match the updateColumn: list passed . updateColumns List of all column names that are updated. Returns Int Example snappySession.update(\"t1\", \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" )","title":"update"},{"location":"reference/API_Reference/apireference_guide/#delete","text":"Deletes all the rows in the table that match passed filter expression. This works only for row tables. Syntax delete(tableName: String, filterExpr: String) Parameters Parameter Description tableName Name of the table. filterExpr SQL WHERE criteria to select rows that will be updated. Returns Int Example snappySession.delete(\u201ct1\u201d, s\"col1=$i\"))","title":"delete"},{"location":"reference/API_Reference/apireference_guide/#queryapproxtstopk","text":"Fetches the topK entries in the Approx TopK synopsis for the specified time interval. The time interval specified here should not be less than the minimum time interval used when creating the TopK synopsis. Note This API is not supported in the Smart Connector mode. Syntax queryApproxTSTopK(topKName: String, startTime: String = null, endTime: String = null, k: Int = -1) Parameters Parameter Description topKName The topK structure that is to be queried. startTime Start time as string in the format yyyy-mm-dd hh:mm:ss . If passed as null , the oldest interval is considered as the start interval. endTime End time as string in the format yyyy-mm-dd hh:mm:ss . If passed as null , the newest interval is considered as the last interval. k Optional. The number of elements to be queried. This is to be passed only for stream summary Returns Dataframe Example snappySession.queryApproxTSTopK(\"topktable\")","title":"queryApproxTSTopK"},{"location":"reference/API_Reference/apireference_guide/#dataframewriter-apis","text":"The following APIs are available for DataFrameWriter: putInto deleteFrom","title":"DataFrameWriter APIs"},{"location":"reference/API_Reference/apireference_guide/#putinto","text":"Puts the content of the DataFrame into the specified table. It requires that the schema of the DataFrame is the same as the schema of the table. Column names are ignored while matching the schemas and put into operation is performed using position based resolution. If some rows are already present in the table, then they are updated. Also, the table on which putInto is implemented should have defined key columns, if its a column table. If it is a row table, then it should have defined a primary key. Syntax putInto(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Unit Example import org.apache.spark.sql.snappy._ df.write.putInto(\u201csnappy_table\u201d)","title":"putInto"},{"location":"reference/API_Reference/apireference_guide/#deletefrom","text":"The deleteFrom API deletes all those records from given snappy table which exists in the input Dataframe. Existence of the record is checked by comparing the key columns (or the primary keys) values. To use this API, key columns(for column table) or primary keys(for row tables) must be defined in the SnappyData table. Also, the source DataFrame must contain all the key columns or primary keys (depending upon the type of snappy table). The column existence is checked using a case-insensitive match of column names. If the source DataFrame contains columns other than the key columns, it will be ignored by the deleteFrom API. Syntax deleteFrom(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Unit Example import org.apache.spark.sql.snappy._ df.write.deleteFrom(\u201csnappy_table\u201d)","title":"deleteFrom"},{"location":"reference/API_Reference/apireference_guide/#snappysessioncatalog-apis","text":"The following APIs are available for SnappySessionCatalog: getKeyColumns getKeyColumnsAndPositions Note These are developer APIs and are subject to change in the future.","title":"SnappySessionCatalog APIs"},{"location":"reference/API_Reference/apireference_guide/#getkeycolumns","text":"Gets primary key or key columns of a SnappyData table. Syntax getKeyColumns(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Sequence of key columns (for column tables) or sequence of primary keys (for row tables). Example snappySession.sessionCatalog.getKeyColumns(\"t1\")","title":"getKeyColumns"},{"location":"reference/API_Reference/apireference_guide/#getkeycolumnsandpositions","text":"Gets primary key or key columns of a SnappyData table along with their position in the table. Syntax getKeyColumnsAndPositions(tableName: String) Parameters Parameter Description tableName Name of the table. Returns Sequence of scala.Tuple2 containing column and column's position in the table for each key columns (for column tables) or sequence of primary keys (for row tables). Example snappySession.sessionCatalog.getKeyColumnsAndPositions(\"t1\")","title":"getKeyColumnsAndPositions"},{"location":"reference/API_Reference/odbc_supported_apis/","text":"ODBC Supported APIs \u00b6 The following APIs are supported for ODBC in Snappy Driver: Function APIs Attribute APIs Function Conformance of ODBC Supported APIs in Snappy Driver \u00b6 Function Conformance level Supported in Snappy Driver Exceptions SQLAllocHandle Core Yes SQLBindCol Core Yes SQLBindParameter Core[1] Yes SQLBrowseConnect Level 1 No SQLBulkOperations Level 1 Yes SQLCancel Core[1] Yes SQLCancelHandle Core[1] Yes SQLCloseCursor Core Yes SQLColAttribute Core[1] Yes SQLColumnPrivileges Level 2 Yes SQLColumns Core Yes SQLConnect Core Yes SQLCopyDesc Core No SQLDataSources Core No It should be implemented by the Driver Manager. SQLDescribeCol Core[1] Yes SQLDescribeParam Level 2 Yes SQLDisconnect Core Yes SQLDriverConnect Core Yes SQLDrivers Core No It should be implemented by the Driver Manager. SQLEndTran Core[1] Yes SQLExecDirect Core Yes SQLExecute Core Yes SQLFetch Core Yes SQLFetchScroll Core[1] Yes SQLForeignKeys Level 2 Yes SQLFreeHandle Core Yes SQLFreeStmt Core Yes SQLGetConnectAttr Core Yes SQLGetCursorName Core Yes SQLGetData Core Yes SQLGetDescField Core No SQLGetDescRec Core No SQLGetDiagField Core Yes SQLGetDiagRec Core Yes SQLGetEnvAttr Core Yes SQLGetFunctions Core Yes SQLGetInfo Core Yes SQL_DEFAULT_TXN_ISOLATION - not supported SQL_TXN_ISOLATION_OPTION - not supported SQL_FORWARD_ONLY_CURSOR_ATTRIBUTES1 supported SQL_FORWARD_ONLY_CURSOR_ATTRIBUTES2 supported SQLGetStmtAttr Core Yes SQLGetTypeInfo Core Yes Supports call only with SQL_ALL_TYPES info type parameter SQLMoreResults Level 1 Yes SQLNativeSql Core Yes SQLNumParams Core Yes SQLNumResultCols Core Yes SQLParamData Core Yes SQLPrepare Core Yes SQLPrimaryKeys Level 1 Yes SQLProcedureColumns Level 1 Yes SQLProcedures Level 1 Yes SQLPutData Core Yes SQLRowCount Core Yes SQLSetConnectAttr Core[2] Yes SQLSetCursorName Core Yes SQLSetDescField Core[1] No SQLSetDescRec Core No SQLSetEnvAttr Core[2] Yes SQLSetPos Level 1[1] Yes SQLSetStmtAttr Core[2] Yes SQLSpecialColumns Core[1] Yes SQLStatistics Core Yes SQLTablePrivileges Level 2 Yes SQLTables Core Yes Attribute Conformance of ODBC Supported APIs in Snappy Driver \u00b6 ODBC Environment Attribute \u00b6 Attributes Conformance Level Supported in Snappy Driver Exceptions SQL_ATTR_CONNECTION_POOLING [1] Yes This is an optional feature and as such is not part of the conformance levels. SQL_ATTR_CP_MATCH [1] Yes SQL_ATTR_ODBC_VER Core Yes SQL_ATTR_OUTPUT_NTS [1] Yes ODBC Connection Attribute \u00b6 Attributes Conformance Level Supported in Snappy Driver Exceptions SQL_ATTR_ACCESS_MODE Core Yes SQL_ATTR_ASYNC_ENABLE Level 1/Level 2[1] Yes Applications that support connection-level asynchronously (required for Level 1) must support setting this attribute to SQL_TRUE by calling SQLSetConnectAttr; the attribute need not be settable to a value other than its default value through SQLSetStmtAttr. Applications that support statement-level asynchronously (required for Level 2) must support setting this attribute to SQL_TRUE using either function. SQL_ATTR_AUTO_IPD Level 2 No SQL_ATTR_AUTOCOMMIT Level 1 Yes For Level 1 interface conformance, the driver must support one value in addition to the driver-defined default value (available by calling SQLGetInfo with the SQL_DEFAULT_TXN_ISOLATION option). For Level 2 interface conformance, the driver must also support SQL_TXN_SERIALIZABLE. SQL_ATTR_CONNECTION_DEAD Level 1 Yes SQL_ATTR_CONNECTION_TIMEOUT Level 2 Yes SQL_ATTR_CURRENT_CATALOG Level 2 Yes SQL_ATTR_LOGIN_TIMEOUT Level 2 Yes SQL_ATTR_ODBC_CURSORS Core Yes SQL_ATTR_PACKET_SIZE Level 2 Yes SQL_ATTR_QUIET_MODE Core Yes SQL_ATTR_TRACE Core Yes SQL_ATTR_TRACEFILE Core Yes SQL_ATTR_TRANSLATE_LIB Core Yes SQL_ATTR_TRANSLATE_OPTION Core Yes SQL_ATTR_TXN_ISOLATION Level 1/Level 2[2] Yes ODBC Statement Attribute \u00b6 Attributes Conformance Level Supported in Snappy Driver Exceptions SQL_ATTR_APP_PARAM_DESC Core Yes SQL_ATTR_APP_ROW_DESC Core Yes SQL_ATTR_ASYNC_ENABLE Level 1/Level 2[1] Yes SQL_ATTR_CONCURRENCY Level 1/Level 2[2] Yes SQL_ATTR_CURSOR_SCROLLABLE Level 1 Yes SQL_ATTR_CURSOR_SENSITIVITY Level 2 Yes SQL_ATTR_CURSOR_TYPE Core/Level 2[3] Yes Applications that support connection-level asynchronously (required for Level 1) must support setting this attribute to SQL_TRUE by calling SQLSetConnectAttr; the attribute need not be settable to a value other than its default value through SQLSetStmtAttr. Applications that support statement-level asynchronously (required for Level 2) must support setting this attribute to SQL_TRUE using either function. SQL_ATTR_ENABLE_AUTO_IPD Level 2 Yes SQL_ATTR_FETCH_BOOKMARK_PTR Level 2 Yes For Level 2 interface conformance, the driver must support SQL_CONCUR_READ_ONLY and at least one other value. SQL_ATTR_IMP_PARAM_DESC Core Yes SQL_ATTR_IMP_ROW_DESC Core Yes For Level 1 interface conformance, the driver must support SQL_CURSOR_FORWARD_ONLY and at least one other value. For Level 2 interface conformance, the driver must support all values defined in this document. SQL_ATTR_KEYSET_SIZE Level 2 Yes SQL_ATTR_MAX_LENGTH Level 1 Yes SQL_ATTR_MAX_ROWS Level 1 Yes SQL_ATTR_METADATA_ID Core Yes SQL_ATTR_NOSCAN Core Yes SQL_ATTR_PARAM_BIND_OFFSET_PTR Core Yes SQL_ATTR_PARAM_BIND_TYPE Core Yes SQL_ATTR_PARAM_OPERATION_PTR Core Yes SQL_ATTR_PARAM_STATUS_PTR Core Yes SQL_ATTR_PARAMS_PROCESSED_PTR Core Yes SQL_ATTR_PARAMSET_SIZE Core Yes SQL_ATTR_QUERY_TIMEOUT Level 2 Yes SQL_ATTR_RETRIEVE_DATA Level 1 Yes SQL_ATTR_ROW_ARRAY_SIZE Core Yes SQL_ATTR_ROW_BIND_OFFSET_PTR Core Yes SQL_ATTR_ROW_BIND_TYPE Core Yes SQL_ATTR_ROW_NUMBER Level 1 Yes SQL_ATTR_ROW_OPERATION_PTR Level 1 Yes SQL_ATTR_ROW_STATUS_PTR Core Yes SQL_ATTR_ROWS_FETCHED_PTR Core Yes SQL_ATTR_SIMULATE_CURSOR Level 2 No SQL_ATTR_USE_BOOKMARKS Level 2 No","title":"ODBC Supported API"},{"location":"reference/API_Reference/odbc_supported_apis/#odbc-supported-apis","text":"The following APIs are supported for ODBC in Snappy Driver: Function APIs Attribute APIs","title":"ODBC Supported APIs"},{"location":"reference/API_Reference/odbc_supported_apis/#function-conformance-of-odbc-supported-apis-in-snappy-driver","text":"Function Conformance level Supported in Snappy Driver Exceptions SQLAllocHandle Core Yes SQLBindCol Core Yes SQLBindParameter Core[1] Yes SQLBrowseConnect Level 1 No SQLBulkOperations Level 1 Yes SQLCancel Core[1] Yes SQLCancelHandle Core[1] Yes SQLCloseCursor Core Yes SQLColAttribute Core[1] Yes SQLColumnPrivileges Level 2 Yes SQLColumns Core Yes SQLConnect Core Yes SQLCopyDesc Core No SQLDataSources Core No It should be implemented by the Driver Manager. SQLDescribeCol Core[1] Yes SQLDescribeParam Level 2 Yes SQLDisconnect Core Yes SQLDriverConnect Core Yes SQLDrivers Core No It should be implemented by the Driver Manager. SQLEndTran Core[1] Yes SQLExecDirect Core Yes SQLExecute Core Yes SQLFetch Core Yes SQLFetchScroll Core[1] Yes SQLForeignKeys Level 2 Yes SQLFreeHandle Core Yes SQLFreeStmt Core Yes SQLGetConnectAttr Core Yes SQLGetCursorName Core Yes SQLGetData Core Yes SQLGetDescField Core No SQLGetDescRec Core No SQLGetDiagField Core Yes SQLGetDiagRec Core Yes SQLGetEnvAttr Core Yes SQLGetFunctions Core Yes SQLGetInfo Core Yes SQL_DEFAULT_TXN_ISOLATION - not supported SQL_TXN_ISOLATION_OPTION - not supported SQL_FORWARD_ONLY_CURSOR_ATTRIBUTES1 supported SQL_FORWARD_ONLY_CURSOR_ATTRIBUTES2 supported SQLGetStmtAttr Core Yes SQLGetTypeInfo Core Yes Supports call only with SQL_ALL_TYPES info type parameter SQLMoreResults Level 1 Yes SQLNativeSql Core Yes SQLNumParams Core Yes SQLNumResultCols Core Yes SQLParamData Core Yes SQLPrepare Core Yes SQLPrimaryKeys Level 1 Yes SQLProcedureColumns Level 1 Yes SQLProcedures Level 1 Yes SQLPutData Core Yes SQLRowCount Core Yes SQLSetConnectAttr Core[2] Yes SQLSetCursorName Core Yes SQLSetDescField Core[1] No SQLSetDescRec Core No SQLSetEnvAttr Core[2] Yes SQLSetPos Level 1[1] Yes SQLSetStmtAttr Core[2] Yes SQLSpecialColumns Core[1] Yes SQLStatistics Core Yes SQLTablePrivileges Level 2 Yes SQLTables Core Yes","title":"Function Conformance of ODBC Supported APIs in Snappy Driver"},{"location":"reference/API_Reference/odbc_supported_apis/#attribute-conformance-of-odbc-supported-apis-in-snappy-driver","text":"","title":"Attribute Conformance of ODBC Supported APIs in Snappy Driver"},{"location":"reference/API_Reference/odbc_supported_apis/#odbc-environment-attribute","text":"Attributes Conformance Level Supported in Snappy Driver Exceptions SQL_ATTR_CONNECTION_POOLING [1] Yes This is an optional feature and as such is not part of the conformance levels. SQL_ATTR_CP_MATCH [1] Yes SQL_ATTR_ODBC_VER Core Yes SQL_ATTR_OUTPUT_NTS [1] Yes","title":"ODBC Environment Attribute"},{"location":"reference/API_Reference/odbc_supported_apis/#odbc-connection-attribute","text":"Attributes Conformance Level Supported in Snappy Driver Exceptions SQL_ATTR_ACCESS_MODE Core Yes SQL_ATTR_ASYNC_ENABLE Level 1/Level 2[1] Yes Applications that support connection-level asynchronously (required for Level 1) must support setting this attribute to SQL_TRUE by calling SQLSetConnectAttr; the attribute need not be settable to a value other than its default value through SQLSetStmtAttr. Applications that support statement-level asynchronously (required for Level 2) must support setting this attribute to SQL_TRUE using either function. SQL_ATTR_AUTO_IPD Level 2 No SQL_ATTR_AUTOCOMMIT Level 1 Yes For Level 1 interface conformance, the driver must support one value in addition to the driver-defined default value (available by calling SQLGetInfo with the SQL_DEFAULT_TXN_ISOLATION option). For Level 2 interface conformance, the driver must also support SQL_TXN_SERIALIZABLE. SQL_ATTR_CONNECTION_DEAD Level 1 Yes SQL_ATTR_CONNECTION_TIMEOUT Level 2 Yes SQL_ATTR_CURRENT_CATALOG Level 2 Yes SQL_ATTR_LOGIN_TIMEOUT Level 2 Yes SQL_ATTR_ODBC_CURSORS Core Yes SQL_ATTR_PACKET_SIZE Level 2 Yes SQL_ATTR_QUIET_MODE Core Yes SQL_ATTR_TRACE Core Yes SQL_ATTR_TRACEFILE Core Yes SQL_ATTR_TRANSLATE_LIB Core Yes SQL_ATTR_TRANSLATE_OPTION Core Yes SQL_ATTR_TXN_ISOLATION Level 1/Level 2[2] Yes","title":"ODBC Connection Attribute"},{"location":"reference/API_Reference/odbc_supported_apis/#odbc-statement-attribute","text":"Attributes Conformance Level Supported in Snappy Driver Exceptions SQL_ATTR_APP_PARAM_DESC Core Yes SQL_ATTR_APP_ROW_DESC Core Yes SQL_ATTR_ASYNC_ENABLE Level 1/Level 2[1] Yes SQL_ATTR_CONCURRENCY Level 1/Level 2[2] Yes SQL_ATTR_CURSOR_SCROLLABLE Level 1 Yes SQL_ATTR_CURSOR_SENSITIVITY Level 2 Yes SQL_ATTR_CURSOR_TYPE Core/Level 2[3] Yes Applications that support connection-level asynchronously (required for Level 1) must support setting this attribute to SQL_TRUE by calling SQLSetConnectAttr; the attribute need not be settable to a value other than its default value through SQLSetStmtAttr. Applications that support statement-level asynchronously (required for Level 2) must support setting this attribute to SQL_TRUE using either function. SQL_ATTR_ENABLE_AUTO_IPD Level 2 Yes SQL_ATTR_FETCH_BOOKMARK_PTR Level 2 Yes For Level 2 interface conformance, the driver must support SQL_CONCUR_READ_ONLY and at least one other value. SQL_ATTR_IMP_PARAM_DESC Core Yes SQL_ATTR_IMP_ROW_DESC Core Yes For Level 1 interface conformance, the driver must support SQL_CURSOR_FORWARD_ONLY and at least one other value. For Level 2 interface conformance, the driver must support all values defined in this document. SQL_ATTR_KEYSET_SIZE Level 2 Yes SQL_ATTR_MAX_LENGTH Level 1 Yes SQL_ATTR_MAX_ROWS Level 1 Yes SQL_ATTR_METADATA_ID Core Yes SQL_ATTR_NOSCAN Core Yes SQL_ATTR_PARAM_BIND_OFFSET_PTR Core Yes SQL_ATTR_PARAM_BIND_TYPE Core Yes SQL_ATTR_PARAM_OPERATION_PTR Core Yes SQL_ATTR_PARAM_STATUS_PTR Core Yes SQL_ATTR_PARAMS_PROCESSED_PTR Core Yes SQL_ATTR_PARAMSET_SIZE Core Yes SQL_ATTR_QUERY_TIMEOUT Level 2 Yes SQL_ATTR_RETRIEVE_DATA Level 1 Yes SQL_ATTR_ROW_ARRAY_SIZE Core Yes SQL_ATTR_ROW_BIND_OFFSET_PTR Core Yes SQL_ATTR_ROW_BIND_TYPE Core Yes SQL_ATTR_ROW_NUMBER Level 1 Yes SQL_ATTR_ROW_OPERATION_PTR Level 1 Yes SQL_ATTR_ROW_STATUS_PTR Core Yes SQL_ATTR_ROWS_FETCHED_PTR Core Yes SQL_ATTR_SIMULATE_CURSOR Level 2 No SQL_ATTR_USE_BOOKMARKS Level 2 No","title":"ODBC Statement Attribute"},{"location":"reference/command_line_utilities/","text":"Command Line Utilities \u00b6 Use the snappy command-line utility to launch SnappyData utilities. To display a full list of snappy commands and options: ./bin/snappy --help The command form to display a particular utility's usage is: ./bin/snappy <utility> --help With no arguments, snappy starts an interactive SQL command shell : ./bin/snappy To specify a system property for an interactive `snappy` session, you must define the JAVA_ARGS environment variable before starting `snappy`. For example, `snappy` uses the `snappy.history` system property to define the file that stores a list of the commands that are executed during an interactive session. To change this property, you would define it as part of the JAVA_ARGS variable: ```pre $ export JAVA_ARGS=\"-Dsnappy.history=/Users/user1/snappystore-history.sql\" $ snappy To launch and exit a snappy utility (rather than start an interactive snappy shell) use the syntax: ./bin/snappy <utility> <arguments for specified utility> To specify a system property when launching a snappy utility, use -J-D property_name = property_value argument. In addition to launching various utilities provided with SnappyData, when launched without any arguments, snappy starts an interactive command shell that you can use to connect to a SnappyData system and execute various commands, including SQL statements. backup and restore Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores. compact-all-disk-stores Performs online compaction of SnappyData disk stores. compact-disk-store Performs offline compaction of a single SnappyData disk store. revoke-missing-disk-store Instruct SnappyData members to stop waiting for a disk store to become available. run Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive SnappyData shell. version Prints information about the SnappyData product version","title":"Command Line Utilities"},{"location":"reference/command_line_utilities/#command-line-utilities","text":"Use the snappy command-line utility to launch SnappyData utilities. To display a full list of snappy commands and options: ./bin/snappy --help The command form to display a particular utility's usage is: ./bin/snappy <utility> --help With no arguments, snappy starts an interactive SQL command shell : ./bin/snappy To specify a system property for an interactive `snappy` session, you must define the JAVA_ARGS environment variable before starting `snappy`. For example, `snappy` uses the `snappy.history` system property to define the file that stores a list of the commands that are executed during an interactive session. To change this property, you would define it as part of the JAVA_ARGS variable: ```pre $ export JAVA_ARGS=\"-Dsnappy.history=/Users/user1/snappystore-history.sql\" $ snappy To launch and exit a snappy utility (rather than start an interactive snappy shell) use the syntax: ./bin/snappy <utility> <arguments for specified utility> To specify a system property when launching a snappy utility, use -J-D property_name = property_value argument. In addition to launching various utilities provided with SnappyData, when launched without any arguments, snappy starts an interactive command shell that you can use to connect to a SnappyData system and execute various commands, including SQL statements. backup and restore Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores. compact-all-disk-stores Performs online compaction of SnappyData disk stores. compact-disk-store Performs offline compaction of a single SnappyData disk store. revoke-missing-disk-store Instruct SnappyData members to stop waiting for a disk store to become available. run Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive SnappyData shell. version Prints information about the SnappyData product version","title":"Command Line Utilities"},{"location":"reference/command_line_utilities/data_extractor/","text":"Data Extractor \u00b6 Refer to Recovering Data During Cluster Failures .","title":"Data Extractor"},{"location":"reference/command_line_utilities/data_extractor/#data-extractor","text":"Refer to Recovering Data During Cluster Failures .","title":"Data Extractor"},{"location":"reference/command_line_utilities/modify_disk_store/","text":"modify-disk-store \u00b6 Modifies the content stored in a disk store. Caution This operation writes to the disk store files. Hence you must use this utility cautiously. Syntax \u00b6 For secured cluster Snappy>create region --name=regionName --type=PARTITION_PERSISTENT_OVERFLOW For non-secured cluster The following table describes the options used for snappy modify-disk-store : Items Description -region Specify the name of the region. -remove This option removes the region from the disk store. All the data stored in the disk store for this region will no longer exist if you use this option. -statisticsEnabled Enables the region's statistics. Values are true or false. Note The name of the disk store, the directories its files are stored in, and the region to target are all required arguments. Description \u00b6 Examples \u00b6 Secured cluster Non-secured cluster","title":"modify-disk-store"},{"location":"reference/command_line_utilities/modify_disk_store/#modify-disk-store","text":"Modifies the content stored in a disk store. Caution This operation writes to the disk store files. Hence you must use this utility cautiously.","title":"modify-disk-store"},{"location":"reference/command_line_utilities/modify_disk_store/#syntax","text":"For secured cluster Snappy>create region --name=regionName --type=PARTITION_PERSISTENT_OVERFLOW For non-secured cluster The following table describes the options used for snappy modify-disk-store : Items Description -region Specify the name of the region. -remove This option removes the region from the disk store. All the data stored in the disk store for this region will no longer exist if you use this option. -statisticsEnabled Enables the region's statistics. Values are true or false. Note The name of the disk store, the directories its files are stored in, and the region to target are all required arguments.","title":"Syntax"},{"location":"reference/command_line_utilities/modify_disk_store/#description","text":"","title":"Description"},{"location":"reference/command_line_utilities/modify_disk_store/#examples","text":"Secured cluster Non-secured cluster","title":"Examples"},{"location":"reference/command_line_utilities/scala-cli/","text":"snappy-scala CLI \u00b6 The snappy-scala CLI that was introduced as an experimental feature in the SnappyData 1.2.0 release, is considered a stable feature since the 1.3.0 release. This is similar to the Spark shell in its capabilities. The Spark documentation defines the Spark shell as follows: Spark\u2019s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python. A developer who is learning the Spark APIs of SparkContext, SparkSession, RDD, DataSet, DataSources, ML, etc. , can use the following utilities: Spark shell to quickly bring up an interactive shell and start learning and experimenting with the APIs. PySpark , provided by spark, for interactive Python where you can interactively learn the Python APIs that are provided by Spark. Spark shell is a spark application that is built on Scala\u2019s REPL (Read-Evaluate-Print loop). It accepts Scala code as input, executes the instructions as per the code, and returns the output of those instructions. After this utility is invoked, the spark driver comes into life, in which a SparkContext, a SparkSession, and a REPL object are initialized, and an interactive shell is provided to the users on the driver VM itself for interactive learning. The high cost, in terms of time and efforts, for each change warranting code compilation and then adding into a jar can be entirely avoided by using the Spark shell. A developer who wants to develop a Spark job can start by writing the code in a Spark shell and improve the job in interactive iterations. After the developer is satisfied with the quality of the code, it can be converted into a Spark Job. The idea of snappy-scala CLI is similar to that of Spark shell with few differences. With the snappy-scala CLI, you can interactively learn all the Spark APIs, the additional SnappyData APIs and also have the same experience of a Scala REPL or a Spark shell REPL app. You can still use the Spark shell and connect to a SnappyData cluster in Smart Connector mode, but you will require a separate Spark cluster for that. Moreover, that will be a different experience wherein the SnappyData cluster is used mostly as a DataSource by the Spark application. snappy-scala CLI is built on top of the exec scala feature and the already existing snappy CLI utility, which connects to the SnappyData cluster using the JDBC driver. snappy-scala CLI is not a true scala interpreter but mimics a scala or a Spark shell type of interpreter. Each line of the code is shipped to the Lead node of the SnappyData cluster. The code is interpreted there, and the result of the interpretation is then brought back to the snappy-scala CLI. It is displayed in the same fashion as that of a Scala REPL. Although the experience of the snappy-scala CLI is similar to that of a Scala or a Spark shell, yet a couple of important features are either missing or are thinly supported. This is because it is currently an experimental feature. The following are a couple of notable differences between the Spark shell and snappy-scala CLI: The auto completion feature, which is rich in a true Scala or Scala based interpreter. It is almost as rich as an IDE, where it can prompt possible completions, method signature, word completion, syntaxes, etc. Support for the list of commands which can be executed on the shell. The following image shows a simple SnappyData cluster, which is started, and then the snappy-scala is launched to connect. Command-line options \u00b6 $ ./bin/snappy-scala -h Usage: snappy-scala [OPTIONS] OPTIONS and Default values -c LOCATOR_OR_SERVER_ENDPOINT (default value is localhost:1527) -u USERNAME (default value is APP) -p PASSWORD (default value is APP) -r SCALA_FILE_PATHS (comma separated paths if multiple) -h, --help (prints script usage) The following options are only available currently: Options Description -c The endpoint of the server host port to which the tool connects. If the option is not specified, then the CLI attempts to connect to localhost:1527 endpoint. -u The user who wants to start the shell. If the user is trying to connect to a secured SnappyData cluster, then this user will be authenticated by the server. -p Password of the user. -r A list of files that can be passed as an argument. The file should contain valid Scala code, which is interpreted line by line by the tool. If there are more than one file to be passed, then the list must be comma-separated. -h prints the usage of the script. Securing the Usage of snappy-scala CLI \u00b6 This is the same as securing the usage of exec scala . In a secured system, by default, only the DB owner has the permission to secure the usage and can grant this privilege to other users as well. Apart from running the Scala code, the CLI also takes predefined commands. Each of these commands has a specific purpose. All these commands start with a colon : . With the help of this, the interpreter differentiates between the code and command. On the snappy-scala prompt, type :help for the list of supported commands. The following image shows the list of the available commands that are displayed when you type :help command: All the commands are listed with their corresponding description when :help is typed on the snappy-scala prompt. This is mostly a subset of what Spark shell provides. Some of the missing ones are :paste , :edit , :load etc. There are few commands which are specific to snappy-scala such as :maximumdisplaywidth , :elapsedtime on etc. The following table lists the commands that are supported by snappy-scala: Commands Description :help Prints the list of all the available commands along with the corresponding descriptions. :implicits The imported implicits in the session so far. :javap Decompiles a java class. :sh Runs a shell command. :silent The CLI becomes silent and does not print the output after each execution. :type Prints the type of the Symbol, which is passed in the argument. For example, if x is an Int then :type x prints INT :kind Prints the kind of the expression type. :warnings Prints any warning in the session. :replay Re-runs all the code from the beginning of the session or from the last point of :reset . Commands are not replayed. The difference with Spark shell on this command is that with snappy-scala, the replay can be done only once, and the buffer containing the commands is then emptied. In case the command is replayed again without running any code, then a message Nothing to replay is displayed. :reset It resets all the state in the interpreter. All types, symbols, imports, etc. created in the session will be lost. :history Prints the Scala code that was run in the session. :run Passes a path to the Scala file as an argument to this command. The scala code in the file is executed one by one by the snappy-scala CLI. The following are additional commands provided in the snappy-scala CLI over the Spark shell commands. Command Description :elapsedtime on Displays the elapsed time of each and every line of code or command which is executed on the interpreter. :quit The session is closed, and you can exit the CLI program. :maximumdisplaywidth Sets the largest display width of the CLI output to the specified value. Usually, this command is used to increase the default value to display long lines that come in the output. :maximumlinewidth Same as above. This command sets the largest display width for each line output from the interpreter to the specified value.","title":"scala CLI"},{"location":"reference/command_line_utilities/scala-cli/#snappy-scala-cli","text":"The snappy-scala CLI that was introduced as an experimental feature in the SnappyData 1.2.0 release, is considered a stable feature since the 1.3.0 release. This is similar to the Spark shell in its capabilities. The Spark documentation defines the Spark shell as follows: Spark\u2019s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python. A developer who is learning the Spark APIs of SparkContext, SparkSession, RDD, DataSet, DataSources, ML, etc. , can use the following utilities: Spark shell to quickly bring up an interactive shell and start learning and experimenting with the APIs. PySpark , provided by spark, for interactive Python where you can interactively learn the Python APIs that are provided by Spark. Spark shell is a spark application that is built on Scala\u2019s REPL (Read-Evaluate-Print loop). It accepts Scala code as input, executes the instructions as per the code, and returns the output of those instructions. After this utility is invoked, the spark driver comes into life, in which a SparkContext, a SparkSession, and a REPL object are initialized, and an interactive shell is provided to the users on the driver VM itself for interactive learning. The high cost, in terms of time and efforts, for each change warranting code compilation and then adding into a jar can be entirely avoided by using the Spark shell. A developer who wants to develop a Spark job can start by writing the code in a Spark shell and improve the job in interactive iterations. After the developer is satisfied with the quality of the code, it can be converted into a Spark Job. The idea of snappy-scala CLI is similar to that of Spark shell with few differences. With the snappy-scala CLI, you can interactively learn all the Spark APIs, the additional SnappyData APIs and also have the same experience of a Scala REPL or a Spark shell REPL app. You can still use the Spark shell and connect to a SnappyData cluster in Smart Connector mode, but you will require a separate Spark cluster for that. Moreover, that will be a different experience wherein the SnappyData cluster is used mostly as a DataSource by the Spark application. snappy-scala CLI is built on top of the exec scala feature and the already existing snappy CLI utility, which connects to the SnappyData cluster using the JDBC driver. snappy-scala CLI is not a true scala interpreter but mimics a scala or a Spark shell type of interpreter. Each line of the code is shipped to the Lead node of the SnappyData cluster. The code is interpreted there, and the result of the interpretation is then brought back to the snappy-scala CLI. It is displayed in the same fashion as that of a Scala REPL. Although the experience of the snappy-scala CLI is similar to that of a Scala or a Spark shell, yet a couple of important features are either missing or are thinly supported. This is because it is currently an experimental feature. The following are a couple of notable differences between the Spark shell and snappy-scala CLI: The auto completion feature, which is rich in a true Scala or Scala based interpreter. It is almost as rich as an IDE, where it can prompt possible completions, method signature, word completion, syntaxes, etc. Support for the list of commands which can be executed on the shell. The following image shows a simple SnappyData cluster, which is started, and then the snappy-scala is launched to connect.","title":"snappy-scala CLI"},{"location":"reference/command_line_utilities/scala-cli/#command-line-options","text":"$ ./bin/snappy-scala -h Usage: snappy-scala [OPTIONS] OPTIONS and Default values -c LOCATOR_OR_SERVER_ENDPOINT (default value is localhost:1527) -u USERNAME (default value is APP) -p PASSWORD (default value is APP) -r SCALA_FILE_PATHS (comma separated paths if multiple) -h, --help (prints script usage) The following options are only available currently: Options Description -c The endpoint of the server host port to which the tool connects. If the option is not specified, then the CLI attempts to connect to localhost:1527 endpoint. -u The user who wants to start the shell. If the user is trying to connect to a secured SnappyData cluster, then this user will be authenticated by the server. -p Password of the user. -r A list of files that can be passed as an argument. The file should contain valid Scala code, which is interpreted line by line by the tool. If there are more than one file to be passed, then the list must be comma-separated. -h prints the usage of the script.","title":"Command-line options"},{"location":"reference/command_line_utilities/scala-cli/#securing-the-usage-of-snappy-scala-cli","text":"This is the same as securing the usage of exec scala . In a secured system, by default, only the DB owner has the permission to secure the usage and can grant this privilege to other users as well. Apart from running the Scala code, the CLI also takes predefined commands. Each of these commands has a specific purpose. All these commands start with a colon : . With the help of this, the interpreter differentiates between the code and command. On the snappy-scala prompt, type :help for the list of supported commands. The following image shows the list of the available commands that are displayed when you type :help command: All the commands are listed with their corresponding description when :help is typed on the snappy-scala prompt. This is mostly a subset of what Spark shell provides. Some of the missing ones are :paste , :edit , :load etc. There are few commands which are specific to snappy-scala such as :maximumdisplaywidth , :elapsedtime on etc. The following table lists the commands that are supported by snappy-scala: Commands Description :help Prints the list of all the available commands along with the corresponding descriptions. :implicits The imported implicits in the session so far. :javap Decompiles a java class. :sh Runs a shell command. :silent The CLI becomes silent and does not print the output after each execution. :type Prints the type of the Symbol, which is passed in the argument. For example, if x is an Int then :type x prints INT :kind Prints the kind of the expression type. :warnings Prints any warning in the session. :replay Re-runs all the code from the beginning of the session or from the last point of :reset . Commands are not replayed. The difference with Spark shell on this command is that with snappy-scala, the replay can be done only once, and the buffer containing the commands is then emptied. In case the command is replayed again without running any code, then a message Nothing to replay is displayed. :reset It resets all the state in the interpreter. All types, symbols, imports, etc. created in the session will be lost. :history Prints the Scala code that was run in the session. :run Passes a path to the Scala file as an argument to this command. The scala code in the file is executed one by one by the snappy-scala CLI. The following are additional commands provided in the snappy-scala CLI over the Spark shell commands. Command Description :elapsedtime on Displays the elapsed time of each and every line of code or command which is executed on the interpreter. :quit The session is closed, and you can exit the CLI program. :maximumdisplaywidth Sets the largest display width of the CLI output to the specified value. Usually, this command is used to increase the default value to display long lines that come in the output. :maximumlinewidth Same as above. This command sets the largest display width for each line output from the interpreter to the specified value.","title":"Securing the Usage of snappy-scala CLI"},{"location":"reference/command_line_utilities/store-backup/","text":"Backup and Restore \u00b6 Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores. An online backup saves the following: For each member with persistent data, the backup includes disk store files for all stores containing persistent table data. Configuration files from the member startup. A restore script (restore.sh) copies the files back to their original locations. Note SnappyData does not support backing up disk stores on systems with live transactions, or when concurrent DML statements are being executed. If a backup of the live transaction or concurrent DML operations, is performed, there is a possibility of partial commits or partial changes of DML operations appearing in the backups. SnappyData does not support taking incremental backups on systems with live transactions, or when concurrent DML statements are being executed. Guidelines Specifying the Backup Directory Backup Directory Structure and Contents Performing a Full Backup Performing an Incremental Backup List of Properties Restoring your Backup Verify the Backup is Successful Guidelines \u00b6 Run this command during a period of low activity in your system. The backup does not block system activities, but it uses file system resources on all hosts in your distributed system and can affect performance. If you try to create backup files from a running system using file copy commands, you can get incomplete and unusable copies. Make sure the target backup directory exists and has the proper permissions for your members to write to it and create subdirectories. It is recommended to compact your disk store before running the backup. Make sure that those SnappyData members that host persistent data are running in the distributed system. Offline members cannot back up their disk stores. (A complete backup can still be performed if all table data is available in the running members). Specifying the Backup Directory \u00b6 The directory you specify for backup can be used multiple times. Each backup first creates a top-level directory for the backup, under the directory you specify, identified to the minute. You can use one of two formats: Use a single physical location, such as a network file server. (For example, < fileServerDirectory >/< SnappyBackupLocation >). Use a directory that is local to all host machines in the system. (For example, < SnappyBackupLocation >). Backup Directory Structure and Contents \u00b6 The backup directory contains a backup of the persistent data. Below is the structure of files and directories backed up in a distributed system: 2018-03-15-05-31-46: 10_80_141_112_10715_ec_v0_7393 10_80_141_112_10962_v1_57099 2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393: config diskstores README.txt restore.sh user 2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/config: 2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/user: 2018-03-15-05-31-46/2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/diskstores/ GFXD-DD-DISKSTORE_4d9fa95e-7746-4d4d-b404-2648d64cf35e GFXD-DEFAULT-DISKSTORE_3c446ce4-43e4-4c14-bce5-e4336b6570e5 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099: config diskstores README.txt restore.sh user 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/config 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/user 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/diskstores: GFXD-DD-DISKSTORE_76705038-10de-4b3e-955b-446546fe4036 GFXD-DEFAULT-DISKSTORE_157fa93d-c8a9-4585-ba78-9c10eb9c2ab6 USERDISKSTORE_216d5484-86f7-4e82-be81-d5bf7c2ba59f USERDISKSTORE-SNAPPY-DELTA_e7e12e86-3907-49e6-8f4c-f8a7a0d4156c Directory Contents config For internal use diskstores - GFXD-DD-DISKSTORE: Diskstores created for DataDictionary - GFXD-DEFAULT-DISKSTORE: The default diskstore. - USERDISKSTORE: Generated for diskstores created by users using CREATE DISKSTORE command. - USERDISKSTORE-SNAPPY-DELTA: Created for delta regions. user For internal use README.txt The file contains information about other files in a directory. restore.sh Script that copies files back to their original locations. Performing a Full Backup \u00b6 For each member with persistent data, the backup includes: Disk store files for all stores containing persistent tables. Backup of all disk stores including the disk stores created for metadata as well as separate disk stores created for row buffer. Configuration files from the member startup (snappydata.properties). These configuration files are not automatically restored, to avoid interfering with any more recent configurations. In particular, if these are extracted from a master jar file, copying the separate files into your working area could override the files in the jar. A restore script (restore.sh), written for the member\u2019s operating system, that copies the files back to their original locations. To perform a full backup: Start the cluster . Start the snappy-shell and connect to the cluster . Stop all transactions running in your distributed system, and do not execute DML statements during the backup. SnappyData does not support backing up a disk store while live transactions are taking place or when concurrent DML statements are being executed. Run the backup command, providing your backup directory location. ./bin/snappy backup <SnappyBackupLocation> -locators=localhost:<peer-discovery-address> Read the message that reports on the success of the operation. If the operation is successful, you see a message like this: The following disk stores were backed up: 1f5dbd41-309b-4997-a50b-95890183f8ce [<hostname>:/<LocatorLogDirectory>/datadictionary] 5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [<hostname>:/<LocatorLogDirectory>] da31492f-3234-4b7e-820b-30c6b85c19a2 [<hostname>:/<ServerLogDirectory>/snappy-internal-delta] 5a5d7ab2-96cf-4a73-8106-7a816a67f098 [<hostname>:/<ServerLogDirectory>/datadictionary] 42510800-40e3-4abf-bcc4-7b7e8c5af951 [<hostname>:/<ServerLogDirectory>] Backup successful. If the operation does not succeed, a message is displayed indicating that the backup was incomplete and is noted in the ending status message. It leaves the file INCOMPLETE_BACKUP in its highest level backup directory. Offline members leave nothing, so you only have this message from the backup operation itself. Although offline members cannot back up their disk stores, a complete backup can be obtained if at least one copy of the data is available in a running member. If the cluster is secure, you also need to specify all the security properties as command-line arguments to the backup command. The security properties you need to provide are the same as those mentioned in the configuration files in the conf directory (locators, servers or leads) when the cluster is launched. The only difference is that any valid user can run this command. That is, the user does not have to be a snappydata cluster administrator to run the backup command. For example: ./bin/snappy backup /snappydata_backup_location/ -locators=locatorhostname:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:389/ -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Optionally, you can encrypt the user's password first and use it in the above command to explicitly avoid putting the password in plain text in the command-line. Here is how you can encrypt the password Performing an Incremental backup \u00b6 An incremental backup saves the difference between the last backup and the current data. An incremental backup copies only operation logs that are not already present in the baseline directories for each member. For incremental backups, the restore script contains explicit references to operation logs in one or more previously-chained incremental backups. When the restore script is run from an incremental backup, it also restores the operation logs from previous incremental backups that are part of the backup chain. If members are missing from the baseline directory because they were offline or did not exist at the time of the baseline backup, those members place full backups of all their files into the incremental backup directory. To perform an incremental backup, execute the backup command but specify the baseline directory as well as your incremental backup directory (both can be the same directory). For example: ./bin/snappy backup -baseline=<SnappyBackupLocation> <SnappyBackupLocation> -locators=<peer-discovery-address> The tool reports on the success of the operation. If the operation is successful, you see a message like this: The following disk stores were backed up: 1f5dbd41-309b-4997-a50b-95890183f8ce [<hostname>:/<LocatorLogDirectory>/datadictionary] 5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [<hostname>:/<LocatorLogDirectory>] da31492f-3234-4b7e-820b-30c6b85c19a2 [<hostname>:/<ServerLogDirectory>/snappy-internal-delta] 5a5d7ab2-96cf-4a73-8106-7a816a67f098 [<hostname>:/<ServerLogDirectory>/datadictionary] 42510800-40e3-4abf-bcc4-7b7e8c5af951 [<hostname>:/<ServerLogDirectory>] Backup successful. A member that fails to complete its backup is noted in this ending status message and leaves the file INCOMPLETE_BACKUP. The next time you perform a backup operation a full backup is performed. To make additional incremental backups, execute the same backup command described in this section by providing the incremental backup directory and the baseline directory. List of Properties \u00b6 Option Description -baseline The directory that contains a baseline backup used for comparison during an incremental backup. The baseline directory corresponds to the backup location you specified when the last backup was performed. (For example, a baseline directory can resemble < fileServerDirectory >/< SnappyDataBackupLocation >.). An incremental backup operation backs up any data that is not already present in the specified -baseline directory. If the member cannot find previously backed up data or if the previously backed up data is corrupt, then command performs a full backup on that member. The command also performs a full backup if you omit the -baseline option. Optionally, you can provide the directory with the time stamp details, to perform an incremental backup (For example, < fileServerDirectory >/< SnappyDataBackupLocation >/< TimeStamp >). -target-directory The directory in which SnappyData stores the backup content. See Specifying the Backup Directory . -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. For example, -locators=localhost:10334 -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -J-D= Sets Java system property. For example: -J-Dgemfire.ack-wait-threshold=20 -J Prefix for any JVM property. For example -J-Xmx4g Restoring your Backup \u00b6 The restore.sh script is generated for each member in the cluster in the timestamp directory. The script (restore.sh) copies files back to their original locations. Navigate to the backup subdirectory with the timestamp of the backup that you want to restore. For example, if you performed multiple incremental backups, navigate to the latest backup directory in order to restore your system to the last available backup. Run each restore script on the host where the backup originated. <SnappyBackupLocation>/<TimestampDirectory>/restore.sh Repeat this procedure as necessary for other members of the distributed system. After all disk stores have been restored, restart all members of the original cluster. You can also do this manually: Restore your disk stores when your members are offline and the system is down. Read the restore scripts to see where the files are placed and make sure the destination locations are ready. The restore scripts do not copy over files with the same names. Run the restore scripts. Run each script on the host where the backup originated. The restore operation copies the files back to their original location. All the disk stores including users created one and disk stores for metadata are also restored. Verify the Backup is Successful \u00b6 To ensure that your backup is successful, you can try the following options: Execute the select count(*) from <TableName>; query and verify the total number of rows. Verify the table details in the SnappyData Monitoring Console . If you have done updates, you can verify to see if those specific updates are available.","title":"backup and restore"},{"location":"reference/command_line_utilities/store-backup/#backup-and-restore","text":"Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores. An online backup saves the following: For each member with persistent data, the backup includes disk store files for all stores containing persistent table data. Configuration files from the member startup. A restore script (restore.sh) copies the files back to their original locations. Note SnappyData does not support backing up disk stores on systems with live transactions, or when concurrent DML statements are being executed. If a backup of the live transaction or concurrent DML operations, is performed, there is a possibility of partial commits or partial changes of DML operations appearing in the backups. SnappyData does not support taking incremental backups on systems with live transactions, or when concurrent DML statements are being executed. Guidelines Specifying the Backup Directory Backup Directory Structure and Contents Performing a Full Backup Performing an Incremental Backup List of Properties Restoring your Backup Verify the Backup is Successful","title":"Backup and Restore"},{"location":"reference/command_line_utilities/store-backup/#guidelines","text":"Run this command during a period of low activity in your system. The backup does not block system activities, but it uses file system resources on all hosts in your distributed system and can affect performance. If you try to create backup files from a running system using file copy commands, you can get incomplete and unusable copies. Make sure the target backup directory exists and has the proper permissions for your members to write to it and create subdirectories. It is recommended to compact your disk store before running the backup. Make sure that those SnappyData members that host persistent data are running in the distributed system. Offline members cannot back up their disk stores. (A complete backup can still be performed if all table data is available in the running members).","title":"Guidelines"},{"location":"reference/command_line_utilities/store-backup/#specifying-the-backup-directory","text":"The directory you specify for backup can be used multiple times. Each backup first creates a top-level directory for the backup, under the directory you specify, identified to the minute. You can use one of two formats: Use a single physical location, such as a network file server. (For example, < fileServerDirectory >/< SnappyBackupLocation >). Use a directory that is local to all host machines in the system. (For example, < SnappyBackupLocation >).","title":"Specifying the Backup Directory"},{"location":"reference/command_line_utilities/store-backup/#backup-directory-structure-and-contents","text":"The backup directory contains a backup of the persistent data. Below is the structure of files and directories backed up in a distributed system: 2018-03-15-05-31-46: 10_80_141_112_10715_ec_v0_7393 10_80_141_112_10962_v1_57099 2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393: config diskstores README.txt restore.sh user 2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/config: 2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/user: 2018-03-15-05-31-46/2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/diskstores/ GFXD-DD-DISKSTORE_4d9fa95e-7746-4d4d-b404-2648d64cf35e GFXD-DEFAULT-DISKSTORE_3c446ce4-43e4-4c14-bce5-e4336b6570e5 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099: config diskstores README.txt restore.sh user 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/config 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/user 2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/diskstores: GFXD-DD-DISKSTORE_76705038-10de-4b3e-955b-446546fe4036 GFXD-DEFAULT-DISKSTORE_157fa93d-c8a9-4585-ba78-9c10eb9c2ab6 USERDISKSTORE_216d5484-86f7-4e82-be81-d5bf7c2ba59f USERDISKSTORE-SNAPPY-DELTA_e7e12e86-3907-49e6-8f4c-f8a7a0d4156c Directory Contents config For internal use diskstores - GFXD-DD-DISKSTORE: Diskstores created for DataDictionary - GFXD-DEFAULT-DISKSTORE: The default diskstore. - USERDISKSTORE: Generated for diskstores created by users using CREATE DISKSTORE command. - USERDISKSTORE-SNAPPY-DELTA: Created for delta regions. user For internal use README.txt The file contains information about other files in a directory. restore.sh Script that copies files back to their original locations.","title":"Backup Directory Structure and Contents"},{"location":"reference/command_line_utilities/store-backup/#performing-a-full-backup","text":"For each member with persistent data, the backup includes: Disk store files for all stores containing persistent tables. Backup of all disk stores including the disk stores created for metadata as well as separate disk stores created for row buffer. Configuration files from the member startup (snappydata.properties). These configuration files are not automatically restored, to avoid interfering with any more recent configurations. In particular, if these are extracted from a master jar file, copying the separate files into your working area could override the files in the jar. A restore script (restore.sh), written for the member\u2019s operating system, that copies the files back to their original locations. To perform a full backup: Start the cluster . Start the snappy-shell and connect to the cluster . Stop all transactions running in your distributed system, and do not execute DML statements during the backup. SnappyData does not support backing up a disk store while live transactions are taking place or when concurrent DML statements are being executed. Run the backup command, providing your backup directory location. ./bin/snappy backup <SnappyBackupLocation> -locators=localhost:<peer-discovery-address> Read the message that reports on the success of the operation. If the operation is successful, you see a message like this: The following disk stores were backed up: 1f5dbd41-309b-4997-a50b-95890183f8ce [<hostname>:/<LocatorLogDirectory>/datadictionary] 5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [<hostname>:/<LocatorLogDirectory>] da31492f-3234-4b7e-820b-30c6b85c19a2 [<hostname>:/<ServerLogDirectory>/snappy-internal-delta] 5a5d7ab2-96cf-4a73-8106-7a816a67f098 [<hostname>:/<ServerLogDirectory>/datadictionary] 42510800-40e3-4abf-bcc4-7b7e8c5af951 [<hostname>:/<ServerLogDirectory>] Backup successful. If the operation does not succeed, a message is displayed indicating that the backup was incomplete and is noted in the ending status message. It leaves the file INCOMPLETE_BACKUP in its highest level backup directory. Offline members leave nothing, so you only have this message from the backup operation itself. Although offline members cannot back up their disk stores, a complete backup can be obtained if at least one copy of the data is available in a running member. If the cluster is secure, you also need to specify all the security properties as command-line arguments to the backup command. The security properties you need to provide are the same as those mentioned in the configuration files in the conf directory (locators, servers or leads) when the cluster is launched. The only difference is that any valid user can run this command. That is, the user does not have to be a snappydata cluster administrator to run the backup command. For example: ./bin/snappy backup /snappydata_backup_location/ -locators=locatorhostname:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:389/ -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Optionally, you can encrypt the user's password first and use it in the above command to explicitly avoid putting the password in plain text in the command-line. Here is how you can encrypt the password","title":"Performing a Full Backup"},{"location":"reference/command_line_utilities/store-backup/#performing-an-incremental-backup","text":"An incremental backup saves the difference between the last backup and the current data. An incremental backup copies only operation logs that are not already present in the baseline directories for each member. For incremental backups, the restore script contains explicit references to operation logs in one or more previously-chained incremental backups. When the restore script is run from an incremental backup, it also restores the operation logs from previous incremental backups that are part of the backup chain. If members are missing from the baseline directory because they were offline or did not exist at the time of the baseline backup, those members place full backups of all their files into the incremental backup directory. To perform an incremental backup, execute the backup command but specify the baseline directory as well as your incremental backup directory (both can be the same directory). For example: ./bin/snappy backup -baseline=<SnappyBackupLocation> <SnappyBackupLocation> -locators=<peer-discovery-address> The tool reports on the success of the operation. If the operation is successful, you see a message like this: The following disk stores were backed up: 1f5dbd41-309b-4997-a50b-95890183f8ce [<hostname>:/<LocatorLogDirectory>/datadictionary] 5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [<hostname>:/<LocatorLogDirectory>] da31492f-3234-4b7e-820b-30c6b85c19a2 [<hostname>:/<ServerLogDirectory>/snappy-internal-delta] 5a5d7ab2-96cf-4a73-8106-7a816a67f098 [<hostname>:/<ServerLogDirectory>/datadictionary] 42510800-40e3-4abf-bcc4-7b7e8c5af951 [<hostname>:/<ServerLogDirectory>] Backup successful. A member that fails to complete its backup is noted in this ending status message and leaves the file INCOMPLETE_BACKUP. The next time you perform a backup operation a full backup is performed. To make additional incremental backups, execute the same backup command described in this section by providing the incremental backup directory and the baseline directory.","title":"Performing an Incremental backup"},{"location":"reference/command_line_utilities/store-backup/#list-of-properties","text":"Option Description -baseline The directory that contains a baseline backup used for comparison during an incremental backup. The baseline directory corresponds to the backup location you specified when the last backup was performed. (For example, a baseline directory can resemble < fileServerDirectory >/< SnappyDataBackupLocation >.). An incremental backup operation backs up any data that is not already present in the specified -baseline directory. If the member cannot find previously backed up data or if the previously backed up data is corrupt, then command performs a full backup on that member. The command also performs a full backup if you omit the -baseline option. Optionally, you can provide the directory with the time stamp details, to perform an incremental backup (For example, < fileServerDirectory >/< SnappyDataBackupLocation >/< TimeStamp >). -target-directory The directory in which SnappyData stores the backup content. See Specifying the Backup Directory . -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. For example, -locators=localhost:10334 -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -J-D= Sets Java system property. For example: -J-Dgemfire.ack-wait-threshold=20 -J Prefix for any JVM property. For example -J-Xmx4g","title":"List of Properties"},{"location":"reference/command_line_utilities/store-backup/#restoring-your-backup","text":"The restore.sh script is generated for each member in the cluster in the timestamp directory. The script (restore.sh) copies files back to their original locations. Navigate to the backup subdirectory with the timestamp of the backup that you want to restore. For example, if you performed multiple incremental backups, navigate to the latest backup directory in order to restore your system to the last available backup. Run each restore script on the host where the backup originated. <SnappyBackupLocation>/<TimestampDirectory>/restore.sh Repeat this procedure as necessary for other members of the distributed system. After all disk stores have been restored, restart all members of the original cluster. You can also do this manually: Restore your disk stores when your members are offline and the system is down. Read the restore scripts to see where the files are placed and make sure the destination locations are ready. The restore scripts do not copy over files with the same names. Run the restore scripts. Run each script on the host where the backup originated. The restore operation copies the files back to their original location. All the disk stores including users created one and disk stores for metadata are also restored.","title":"Restoring your Backup"},{"location":"reference/command_line_utilities/store-backup/#verify-the-backup-is-successful","text":"To ensure that your backup is successful, you can try the following options: Execute the select count(*) from <TableName>; query and verify the total number of rows. Verify the table details in the SnappyData Monitoring Console . If you have done updates, you can verify to see if those specific updates are available.","title":"Verify the Backup is Successful"},{"location":"reference/command_line_utilities/store-compact-all-disk-stores/","text":"compact-all-disk-stores \u00b6 Perform online compaction of SnappyData disk stores. Syntax \u00b6 For secured cluster ./bin/snappy compact-all-disk-stores -locators==<addresses> -auth-provider=<authprovider> -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> For non-secured cluster ./bin/snappy compact-all-disk-stores== <-locators=<addresses>> [-bind-address=<address>] [-<prop-name>=<prop-value>]* The table describes options for snappy compact-all-disk-stores . Option Description -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default snappy uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name prop-value Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites . Description \u00b6 When a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files. Manual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains. Offline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started. Online Compaction \u00b6 To run manual online compaction, ALLOWFORCECOMPACTION should be set to true while creating a diskstore You can run manual online compaction at any time while the system is running. Oplogs that are eligible for compaction, based on the COMPACTIONTHRESHOLD, are compacted into the current oplog. Example \u00b6 Secured cluster ./bin/snappy compact-all-disk-stores -locators=locatorhostname:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123 // The following output is displayed: Connecting to distributed system: locators=localhost[10334] 18/11/15 17:54:02.964 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/15 17:54:03.757 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@11a82d0f configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) Compaction complete. The following disk stores compacted some files: Non-secured cluster ./bin/snappy compact-all-disk-stores==locators=locatorhostname:10334*","title":"compact-all-disk-stores"},{"location":"reference/command_line_utilities/store-compact-all-disk-stores/#compact-all-disk-stores","text":"Perform online compaction of SnappyData disk stores.","title":"compact-all-disk-stores"},{"location":"reference/command_line_utilities/store-compact-all-disk-stores/#syntax","text":"For secured cluster ./bin/snappy compact-all-disk-stores -locators==<addresses> -auth-provider=<authprovider> -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> For non-secured cluster ./bin/snappy compact-all-disk-stores== <-locators=<addresses>> [-bind-address=<address>] [-<prop-name>=<prop-value>]* The table describes options for snappy compact-all-disk-stores . Option Description -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default snappy uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name prop-value Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites .","title":"Syntax"},{"location":"reference/command_line_utilities/store-compact-all-disk-stores/#description","text":"When a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files. Manual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains. Offline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started.","title":"Description"},{"location":"reference/command_line_utilities/store-compact-all-disk-stores/#online-compaction","text":"To run manual online compaction, ALLOWFORCECOMPACTION should be set to true while creating a diskstore You can run manual online compaction at any time while the system is running. Oplogs that are eligible for compaction, based on the COMPACTIONTHRESHOLD, are compacted into the current oplog.","title":"Online Compaction"},{"location":"reference/command_line_utilities/store-compact-all-disk-stores/#example","text":"Secured cluster ./bin/snappy compact-all-disk-stores -locators=locatorhostname:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123 // The following output is displayed: Connecting to distributed system: locators=localhost[10334] 18/11/15 17:54:02.964 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/15 17:54:03.757 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@11a82d0f configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) Compaction complete. The following disk stores compacted some files: Non-secured cluster ./bin/snappy compact-all-disk-stores==locators=locatorhostname:10334*","title":"Example"},{"location":"reference/command_line_utilities/store-compact-disk-store/","text":"compact-disk-store \u00b6 Perform offline compaction of a single SnappyData disk store. Syntax \u00b6 ./bin/snappy compact-disk-store <diskStoreName> <directory>+ [-maxOplogSize=<int>] Description \u00b6 Note Do not perform offline compaction on the baseline directory of an incremental backup. When a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files. Manual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains. Offline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started. Note You must provide all of the directories in the disk store. If no oplog max size is specified, SnappyData uses the system default. Offline compaction can consume a large amount of memory. If you get a java.lang.OutOfMemory error while running this command, you made need to increase the heap size by setting the -Xmx and -Xms options in the JAVA_ARGS environment variable. Command Line Utilites provides more information about setting Java options. Example \u00b6 ./bin/snappy compact-disk-store myDiskStoreName /firstDir /secondDir maxOplogSize=maxMegabytesForOplog The output of this command is similar to: Offline compaction removed 12 records. Total number of region entries in this disk store is: 7","title":"compact-disk-store"},{"location":"reference/command_line_utilities/store-compact-disk-store/#compact-disk-store","text":"Perform offline compaction of a single SnappyData disk store.","title":"compact-disk-store"},{"location":"reference/command_line_utilities/store-compact-disk-store/#syntax","text":"./bin/snappy compact-disk-store <diskStoreName> <directory>+ [-maxOplogSize=<int>]","title":"Syntax"},{"location":"reference/command_line_utilities/store-compact-disk-store/#description","text":"Note Do not perform offline compaction on the baseline directory of an incremental backup. When a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files. Manual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains. Offline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started. Note You must provide all of the directories in the disk store. If no oplog max size is specified, SnappyData uses the system default. Offline compaction can consume a large amount of memory. If you get a java.lang.OutOfMemory error while running this command, you made need to increase the heap size by setting the -Xmx and -Xms options in the JAVA_ARGS environment variable. Command Line Utilites provides more information about setting Java options.","title":"Description"},{"location":"reference/command_line_utilities/store-compact-disk-store/#example","text":"./bin/snappy compact-disk-store myDiskStoreName /firstDir /secondDir maxOplogSize=maxMegabytesForOplog The output of this command is similar to: Offline compaction removed 12 records. Total number of region entries in this disk store is: 7","title":"Example"},{"location":"reference/command_line_utilities/store-list-missing-disk-stores/","text":"list-missing-disk-stores \u00b6 Lists all disk stores with the most recent data for which other members are waiting. Syntax \u00b6 Secured cluster ./bin/snappy list-missing-disk-stores -locators=<addresses> -auth-provider=<auth-provider> -user=<username> -password=<password> -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:<ldap-server-port>/ -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Non-secured cluster ./bin/snappy list-missing-disk-stores -locators=localhost:bind address If no locator option is specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect. The table describes options for snappy list-missing-disk-stores. Option Description -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites . Example \u00b6 Secured cluster ./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123 Connecting to distributed system: locators=localhost[10334] 18/11/15 18:08:26.802 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/15 18:08:27.575 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) The distributed system did not have any missing disk stores Non-secured Cluster ./bin/snappy list-missing-disk-stores -locators=localhost:10334 Connecting to distributed system: locators=localhost[10334] log4j:WARN No appenders could be found for logger (com.gemstone.org.jgroups.util.GemFireTracer). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 18/11/19 14:08:33 INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@33a053d configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) The distributed system did not have any missing disk stores","title":"list-missing-disk-stores"},{"location":"reference/command_line_utilities/store-list-missing-disk-stores/#list-missing-disk-stores","text":"Lists all disk stores with the most recent data for which other members are waiting.","title":"list-missing-disk-stores"},{"location":"reference/command_line_utilities/store-list-missing-disk-stores/#syntax","text":"Secured cluster ./bin/snappy list-missing-disk-stores -locators=<addresses> -auth-provider=<auth-provider> -user=<username> -password=<password> -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:<ldap-server-port>/ -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Non-secured cluster ./bin/snappy list-missing-disk-stores -locators=localhost:bind address If no locator option is specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect. The table describes options for snappy list-missing-disk-stores. Option Description -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites .","title":"Syntax"},{"location":"reference/command_line_utilities/store-list-missing-disk-stores/#example","text":"Secured cluster ./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123 Connecting to distributed system: locators=localhost[10334] 18/11/15 18:08:26.802 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/15 18:08:27.575 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) The distributed system did not have any missing disk stores Non-secured Cluster ./bin/snappy list-missing-disk-stores -locators=localhost:10334 Connecting to distributed system: locators=localhost[10334] log4j:WARN No appenders could be found for logger (com.gemstone.org.jgroups.util.GemFireTracer). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 18/11/19 14:08:33 INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@33a053d configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) The distributed system did not have any missing disk stores","title":"Example"},{"location":"reference/command_line_utilities/store-revoke-missing-disk-stores/","text":"revoke-missing-disk-store \u00b6 Instruct SnappyData members to stop waiting for a disk store to become available. Syntax \u00b6 Secured cluster ./bin/snappy revoke-missing-disk-store -locators=<addresses> -auth-provider=<auth-provider> -user=<username> -password=<password> -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:<ldap-server-port>/ -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Non-secured cluster ./bin/snappy revoke-missing-disk-store <disk-store-id> <-locators=<addresses>> [-bind-address=<address>] [-<prop-name>=<prop-value>]* The table describes options and arguments for snappy revoke-missing-disk-store . If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect. Option Description -disk-store-id (Required.) Specifies the unique ID of the disk store to revoke. -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites . Example \u00b6 Secured cluster The following example depicts how to revoke the missing disk stores in a secured cluster: Using the following command, you must first list the missing disk stores: ./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123 Next, run the revoke-missing-disk-store command to revoke the missing disk stores in case more recent data is available: ./bin/snappy revoke-missing-disk-store -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:389/ -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Connecting to distributed system: locators=localhost[10334] 18/11/16 16:24:37.187 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/16 16:24:38.025 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) revocation was successful and no disk stores are now missing Finally, you can use the same list-missing-disk-stores command to confirm that no disk stores are missing. Non-secured cluster The following example depicts how to revoke the missing disk stores in a non-secured cluster: Using the following command, you must first list the missing disk stores: ./bin/snappy list-missing-disk-stores -locators=localhost:10334 Connecting to distributed system: -locators=localhost:10334 1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary] Next, run the revoke-missing-disk-store command to revoke the missing disk stores in case more recent data is available: ./bin/snappy revoke-missing-disk-store 1f811502-f126-4ce4-9839-9549335b734d -locators=localhost:10334 Connecting to distributed system: -locators=localhost:10334 revocation was successful and no disk stores are now missing Finally, use the list-missing-disk-stores command to confirm that none of the disk stores are missing.","title":"revoke-missing-disk-store"},{"location":"reference/command_line_utilities/store-revoke-missing-disk-stores/#revoke-missing-disk-store","text":"Instruct SnappyData members to stop waiting for a disk store to become available.","title":"revoke-missing-disk-store"},{"location":"reference/command_line_utilities/store-revoke-missing-disk-stores/#syntax","text":"Secured cluster ./bin/snappy revoke-missing-disk-store -locators=<addresses> -auth-provider=<auth-provider> -user=<username> -password=<password> -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:<ldap-server-port>/ -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Non-secured cluster ./bin/snappy revoke-missing-disk-store <disk-store-id> <-locators=<addresses>> [-bind-address=<address>] [-<prop-name>=<prop-value>]* The table describes options and arguments for snappy revoke-missing-disk-store . If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect. Option Description -disk-store-id (Required.) Specifies the unique ID of the disk store to revoke. -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites .","title":"Syntax"},{"location":"reference/command_line_utilities/store-revoke-missing-disk-stores/#example","text":"Secured cluster The following example depicts how to revoke the missing disk stores in a secured cluster: Using the following command, you must first list the missing disk stores: ./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123 Next, run the revoke-missing-disk-store command to revoke the missing disk stores in case more recent data is available: ./bin/snappy revoke-missing-disk-store -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:389/ -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Connecting to distributed system: locators=localhost[10334] 18/11/16 16:24:37.187 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/16 16:24:38.025 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) revocation was successful and no disk stores are now missing Finally, you can use the same list-missing-disk-stores command to confirm that no disk stores are missing. Non-secured cluster The following example depicts how to revoke the missing disk stores in a non-secured cluster: Using the following command, you must first list the missing disk stores: ./bin/snappy list-missing-disk-stores -locators=localhost:10334 Connecting to distributed system: -locators=localhost:10334 1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary] Next, run the revoke-missing-disk-store command to revoke the missing disk stores in case more recent data is available: ./bin/snappy revoke-missing-disk-store 1f811502-f126-4ce4-9839-9549335b734d -locators=localhost:10334 Connecting to distributed system: -locators=localhost:10334 revocation was successful and no disk stores are now missing Finally, use the list-missing-disk-stores command to confirm that none of the disk stores are missing.","title":"Example"},{"location":"reference/command_line_utilities/store-run/","text":"run \u00b6 Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell. Syntax \u00b6 ./bin/snappy run -file=<path or URL> [-auth-provider=<name>] [-client-bind-address=<address>] [-client-port=<port>] [-encoding=<charset>] [-extra-conn-props=<properties>] [-help] [-ignore-errors] [-J-D<property=value>] [-password[=<password>]] [-path=<path>] [-user=<username>] This table describes options for the snappy run command. Default values are used if you do not specify an option. Option Description -file The local path of a SQL command file to execute, or a URL that links to the SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell. This argument is required. -auth-provider Sets the authentication provider to use for peer-to-peer connections as well as client-server connections. Valid values are BUILTIN and LDAP. All other members of the SnappyData distributed system must use the same authentication provider and user definitions. If you omit this option, the connection uses no authentication mechanism. -client-bind-address Set the hostname or IP address to which the locator or server listens on for JDBC/ODBC/thrift client connections. -client-port The port on which a SnappyData locator listens for client connections. The default is 1527. Use this option with -client-bind-address to attach to a SnappyData cluster as a thin client and perform the command. -encoding The character set encoding of the SQL script file ( -file argument). The default is UTF-8. Other possible values are: US-ASCII, ISO-8859-1, UTF-8, UTF-16BE, UTF-16LE, UTF-16. See the java.nio.charset.Charset reference for more information. -extra-conn-props A semicolon-separated list of properties to use when connecting to the SnappyData distributed system. help, --help Display the help message for this snappy command. -ignore-errors Include this option to ignore any errors that may occur while executing statements in the file, and continue executing the remaining statements. If you omit this option, then snappy immediately terminates the script's execution if an exception occurs. -J-D;property=value; Sets Java system property to the specified value. -password If the servers or locators have been configured to use authentication, this option specifies the password for the user (specified with the -user option) to use for booting the server and joining the distributed system. The password value is optional. If you omit the password, you are prompted to enter a password from the console. -path Configures the working directory for any other SQL command files executed from within the script. The -path entry is prepended to any SQL script file name executed that the script executes in a run command. -user If the servers or locators have been configured to use authentication, this option specifies the username to use for booting the server and joining the distributed system. Description \u00b6 Specify the below command to connect to a SnappyData Distributed system and execute a SQL command file: Use both -client-bind-address and -client-port to connect to a SnappyData cluster as a thin client and perform the command. The -file argument specifies the location of the SQL script file to execute. If the script file itself calls other script files using run 'filename' , also consider using the -path option to specify the location of the embedded script files. If an exception occurs while executing the script, SnappyData immediately stops executing script commands, unless you include the -ignore-errors option. Examples \u00b6 This command connects to a SnappyData network server running on localhost:1527 and executes commands in the create_and_load_column_table.sql file: ./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 If the script calls for dependent scripts (for example load_countries.sql, load_cities.sql) and if the command is executed outside the directory in which the dependent scripts are located, specify the working directory using the -path option. ./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -path=/home/user1/snappydata/examples/quickstart -client-bind-address=localhost -client-port=1527 You can also run the command by providing the username and password. ./bin/snappy run -file=/home/supriya/snappy/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 -user=user1 -password=user123","title":"run"},{"location":"reference/command_line_utilities/store-run/#run","text":"Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.","title":"run"},{"location":"reference/command_line_utilities/store-run/#syntax","text":"./bin/snappy run -file=<path or URL> [-auth-provider=<name>] [-client-bind-address=<address>] [-client-port=<port>] [-encoding=<charset>] [-extra-conn-props=<properties>] [-help] [-ignore-errors] [-J-D<property=value>] [-password[=<password>]] [-path=<path>] [-user=<username>] This table describes options for the snappy run command. Default values are used if you do not specify an option. Option Description -file The local path of a SQL command file to execute, or a URL that links to the SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell. This argument is required. -auth-provider Sets the authentication provider to use for peer-to-peer connections as well as client-server connections. Valid values are BUILTIN and LDAP. All other members of the SnappyData distributed system must use the same authentication provider and user definitions. If you omit this option, the connection uses no authentication mechanism. -client-bind-address Set the hostname or IP address to which the locator or server listens on for JDBC/ODBC/thrift client connections. -client-port The port on which a SnappyData locator listens for client connections. The default is 1527. Use this option with -client-bind-address to attach to a SnappyData cluster as a thin client and perform the command. -encoding The character set encoding of the SQL script file ( -file argument). The default is UTF-8. Other possible values are: US-ASCII, ISO-8859-1, UTF-8, UTF-16BE, UTF-16LE, UTF-16. See the java.nio.charset.Charset reference for more information. -extra-conn-props A semicolon-separated list of properties to use when connecting to the SnappyData distributed system. help, --help Display the help message for this snappy command. -ignore-errors Include this option to ignore any errors that may occur while executing statements in the file, and continue executing the remaining statements. If you omit this option, then snappy immediately terminates the script's execution if an exception occurs. -J-D;property=value; Sets Java system property to the specified value. -password If the servers or locators have been configured to use authentication, this option specifies the password for the user (specified with the -user option) to use for booting the server and joining the distributed system. The password value is optional. If you omit the password, you are prompted to enter a password from the console. -path Configures the working directory for any other SQL command files executed from within the script. The -path entry is prepended to any SQL script file name executed that the script executes in a run command. -user If the servers or locators have been configured to use authentication, this option specifies the username to use for booting the server and joining the distributed system.","title":"Syntax"},{"location":"reference/command_line_utilities/store-run/#description","text":"Specify the below command to connect to a SnappyData Distributed system and execute a SQL command file: Use both -client-bind-address and -client-port to connect to a SnappyData cluster as a thin client and perform the command. The -file argument specifies the location of the SQL script file to execute. If the script file itself calls other script files using run 'filename' , also consider using the -path option to specify the location of the embedded script files. If an exception occurs while executing the script, SnappyData immediately stops executing script commands, unless you include the -ignore-errors option.","title":"Description"},{"location":"reference/command_line_utilities/store-run/#examples","text":"This command connects to a SnappyData network server running on localhost:1527 and executes commands in the create_and_load_column_table.sql file: ./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 If the script calls for dependent scripts (for example load_countries.sql, load_cities.sql) and if the command is executed outside the directory in which the dependent scripts are located, specify the working directory using the -path option. ./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -path=/home/user1/snappydata/examples/quickstart -client-bind-address=localhost -client-port=1527 You can also run the command by providing the username and password. ./bin/snappy run -file=/home/supriya/snappy/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 -user=user1 -password=user123","title":"Examples"},{"location":"reference/command_line_utilities/store-unblock-disk-stores/","text":"unblock-disk-store \u00b6 Indicates a member waiting for other diskStoreID to go ahead with the initialization. When a member recovers from a set of persistent files, it waits for other members that were also persisting the same region to start up. If the persistent files for those other members were lost or not available, this method can be used to tell the members to stop waiting for that data and consider its own data as latest. Syntax \u00b6 Secured cluster ./bin/snappy unblock-disk-store<disk-store-id> -locators=localhost:<addresses> -auth-provider=<auth-provider> -user=<username> -password=<password> -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:<ldap-server-port>/ -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Non-secured cluster ./bin/snappy unblock-disk-store <disk-store-id> <-locators=<addresses>> [-bind-address=<address>] [-<prop-name>=<prop-value>]* The table describes options and arguments for snappy unblock-disk-store. If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect. Option Description -disk-store-id (Required.) Specifies the unique ID of the disk store to unblock. -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites . Example \u00b6 Secured cluster ./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:389/ -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Connecting to distributed system: locators=localhost[10334] 18/11/16 16:26:56.050 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/16 16:26:56.863 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) Unblock was successful and no disk stores are now waiting Non-secured cluster ./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334 Connecting to distributed system: -locators=localhost:10334 Unblock was successful and no disk stores are now waiting","title":"unblock-disk-store"},{"location":"reference/command_line_utilities/store-unblock-disk-stores/#unblock-disk-store","text":"Indicates a member waiting for other diskStoreID to go ahead with the initialization. When a member recovers from a set of persistent files, it waits for other members that were also persisting the same region to start up. If the persistent files for those other members were lost or not available, this method can be used to tell the members to stop waiting for that data and consider its own data as latest.","title":"unblock-disk-store"},{"location":"reference/command_line_utilities/store-unblock-disk-stores/#syntax","text":"Secured cluster ./bin/snappy unblock-disk-store<disk-store-id> -locators=localhost:<addresses> -auth-provider=<auth-provider> -user=<username> -password=<password> -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:<ldap-server-port>/ -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Non-secured cluster ./bin/snappy unblock-disk-store <disk-store-id> <-locators=<addresses>> [-bind-address=<address>] [-<prop-name>=<prop-value>]* The table describes options and arguments for snappy unblock-disk-store. If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect. Option Description -disk-store-id (Required.) Specifies the unique ID of the disk store to unblock. -locators List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. -bind-address The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address. -prop-name Any other SnappyData distributed system property. Authentication properties Refer Authentication Properites .","title":"Syntax"},{"location":"reference/command_line_utilities/store-unblock-disk-stores/#example","text":"Secured cluster ./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://<ldap-server-host>:389/ -user=<username> -password=<password> -gemfirexd.auth-ldap-search-base=<search-base-values> -gemfirexd.auth-ldap-search-dn=<search-dn-values> -gemfirexd.auth-ldap-search-pw=<password> Connecting to distributed system: locators=localhost[10334] 18/11/16 16:26:56.050 IST main<tid=0x1> INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP 18/11/16 16:26:56.863 IST main<tid=0x1> INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration: Total Usable Heap = 786.2 MB (824374722) Storage Pool = 393.1 MB (412187361) Execution Pool = 393.1 MB (412187361) Max Storage Pool Size = 628.9 MB (659499777) Unblock was successful and no disk stores are now waiting Non-secured cluster ./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334 Connecting to distributed system: -locators=localhost:10334 Unblock was successful and no disk stores are now waiting","title":"Example"},{"location":"reference/command_line_utilities/store-version/","text":"version \u00b6 Prints information about the SnappyData product version. Syntax \u00b6 ./bin/snappy version Example \u00b6 ./bin/snappy version SnappyData Platform Version 1.1.1 SnappyData RowStore 1.6.3 SnappyData Column Store 1.1.1","title":"version"},{"location":"reference/command_line_utilities/store-version/#version","text":"Prints information about the SnappyData product version.","title":"version"},{"location":"reference/command_line_utilities/store-version/#syntax","text":"./bin/snappy version","title":"Syntax"},{"location":"reference/command_line_utilities/store-version/#example","text":"./bin/snappy version SnappyData Platform Version 1.1.1 SnappyData RowStore 1.6.3 SnappyData Column Store 1.1.1","title":"Example"},{"location":"reference/command_line_utilities/validate_diskstore/","text":"validate-disk-store \u00b6 Verifies the health of an offline disk store and provides information about the tables using that disk store. Syntax \u00b6 ./bin/snappypy validate-disk-store <diskStoreName> <directory>+ In the syntax, you must provide the name of disk store to be validated and the path which stores the disk store files. Description \u00b6 The SnappyData validate-disk-store command verifies the health of your offline disk store and gives you information about the following: * Tables that are using that disk store * Total number of rows * Number of records that would be removed, if you have compacted the store. You can use this command: Before compacting an offline disk store, to determine whether it is worthwhile. Before restoring a disk store. Any time you want to ensure the disk store is in good shape. Example \u00b6 ./bin/snappy validate-disk-store GFXD-DEFAULT-DISKSTORE /home/xyz/<snappydata_install_dir>/work/localhost-server-1 This command displays an output as shown in the following example: log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. /__UUID_PERSIST: entryCount=42 /APP/SNAPPYSYS_INTERNAL____AIRLINE_COLUMN_STORE_ entryCount=0 bucketCount=8 /partitioned_region entryCount=6 bucketCount=10 Disk store contains 1 compactable records. Total number of region entries in this disk store is: 6","title":"validate-disk-store"},{"location":"reference/command_line_utilities/validate_diskstore/#validate-disk-store","text":"Verifies the health of an offline disk store and provides information about the tables using that disk store.","title":"validate-disk-store"},{"location":"reference/command_line_utilities/validate_diskstore/#syntax","text":"./bin/snappypy validate-disk-store <diskStoreName> <directory>+ In the syntax, you must provide the name of disk store to be validated and the path which stores the disk store files.","title":"Syntax"},{"location":"reference/command_line_utilities/validate_diskstore/#description","text":"The SnappyData validate-disk-store command verifies the health of your offline disk store and gives you information about the following: * Tables that are using that disk store * Total number of rows * Number of records that would be removed, if you have compacted the store. You can use this command: Before compacting an offline disk store, to determine whether it is worthwhile. Before restoring a disk store. Any time you want to ensure the disk store is in good shape.","title":"Description"},{"location":"reference/command_line_utilities/validate_diskstore/#example","text":"./bin/snappy validate-disk-store GFXD-DEFAULT-DISKSTORE /home/xyz/<snappydata_install_dir>/work/localhost-server-1 This command displays an output as shown in the following example: log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. /__UUID_PERSIST: entryCount=42 /APP/SNAPPYSYS_INTERNAL____AIRLINE_COLUMN_STORE_ entryCount=0 bucketCount=8 /partitioned_region entryCount=6 bucketCount=10 Disk store contains 1 compactable records. Total number of region entries in this disk store is: 6","title":"Example"},{"location":"reference/configuration_parameters/","text":"Configuration Properties \u00b6 You use JDBC connection properties, connection boot properties, and Java system properties to configure SnappyData members and connections. Property Types Property Prefixes Using Non-ASCII Strings in SnappyData Property Files List of Property Names Property Types \u00b6 SnappyData configuration properties are divided into the following property types: Connection properties . Connection properties configure the features of a SnappyData member or a SnappyData client connection when you start or connect to a SnappyData member. You can define connection properties directly in the JDBC connection URL (or in the \"connect\" command in an interactive SnappyData session). You can also define connection properties in the gemfirexd.properties file or as Java system properties. For example, use -J-D property_name = property_value when you execute a snappy utility. Or, use the JAVA_ARGS environment variable to define a Java system property for an interactive snappy session (JAVA_ARGS=\"-D property_name = property_value \"). Note You must add a prefix to certain connection property names in order to specify those properties as Java system properties. See Property Prefixes . Connection properties can be further categorized as either boot properties or client properties : Boot properties . A boot connection property configures features of a SnappyData member, and can only be applied along with the first connection that starts a SnappyData member. Boot properties have no effect when they are specified on connections to a member after the member has started. Boot properties have no effect when they are specified on a thin client connection. Client properties . A client connection property configures features of the client connection itself and can be used with the JDBC thin client drive (for example, using a JDBC thin client connection URL or the connect client command from an interactive snappy session). System properties . Certain SnappyData configuration properties must be specified either as Java system properties (using -J-D property_name = property_value with a snappy utility or setting JAVA_ARGS=\"-D property_name = property_value \" for an interactive snappy session). You cannot define these properties in a JDBC URL connection. Many of SnappyData system properties affect features of the SnappyData member at boot time and can be optionally defined in the gemfirexd.properties file. See the property description to determine whether or not a system property can be defined in gemfirexd.properties. The names of SnappyData system properties always include the snappydata. prefix. For example, all properties that configure LDAP server information for user authentication must be specified as Java system properties, rather than JDBC properties, when you boot a server. Certain properties have additional behaviors or restrictions. See the individual property descriptions for more information. Property Prefixes \u00b6 You must add a prefix to connection and boot property names when you define those properties as Java system properties. The Prefix row in each property table lists a prefix value ( snappydata. or gemfire. ) when one is required. Do not use an indicated prefix when you specify the property in a connection string. If no prefix is specified, use only the indicated property name in all circumstances. For example, use \"host-data\" whether you define this property in gemfirexd.properties, as a Java system property, or as a property definition for FabricServer. Using Non-ASCII Strings in SnappyData Property Files \u00b6 You can specify Unicode (non-ASCII) characters in SnappyData property files by using a \\uXXXX escape sequence. For a supplementary character, you need two escape sequences, one for each of the two UTF-16 code units. The XXXX denotes the 4 hexadecimal digits for the value of the UTF-16 code unit. For example, a properties file might have the following entries: s1=hello there s2=\\u3053\\u3093\\u306b\\u3061\\u306f pre For example, in `gemfirexd.properties`, you might write: ```pre log-file=my\\u00df.log to indicate the desired property definition of log-file=my.log . If you have edited and saved the file in a non-ASCII encoding, you can convert it to ASCII with the native2ascii tool included in your Oracle Java distribution. For example, you might want to do this when editing a properties file in Shift_JIS, a popular Japanese encoding. List of Property Names \u00b6 Below is the list of all the configuration properties and links for each property reference page. ack-severe-alert-threshold ack-wait-threshold allow-explicit-commit archive-disk-space-limit archive-file-size-limit bind-address enable-network-partition-detection enable-stats enable-time-statistics enable-timestats enforce-unique-host init-scripts load-balance locators log-file log-level member-timeout membership-port-range password read-timeout redundancy-zone secondary-locators skip-constraint-checks skip-locks socket-buffer-size socket-lease-time gemfirexd.datadictionary.allow-startup-errors gemfirexd.default-startup-recovery-delay snappy.history gemfirexd.max-lock-wait gemfirexd.query-cancellation-interval gemfirexd.query-timeout ssl-enabled ssl-ciphers ssl-protocols ssl-require-authentication start-locator statistic-archive-file statistic-sample-rate statistic-sampling-enabled sync-commits sys-disk-dir user","title":"Configuration Properties"},{"location":"reference/configuration_parameters/#configuration-properties","text":"You use JDBC connection properties, connection boot properties, and Java system properties to configure SnappyData members and connections. Property Types Property Prefixes Using Non-ASCII Strings in SnappyData Property Files List of Property Names","title":"Configuration Properties"},{"location":"reference/configuration_parameters/#property-types","text":"SnappyData configuration properties are divided into the following property types: Connection properties . Connection properties configure the features of a SnappyData member or a SnappyData client connection when you start or connect to a SnappyData member. You can define connection properties directly in the JDBC connection URL (or in the \"connect\" command in an interactive SnappyData session). You can also define connection properties in the gemfirexd.properties file or as Java system properties. For example, use -J-D property_name = property_value when you execute a snappy utility. Or, use the JAVA_ARGS environment variable to define a Java system property for an interactive snappy session (JAVA_ARGS=\"-D property_name = property_value \"). Note You must add a prefix to certain connection property names in order to specify those properties as Java system properties. See Property Prefixes . Connection properties can be further categorized as either boot properties or client properties : Boot properties . A boot connection property configures features of a SnappyData member, and can only be applied along with the first connection that starts a SnappyData member. Boot properties have no effect when they are specified on connections to a member after the member has started. Boot properties have no effect when they are specified on a thin client connection. Client properties . A client connection property configures features of the client connection itself and can be used with the JDBC thin client drive (for example, using a JDBC thin client connection URL or the connect client command from an interactive snappy session). System properties . Certain SnappyData configuration properties must be specified either as Java system properties (using -J-D property_name = property_value with a snappy utility or setting JAVA_ARGS=\"-D property_name = property_value \" for an interactive snappy session). You cannot define these properties in a JDBC URL connection. Many of SnappyData system properties affect features of the SnappyData member at boot time and can be optionally defined in the gemfirexd.properties file. See the property description to determine whether or not a system property can be defined in gemfirexd.properties. The names of SnappyData system properties always include the snappydata. prefix. For example, all properties that configure LDAP server information for user authentication must be specified as Java system properties, rather than JDBC properties, when you boot a server. Certain properties have additional behaviors or restrictions. See the individual property descriptions for more information.","title":"Property Types"},{"location":"reference/configuration_parameters/#property-prefixes","text":"You must add a prefix to connection and boot property names when you define those properties as Java system properties. The Prefix row in each property table lists a prefix value ( snappydata. or gemfire. ) when one is required. Do not use an indicated prefix when you specify the property in a connection string. If no prefix is specified, use only the indicated property name in all circumstances. For example, use \"host-data\" whether you define this property in gemfirexd.properties, as a Java system property, or as a property definition for FabricServer.","title":"Property Prefixes"},{"location":"reference/configuration_parameters/#using-non-ascii-strings-in-snappydata-property-files","text":"You can specify Unicode (non-ASCII) characters in SnappyData property files by using a \\uXXXX escape sequence. For a supplementary character, you need two escape sequences, one for each of the two UTF-16 code units. The XXXX denotes the 4 hexadecimal digits for the value of the UTF-16 code unit. For example, a properties file might have the following entries: s1=hello there s2=\\u3053\\u3093\\u306b\\u3061\\u306f pre For example, in `gemfirexd.properties`, you might write: ```pre log-file=my\\u00df.log to indicate the desired property definition of log-file=my.log . If you have edited and saved the file in a non-ASCII encoding, you can convert it to ASCII with the native2ascii tool included in your Oracle Java distribution. For example, you might want to do this when editing a properties file in Shift_JIS, a popular Japanese encoding.","title":"Using Non-ASCII Strings in SnappyData Property Files"},{"location":"reference/configuration_parameters/#list-of-property-names","text":"Below is the list of all the configuration properties and links for each property reference page. ack-severe-alert-threshold ack-wait-threshold allow-explicit-commit archive-disk-space-limit archive-file-size-limit bind-address enable-network-partition-detection enable-stats enable-time-statistics enable-timestats enforce-unique-host init-scripts load-balance locators log-file log-level member-timeout membership-port-range password read-timeout redundancy-zone secondary-locators skip-constraint-checks skip-locks socket-buffer-size socket-lease-time gemfirexd.datadictionary.allow-startup-errors gemfirexd.default-startup-recovery-delay snappy.history gemfirexd.max-lock-wait gemfirexd.query-cancellation-interval gemfirexd.query-timeout ssl-enabled ssl-ciphers ssl-protocols ssl-require-authentication start-locator statistic-archive-file statistic-sample-rate statistic-sampling-enabled sync-commits sys-disk-dir user","title":"List of Property Names"},{"location":"reference/configuration_parameters/ack-severe-alert-threshold/","text":"ack-severe-alert-threshold \u00b6 Description \u00b6 The number of seconds the distributed system waits after the ack-wait-threshold for a message to be acknowledged before it issues an alert at a severe level. A value of zero disables this feature. Default Value \u00b6 0 (disabled) Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"ack-severe-alert-threshold"},{"location":"reference/configuration_parameters/ack-severe-alert-threshold/#ack-severe-alert-threshold","text":"","title":"ack-severe-alert-threshold"},{"location":"reference/configuration_parameters/ack-severe-alert-threshold/#description","text":"The number of seconds the distributed system waits after the ack-wait-threshold for a message to be acknowledged before it issues an alert at a severe level. A value of zero disables this feature.","title":"Description"},{"location":"reference/configuration_parameters/ack-severe-alert-threshold/#default-value","text":"0 (disabled)","title":"Default Value"},{"location":"reference/configuration_parameters/ack-severe-alert-threshold/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/ack-severe-alert-threshold/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/ack-wait-threshold/","text":"ack-wait-threshold \u00b6 Description \u00b6 The number of seconds a distributed message waits for an acknowledgment before it sends an alert to signal that something might be wrong with the system member that is unresponsive. After sending this alert the waiter continues to wait. The alerts are logged in the system members log as warnings. Valid values are in the range 0...2147483647 Default Value \u00b6 15 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"ack-wait-threshold"},{"location":"reference/configuration_parameters/ack-wait-threshold/#ack-wait-threshold","text":"","title":"ack-wait-threshold"},{"location":"reference/configuration_parameters/ack-wait-threshold/#description","text":"The number of seconds a distributed message waits for an acknowledgment before it sends an alert to signal that something might be wrong with the system member that is unresponsive. After sending this alert the waiter continues to wait. The alerts are logged in the system members log as warnings. Valid values are in the range 0...2147483647","title":"Description"},{"location":"reference/configuration_parameters/ack-wait-threshold/#default-value","text":"15","title":"Default Value"},{"location":"reference/configuration_parameters/ack-wait-threshold/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/ack-wait-threshold/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/allow-explicit-commit/","text":"allow-explicit-commit \u00b6 Description \u00b6 Using this property, you can specify whether to allow the execution of unsupported operations. Such operations may otherwise produce an error when you have set the JDBC autocommit to false using java.sql.Connection#setAutoCommit API. If you set the autocommit to false, the operations in a column table produces an error as follows: Operations on column tables are not supported when query routing is disabled or autocommit is false . To allow such operations, set the allow-explicit-commit property to true. Note Although this property allows using the JDBC autocommit(false) and commit/rollback APIs, all of these are no-op with no change in the product behavior. This means that autocommit is always true even if the user sets it explicitly to false. This property is useful in scenarios as SQL client tools that may use transactions isolation levels (read committed / repeatable read) and explicitly set autocommit to false. In such cases, without this property, the SQL operations produce an error. Example \u00b6 This property can be used in connection URLs while connecting to SnappyData JDBC server. In such a case JDBC URL appears as follows: jdbc:snappydata://locatoHostName:1527/allow-explicit-commit=true This property can also be passed to java.sql.DriverManager#getConnection(java.lang.String, java.util.Properties) API in the properties object. Default Value \u00b6 false Property Type \u00b6 Connection Prefix \u00b6 NA","title":"allow-explicit-commit"},{"location":"reference/configuration_parameters/allow-explicit-commit/#allow-explicit-commit","text":"","title":"allow-explicit-commit"},{"location":"reference/configuration_parameters/allow-explicit-commit/#description","text":"Using this property, you can specify whether to allow the execution of unsupported operations. Such operations may otherwise produce an error when you have set the JDBC autocommit to false using java.sql.Connection#setAutoCommit API. If you set the autocommit to false, the operations in a column table produces an error as follows: Operations on column tables are not supported when query routing is disabled or autocommit is false . To allow such operations, set the allow-explicit-commit property to true. Note Although this property allows using the JDBC autocommit(false) and commit/rollback APIs, all of these are no-op with no change in the product behavior. This means that autocommit is always true even if the user sets it explicitly to false. This property is useful in scenarios as SQL client tools that may use transactions isolation levels (read committed / repeatable read) and explicitly set autocommit to false. In such cases, without this property, the SQL operations produce an error.","title":"Description"},{"location":"reference/configuration_parameters/allow-explicit-commit/#example","text":"This property can be used in connection URLs while connecting to SnappyData JDBC server. In such a case JDBC URL appears as follows: jdbc:snappydata://locatoHostName:1527/allow-explicit-commit=true This property can also be passed to java.sql.DriverManager#getConnection(java.lang.String, java.util.Properties) API in the properties object.","title":"Example"},{"location":"reference/configuration_parameters/allow-explicit-commit/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/allow-explicit-commit/#property-type","text":"Connection","title":"Property Type"},{"location":"reference/configuration_parameters/allow-explicit-commit/#prefix","text":"NA","title":"Prefix"},{"location":"reference/configuration_parameters/archive-disk-space-limit/","text":"archive-disk-space-limit \u00b6 Description \u00b6 The maximum size in megabytes of all inactive statistic archive files combined. If this limit is exceeded, inactive archive files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited. Default Value \u00b6 0 (unlimited) Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"archive-disk-space-limit"},{"location":"reference/configuration_parameters/archive-disk-space-limit/#archive-disk-space-limit","text":"","title":"archive-disk-space-limit"},{"location":"reference/configuration_parameters/archive-disk-space-limit/#description","text":"The maximum size in megabytes of all inactive statistic archive files combined. If this limit is exceeded, inactive archive files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited.","title":"Description"},{"location":"reference/configuration_parameters/archive-disk-space-limit/#default-value","text":"0 (unlimited)","title":"Default Value"},{"location":"reference/configuration_parameters/archive-disk-space-limit/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/archive-disk-space-limit/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/archive-file-size-limit/","text":"archive-file-size-limit \u00b6 Description \u00b6 The maximum size in megabytes of a single statistic archive file. Once this limit is exceeded, a new statistic archive file is created, and the current archive file becomes inactive. If set to zero, the file size is unlimited. Default Value \u00b6 0 (unlimited) Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"archive-file-size-limit"},{"location":"reference/configuration_parameters/archive-file-size-limit/#archive-file-size-limit","text":"","title":"archive-file-size-limit"},{"location":"reference/configuration_parameters/archive-file-size-limit/#description","text":"The maximum size in megabytes of a single statistic archive file. Once this limit is exceeded, a new statistic archive file is created, and the current archive file becomes inactive. If set to zero, the file size is unlimited.","title":"Description"},{"location":"reference/configuration_parameters/archive-file-size-limit/#default-value","text":"0 (unlimited)","title":"Default Value"},{"location":"reference/configuration_parameters/archive-file-size-limit/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/archive-file-size-limit/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/bind-address/","text":"bind-address \u00b6 Description \u00b6 Note This setting is relevant only for multi-homed hosts (machines with multiple network interface cards). Adapter card the cache binds to for peer-to-peer communication. It also specifies the default location for SnappyData servers to listen on, which is used unless overridden by the server-bind-address . Specify the IP address, not the hostname, because each network card may not have a unique hostname. An empty string (the default) causes the member to listen on the default card for the machine. This attribute is a machine-wide attribute used for system member and client/server communication. It has no effect on the locator location unless the locator is embedded in a member process. Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"bind-address"},{"location":"reference/configuration_parameters/bind-address/#bind-address","text":"","title":"bind-address"},{"location":"reference/configuration_parameters/bind-address/#description","text":"Note This setting is relevant only for multi-homed hosts (machines with multiple network interface cards). Adapter card the cache binds to for peer-to-peer communication. It also specifies the default location for SnappyData servers to listen on, which is used unless overridden by the server-bind-address . Specify the IP address, not the hostname, because each network card may not have a unique hostname. An empty string (the default) causes the member to listen on the default card for the machine. This attribute is a machine-wide attribute used for system member and client/server communication. It has no effect on the locator location unless the locator is embedded in a member process.","title":"Description"},{"location":"reference/configuration_parameters/bind-address/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/bind-address/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/bind-address/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/enable-network-partition-detection/","text":"enable-network-partition-detection \u00b6 Description \u00b6 Boolean instructing the system to detect and handle splits in the distributed system, typically caused by a partitioning of the network (split brain) where the distributed system is running. Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"enable-network-partition-detection"},{"location":"reference/configuration_parameters/enable-network-partition-detection/#enable-network-partition-detection","text":"","title":"enable-network-partition-detection"},{"location":"reference/configuration_parameters/enable-network-partition-detection/#description","text":"Boolean instructing the system to detect and handle splits in the distributed system, typically caused by a partitioning of the network (split brain) where the distributed system is running.","title":"Description"},{"location":"reference/configuration_parameters/enable-network-partition-detection/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/enable-network-partition-detection/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/enable-network-partition-detection/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/enable-stats/","text":"enable-stats \u00b6 Description \u00b6 This property can only be used with a peer client connection; you cannot use it from a thin client. Enables statistics collection at the statement level for the associated connection. Default Value \u00b6 false Property Type \u00b6 connection Prefix \u00b6 snappydata.","title":"enable-stats"},{"location":"reference/configuration_parameters/enable-stats/#enable-stats","text":"","title":"enable-stats"},{"location":"reference/configuration_parameters/enable-stats/#description","text":"This property can only be used with a peer client connection; you cannot use it from a thin client. Enables statistics collection at the statement level for the associated connection.","title":"Description"},{"location":"reference/configuration_parameters/enable-stats/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/enable-stats/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/enable-stats/#prefix","text":"snappydata.","title":"Prefix"},{"location":"reference/configuration_parameters/enable-time-statistics/","text":"enable-time-statistics \u00b6 Description \u00b6 Boolean instructing the system to track time-based statistics for the distributed system. Disabled by default for performance. Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"enable-time-statistics"},{"location":"reference/configuration_parameters/enable-time-statistics/#enable-time-statistics","text":"","title":"enable-time-statistics"},{"location":"reference/configuration_parameters/enable-time-statistics/#description","text":"Boolean instructing the system to track time-based statistics for the distributed system. Disabled by default for performance.","title":"Description"},{"location":"reference/configuration_parameters/enable-time-statistics/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/enable-time-statistics/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/enable-time-statistics/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/enable-timestats/","text":"enable-timestats \u00b6 Description \u00b6 Boolean instructing the system to track time-based statistics for the current connection. Disabled by default for performance. See Evaluating Statistics for the System and Applications . Default Value \u00b6 false Property Type \u00b6 connection Prefix \u00b6 gemfire.","title":"enable-timestats"},{"location":"reference/configuration_parameters/enable-timestats/#enable-timestats","text":"","title":"enable-timestats"},{"location":"reference/configuration_parameters/enable-timestats/#description","text":"Boolean instructing the system to track time-based statistics for the current connection. Disabled by default for performance. See Evaluating Statistics for the System and Applications .","title":"Description"},{"location":"reference/configuration_parameters/enable-timestats/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/enable-timestats/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/enable-timestats/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/enforce-unique-host/","text":"enforce-unique-host \u00b6 Description \u00b6 Determines whether SnappyData puts redundant copies of the same data in different members running on the same physical machine. By default, SnappyData tries to put redundant copies on different machines, but it puts them on the same machine if no other machines are available. Setting this property to true prevents this and requires different machines for redundant copies. Usage \u00b6 In the conf/servers file you can set this as: localhost -locators=localhost:3241,localhost:3242 -gemfire.enforce-unique-host=true Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"enforce-unique-host"},{"location":"reference/configuration_parameters/enforce-unique-host/#enforce-unique-host","text":"","title":"enforce-unique-host"},{"location":"reference/configuration_parameters/enforce-unique-host/#description","text":"Determines whether SnappyData puts redundant copies of the same data in different members running on the same physical machine. By default, SnappyData tries to put redundant copies on different machines, but it puts them on the same machine if no other machines are available. Setting this property to true prevents this and requires different machines for redundant copies.","title":"Description"},{"location":"reference/configuration_parameters/enforce-unique-host/#usage","text":"In the conf/servers file you can set this as: localhost -locators=localhost:3241,localhost:3242 -gemfire.enforce-unique-host=true","title":"Usage"},{"location":"reference/configuration_parameters/enforce-unique-host/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/enforce-unique-host/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/enforce-unique-host/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/gemfirexd.properties/","text":"gemfirexd.properties \u00b6 Description \u00b6 Overrides the default lookup for application file properties in the file named 'gemfirexd.properties.' Default Value \u00b6 gemfirexd.properties Property Type \u00b6 system Note You must define this property as a Java system property (for example by using -J-D property_name = property_value with a snappy utility, or by setting JAVA_ARGS=\"-D property_name = property_value \"). Prefix \u00b6 n/a","title":"gemfirexd.properties"},{"location":"reference/configuration_parameters/gemfirexd.properties/#gemfirexdproperties","text":"","title":"gemfirexd.properties"},{"location":"reference/configuration_parameters/gemfirexd.properties/#description","text":"Overrides the default lookup for application file properties in the file named 'gemfirexd.properties.'","title":"Description"},{"location":"reference/configuration_parameters/gemfirexd.properties/#default-value","text":"gemfirexd.properties","title":"Default Value"},{"location":"reference/configuration_parameters/gemfirexd.properties/#property-type","text":"system Note You must define this property as a Java system property (for example by using -J-D property_name = property_value with a snappy utility, or by setting JAVA_ARGS=\"-D property_name = property_value \").","title":"Property Type"},{"location":"reference/configuration_parameters/gemfirexd.properties/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/hostname-for-clients/","text":"hostname-for-clients \u00b6 The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where the servers/locators are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in client-bind-address resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required","title":"hostname-for-clients"},{"location":"reference/configuration_parameters/hostname-for-clients/#hostname-for-clients","text":"The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where the servers/locators are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in client-bind-address resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required","title":"hostname-for-clients"},{"location":"reference/configuration_parameters/init-scripts/","text":"init-scripts \u00b6 Description \u00b6 One or more SQL script files to execute after loading DDL from the data dictionary. Use a comma-separated list of files to supply multiple values. Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 snappydata.","title":"init-scripts"},{"location":"reference/configuration_parameters/init-scripts/#init-scripts","text":"","title":"init-scripts"},{"location":"reference/configuration_parameters/init-scripts/#description","text":"One or more SQL script files to execute after loading DDL from the data dictionary. Use a comma-separated list of files to supply multiple values.","title":"Description"},{"location":"reference/configuration_parameters/init-scripts/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/init-scripts/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/init-scripts/#prefix","text":"snappydata.","title":"Prefix"},{"location":"reference/configuration_parameters/load-balance/","text":"load-balance \u00b6 Description \u00b6 Specifies whether load balancing is performed for the JDBC/ODBC client connection. With the default value (\"true\") clients are automatically connected to a less-loaded server if the locators are used for member discovery. Note Load balancing is provided only for SnappyData distributed systems that use locators for member discovery. When load balancing is enabled, clients may not be able to connect to a specific server even if they provide that server's unique port number for client connections. As a best practice, clients should always request connections using a locator address and port when load balancing is enabled. With 1.1.0 release, the load-balance is set to false by default, when you connect to a server's hostname:port. And it is set to true, when you connect to a locator's hostname:port. The locator then redirects the client to a less-loaded server with which the client makes the connection. With 1.1.1 release, same behavior is implemented for ODBC driver as well. Note You must specify load-balance=true in ODBC properties, if the locator address and port is provided. For example: snappy> connect client 'server_hostname:1527;load-balance=false' Default Value \u00b6 false Property Type \u00b6 connection Prefix \u00b6 n/a","title":"load-balance"},{"location":"reference/configuration_parameters/load-balance/#load-balance","text":"","title":"load-balance"},{"location":"reference/configuration_parameters/load-balance/#description","text":"Specifies whether load balancing is performed for the JDBC/ODBC client connection. With the default value (\"true\") clients are automatically connected to a less-loaded server if the locators are used for member discovery. Note Load balancing is provided only for SnappyData distributed systems that use locators for member discovery. When load balancing is enabled, clients may not be able to connect to a specific server even if they provide that server's unique port number for client connections. As a best practice, clients should always request connections using a locator address and port when load balancing is enabled. With 1.1.0 release, the load-balance is set to false by default, when you connect to a server's hostname:port. And it is set to true, when you connect to a locator's hostname:port. The locator then redirects the client to a less-loaded server with which the client makes the connection. With 1.1.1 release, same behavior is implemented for ODBC driver as well. Note You must specify load-balance=true in ODBC properties, if the locator address and port is provided. For example: snappy> connect client 'server_hostname:1527;load-balance=false'","title":"Description"},{"location":"reference/configuration_parameters/load-balance/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/load-balance/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/load-balance/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/locators/","text":"locators \u00b6 Description \u00b6 List of locators used by system members. The list must be configured consistently for every member of the distributed system. If the list is empty, locators are not used. For each locator, provide a host name and/or address (separated by '@', if you use both), followed by a port number in brackets. Examples: locators=address1[port1],address2[port2] locators=hostName1@address1[port1],name2@address2[port2] locators=hostName1[port1],name2[port2] Note On multi-homed hosts, this last notation uses the default address. If you use bind addresses for your locators, explicitly specify the addresses in the locator's list - do not use just the hostname. Usage \u00b6 To start multiple locators in a cluster modify the following files: conf/locators localhost -peer-discovery-address=localhost -peer-discovery-port=3241 -locators=localhost:3242 localhost -peer-discovery-address=localhost -peer-discovery-port=3242 -locators=localhost:3241 conf/servers localhost -locators=localhost:3241,localhost:3242 conf/leads localhost -locators=localhost:3241,localhost:3242 Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"locators"},{"location":"reference/configuration_parameters/locators/#locators","text":"","title":"locators"},{"location":"reference/configuration_parameters/locators/#description","text":"List of locators used by system members. The list must be configured consistently for every member of the distributed system. If the list is empty, locators are not used. For each locator, provide a host name and/or address (separated by '@', if you use both), followed by a port number in brackets. Examples: locators=address1[port1],address2[port2] locators=hostName1@address1[port1],name2@address2[port2] locators=hostName1[port1],name2[port2] Note On multi-homed hosts, this last notation uses the default address. If you use bind addresses for your locators, explicitly specify the addresses in the locator's list - do not use just the hostname.","title":"Description"},{"location":"reference/configuration_parameters/locators/#usage","text":"To start multiple locators in a cluster modify the following files: conf/locators localhost -peer-discovery-address=localhost -peer-discovery-port=3241 -locators=localhost:3242 localhost -peer-discovery-address=localhost -peer-discovery-port=3242 -locators=localhost:3241 conf/servers localhost -locators=localhost:3241,localhost:3242 conf/leads localhost -locators=localhost:3241,localhost:3242","title":"Usage"},{"location":"reference/configuration_parameters/locators/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/locators/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/locators/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/log-disk-space-limit/","text":"log-disk-space-limit \u00b6 Description \u00b6 Maximum size in megabytes of all inactive log files combined. If this limit is exceeded, inactive log files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited. Default Value \u00b6 0 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"log-disk-space-limit"},{"location":"reference/configuration_parameters/log-disk-space-limit/#log-disk-space-limit","text":"","title":"log-disk-space-limit"},{"location":"reference/configuration_parameters/log-disk-space-limit/#description","text":"Maximum size in megabytes of all inactive log files combined. If this limit is exceeded, inactive log files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited.","title":"Description"},{"location":"reference/configuration_parameters/log-disk-space-limit/#default-value","text":"0","title":"Default Value"},{"location":"reference/configuration_parameters/log-disk-space-limit/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/log-disk-space-limit/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/log-file-size-limit/","text":"log-file-size-limit \u00b6 Description \u00b6 The maximum size in megabytes of a log file before it is closed and logging rolls on to a new (child) log file. If set to 0, log rolling is disabled. Default Value \u00b6 0 Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"log-file-size-limit"},{"location":"reference/configuration_parameters/log-file-size-limit/#log-file-size-limit","text":"","title":"log-file-size-limit"},{"location":"reference/configuration_parameters/log-file-size-limit/#description","text":"The maximum size in megabytes of a log file before it is closed and logging rolls on to a new (child) log file. If set to 0, log rolling is disabled.","title":"Description"},{"location":"reference/configuration_parameters/log-file-size-limit/#default-value","text":"0","title":"Default Value"},{"location":"reference/configuration_parameters/log-file-size-limit/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/log-file-size-limit/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/log-file/","text":"log-file \u00b6 Description \u00b6 File to use for writing log messages. If this property is not set, the default is used. Each member type has its own default output: leader: snappyleader.log locator: snappylocator.log server: snappyserver.log Usage \u00b6 localhost -log-file=/home/user1/snappy/server/snappy-server.log Default Value \u00b6 not set Property Type \u00b6 connection Prefix \u00b6 n/a","title":"log-file"},{"location":"reference/configuration_parameters/log-file/#log-file","text":"","title":"log-file"},{"location":"reference/configuration_parameters/log-file/#description","text":"File to use for writing log messages. If this property is not set, the default is used. Each member type has its own default output: leader: snappyleader.log locator: snappylocator.log server: snappyserver.log","title":"Description"},{"location":"reference/configuration_parameters/log-file/#usage","text":"localhost -log-file=/home/user1/snappy/server/snappy-server.log","title":"Usage"},{"location":"reference/configuration_parameters/log-file/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/log-file/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/log-file/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/log-level/","text":"log-level \u00b6 Description \u00b6 The level of detail of the messages written to the system member's log. Setting log-level to one of the ordered levels causes all messages of that level and greater severity to be printed. Valid values from lowest to highest are fine, config, info, warning, error, severe, and none. Usage \u00b6 localhost -log-level=fine Default Value \u00b6 config Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"log-level"},{"location":"reference/configuration_parameters/log-level/#log-level","text":"","title":"log-level"},{"location":"reference/configuration_parameters/log-level/#description","text":"The level of detail of the messages written to the system member's log. Setting log-level to one of the ordered levels causes all messages of that level and greater severity to be printed. Valid values from lowest to highest are fine, config, info, warning, error, severe, and none.","title":"Description"},{"location":"reference/configuration_parameters/log-level/#usage","text":"localhost -log-level=fine","title":"Usage"},{"location":"reference/configuration_parameters/log-level/#default-value","text":"config","title":"Default Value"},{"location":"reference/configuration_parameters/log-level/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/log-level/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/member-timeout/","text":"member-timeout \u00b6 Description \u00b6 Interval, in milliseconds, between two attempts to determine whether another system member is alive. When another member appears to be gone, SnappyData tries to contact it twice before quitting. Valid values are in the range 1000..600000. Default Value \u00b6 5000 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"member-timeout"},{"location":"reference/configuration_parameters/member-timeout/#member-timeout","text":"","title":"member-timeout"},{"location":"reference/configuration_parameters/member-timeout/#description","text":"Interval, in milliseconds, between two attempts to determine whether another system member is alive. When another member appears to be gone, SnappyData tries to contact it twice before quitting. Valid values are in the range 1000..600000.","title":"Description"},{"location":"reference/configuration_parameters/member-timeout/#default-value","text":"5000","title":"Default Value"},{"location":"reference/configuration_parameters/member-timeout/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/member-timeout/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/membership-port-range/","text":"membership-port-range \u00b6 Description \u00b6 The range of ports available for unicast UDP messaging and for TCP failure detection. Specify this value as two integers separated by a minus sign. Different members can use different ranges. SnappyData randomly chooses two unique integers from this range for the member, one for UDP unicast messaging and the other for TCP failure detection messaging. Additionally, the system uniquely identifies the member using the combined host IP address and UDP port number. You may want to restrict the range of ports that SnappyData uses so the product can run in an environment where routers only allow traffic on certain ports. Default Value \u00b6 1024-65535 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"membership-port-range"},{"location":"reference/configuration_parameters/membership-port-range/#membership-port-range","text":"","title":"membership-port-range"},{"location":"reference/configuration_parameters/membership-port-range/#description","text":"The range of ports available for unicast UDP messaging and for TCP failure detection. Specify this value as two integers separated by a minus sign. Different members can use different ranges. SnappyData randomly chooses two unique integers from this range for the member, one for UDP unicast messaging and the other for TCP failure detection messaging. Additionally, the system uniquely identifies the member using the combined host IP address and UDP port number. You may want to restrict the range of ports that SnappyData uses so the product can run in an environment where routers only allow traffic on certain ports.","title":"Description"},{"location":"reference/configuration_parameters/membership-port-range/#default-value","text":"1024-65535","title":"Default Value"},{"location":"reference/configuration_parameters/membership-port-range/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/membership-port-range/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/password/","text":"password \u00b6 Description \u00b6 A password for the user name given at boot or connection time. Use this attribute in conjunction with the user attribute. Default Value \u00b6 not set Property Type \u00b6 connection Prefix \u00b6 n/a","title":"password"},{"location":"reference/configuration_parameters/password/#password","text":"","title":"password"},{"location":"reference/configuration_parameters/password/#description","text":"A password for the user name given at boot or connection time. Use this attribute in conjunction with the user attribute.","title":"Description"},{"location":"reference/configuration_parameters/password/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/password/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/password/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/persist-dd/","text":"persist-dd \u00b6 Description \u00b6 Enables or disables disk persistence for the data dictionary. By default, all data stores (SnappyData members booted with host-data=true) set this value to \"true\" to enable persistence. All SnappyData data stores in the same cluster must use the same persistence setting. When persistence is enabled, if all data stores in a cluster are down, then clients cannot execute DDL statements in the cluster until a data store becomes available. This ensures that the persisent data dictionary can be recovered when the final data store rejoins the cluster. Note You cannot enable persistence for SnappyData accessors (members that booted with host-data=false). When persist-dd is set to \"false,\" then no tables can be declared as persistent. However, overflow can still be configured if you explicitly define the sys-disk-dir attribute. When a new SnappyData member joins an existing cluster, the data dictionary is obtained either from other members, or it is retrieved from persisted data if the new member is determined to have to most current copy of the data in the cluster. Default Value \u00b6 true Property Type \u00b6 connection (boot) Prefix \u00b6 snappydata.","title":"persist-dd"},{"location":"reference/configuration_parameters/persist-dd/#persist-dd","text":"","title":"persist-dd"},{"location":"reference/configuration_parameters/persist-dd/#description","text":"Enables or disables disk persistence for the data dictionary. By default, all data stores (SnappyData members booted with host-data=true) set this value to \"true\" to enable persistence. All SnappyData data stores in the same cluster must use the same persistence setting. When persistence is enabled, if all data stores in a cluster are down, then clients cannot execute DDL statements in the cluster until a data store becomes available. This ensures that the persisent data dictionary can be recovered when the final data store rejoins the cluster. Note You cannot enable persistence for SnappyData accessors (members that booted with host-data=false). When persist-dd is set to \"false,\" then no tables can be declared as persistent. However, overflow can still be configured if you explicitly define the sys-disk-dir attribute. When a new SnappyData member joins an existing cluster, the data dictionary is obtained either from other members, or it is retrieved from persisted data if the new member is determined to have to most current copy of the data in the cluster.","title":"Description"},{"location":"reference/configuration_parameters/persist-dd/#default-value","text":"true","title":"Default Value"},{"location":"reference/configuration_parameters/persist-dd/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/persist-dd/#prefix","text":"snappydata.","title":"Prefix"},{"location":"reference/configuration_parameters/prefer-netserver-ipaddress/","text":"prefer-netserver-ipaddress \u00b6 Description \u00b6 Determines whether locators (or intermediate servers, if locators are not used for discovery) provide server hostnames or server IP addresses to clients who are connecting to the distributed system. With the default setting (false), a locator provides hostnames to a client. Hostnames are generally preferable to use because it is more likely that an IP address may not be valid outside of a firewall, where clients typically reside. However, if you configure a system using only IP addresses, or there is a likelihood of a mismatch between hostnames and IP addresses (either due to misconfigured /etc/hosts files or DHCP hostnames are overwritten), then set this property to true to provide IP addresses for client connections. Note This is a server-side boot property. You cannot use this property in a thin client connection string. Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 snappydata.","title":"prefer-netserver-ipaddress"},{"location":"reference/configuration_parameters/prefer-netserver-ipaddress/#prefer-netserver-ipaddress","text":"","title":"prefer-netserver-ipaddress"},{"location":"reference/configuration_parameters/prefer-netserver-ipaddress/#description","text":"Determines whether locators (or intermediate servers, if locators are not used for discovery) provide server hostnames or server IP addresses to clients who are connecting to the distributed system. With the default setting (false), a locator provides hostnames to a client. Hostnames are generally preferable to use because it is more likely that an IP address may not be valid outside of a firewall, where clients typically reside. However, if you configure a system using only IP addresses, or there is a likelihood of a mismatch between hostnames and IP addresses (either due to misconfigured /etc/hosts files or DHCP hostnames are overwritten), then set this property to true to provide IP addresses for client connections. Note This is a server-side boot property. You cannot use this property in a thin client connection string.","title":"Description"},{"location":"reference/configuration_parameters/prefer-netserver-ipaddress/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/prefer-netserver-ipaddress/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/prefer-netserver-ipaddress/#prefix","text":"snappydata.","title":"Prefix"},{"location":"reference/configuration_parameters/read-timeout/","text":"read-timeout \u00b6 Description \u00b6 Defines the read-timeout for a thin-client connection, in seconds. If no response is received after this time, the connection is dropped and SnappyData can attempt to reconnect. Default Value \u00b6 not set Property Type \u00b6 connection Prefix \u00b6 n/a","title":"read-timeout"},{"location":"reference/configuration_parameters/read-timeout/#read-timeout","text":"","title":"read-timeout"},{"location":"reference/configuration_parameters/read-timeout/#description","text":"Defines the read-timeout for a thin-client connection, in seconds. If no response is received after this time, the connection is dropped and SnappyData can attempt to reconnect.","title":"Description"},{"location":"reference/configuration_parameters/read-timeout/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/read-timeout/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/read-timeout/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/redundancy-zone/","text":"redundancy-zone \u00b6 Description \u00b6 Defines this member's redundancy zone. Used to separate members into different groups for satisfying partitioned table redundancy. If this property is set, SnappyData does not put redundant copies of data in members with the same redundancy zone setting. For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. You set one redundancy zone in the conf/servers file for all members that run on one rack: -gemfire.redundancy-zone=rack1 You can also set another redundancy zone in the conf/servers file for all members that run on another rack: -gemfire.redundancy-zone=rack2 Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire. Example \u00b6 localhost1 -gemfire.redundancy-zone=rack1 localhost1 -gemfire.redundancy-zone=rack1 localhost2 -gemfire.redundancy-zone=rack2 localhost2 -gemfire.redundancy-zone=rack2","title":"redundancy-zone"},{"location":"reference/configuration_parameters/redundancy-zone/#redundancy-zone","text":"","title":"redundancy-zone"},{"location":"reference/configuration_parameters/redundancy-zone/#description","text":"Defines this member's redundancy zone. Used to separate members into different groups for satisfying partitioned table redundancy. If this property is set, SnappyData does not put redundant copies of data in members with the same redundancy zone setting. For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. You set one redundancy zone in the conf/servers file for all members that run on one rack: -gemfire.redundancy-zone=rack1 You can also set another redundancy zone in the conf/servers file for all members that run on another rack: -gemfire.redundancy-zone=rack2","title":"Description"},{"location":"reference/configuration_parameters/redundancy-zone/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/redundancy-zone/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/redundancy-zone/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/redundancy-zone/#example","text":"localhost1 -gemfire.redundancy-zone=rack1 localhost1 -gemfire.redundancy-zone=rack1 localhost2 -gemfire.redundancy-zone=rack2 localhost2 -gemfire.redundancy-zone=rack2","title":"Example"},{"location":"reference/configuration_parameters/secondary-locators/","text":"secondary-locators \u00b6 Description \u00b6 Provides the address and port number of one or more secondary SnappyData locator members to use for connecting to the distributed system. The secondary locator addresses are used to establish an initial connection to SnappyData if the primary locator is not available. Specify the address and port number of each locator using either the locator[port] or locator:port format. Use a comma-separated list to specify multiple secondary locators. Default Value \u00b6 not set Property Type \u00b6 connection Prefix \u00b6 n/a","title":"secondary-locators"},{"location":"reference/configuration_parameters/secondary-locators/#secondary-locators","text":"","title":"secondary-locators"},{"location":"reference/configuration_parameters/secondary-locators/#description","text":"Provides the address and port number of one or more secondary SnappyData locator members to use for connecting to the distributed system. The secondary locator addresses are used to establish an initial connection to SnappyData if the primary locator is not available. Specify the address and port number of each locator using either the locator[port] or locator:port format. Use a comma-separated list to specify multiple secondary locators.","title":"Description"},{"location":"reference/configuration_parameters/secondary-locators/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/secondary-locators/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/secondary-locators/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/shutdown/","text":"shutdown \u00b6 Description \u00b6 Stops SnappyData in the current process. A successful shutdown always results in a SQLException indicating that StopsSnappyData has shut down and that there is no longer a connection active. shutdown=true overrides all other attributes that might be specified in the JDBC connection. Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"shutdown"},{"location":"reference/configuration_parameters/shutdown/#shutdown","text":"","title":"shutdown"},{"location":"reference/configuration_parameters/shutdown/#description","text":"Stops SnappyData in the current process. A successful shutdown always results in a SQLException indicating that StopsSnappyData has shut down and that there is no longer a connection active. shutdown=true overrides all other attributes that might be specified in the JDBC connection.","title":"Description"},{"location":"reference/configuration_parameters/shutdown/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/shutdown/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/shutdown/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/skip-constraint-checks/","text":"skip-constraint-checks \u00b6 Description \u00b6 When skip-constraint-checks is set to true, SnappyData ignores all primary key, foreign key, and unique constraints for all SQL statements that are executed over the connection. This connection property is typically used only when importing data into a SnappyData system, in order to speed the execution of insert statements. Note When you set this property on a connection, you must ensure that no SQL operations violate the foreign key and unique constraints defined in the system. For primary key constraints, SnappyData uses the PUT INTO DML syntax to ensure that only the last primary key value inserted or updated remains in the system; this preserves primary key constraints. However, foreign key and unique constraints can be violated when this property is set. This can lead to undefined behavior in the system, because other connections that do not enable skip-constraint-checks still require constraint checks for correct operation. One exception to the skip-constraint-checks behavior is that SnappyData will throw a constraint violation error if a SQL statement would cause a local unique index to have duplicate values. This type of local index is created when you specify a unique index on a replicated table, or on partitioned tables where the number unique index columns is greater than or equal to the table's partitioning columns. The exception does not apply to updating global indexes, because SnappyData uses the PUT INTO DML syntax to update global indexes when skip-constraint-checks is enabled. Using PUT INTO ensures that only the last index entry for a given value remains in the index, which preserves uniqueness. Default Value \u00b6 false Property Type \u00b6 connection Prefix \u00b6 n/a","title":"skip-constraint-checks"},{"location":"reference/configuration_parameters/skip-constraint-checks/#skip-constraint-checks","text":"","title":"skip-constraint-checks"},{"location":"reference/configuration_parameters/skip-constraint-checks/#description","text":"When skip-constraint-checks is set to true, SnappyData ignores all primary key, foreign key, and unique constraints for all SQL statements that are executed over the connection. This connection property is typically used only when importing data into a SnappyData system, in order to speed the execution of insert statements. Note When you set this property on a connection, you must ensure that no SQL operations violate the foreign key and unique constraints defined in the system. For primary key constraints, SnappyData uses the PUT INTO DML syntax to ensure that only the last primary key value inserted or updated remains in the system; this preserves primary key constraints. However, foreign key and unique constraints can be violated when this property is set. This can lead to undefined behavior in the system, because other connections that do not enable skip-constraint-checks still require constraint checks for correct operation. One exception to the skip-constraint-checks behavior is that SnappyData will throw a constraint violation error if a SQL statement would cause a local unique index to have duplicate values. This type of local index is created when you specify a unique index on a replicated table, or on partitioned tables where the number unique index columns is greater than or equal to the table's partitioning columns. The exception does not apply to updating global indexes, because SnappyData uses the PUT INTO DML syntax to update global indexes when skip-constraint-checks is enabled. Using PUT INTO ensures that only the last index entry for a given value remains in the index, which preserves uniqueness.","title":"Description"},{"location":"reference/configuration_parameters/skip-constraint-checks/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/skip-constraint-checks/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/skip-constraint-checks/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/skip-listeners/","text":"skip-listeners \u00b6 Description \u00b6 If true, SnappyData does not propagate DML operations to configured DBSynchronizer, AsyncEventListener, or EventCallBack (Writer and Listener) implementations. This is a connection-level property and applies only to DML operations generated by the specific connection that enables the property. Note This property does not affect WAN replication using a configured gateway. SnappyData always propagates DML operations to an enabled WAN site. Default Value \u00b6 false Property Type \u00b6 connection Prefix \u00b6 n/a","title":"skip-listeners"},{"location":"reference/configuration_parameters/skip-listeners/#skip-listeners","text":"","title":"skip-listeners"},{"location":"reference/configuration_parameters/skip-listeners/#description","text":"If true, SnappyData does not propagate DML operations to configured DBSynchronizer, AsyncEventListener, or EventCallBack (Writer and Listener) implementations. This is a connection-level property and applies only to DML operations generated by the specific connection that enables the property. Note This property does not affect WAN replication using a configured gateway. SnappyData always propagates DML operations to an enabled WAN site.","title":"Description"},{"location":"reference/configuration_parameters/skip-listeners/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/skip-listeners/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/skip-listeners/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/skip-locks/","text":"skip-locks \u00b6 Description \u00b6 Note This property is provided only for the purpose of cancelling a long-running query in cases where the query causes a DDL operation to hold a DataDictionary lock, preventing new logins to the system. Using this property outside of its intended purpose can lead to data corruption, especially if DDL is performed while the property is enabled. skip-locks forces the associated connection to avoid acquiring DataDictionary and table locks, enabling a JVM owner user to log into a system where a blocked DDL operation holds a DataDictionary lock and prevents new connections. Any operation that attempts to acquire a table or DataDictionary lock from the connection logs a warning and sends a SQLWarning in the statement. Transaction locks are still obtained as usual. Use this property to connect directly to a SnappyData server, rather than a locator. (The property disables the load-balance property by default, as load balancing can cause local deadlocks even when skip-locks is enabled.) This property is restricted to JVM owners. Attempting to set the property without JVM owner credentials fails with the error, \"Connection refused: administrator access required for skip-locks.\" If authorization is disabled, the default user \"APP\" is the JVM owner. Default Value \u00b6 false Property Type \u00b6 connection Prefix \u00b6 n/a","title":"skip-locks"},{"location":"reference/configuration_parameters/skip-locks/#skip-locks","text":"","title":"skip-locks"},{"location":"reference/configuration_parameters/skip-locks/#description","text":"Note This property is provided only for the purpose of cancelling a long-running query in cases where the query causes a DDL operation to hold a DataDictionary lock, preventing new logins to the system. Using this property outside of its intended purpose can lead to data corruption, especially if DDL is performed while the property is enabled. skip-locks forces the associated connection to avoid acquiring DataDictionary and table locks, enabling a JVM owner user to log into a system where a blocked DDL operation holds a DataDictionary lock and prevents new connections. Any operation that attempts to acquire a table or DataDictionary lock from the connection logs a warning and sends a SQLWarning in the statement. Transaction locks are still obtained as usual. Use this property to connect directly to a SnappyData server, rather than a locator. (The property disables the load-balance property by default, as load balancing can cause local deadlocks even when skip-locks is enabled.) This property is restricted to JVM owners. Attempting to set the property without JVM owner credentials fails with the error, \"Connection refused: administrator access required for skip-locks.\" If authorization is disabled, the default user \"APP\" is the JVM owner.","title":"Description"},{"location":"reference/configuration_parameters/skip-locks/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/skip-locks/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/skip-locks/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappy.history/","text":"snappy.history \u00b6 Description \u00b6 The path and filename in which the snappy utility stores a list of the commands executed during an interactive snappy session. Specify this system property in the JAVA_ARGS environment variable before you execute snappy (for example, JAVA_ARGS=\"-Dsnappy.history= path-to-file \"). Specify an empty string value to disable recording a history of commands. See Snappy-SQL Shell Interactive Commands . Default Value \u00b6 %UserProfile%\\.snappy.history (Windows) $HOME/.snappy.history (Linux) Property Type \u00b6 system Prefix \u00b6 n/a Example \u00b6 export JAVA_ARGS=\"-Dsnappy.history=*path-to-file*\"","title":"snappy.history"},{"location":"reference/configuration_parameters/snappy.history/#snappyhistory","text":"","title":"snappy.history"},{"location":"reference/configuration_parameters/snappy.history/#description","text":"The path and filename in which the snappy utility stores a list of the commands executed during an interactive snappy session. Specify this system property in the JAVA_ARGS environment variable before you execute snappy (for example, JAVA_ARGS=\"-Dsnappy.history= path-to-file \"). Specify an empty string value to disable recording a history of commands. See Snappy-SQL Shell Interactive Commands .","title":"Description"},{"location":"reference/configuration_parameters/snappy.history/#default-value","text":"%UserProfile%\\.snappy.history (Windows) $HOME/.snappy.history (Linux)","title":"Default Value"},{"location":"reference/configuration_parameters/snappy.history/#property-type","text":"system","title":"Property Type"},{"location":"reference/configuration_parameters/snappy.history/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappy.history/#example","text":"export JAVA_ARGS=\"-Dsnappy.history=*path-to-file*\"","title":"Example"},{"location":"reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/","text":"gemfirexd.datadictionary.allow-startup-errors \u00b6 Description \u00b6 Enables a SnappyData member to start up, ignoring DDL statements that fail during member initialization. This property enables you to resolve startup problems manually, after forcing the member to start. Typical DDL initialization problems occur when a required disk store file is unavailable, or when SnappyData cannot initialize a DBSynchronizer configuration due to the external RDBMS being unavailable. Use gemfirexd.datadictionary.allow-startup-errors to drop and recreate the disk store or DBSynchronizer configuration after startup. Default Value \u00b6 false Property Type \u00b6 system Note You must define this property as a Java system property (for example by using -J-D property_name = property_value with a snappy utility, or by setting JAVA_ARGS=\"-D property_name = property_value \"). Prefix \u00b6 n/a","title":"gemfirexd.datadictionary.allow-startup-errors"},{"location":"reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#gemfirexddatadictionaryallow-startup-errors","text":"","title":"gemfirexd.datadictionary.allow-startup-errors"},{"location":"reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#description","text":"Enables a SnappyData member to start up, ignoring DDL statements that fail during member initialization. This property enables you to resolve startup problems manually, after forcing the member to start. Typical DDL initialization problems occur when a required disk store file is unavailable, or when SnappyData cannot initialize a DBSynchronizer configuration due to the external RDBMS being unavailable. Use gemfirexd.datadictionary.allow-startup-errors to drop and recreate the disk store or DBSynchronizer configuration after startup.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#property-type","text":"system Note You must define this property as a Java system property (for example by using -J-D property_name = property_value with a snappy utility, or by setting JAVA_ARGS=\"-D property_name = property_value \").","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappydata.default-startup-recovery-delay/","text":"gemfirexd.default-startup-recovery-delay \u00b6 Description \u00b6 The number of milliseconds to wait after a member joins the distributed system before recovering redundancy for partitioned tables having redundant copies. The default of 120000 (2 minutes) configures redundancy recovery to be started after 2 minutes whenever a new partitioned table host joins. A value of 0 configures immediate redundancy recovery while a value of -1 disables automatic recovery after a new member joins the distributed system. Default Value \u00b6 120000 Property Type \u00b6 system Prefix \u00b6 n/a","title":"gemfirexd.default-startup-recovery-delay"},{"location":"reference/configuration_parameters/snappydata.default-startup-recovery-delay/#gemfirexddefault-startup-recovery-delay","text":"","title":"gemfirexd.default-startup-recovery-delay"},{"location":"reference/configuration_parameters/snappydata.default-startup-recovery-delay/#description","text":"The number of milliseconds to wait after a member joins the distributed system before recovering redundancy for partitioned tables having redundant copies. The default of 120000 (2 minutes) configures redundancy recovery to be started after 2 minutes whenever a new partitioned table host joins. A value of 0 configures immediate redundancy recovery while a value of -1 disables automatic recovery after a new member joins the distributed system.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.default-startup-recovery-delay/#default-value","text":"120000","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.default-startup-recovery-delay/#property-type","text":"system","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.default-startup-recovery-delay/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappydata.max-lock-wait/","text":"gemfirexd.max-lock-wait \u00b6 Description \u00b6 Maximum time in milliseconds that a DDL statement waits for a distributed lock on the data dictionary or a table. Default Value \u00b6 300000 Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"gemfirexd.max-lock-wait"},{"location":"reference/configuration_parameters/snappydata.max-lock-wait/#gemfirexdmax-lock-wait","text":"","title":"gemfirexd.max-lock-wait"},{"location":"reference/configuration_parameters/snappydata.max-lock-wait/#description","text":"Maximum time in milliseconds that a DDL statement waits for a distributed lock on the data dictionary or a table.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.max-lock-wait/#default-value","text":"300000","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.max-lock-wait/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.max-lock-wait/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappydata.query-cancellation-interval/","text":"gemfirexd.query-cancellation-interval \u00b6 Description \u00b6 After used memory used passes a critical limit, SnappyData begins cancelling queries to free memory. This attribute specifies the period in milliseconds after which SnappyData cancels a query during periods of critical memory usage. With the default value, SnappyData cancels a query every 100 milliseconds when necessary to free memory. Default Value \u00b6 100 Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"gemfirexd.query-cancellation-interval"},{"location":"reference/configuration_parameters/snappydata.query-cancellation-interval/#gemfirexdquery-cancellation-interval","text":"","title":"gemfirexd.query-cancellation-interval"},{"location":"reference/configuration_parameters/snappydata.query-cancellation-interval/#description","text":"After used memory used passes a critical limit, SnappyData begins cancelling queries to free memory. This attribute specifies the period in milliseconds after which SnappyData cancels a query during periods of critical memory usage. With the default value, SnappyData cancels a query every 100 milliseconds when necessary to free memory.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.query-cancellation-interval/#default-value","text":"100","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.query-cancellation-interval/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.query-cancellation-interval/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappydata.query-timeout/","text":"gemfirexd.query-timeout \u00b6 Description \u00b6 After a query has run for longer than the specified amount of time in seconds, SnappyData automatically cancels the query. Default Value \u00b6 n/a Property Type \u00b6 system Prefix \u00b6 n/a","title":"gemfirexd.query-timeout"},{"location":"reference/configuration_parameters/snappydata.query-timeout/#gemfirexdquery-timeout","text":"","title":"gemfirexd.query-timeout"},{"location":"reference/configuration_parameters/snappydata.query-timeout/#description","text":"After a query has run for longer than the specified amount of time in seconds, SnappyData automatically cancels the query.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.query-timeout/#default-value","text":"n/a","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.query-timeout/#property-type","text":"system","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.query-timeout/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappydata.storage.tempDirectory/","text":"snappydata.storage.tempDirectory \u00b6 Description \u00b6 The temporary directory for overflowing intermediate data. For example, this directory is used during join processing to temporarily store sorted data in multiple files. Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"snappydata.storage.tempDirectory"},{"location":"reference/configuration_parameters/snappydata.storage.tempDirectory/#snappydatastoragetempdirectory","text":"","title":"snappydata.storage.tempDirectory"},{"location":"reference/configuration_parameters/snappydata.storage.tempDirectory/#description","text":"The temporary directory for overflowing intermediate data. For example, this directory is used during join processing to temporarily store sorted data in multiple files.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.storage.tempDirectory/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.storage.tempDirectory/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.storage.tempDirectory/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/snappydata.system.home/","text":"snappydata.system.home \u00b6 Description \u00b6 Specifies the SnappyData home directory. Any relative paths in the system are accessed using this property value. Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"snappydata.system.home"},{"location":"reference/configuration_parameters/snappydata.system.home/#snappydatasystemhome","text":"","title":"snappydata.system.home"},{"location":"reference/configuration_parameters/snappydata.system.home/#description","text":"Specifies the SnappyData home directory. Any relative paths in the system are accessed using this property value.","title":"Description"},{"location":"reference/configuration_parameters/snappydata.system.home/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/snappydata.system.home/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/snappydata.system.home/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/socket-buffer-size/","text":"socket-buffer-size \u00b6 Description \u00b6 Receive buffer sizes in bytes of the TCP/IP connections used for data transmission. To minimize the buffer size allocation needed for distributing large, serializable messages, the messages are sent in chunks. This setting determines the size of the chunks. Larger buffers can handle large messages more quickly, but take up more memory. Default Value \u00b6 32768 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"socket-buffer-size"},{"location":"reference/configuration_parameters/socket-buffer-size/#socket-buffer-size","text":"","title":"socket-buffer-size"},{"location":"reference/configuration_parameters/socket-buffer-size/#description","text":"Receive buffer sizes in bytes of the TCP/IP connections used for data transmission. To minimize the buffer size allocation needed for distributing large, serializable messages, the messages are sent in chunks. This setting determines the size of the chunks. Larger buffers can handle large messages more quickly, but take up more memory.","title":"Description"},{"location":"reference/configuration_parameters/socket-buffer-size/#default-value","text":"32768","title":"Default Value"},{"location":"reference/configuration_parameters/socket-buffer-size/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/socket-buffer-size/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/socket-lease-time/","text":"socket-lease-time \u00b6 Description \u00b6 The time, in milliseconds, that a thread can have exclusive access to a socket it is not actively using. A value of zero causes socket leases to never expire. This property is ignored if conserve-sockets is true. Valid values are in the range 0..600000. Default Value \u00b6 60000 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"socket-lease-time"},{"location":"reference/configuration_parameters/socket-lease-time/#socket-lease-time","text":"","title":"socket-lease-time"},{"location":"reference/configuration_parameters/socket-lease-time/#description","text":"The time, in milliseconds, that a thread can have exclusive access to a socket it is not actively using. A value of zero causes socket leases to never expire. This property is ignored if conserve-sockets is true. Valid values are in the range 0..600000.","title":"Description"},{"location":"reference/configuration_parameters/socket-lease-time/#default-value","text":"60000","title":"Default Value"},{"location":"reference/configuration_parameters/socket-lease-time/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/socket-lease-time/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/ssl/","text":"ssl \u00b6 Description \u00b6 Determines whether this connection is encrypted or not, and whether certificate-based peer authentication is enabled. Possible values are Off, Basic, and peerAuthentication. Default Value \u00b6 Off Property Type \u00b6 Connection Prefix \u00b6","title":"ssl"},{"location":"reference/configuration_parameters/ssl/#ssl","text":"","title":"ssl"},{"location":"reference/configuration_parameters/ssl/#description","text":"Determines whether this connection is encrypted or not, and whether certificate-based peer authentication is enabled. Possible values are Off, Basic, and peerAuthentication.","title":"Description"},{"location":"reference/configuration_parameters/ssl/#default-value","text":"Off","title":"Default Value"},{"location":"reference/configuration_parameters/ssl/#property-type","text":"Connection","title":"Property Type"},{"location":"reference/configuration_parameters/ssl/#prefix","text":"","title":"Prefix"},{"location":"reference/configuration_parameters/ssl_ciphers/","text":"ssl-ciphers \u00b6 Description \u00b6 Used for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL ciphers for member connections. A setting of 'any' uses any ciphers that are enabled by default in the configured JSSE provider. Default Value \u00b6 any Property Type \u00b6 connection (boot) Prefix \u00b6 NA","title":"ssl-ciphers"},{"location":"reference/configuration_parameters/ssl_ciphers/#ssl-ciphers","text":"","title":"ssl-ciphers"},{"location":"reference/configuration_parameters/ssl_ciphers/#description","text":"Used for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL ciphers for member connections. A setting of 'any' uses any ciphers that are enabled by default in the configured JSSE provider.","title":"Description"},{"location":"reference/configuration_parameters/ssl_ciphers/#default-value","text":"any","title":"Default Value"},{"location":"reference/configuration_parameters/ssl_ciphers/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/ssl_ciphers/#prefix","text":"NA","title":"Prefix"},{"location":"reference/configuration_parameters/ssl_enabled/","text":"ssl-enabled \u00b6 Description \u00b6 Used for configuring SSL encryption between SnappyData members. Boolean indicates whether to use SSL for member connections. The default is set to false. To enable the SSL encryption, you must set it to true. This attribute must be consistent across the distributed system members. Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 NA","title":"ssl-enabled"},{"location":"reference/configuration_parameters/ssl_enabled/#ssl-enabled","text":"","title":"ssl-enabled"},{"location":"reference/configuration_parameters/ssl_enabled/#description","text":"Used for configuring SSL encryption between SnappyData members. Boolean indicates whether to use SSL for member connections. The default is set to false. To enable the SSL encryption, you must set it to true. This attribute must be consistent across the distributed system members.","title":"Description"},{"location":"reference/configuration_parameters/ssl_enabled/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/ssl_enabled/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/ssl_enabled/#prefix","text":"NA","title":"Prefix"},{"location":"reference/configuration_parameters/ssl_protocols/","text":"ssl-protocols \u00b6 Description \u00b6 Used for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL protocols for member connections. A setting of 'any' uses any protocol that is enabled by default in the configured JSSE provider. Default Value \u00b6 any Property Type \u00b6 connection (boot) Prefix \u00b6 NA","title":"ssl-protocols"},{"location":"reference/configuration_parameters/ssl_protocols/#ssl-protocols","text":"","title":"ssl-protocols"},{"location":"reference/configuration_parameters/ssl_protocols/#description","text":"Used for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL protocols for member connections. A setting of 'any' uses any protocol that is enabled by default in the configured JSSE provider.","title":"Description"},{"location":"reference/configuration_parameters/ssl_protocols/#default-value","text":"any","title":"Default Value"},{"location":"reference/configuration_parameters/ssl_protocols/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/ssl_protocols/#prefix","text":"NA","title":"Prefix"},{"location":"reference/configuration_parameters/ssl_require_auth/","text":"ssl-require-authentication \u00b6 Description \u00b6 Used for configuring SSL encryption between SnappyData members. Boolean indicating whether to require authentication for member connections. Default Value \u00b6 true Property Type \u00b6 connection (boot) Prefix \u00b6 NA","title":"ssl-require-authentication"},{"location":"reference/configuration_parameters/ssl_require_auth/#ssl-require-authentication","text":"","title":"ssl-require-authentication"},{"location":"reference/configuration_parameters/ssl_require_auth/#description","text":"Used for configuring SSL encryption between SnappyData members. Boolean indicating whether to require authentication for member connections.","title":"Description"},{"location":"reference/configuration_parameters/ssl_require_auth/#default-value","text":"true","title":"Default Value"},{"location":"reference/configuration_parameters/ssl_require_auth/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/ssl_require_auth/#prefix","text":"NA","title":"Prefix"},{"location":"reference/configuration_parameters/start-locator/","text":"start-locator \u00b6 Description \u00b6 If set, automatically starts a locator in the current process when the member connects to the distributed system and stops the locator when the member disconnects. To use, specify the locator with an optional address or host specification and a required port number, in one of these formats: start-locator=address[port1] start-locator=port1 If you only specify the port, the address assigned to the member is used for the locator. If not already there, this locator is automatically added to the list of locators in this set of SnappyData properties. Default Value \u00b6 not set Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"start-locator"},{"location":"reference/configuration_parameters/start-locator/#start-locator","text":"","title":"start-locator"},{"location":"reference/configuration_parameters/start-locator/#description","text":"If set, automatically starts a locator in the current process when the member connects to the distributed system and stops the locator when the member disconnects. To use, specify the locator with an optional address or host specification and a required port number, in one of these formats: start-locator=address[port1] start-locator=port1 If you only specify the port, the address assigned to the member is used for the locator. If not already there, this locator is automatically added to the list of locators in this set of SnappyData properties.","title":"Description"},{"location":"reference/configuration_parameters/start-locator/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/start-locator/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/start-locator/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/statistic-archive-file/","text":"statistic-archive-file \u00b6 Description \u00b6 The file to which the running system member writes statistic samples. An empty string disables archiving. Adding .gz suffix to the file name causes it to be compressed. This property is commonly used with archive-disk-space-limit and archive-file-size-limit . Default Value \u00b6 not set (this disables statistic archiving) Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"statistic-archive-file"},{"location":"reference/configuration_parameters/statistic-archive-file/#statistic-archive-file","text":"","title":"statistic-archive-file"},{"location":"reference/configuration_parameters/statistic-archive-file/#description","text":"The file to which the running system member writes statistic samples. An empty string disables archiving. Adding .gz suffix to the file name causes it to be compressed. This property is commonly used with archive-disk-space-limit and archive-file-size-limit .","title":"Description"},{"location":"reference/configuration_parameters/statistic-archive-file/#default-value","text":"not set (this disables statistic archiving)","title":"Default Value"},{"location":"reference/configuration_parameters/statistic-archive-file/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/statistic-archive-file/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/statistic-sample-rate/","text":"statistic-sample-rate \u00b6 Description \u00b6 Boot property that specifies how often to sample statistics, in milliseconds. Valid values are in the range 1000..60000. Default Value \u00b6 1000 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"statistic-sample-rate"},{"location":"reference/configuration_parameters/statistic-sample-rate/#statistic-sample-rate","text":"","title":"statistic-sample-rate"},{"location":"reference/configuration_parameters/statistic-sample-rate/#description","text":"Boot property that specifies how often to sample statistics, in milliseconds. Valid values are in the range 1000..60000.","title":"Description"},{"location":"reference/configuration_parameters/statistic-sample-rate/#default-value","text":"1000","title":"Default Value"},{"location":"reference/configuration_parameters/statistic-sample-rate/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/statistic-sample-rate/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/statistic-sampling-enabled/","text":"statistic-sampling-enabled \u00b6 Description \u00b6 Determines whether to collect and archive statistics on the member. Turning statistics sampling off saves on resources, but it also takes away potentially valuable information for ongoing system tuning and about unexpected system problems. Default Value \u00b6 true Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"statistic-sampling-enabled"},{"location":"reference/configuration_parameters/statistic-sampling-enabled/#statistic-sampling-enabled","text":"","title":"statistic-sampling-enabled"},{"location":"reference/configuration_parameters/statistic-sampling-enabled/#description","text":"Determines whether to collect and archive statistics on the member. Turning statistics sampling off saves on resources, but it also takes away potentially valuable information for ongoing system tuning and about unexpected system problems.","title":"Description"},{"location":"reference/configuration_parameters/statistic-sampling-enabled/#default-value","text":"true","title":"Default Value"},{"location":"reference/configuration_parameters/statistic-sampling-enabled/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/statistic-sampling-enabled/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/sync-commits/","text":"sync-commits \u00b6 Description \u00b6 Determines whether second-phase commit actions occur in the background for the current connection, or whether the connection waits for second-phase commit actions to complete. By default (sync-commits=false) SnappyData performs second-phase commits in the background, but ensures that the connection that issued the transaction only sees completed results. This means that other threads or connections may see different results until the second-phase commit actions complete. Using sync-commits=true ensures that the current thin client or peer client connection waits until all second-phase commit actions complete. Default Value \u00b6 false Property Type \u00b6 connection (boot) Prefix \u00b6 n/a","title":"sync-commits"},{"location":"reference/configuration_parameters/sync-commits/#sync-commits","text":"","title":"sync-commits"},{"location":"reference/configuration_parameters/sync-commits/#description","text":"Determines whether second-phase commit actions occur in the background for the current connection, or whether the connection waits for second-phase commit actions to complete. By default (sync-commits=false) SnappyData performs second-phase commits in the background, but ensures that the connection that issued the transaction only sees completed results. This means that other threads or connections may see different results until the second-phase commit actions complete. Using sync-commits=true ensures that the current thin client or peer client connection waits until all second-phase commit actions complete.","title":"Description"},{"location":"reference/configuration_parameters/sync-commits/#default-value","text":"false","title":"Default Value"},{"location":"reference/configuration_parameters/sync-commits/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/sync-commits/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/configuration_parameters/sys-disk-dir/","text":"sys-disk-dir \u00b6 Description \u00b6 Specifies the base path of the default disk store. This directory also holds the data dictionary subdirectory, which stores the persistent data dictionary. Other SnappyData features also use this directory for storing files. For example, gateway queue overflow and overflow tables use this attribute by default. You can override sys-disk-dir for table overflow using options in a table's CREATE TABLE statement. Usage \u00b6 -spark.snappydata.store.sys-disk-dir= Default Value \u00b6 The SnappyData working directory. Property Type \u00b6 connection (boot) Prefix \u00b6 snappydata.","title":"sys-disk-dir"},{"location":"reference/configuration_parameters/sys-disk-dir/#sys-disk-dir","text":"","title":"sys-disk-dir"},{"location":"reference/configuration_parameters/sys-disk-dir/#description","text":"Specifies the base path of the default disk store. This directory also holds the data dictionary subdirectory, which stores the persistent data dictionary. Other SnappyData features also use this directory for storing files. For example, gateway queue overflow and overflow tables use this attribute by default. You can override sys-disk-dir for table overflow using options in a table's CREATE TABLE statement.","title":"Description"},{"location":"reference/configuration_parameters/sys-disk-dir/#usage","text":"-spark.snappydata.store.sys-disk-dir=","title":"Usage"},{"location":"reference/configuration_parameters/sys-disk-dir/#default-value","text":"The SnappyData working directory.","title":"Default Value"},{"location":"reference/configuration_parameters/sys-disk-dir/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/sys-disk-dir/#prefix","text":"snappydata.","title":"Prefix"},{"location":"reference/configuration_parameters/tcp-port/","text":"tcp-port \u00b6 Description \u00b6 TCP port to listen on for cache communications. If set to zero, the operating system selects an available port. Each process on a machine must have its own TCP port. Some operating systems restrict the range of ports usable by non-privileged users, and using restricted port numbers can cause runtime errors in SnappyData startup. Valid values are in the range 0..65535. Default Value \u00b6 0 Property Type \u00b6 connection (boot) Prefix \u00b6 gemfire.","title":"tcp-port"},{"location":"reference/configuration_parameters/tcp-port/#tcp-port","text":"","title":"tcp-port"},{"location":"reference/configuration_parameters/tcp-port/#description","text":"TCP port to listen on for cache communications. If set to zero, the operating system selects an available port. Each process on a machine must have its own TCP port. Some operating systems restrict the range of ports usable by non-privileged users, and using restricted port numbers can cause runtime errors in SnappyData startup. Valid values are in the range 0..65535.","title":"Description"},{"location":"reference/configuration_parameters/tcp-port/#default-value","text":"0","title":"Default Value"},{"location":"reference/configuration_parameters/tcp-port/#property-type","text":"connection (boot)","title":"Property Type"},{"location":"reference/configuration_parameters/tcp-port/#prefix","text":"gemfire.","title":"Prefix"},{"location":"reference/configuration_parameters/user/","text":"user \u00b6 Description \u00b6 The user name for the member or connection. A valid username and password are required when user authentication is turned on. Use this attribute in conjunction with the password attribute. Default Value \u00b6 not set Property Type \u00b6 connection Prefix \u00b6 n/a","title":"user"},{"location":"reference/configuration_parameters/user/#user","text":"","title":"user"},{"location":"reference/configuration_parameters/user/#description","text":"The user name for the member or connection. A valid username and password are required when user authentication is turned on. Use this attribute in conjunction with the password attribute.","title":"Description"},{"location":"reference/configuration_parameters/user/#default-value","text":"not set","title":"Default Value"},{"location":"reference/configuration_parameters/user/#property-type","text":"connection","title":"Property Type"},{"location":"reference/configuration_parameters/user/#prefix","text":"n/a","title":"Prefix"},{"location":"reference/inbuilt_system_procedures/","text":"Built-in System Procedures and Built-in Functions \u00b6 SnappyData provides built-in system procedures to help you manage the distributed system. For example, you can use system procedures to install required JAR files. SnappyData also supports built-in function, which lets you perform various kinds of data transformations directly in SELECT statements. Note If you enable SQL authorization, you must use the GRANT command to grant normal users permission to use these procedures. All system procedures are part of the SYS schema. The following built-in procedures are available: DUMP_STACKS REBALANCE_ALL_BUCKETS SET_CRITICAL_HEAP_PERCENTAGE SET_TRACE_FLAG REMOVE_METASTORE_ENTRY EXPORT_DDLS EXPORT_DATA The following built-in functions are available: DSID CURRENT_USER CURRENT_USER_LDAP_GROUPS","title":"Built-in System Procedures and Built-in Functions"},{"location":"reference/inbuilt_system_procedures/#built-in-system-procedures-and-built-in-functions","text":"SnappyData provides built-in system procedures to help you manage the distributed system. For example, you can use system procedures to install required JAR files. SnappyData also supports built-in function, which lets you perform various kinds of data transformations directly in SELECT statements. Note If you enable SQL authorization, you must use the GRANT command to grant normal users permission to use these procedures. All system procedures are part of the SYS schema. The following built-in procedures are available: DUMP_STACKS REBALANCE_ALL_BUCKETS SET_CRITICAL_HEAP_PERCENTAGE SET_TRACE_FLAG REMOVE_METASTORE_ENTRY EXPORT_DDLS EXPORT_DATA The following built-in functions are available: DSID CURRENT_USER CURRENT_USER_LDAP_GROUPS","title":"Built-in System Procedures and Built-in Functions"},{"location":"reference/inbuilt_system_procedures/current_user/","text":"CURRENT_USER \u00b6 The CURRENT_USER function returns the name of the user that owns the session executing the current SQL statement. Example \u00b6 snappy> SELECT CURRENT_USER(); current_user() ----------------------------------------------------------------------------------------- USER1 1 row selected Also see: Built-in System Procedures and Built-in Functions","title":"CURRENT_USER"},{"location":"reference/inbuilt_system_procedures/current_user/#current_user","text":"The CURRENT_USER function returns the name of the user that owns the session executing the current SQL statement.","title":"CURRENT_USER"},{"location":"reference/inbuilt_system_procedures/current_user/#example","text":"snappy> SELECT CURRENT_USER(); current_user() ----------------------------------------------------------------------------------------- USER1 1 row selected Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/current_user_ldap_groups/","text":"CURRENT_USER_LDAP_GROUPS \u00b6 The CURRENT_USER_LDAP_GROUPS function returns all the ldap groups (as an ARRAY) of the user who is executing the current SQL statement. Example \u00b6 snappy> SELECT array_contains(CURRENT_USER_LDAP_GROUPS(), 'GROUP1'); ---------------------------- true 1 row selected Also see: Built-in System Procedures and Built-in Functions","title":"CURRENT_USER_LDAP_GROUPS"},{"location":"reference/inbuilt_system_procedures/current_user_ldap_groups/#current_user_ldap_groups","text":"The CURRENT_USER_LDAP_GROUPS function returns all the ldap groups (as an ARRAY) of the user who is executing the current SQL statement.","title":"CURRENT_USER_LDAP_GROUPS"},{"location":"reference/inbuilt_system_procedures/current_user_ldap_groups/#example","text":"snappy> SELECT array_contains(CURRENT_USER_LDAP_GROUPS(), 'GROUP1'); ---------------------------- true 1 row selected Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/diskstore-fsync/","text":"SYS.DISKSTORE_FSYNC \u00b6 Flush and fsync all data to configured disk stores, including data in asynchronous persistence queues. Use this procedure to ensure that all in-memory data is written to configured disk stores. You may want to fsync data in disk stores before copying or backing up disk store files at the operating system level, to ensure that they represent the current state of the SnappyData data. Syntax \u00b6 SYS.diskstore_fsync ( IN DISKSTORENAME VARCHAR(128) ) DISKSTORENAME The name of the disk store, as specified in the CREATE DISKSTORE statement. (Default or system-generated disk store names can be queried from the SYSDISKSTORES table.) Example \u00b6 Performs an fsync of the the data dictionary disk store file : snappy> call sys.diskstore_fsync('GFXD-DD-DISKSTORE');","title":"SYS.DISKSTORE_FSYNC"},{"location":"reference/inbuilt_system_procedures/diskstore-fsync/#sysdiskstore_fsync","text":"Flush and fsync all data to configured disk stores, including data in asynchronous persistence queues. Use this procedure to ensure that all in-memory data is written to configured disk stores. You may want to fsync data in disk stores before copying or backing up disk store files at the operating system level, to ensure that they represent the current state of the SnappyData data.","title":"SYS.DISKSTORE_FSYNC"},{"location":"reference/inbuilt_system_procedures/diskstore-fsync/#syntax","text":"SYS.diskstore_fsync ( IN DISKSTORENAME VARCHAR(128) ) DISKSTORENAME The name of the disk store, as specified in the CREATE DISKSTORE statement. (Default or system-generated disk store names can be queried from the SYSDISKSTORES table.)","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/diskstore-fsync/#example","text":"Performs an fsync of the the data dictionary disk store file : snappy> call sys.diskstore_fsync('GFXD-DD-DISKSTORE');","title":"Example"},{"location":"reference/inbuilt_system_procedures/dsid/","text":"DSID \u00b6 The DSID function returns the string form of the distributed member process identity uniquely represented in the distributed system. Note This function is not supported in the Smart Connector mode. It is only supported for embedded mode, JDBC and ODBC. In some rare cases, if the bucket has just moved while the query was being scheduled, then remote bucket fetch cannot be performed by the query partition but it still displays the DSID() of the node where the partition was executed. Example \u00b6 snappy>select count(*), dsid() from AIRLINE group by dsid(); count(1) |DSID() ------------------------------------------------- 347749 |192.168.1.98(3625)<v3>:8739 348440 |192.168.1.98(3849)<v4>:50958 303811 |192.168.1.98(3255)<v1>:2428 3 rows selected Also see: Built-in System Procedures and Built-in Functions","title":"DSID"},{"location":"reference/inbuilt_system_procedures/dsid/#dsid","text":"The DSID function returns the string form of the distributed member process identity uniquely represented in the distributed system. Note This function is not supported in the Smart Connector mode. It is only supported for embedded mode, JDBC and ODBC. In some rare cases, if the bucket has just moved while the query was being scheduled, then remote bucket fetch cannot be performed by the query partition but it still displays the DSID() of the node where the partition was executed.","title":"DSID"},{"location":"reference/inbuilt_system_procedures/dsid/#example","text":"snappy>select count(*), dsid() from AIRLINE group by dsid(); count(1) |DSID() ------------------------------------------------- 347749 |192.168.1.98(3625)<v3>:8739 348440 |192.168.1.98(3849)<v4>:50958 303811 |192.168.1.98(3255)<v1>:2428 3 rows selected Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/dump-stacks/","text":"SYS.DUMP_STACKS \u00b6 Writes thread stacks, locks, and transaction states to the SnappyData log file. You can write stack information either for the current SnappyData member or for all SnappyData members in the distributed system. Syntax \u00b6 SYS.DUMP_STACKS ( IN ALL BOOLEAN ) ALL Specifies boolean value: true to log stack trace information for all SnappyData members, or false to log information only for the local SnappyData member. Example \u00b6 This command writes thread stack information only for the local SnappyData member. The stack information is written to the SnappyData log file (by default snappyserver.log in the member startup directory): snappy> call sys.dump_stacks('false'); Also see: Built-in System Procedures and Built-in Functions","title":"DUMP_STACKS"},{"location":"reference/inbuilt_system_procedures/dump-stacks/#sysdump_stacks","text":"Writes thread stacks, locks, and transaction states to the SnappyData log file. You can write stack information either for the current SnappyData member or for all SnappyData members in the distributed system.","title":"SYS.DUMP_STACKS"},{"location":"reference/inbuilt_system_procedures/dump-stacks/#syntax","text":"SYS.DUMP_STACKS ( IN ALL BOOLEAN ) ALL Specifies boolean value: true to log stack trace information for all SnappyData members, or false to log information only for the local SnappyData member.","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/dump-stacks/#example","text":"This command writes thread stack information only for the local SnappyData member. The stack information is written to the SnappyData log file (by default snappyserver.log in the member startup directory): snappy> call sys.dump_stacks('false'); Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/export_data/","text":"SYS.EXPORT_DATA \u00b6 You can use the EXPORT_DATA system procedure to export the tables in a specified format into the provided path. The procedure also generates helper scripts which you can use to load the extracted data into a new cluster. Verify there are non-empty directories for the respective tables for all the tables shown on the UI. After the data is loaded into the new cluster successfully, drop all the external tables. The EXPORT_DATA procedure accepts the following arguments: Arguments Description exportURI Specify the Spark supported URI to export data. For example S3, HDFS, NFS, Local FileSystem etc. formatType Specify Spark supported formats. For example CSV, Parquet, JSON etc. tableNames Provide comma-separated table names (<schema>.<tablename>) or specify all to export all the tables. ignoreError Specify this setting as true to ignore errors while reading a table and move on to reading the next table. Any errors that occur while exporting a table is logged and the next table is exported. When a table is skipped due to failures, no data is available, and you can retry exporting it after troubleshooting. You must also check both the lead as well as the server logs for failures. Note The directory created is of the pattern <exportURI_TimeInMillis> . Syntax \u00b6 call sys.EXPORT_DATA('<exportURI>', '<formatType>', '<tableNames>', '<ignoreError>'); Examples \u00b6 call sys.EXPORT_DATA('/home/xyz/extracted/data/', 'csv', 'all', 'true'); call sys.EXPORT_DATA('/home/xyz/extracted/data/', 'parquet', 'CT,RT', 'false'); Folder Structure \u00b6 ls /home/xyz/extracted/data_1571059786952/ APP.CT/ APP.RT/ APP.RTP/ ls /home/xyz/extracted/data_1571059786952/APP.CT/ part-00000-e6923433-5638-46ce-a719-b203c8968c88.csv part-00001-e6923433-5638-46ce-a719-b203c8968c88.csv ls /home/xyz/extracted/data_1571059786952_load_scripts/ part-00000 _SUCCESS Using generated load scripts run \u2018/home/xyz/extracted/data_1571059786952_load_scripts/part-00000\u2019; For statistics search the leader log for the following message: Successfully exported <N> tables. Exported tables are: <TableNames> Failed to export <N> tables. Failed tables are <TableNames>","title":"EXPORT_DATA"},{"location":"reference/inbuilt_system_procedures/export_data/#sysexport_data","text":"You can use the EXPORT_DATA system procedure to export the tables in a specified format into the provided path. The procedure also generates helper scripts which you can use to load the extracted data into a new cluster. Verify there are non-empty directories for the respective tables for all the tables shown on the UI. After the data is loaded into the new cluster successfully, drop all the external tables. The EXPORT_DATA procedure accepts the following arguments: Arguments Description exportURI Specify the Spark supported URI to export data. For example S3, HDFS, NFS, Local FileSystem etc. formatType Specify Spark supported formats. For example CSV, Parquet, JSON etc. tableNames Provide comma-separated table names (<schema>.<tablename>) or specify all to export all the tables. ignoreError Specify this setting as true to ignore errors while reading a table and move on to reading the next table. Any errors that occur while exporting a table is logged and the next table is exported. When a table is skipped due to failures, no data is available, and you can retry exporting it after troubleshooting. You must also check both the lead as well as the server logs for failures. Note The directory created is of the pattern <exportURI_TimeInMillis> .","title":"SYS.EXPORT_DATA"},{"location":"reference/inbuilt_system_procedures/export_data/#syntax","text":"call sys.EXPORT_DATA('<exportURI>', '<formatType>', '<tableNames>', '<ignoreError>');","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/export_data/#examples","text":"call sys.EXPORT_DATA('/home/xyz/extracted/data/', 'csv', 'all', 'true'); call sys.EXPORT_DATA('/home/xyz/extracted/data/', 'parquet', 'CT,RT', 'false');","title":"Examples"},{"location":"reference/inbuilt_system_procedures/export_data/#folder-structure","text":"ls /home/xyz/extracted/data_1571059786952/ APP.CT/ APP.RT/ APP.RTP/ ls /home/xyz/extracted/data_1571059786952/APP.CT/ part-00000-e6923433-5638-46ce-a719-b203c8968c88.csv part-00001-e6923433-5638-46ce-a719-b203c8968c88.csv ls /home/xyz/extracted/data_1571059786952_load_scripts/ part-00000 _SUCCESS Using generated load scripts run \u2018/home/xyz/extracted/data_1571059786952_load_scripts/part-00000\u2019; For statistics search the leader log for the following message: Successfully exported <N> tables. Exported tables are: <TableNames> Failed to export <N> tables. Failed tables are <TableNames>","title":"Folder Structure"},{"location":"reference/inbuilt_system_procedures/export_ddl/","text":"SYS.EXPORT_DDLS \u00b6 You can use the EXPORT_DDLs system procedure to export table definitions in text format. The EXPORT_DDLs system procedure takes a single argument, exportURI . You can provide any spark supported URI such as s3, local path, or HDFS. All the DDLs such as TABLE, VIEW, DATABASE, FUNCTION, DEPLOY, ALTER, UPDATE, GRANT are exported to a text file in exportURI with the name part-00000 . Verify there are respective DDLs in the generated file for all the tables shown on the UI. Note The directory created is of the pattern <exportURI_TimeInMillis> . Syntax \u00b6 call sys.EXPORT_DDLS('<exportURI>'); Examples \u00b6 call sys.EXPORT_DDLS('/home/xyz/extracted/ddls'); Folder Structure \u00b6 ls /home/xyz/extracted/ddls_1571059691610/ part-00000 _SUCCESS Reloading extracted DDLs run \u2018/home/xyz/extracted/ddls_1571059691610/part-00000\u2019; For statistics search the leader log for message Successfully exported <N> DDL statements.","title":"EXPORT_DDLS"},{"location":"reference/inbuilt_system_procedures/export_ddl/#sysexport_ddls","text":"You can use the EXPORT_DDLs system procedure to export table definitions in text format. The EXPORT_DDLs system procedure takes a single argument, exportURI . You can provide any spark supported URI such as s3, local path, or HDFS. All the DDLs such as TABLE, VIEW, DATABASE, FUNCTION, DEPLOY, ALTER, UPDATE, GRANT are exported to a text file in exportURI with the name part-00000 . Verify there are respective DDLs in the generated file for all the tables shown on the UI. Note The directory created is of the pattern <exportURI_TimeInMillis> .","title":"SYS.EXPORT_DDLS"},{"location":"reference/inbuilt_system_procedures/export_ddl/#syntax","text":"call sys.EXPORT_DDLS('<exportURI>');","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/export_ddl/#examples","text":"call sys.EXPORT_DDLS('/home/xyz/extracted/ddls');","title":"Examples"},{"location":"reference/inbuilt_system_procedures/export_ddl/#folder-structure","text":"ls /home/xyz/extracted/ddls_1571059691610/ part-00000 _SUCCESS Reloading extracted DDLs run \u2018/home/xyz/extracted/ddls_1571059691610/part-00000\u2019; For statistics search the leader log for message Successfully exported <N> DDL statements.","title":"Folder Structure"},{"location":"reference/inbuilt_system_procedures/rebalance-all-buckets/","text":"SYS.REBALANCE_ALL_BUCKETS \u00b6 Rebalance partitioned table data on available SnappyData members. Syntax \u00b6 SYS.REBALANCE_ALL_BUCKETS() Rebalancing is a SnappyData member operation that affects partitioned tables created in the cluster. Rebalancing performs two tasks: If the partitioned table's redundancy setting is not satisfied, rebalancing does what it can to recover redundancy. Rebalancing moves the partitioned table's data buckets between host members as needed to establish the best balance of data across the distributed system. For efficiency, when starting multiple members, trigger the rebalance a single time, after you have added all members. Example \u00b6 snappy> call sys.rebalance_all_buckets(); Also see: Built-in System Procedures and Built-in Functions","title":"REBALANCE_ALL_BUCKETS"},{"location":"reference/inbuilt_system_procedures/rebalance-all-buckets/#sysrebalance_all_buckets","text":"Rebalance partitioned table data on available SnappyData members.","title":"SYS.REBALANCE_ALL_BUCKETS"},{"location":"reference/inbuilt_system_procedures/rebalance-all-buckets/#syntax","text":"SYS.REBALANCE_ALL_BUCKETS() Rebalancing is a SnappyData member operation that affects partitioned tables created in the cluster. Rebalancing performs two tasks: If the partitioned table's redundancy setting is not satisfied, rebalancing does what it can to recover redundancy. Rebalancing moves the partitioned table's data buckets between host members as needed to establish the best balance of data across the distributed system. For efficiency, when starting multiple members, trigger the rebalance a single time, after you have added all members.","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/rebalance-all-buckets/#example","text":"snappy> call sys.rebalance_all_buckets(); Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/set-trace-flag/","text":"SYS.SET_TRACE_FLAG \u00b6 This procedure enables or disables a specific trace flag for the distributed system as a whole. You must be a system user to execute this procedure. Syntax \u00b6 CALL SYS.SET_TRACE_FLAG ( IN TRACE_FLAG VARCHAR(256), IN ON BOOLEAN ) TRACE_FLAG Specifies name of the trace flag to enable or disable. ON Specifies boolean value: true or 1 to enable the trace flag, or false or 0 to disable it. Example \u00b6 This command traces all JAR installation, update, and removal operations in the SnappyData distributed system: snappy> call sys.set_trace_flag ('TraceJars', 'true'); Also see: Built-in System Procedures and Built-in Functions","title":"SET_TRACE_FLAG"},{"location":"reference/inbuilt_system_procedures/set-trace-flag/#sysset_trace_flag","text":"This procedure enables or disables a specific trace flag for the distributed system as a whole. You must be a system user to execute this procedure.","title":"SYS.SET_TRACE_FLAG"},{"location":"reference/inbuilt_system_procedures/set-trace-flag/#syntax","text":"CALL SYS.SET_TRACE_FLAG ( IN TRACE_FLAG VARCHAR(256), IN ON BOOLEAN ) TRACE_FLAG Specifies name of the trace flag to enable or disable. ON Specifies boolean value: true or 1 to enable the trace flag, or false or 0 to disable it.","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/set-trace-flag/#example","text":"This command traces all JAR installation, update, and removal operations in the SnappyData distributed system: snappy> call sys.set_trace_flag ('TraceJars', 'true'); Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/set_critical_heap_percentage/","text":"SYS.SET_CRITICAL_HEAP_PERCENTAGE \u00b6 Sets the percentage threshold of Java heap memory usage that triggers LowMemoryException s on a SnappyData data store. This procedure executes only on the local SnappyData data store member. This procedure sets the percentage threshold of critical Java heap memory usage on the local SnappyData data store. If the amount of heap memory being used exceeds the percentage, the member will report LowMemoryExceptions during local or client put operations into heap tables. The member will also inform other members in the distributed system that it has reached the critical threshold. When a data store is started with the -heap-size option, the default critical threshold is 90%. Syntax \u00b6 SYS.SET_CRITICAL_HEAP_PERCENTAGE ( IN PERCENTAGE REAL NOT NULL ) PERCENTAGE The percentage of used heap space that triggers LowMemoryException s on the local SnappyData data store. Example \u00b6 This command sets the critical threshold for heap memory usage on the local SnappyData member to 99.9%: snappy>call sys.set_critical_heap_percentage (99.9); Also see: Built-in System Procedures and Built-in Functions","title":"SET_CRITICAL_HEAP_PERCENTAGE"},{"location":"reference/inbuilt_system_procedures/set_critical_heap_percentage/#sysset_critical_heap_percentage","text":"Sets the percentage threshold of Java heap memory usage that triggers LowMemoryException s on a SnappyData data store. This procedure executes only on the local SnappyData data store member. This procedure sets the percentage threshold of critical Java heap memory usage on the local SnappyData data store. If the amount of heap memory being used exceeds the percentage, the member will report LowMemoryExceptions during local or client put operations into heap tables. The member will also inform other members in the distributed system that it has reached the critical threshold. When a data store is started with the -heap-size option, the default critical threshold is 90%.","title":"SYS.SET_CRITICAL_HEAP_PERCENTAGE"},{"location":"reference/inbuilt_system_procedures/set_critical_heap_percentage/#syntax","text":"SYS.SET_CRITICAL_HEAP_PERCENTAGE ( IN PERCENTAGE REAL NOT NULL ) PERCENTAGE The percentage of used heap space that triggers LowMemoryException s on the local SnappyData data store.","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/set_critical_heap_percentage/#example","text":"This command sets the critical threshold for heap memory usage on the local SnappyData member to 99.9%: snappy>call sys.set_critical_heap_percentage (99.9); Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/inbuilt_system_procedures/set_eviction_heap_percentage/","text":"SYS.SET_EVICTION_HEAP_PERCENTAGE \u00b6 Sets the percentage threshold of Java heap memory usage that triggers a SnappyData data store to perform LRU eviction on tables that are configured for LRU_HEAP eviction. This procedure executes only on the local SnappyData data store member. This procedure sets the percentage threshold for evicting table data from the Java heap for the local SnappyData data store. When the used heap reaches the percentage, SnappyData begins to evict rows, using a LRU algorithm, from tables that are configured with LRU_HEAP eviction. The default eviction heap percentage is 81% of the critical heap percentage value. Syntax \u00b6 SYS.SET_EVICTION_HEAP_PERCENTAGE ( IN PERCENTAGE REAL NOT NULL ) PERCENTAGE The percentage of used heap space that triggers eviction on the local SnappyData data store. Example \u00b6 This command triggers eviction on any SnappyData member when the local member's heap usage reaches 90%: call sys.set_eviction_heap_percentage (90);","title":"SYS.SET_EVICTION_HEAP_PERCENTAGE"},{"location":"reference/inbuilt_system_procedures/set_eviction_heap_percentage/#sysset_eviction_heap_percentage","text":"Sets the percentage threshold of Java heap memory usage that triggers a SnappyData data store to perform LRU eviction on tables that are configured for LRU_HEAP eviction. This procedure executes only on the local SnappyData data store member. This procedure sets the percentage threshold for evicting table data from the Java heap for the local SnappyData data store. When the used heap reaches the percentage, SnappyData begins to evict rows, using a LRU algorithm, from tables that are configured with LRU_HEAP eviction. The default eviction heap percentage is 81% of the critical heap percentage value.","title":"SYS.SET_EVICTION_HEAP_PERCENTAGE"},{"location":"reference/inbuilt_system_procedures/set_eviction_heap_percentage/#syntax","text":"SYS.SET_EVICTION_HEAP_PERCENTAGE ( IN PERCENTAGE REAL NOT NULL ) PERCENTAGE The percentage of used heap space that triggers eviction on the local SnappyData data store.","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/set_eviction_heap_percentage/#example","text":"This command triggers eviction on any SnappyData member when the local member's heap usage reaches 90%: call sys.set_eviction_heap_percentage (90);","title":"Example"},{"location":"reference/inbuilt_system_procedures/setstatstimingproc/","text":"SYSCS_UTIL.SET_STATISTICS_TIMING \u00b6 When statistics timing is turned on, you can track the timing of various aspects of a statement execution. When statistics timing is turned off, all timing values are set to zero. Statistics timing is an attribute associated with a connection that you turn on and off by using the SYSCS_UTIL.SET_STATISTICS_TIMING system procedure. Statistics timing is turned off by default. Turn statistics timing on only when the statistics are being collected with [SYSCS_UTIL.SET_EXPLAIN_CONNECTION] . If you turn statistics timing on before enabling statistics collection with SYSCS_UTIL.SET_EXPLAIN_CONNECTION, the procedure has no effect. To Be Confirmed Turn statistics timing on by calling this procedure with a non-zero argument. Turn statistics timing off by calling the procedure with a zero argument. To be confirmed See Capture Query Plans for All Statements . Syntax \u00b6 SYSCS_UTIL.SET_STATISTICS_TIMING(IN SMALLINT ENABLE) Example \u00b6 To enable statistics timing: CALL SYSCS_UTIL.SET_STATISTICS_TIMING(1);","title":"SYSCS_UTIL.SET_STATISTICS_TIMING"},{"location":"reference/inbuilt_system_procedures/setstatstimingproc/#syscs_utilset_statistics_timing","text":"When statistics timing is turned on, you can track the timing of various aspects of a statement execution. When statistics timing is turned off, all timing values are set to zero. Statistics timing is an attribute associated with a connection that you turn on and off by using the SYSCS_UTIL.SET_STATISTICS_TIMING system procedure. Statistics timing is turned off by default. Turn statistics timing on only when the statistics are being collected with [SYSCS_UTIL.SET_EXPLAIN_CONNECTION] . If you turn statistics timing on before enabling statistics collection with SYSCS_UTIL.SET_EXPLAIN_CONNECTION, the procedure has no effect. To Be Confirmed Turn statistics timing on by calling this procedure with a non-zero argument. Turn statistics timing off by calling the procedure with a zero argument. To be confirmed See Capture Query Plans for All Statements .","title":"SYSCS_UTIL.SET_STATISTICS_TIMING"},{"location":"reference/inbuilt_system_procedures/setstatstimingproc/#syntax","text":"SYSCS_UTIL.SET_STATISTICS_TIMING(IN SMALLINT ENABLE)","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/setstatstimingproc/#example","text":"To enable statistics timing: CALL SYSCS_UTIL.SET_STATISTICS_TIMING(1);","title":"Example"},{"location":"reference/inbuilt_system_procedures/sys_remove_metastore_entry/","text":"SYS.REMOVE_METASTORE_ENTRY \u00b6 This procedure drops a table from the external catalog if it exists (without checking that the table exists in the catalog). However, it does not handle related policies and base tables; hence they must be dropped separately. You must connect to the server from the Snappy shell, check the policies and base tables, and drop them separately. // Viewing and dropping policies SELECT * FROM SYS.SYSPOLICIES; DROP POLICY <policy name>; Syntax \u00b6 call sys.REMOVE_METASTORE_ENTRY('<dbName>.<tableName>', '<forceDrop boolean>'); Example \u00b6 Considering the case when cluster fails to come up, and the log mentions the pattern: \"AnalysisException: Table (dbName).(tableName) might be inconsistent in hive catalog. Use system procedure SYS.REMOVE_METASTORE_ENTRY to remove inconsistency\" , then use the following steps to resolve the issue. Check if the server is running: cd $SNAPPY_HOME ./sbin/snappy-status-all.sh Launch Snappy shell and connect to the server: ./bin/snappy ; connect client '<server hostname>:<server port>'; Check and find if there are any policies on the table which caused catalog inconsistency and drop policies: SELECT * FROM SYS.SYSPOLICIES; DROP POLICY <policy name>; Call the procedure to drop the table from the catalog: call sys.REMOVE_METASTORE_ENTRY('<dbName>.<tableName>', 'false'); Restart SnappyData cluster and check the status. The cluster starts successfully. Also see: Built-in System Procedures and Built-in Functions","title":"REMOVE_METASTORE_ENTRY"},{"location":"reference/inbuilt_system_procedures/sys_remove_metastore_entry/#sysremove_metastore_entry","text":"This procedure drops a table from the external catalog if it exists (without checking that the table exists in the catalog). However, it does not handle related policies and base tables; hence they must be dropped separately. You must connect to the server from the Snappy shell, check the policies and base tables, and drop them separately. // Viewing and dropping policies SELECT * FROM SYS.SYSPOLICIES; DROP POLICY <policy name>;","title":"SYS.REMOVE_METASTORE_ENTRY"},{"location":"reference/inbuilt_system_procedures/sys_remove_metastore_entry/#syntax","text":"call sys.REMOVE_METASTORE_ENTRY('<dbName>.<tableName>', '<forceDrop boolean>');","title":"Syntax"},{"location":"reference/inbuilt_system_procedures/sys_remove_metastore_entry/#example","text":"Considering the case when cluster fails to come up, and the log mentions the pattern: \"AnalysisException: Table (dbName).(tableName) might be inconsistent in hive catalog. Use system procedure SYS.REMOVE_METASTORE_ENTRY to remove inconsistency\" , then use the following steps to resolve the issue. Check if the server is running: cd $SNAPPY_HOME ./sbin/snappy-status-all.sh Launch Snappy shell and connect to the server: ./bin/snappy ; connect client '<server hostname>:<server port>'; Check and find if there are any policies on the table which caused catalog inconsistency and drop policies: SELECT * FROM SYS.SYSPOLICIES; DROP POLICY <policy name>; Call the procedure to drop the table from the catalog: call sys.REMOVE_METASTORE_ENTRY('<dbName>.<tableName>', 'false'); Restart SnappyData cluster and check the status. The cluster starts successfully. Also see: Built-in System Procedures and Built-in Functions","title":"Example"},{"location":"reference/interactive_commands/","text":"Snappy-SQL Shell Interactive Commands \u00b6 snappy implements an interactive command-line tool that is based on the Apache Derby ij tool. Use snappy to run scripts or interactive queries against a SnappyData cluster. Start the interactive snappy command prompt by using the snappy script without supplying any other options: snappy The system property snappy.history specifies a file in which to store all of the commands executed during an interactive snappy session. For example: $ export JAVA_ARGS=\"-Dsnappy.history=/Users/user1/snappydata-history.sql\" $ snappy By default the history file is named .snappy.history, and it is stored in the current user's home directory. snappy accepts several commands to control its use of JDBC. It recognizes a semicolon as the end of a snappy or SQL command. It treats semicolons within SQL comments, strings, and delimited identifiers as part of those constructs and not as the end of the command. Semicolons are required at the end of a snappy or SQL statement. All snappy commands, identifiers, and keywords are case-insensitive. Commands can span multiple lines without using any special escape character for ends of lines. This means that if a string spans a line, the new line contents show up in the value in the string. snappy treats any command that it does not recognize as a SQL command that is passed to the underlying connection. This means that any syntactic errors in snappy commands are handed to the SQL engine and generally result in SQL parsing errors. autocommit Turns the connection's auto-commit mode on or off. commit Issues a java.sql.Connection.commit request. connect client Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the host:port values. connect Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the host:port values. describe Provides a description of the specified table or view. disconnect Disconnects from the database. elapsedtime Displays the total time elapsed during statement execution. exit Completes the snappy application and halts processing. MaximumDisplayWidth Sets the largest display width for columns to the specified value. rollback Issues a java.sql.Connection.rollback request. run Treats the value of the string as a valid file name, and redirects snappy processing to read from that file until it ends or an exit command is executed. set connection Specifies which connection to make current when more than one connection is open. show Displays information about active connections and database objects.","title":"Snappy-SQL Shell Interactive Commands"},{"location":"reference/interactive_commands/#snappy-sql-shell-interactive-commands","text":"snappy implements an interactive command-line tool that is based on the Apache Derby ij tool. Use snappy to run scripts or interactive queries against a SnappyData cluster. Start the interactive snappy command prompt by using the snappy script without supplying any other options: snappy The system property snappy.history specifies a file in which to store all of the commands executed during an interactive snappy session. For example: $ export JAVA_ARGS=\"-Dsnappy.history=/Users/user1/snappydata-history.sql\" $ snappy By default the history file is named .snappy.history, and it is stored in the current user's home directory. snappy accepts several commands to control its use of JDBC. It recognizes a semicolon as the end of a snappy or SQL command. It treats semicolons within SQL comments, strings, and delimited identifiers as part of those constructs and not as the end of the command. Semicolons are required at the end of a snappy or SQL statement. All snappy commands, identifiers, and keywords are case-insensitive. Commands can span multiple lines without using any special escape character for ends of lines. This means that if a string spans a line, the new line contents show up in the value in the string. snappy treats any command that it does not recognize as a SQL command that is passed to the underlying connection. This means that any syntactic errors in snappy commands are handed to the SQL engine and generally result in SQL parsing errors. autocommit Turns the connection's auto-commit mode on or off. commit Issues a java.sql.Connection.commit request. connect client Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the host:port values. connect Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the host:port values. describe Provides a description of the specified table or view. disconnect Disconnects from the database. elapsedtime Displays the total time elapsed during statement execution. exit Completes the snappy application and halts processing. MaximumDisplayWidth Sets the largest display width for columns to the specified value. rollback Issues a java.sql.Connection.rollback request. run Treats the value of the string as a valid file name, and redirects snappy processing to read from that file until it ends or an exit command is executed. set connection Specifies which connection to make current when more than one connection is open. show Displays information about active connections and database objects.","title":"Snappy-SQL Shell Interactive Commands"},{"location":"reference/interactive_commands/autocommit/","text":"autocommit \u00b6 Syntax \u00b6 AUTOCOMMIT { ON | OFF } Description \u00b6 Turns the connection's auto-commit mode on or off. JDBC specifies that the default auto-commit mode is ON . Certain types of processing require that auto-commit mode be OFF . If auto-commit mode is changed from off to on when a transaction is outstanding, that work is committed when the current transaction commits, not at the time auto-commit is turned on. Use Commit or Rollback before turning on auto-commit when there is a transaction outstanding, so that all prior work is completed before the return to auto-commit mode. Example \u00b6 snappy> AUTOCOMMIT off; snappy> INSERT INTO greetings values (DEFAULT, 'hello'); 1 row inserted/updated/deleted snappy> COMMIT;","title":"autocommit"},{"location":"reference/interactive_commands/autocommit/#autocommit","text":"","title":"autocommit"},{"location":"reference/interactive_commands/autocommit/#syntax","text":"AUTOCOMMIT { ON | OFF }","title":"Syntax"},{"location":"reference/interactive_commands/autocommit/#description","text":"Turns the connection's auto-commit mode on or off. JDBC specifies that the default auto-commit mode is ON . Certain types of processing require that auto-commit mode be OFF . If auto-commit mode is changed from off to on when a transaction is outstanding, that work is committed when the current transaction commits, not at the time auto-commit is turned on. Use Commit or Rollback before turning on auto-commit when there is a transaction outstanding, so that all prior work is completed before the return to auto-commit mode.","title":"Description"},{"location":"reference/interactive_commands/autocommit/#example","text":"snappy> AUTOCOMMIT off; snappy> INSERT INTO greetings values (DEFAULT, 'hello'); 1 row inserted/updated/deleted snappy> COMMIT;","title":"Example"},{"location":"reference/interactive_commands/commit/","text":"commit \u00b6 Syntax \u00b6 COMMIT Description \u00b6 Issues a java.sql.Connection.commit request. Use this command only if auto-commit is off . A java.sql.Connection.commit request commits the currently active transaction and initiates a new transaction. Example \u00b6 snappy> AUTOCOMMIT off; snappy> INSERT INTO greetings values (DEFAULT, 'hello'); 1 row inserted/updated/deleted snappy> COMMIT;","title":"commit"},{"location":"reference/interactive_commands/commit/#commit","text":"","title":"commit"},{"location":"reference/interactive_commands/commit/#syntax","text":"COMMIT","title":"Syntax"},{"location":"reference/interactive_commands/commit/#description","text":"Issues a java.sql.Connection.commit request. Use this command only if auto-commit is off . A java.sql.Connection.commit request commits the currently active transaction and initiates a new transaction.","title":"Description"},{"location":"reference/interactive_commands/commit/#example","text":"snappy> AUTOCOMMIT off; snappy> INSERT INTO greetings values (DEFAULT, 'hello'); 1 row inserted/updated/deleted snappy> COMMIT;","title":"Example"},{"location":"reference/interactive_commands/connect/","text":"connect \u00b6 Connects to the database indicated by the ConnectionURLString . Syntax \u00b6 CONNECT ConnectionURLString [ PROTOCOL Identifier ] [ AS Identifier ] Description \u00b6 Connects to the database indicated by the ConnectionURLString . You have the option of specifying a name for your connection. Use the Set Connection command to switch between connections. If you do not name a connection, the system generates a name automatically. Note If the connection requires a user name and password, supply those in the connection URL string, as shown in the example. If the connect succeeds, the connection becomes the current one and snappy displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt. All further commands are processed against the new, current connection. Example \u00b6 snappy> protocol 'jdbc:derby:'; snappy> connect '//armenia:29303/myDB;user=a;password=a' as db5Connection; snappy> show connections; DB5CONNECTION* - jdbc:derby://armenia:29303/myDB * = current connection","title":"connect"},{"location":"reference/interactive_commands/connect/#connect","text":"Connects to the database indicated by the ConnectionURLString .","title":"connect"},{"location":"reference/interactive_commands/connect/#syntax","text":"CONNECT ConnectionURLString [ PROTOCOL Identifier ] [ AS Identifier ]","title":"Syntax"},{"location":"reference/interactive_commands/connect/#description","text":"Connects to the database indicated by the ConnectionURLString . You have the option of specifying a name for your connection. Use the Set Connection command to switch between connections. If you do not name a connection, the system generates a name automatically. Note If the connection requires a user name and password, supply those in the connection URL string, as shown in the example. If the connect succeeds, the connection becomes the current one and snappy displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt. All further commands are processed against the new, current connection.","title":"Description"},{"location":"reference/interactive_commands/connect/#example","text":"snappy> protocol 'jdbc:derby:'; snappy> connect '//armenia:29303/myDB;user=a;password=a' as db5Connection; snappy> show connections; DB5CONNECTION* - jdbc:derby://armenia:29303/myDB * = current connection","title":"Example"},{"location":"reference/interactive_commands/connect_client/","text":"connect client \u00b6 Syntax \u00b6 CONNECT CLIENT 'host:port[;property=value]*' [ AS connectionName ] Description \u00b6 Uses the JDBC SnappyData thin client driver to connect to a SnappyData member indicated by the host:port values. You can specify an optional name for your connection. Use the set connection to switch between multiple connections. If you do not name a connection, the system generates a name automatically. If the connection requires a user name and password, supply those with the optional properties. If the connect succeeds, the connection becomes the current one and snappy displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt. All further commands are processed against the new, current connection. Example \u00b6 SnappyData version 1.1.1 snappy> connect client 'localhost:1527' as clientConnection; snappy> show connections; CLIENTCONNECTION* - jdbc:snappydata:thrift://localhost[1527] * = current connection","title":"connect client"},{"location":"reference/interactive_commands/connect_client/#connect-client","text":"","title":"connect client"},{"location":"reference/interactive_commands/connect_client/#syntax","text":"CONNECT CLIENT 'host:port[;property=value]*' [ AS connectionName ]","title":"Syntax"},{"location":"reference/interactive_commands/connect_client/#description","text":"Uses the JDBC SnappyData thin client driver to connect to a SnappyData member indicated by the host:port values. You can specify an optional name for your connection. Use the set connection to switch between multiple connections. If you do not name a connection, the system generates a name automatically. If the connection requires a user name and password, supply those with the optional properties. If the connect succeeds, the connection becomes the current one and snappy displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt. All further commands are processed against the new, current connection.","title":"Description"},{"location":"reference/interactive_commands/connect_client/#example","text":"SnappyData version 1.1.1 snappy> connect client 'localhost:1527' as clientConnection; snappy> show connections; CLIENTCONNECTION* - jdbc:snappydata:thrift://localhost[1527] * = current connection","title":"Example"},{"location":"reference/interactive_commands/describe/","text":"describe \u00b6 Provides a description of the specified table or view. Syntax \u00b6 DESCRIBE { table-Name | view-Name } Description \u00b6 Provides a description of the specified table or view. For a list of tables in the current schema, use the Show Tables command. For a list of views in the current schema, use the Show Views command. For a list of available schemas, use the Show Schemas command. If the table or view is in a particular schema, qualify it with the schema name. If the table or view name is case-sensitive, enclose it in single quotes. You can display all the columns from all the tables and views in a single schema in a single display by using the wildcard character '*'. Example \u00b6 snappy> describe maps; COLUMN_NAME |TYPE_NAME|DEC&|NUM&|COLUM&|COLUMN_DEF|CHAR_OCTE&|IS_NULL& ------------------------------------------------------------------------------ MAP_ID |INTEGER |0 |10 |10 |AUTOINCRE&|NULL |NO MAP_NAME |VARCHAR |NULL|NULL|24 |NULL |48 |NO REGION |VARCHAR |NULL|NULL|26 |NULL |52 |YES AREA |DECIMAL |4 |10 |8 |NULL |NULL |NO PHOTO_FORMAT |VARCHAR |NULL|NULL|26 |NULL |52 |NO PICTURE |BLOB |NULL|NULL|102400|NULL |NULL |YES 6 rows selected snappy>","title":"describe"},{"location":"reference/interactive_commands/describe/#describe","text":"Provides a description of the specified table or view.","title":"describe"},{"location":"reference/interactive_commands/describe/#syntax","text":"DESCRIBE { table-Name | view-Name }","title":"Syntax"},{"location":"reference/interactive_commands/describe/#description","text":"Provides a description of the specified table or view. For a list of tables in the current schema, use the Show Tables command. For a list of views in the current schema, use the Show Views command. For a list of available schemas, use the Show Schemas command. If the table or view is in a particular schema, qualify it with the schema name. If the table or view name is case-sensitive, enclose it in single quotes. You can display all the columns from all the tables and views in a single schema in a single display by using the wildcard character '*'.","title":"Description"},{"location":"reference/interactive_commands/describe/#example","text":"snappy> describe maps; COLUMN_NAME |TYPE_NAME|DEC&|NUM&|COLUM&|COLUMN_DEF|CHAR_OCTE&|IS_NULL& ------------------------------------------------------------------------------ MAP_ID |INTEGER |0 |10 |10 |AUTOINCRE&|NULL |NO MAP_NAME |VARCHAR |NULL|NULL|24 |NULL |48 |NO REGION |VARCHAR |NULL|NULL|26 |NULL |52 |YES AREA |DECIMAL |4 |10 |8 |NULL |NULL |NO PHOTO_FORMAT |VARCHAR |NULL|NULL|26 |NULL |52 |NO PICTURE |BLOB |NULL|NULL|102400|NULL |NULL |YES 6 rows selected snappy>","title":"Example"},{"location":"reference/interactive_commands/disconnect/","text":"disconnect \u00b6 Disconnects from the database. Syntax \u00b6 DISCONNECT [ ALL | CURRENT | ConnectionIdentifier ] Description \u00b6 Disconnects from the database. Specifically, issues a java.sql.Connection.close request against the connection indicated on the command line. There must be a current connection at the time the request is made. If ALL is specified, all known connections are closed and there will be no current connection. Disconnect CURRENT is the same as Disconnect without indicating a connection; the default connection is closed. If a connection name is specified with an identifier, the command disconnects the named connection. The name must be the name of a connection in the current session provided with the Connect command. If the Connect command without the AS clause was used, you can supply the name the system generated for the connection. If the current connection is the named connection when the command completes, there will be no current connection and you must issue a Connect command. Example \u00b6 snappy> DISCONNECT CONNECTION1;","title":"disconnect"},{"location":"reference/interactive_commands/disconnect/#disconnect","text":"Disconnects from the database.","title":"disconnect"},{"location":"reference/interactive_commands/disconnect/#syntax","text":"DISCONNECT [ ALL | CURRENT | ConnectionIdentifier ]","title":"Syntax"},{"location":"reference/interactive_commands/disconnect/#description","text":"Disconnects from the database. Specifically, issues a java.sql.Connection.close request against the connection indicated on the command line. There must be a current connection at the time the request is made. If ALL is specified, all known connections are closed and there will be no current connection. Disconnect CURRENT is the same as Disconnect without indicating a connection; the default connection is closed. If a connection name is specified with an identifier, the command disconnects the named connection. The name must be the name of a connection in the current session provided with the Connect command. If the Connect command without the AS clause was used, you can supply the name the system generated for the connection. If the current connection is the named connection when the command completes, there will be no current connection and you must issue a Connect command.","title":"Description"},{"location":"reference/interactive_commands/disconnect/#example","text":"snappy> DISCONNECT CONNECTION1;","title":"Example"},{"location":"reference/interactive_commands/elapsedtime/","text":"elapsedtime \u00b6 Displays the total time elapsed during statement execution. Syntax \u00b6 ELAPSEDTIME { ON | OFF } Description \u00b6 When elapsedtime is turned on, snappy displays the total time elapsed during statement execution. The default value is OFF. Example \u00b6 snappy> elapsedtime on; snappy> select * from airlines; A&|AIRLINE_FULL |BASIC_RATE |DISTANCE_DISCOUNT |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT&|ECONOMY_SE&|BUSINESS_S&|FIRSTCLASS& ----------------------------------------------------------------------------------------------------------------------------------------------------------- NA|New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 US|Union Standard Airlines |0.19 |0.05 |0.4 |1.6 |20 |10 |5 AA|Amazonian Airways |0.18 |0.03 |0.5 |1.5 |20 |10 |5 3 rows selected ELAPSED TIME = 2 milliseconds","title":"elapsedtime"},{"location":"reference/interactive_commands/elapsedtime/#elapsedtime","text":"Displays the total time elapsed during statement execution.","title":"elapsedtime"},{"location":"reference/interactive_commands/elapsedtime/#syntax","text":"ELAPSEDTIME { ON | OFF }","title":"Syntax"},{"location":"reference/interactive_commands/elapsedtime/#description","text":"When elapsedtime is turned on, snappy displays the total time elapsed during statement execution. The default value is OFF.","title":"Description"},{"location":"reference/interactive_commands/elapsedtime/#example","text":"snappy> elapsedtime on; snappy> select * from airlines; A&|AIRLINE_FULL |BASIC_RATE |DISTANCE_DISCOUNT |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT&|ECONOMY_SE&|BUSINESS_S&|FIRSTCLASS& ----------------------------------------------------------------------------------------------------------------------------------------------------------- NA|New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 US|Union Standard Airlines |0.19 |0.05 |0.4 |1.6 |20 |10 |5 AA|Amazonian Airways |0.18 |0.03 |0.5 |1.5 |20 |10 |5 3 rows selected ELAPSED TIME = 2 milliseconds","title":"Example"},{"location":"reference/interactive_commands/exit/","text":"exit \u00b6 Completes the snappy application and halts processing. Syntax \u00b6 EXIT Description \u00b6 Causes the snappy application to complete and processing to halt. Issuing this command from within a file started with the Run command or on the command line causes the outermost input loop to halt. snappy exits when the Exit command is entered or if given a command file on the Java invocation line, when the end of the command file is reached. Example \u00b6 snappy> DISCONNECT CONNECTION1; snappy> EXIT;","title":"exit"},{"location":"reference/interactive_commands/exit/#exit","text":"Completes the snappy application and halts processing.","title":"exit"},{"location":"reference/interactive_commands/exit/#syntax","text":"EXIT","title":"Syntax"},{"location":"reference/interactive_commands/exit/#description","text":"Causes the snappy application to complete and processing to halt. Issuing this command from within a file started with the Run command or on the command line causes the outermost input loop to halt. snappy exits when the Exit command is entered or if given a command file on the Java invocation line, when the end of the command file is reached.","title":"Description"},{"location":"reference/interactive_commands/exit/#example","text":"snappy> DISCONNECT CONNECTION1; snappy> EXIT;","title":"Example"},{"location":"reference/interactive_commands/maximumdisplaywidth/","text":"MaximumDisplayWidth \u00b6 Sets the largest display width for columns to the specified value. Syntax \u00b6 MAXIMUMDISPLAYWIDTH integer_value Description \u00b6 Sets the largest display width for columns to the specified value. You generally use this command to increase the default value in order to display large blocks of text. Example \u00b6 snappy> insert into airlineref values('A-1', 'NOW IS THE TIME'); 1 row inserted/updated/deleted snappy> maximumdisplaywidth 4; snappy> select * from AIRLINEREF where code='A-1'; CODE|DES& --------- A-1 |NOW& 1 row selected snappy> maximumdisplaywidth 30; snappy> select * from AIRLINEREF where code='A-1'; CODE |DESCRIPTION ------------------------------------ A-1 |NOW IS THE TIME 1 row selected","title":"MaximumDisplayWidth"},{"location":"reference/interactive_commands/maximumdisplaywidth/#maximumdisplaywidth","text":"Sets the largest display width for columns to the specified value.","title":"MaximumDisplayWidth"},{"location":"reference/interactive_commands/maximumdisplaywidth/#syntax","text":"MAXIMUMDISPLAYWIDTH integer_value","title":"Syntax"},{"location":"reference/interactive_commands/maximumdisplaywidth/#description","text":"Sets the largest display width for columns to the specified value. You generally use this command to increase the default value in order to display large blocks of text.","title":"Description"},{"location":"reference/interactive_commands/maximumdisplaywidth/#example","text":"snappy> insert into airlineref values('A-1', 'NOW IS THE TIME'); 1 row inserted/updated/deleted snappy> maximumdisplaywidth 4; snappy> select * from AIRLINEREF where code='A-1'; CODE|DES& --------- A-1 |NOW& 1 row selected snappy> maximumdisplaywidth 30; snappy> select * from AIRLINEREF where code='A-1'; CODE |DESCRIPTION ------------------------------------ A-1 |NOW IS THE TIME 1 row selected","title":"Example"},{"location":"reference/interactive_commands/rollback/","text":"rollback \u00b6 Issues a java.sql.Connection.rollback request. Syntax \u00b6 ROLLBACK Description \u00b6 Issues a java.sql.Connection.rollback request. Use only if auto-commit is off. A java.sql.Connection.rollback request undoes the currently active transaction and initiates a new transaction. Example \u00b6 snappy> SET ISOLATION read committed; 0 rows inserted/updated/deleted snappy> VALUES CURRENT ISOLATION; 1 ---- CS 1 row selected snappy> AUTOCOMMIT off; snappy> insert into airlines VALUES ('AN', 'Another New Airline', 0.20, 0.07, 0.6, 1.7, 20, 10, 5); 1 row inserted/updated/deleted snappy> select * from airlines; A&|AIRLINE_FULL |BASIC_RATE |DISTANCE_DISCOUNT |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT&|ECONOMY_SE&|BUSINESS_S&|FIRSTCLASS& ----------------------------------------------------------------------------------------------------------------------------------------------------------- NA|New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 US|Union Standard Airlines |0.19 |0.05 |0.4 |1.6 |20 |10 |5 AA|Amazonian Airways |0.18 |0.03 |0.5 |1.5 |20 |10 |5 AN|Another New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 4 rows selected snappy> ROLLBACK; snappy> select * from airlines; A&|AIRLINE_FULL |BASIC_RATE |DISTANCE_DISCOUNT |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT&|ECONOMY_SE&|BUSINESS_S&|FIRSTCLASS& ----------------------------------------------------------------------------------------------------------------------------------------------------------- NA|New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 US|Union Standard Airlines |0.19 |0.05 |0.4 |1.6 |20 |10 |5 AA|Amazonian Airways |0.18 |0.03 |0.5 |1.5 |20 |10 |5 3 rows selected","title":"rollback"},{"location":"reference/interactive_commands/rollback/#rollback","text":"Issues a java.sql.Connection.rollback request.","title":"rollback"},{"location":"reference/interactive_commands/rollback/#syntax","text":"ROLLBACK","title":"Syntax"},{"location":"reference/interactive_commands/rollback/#description","text":"Issues a java.sql.Connection.rollback request. Use only if auto-commit is off. A java.sql.Connection.rollback request undoes the currently active transaction and initiates a new transaction.","title":"Description"},{"location":"reference/interactive_commands/rollback/#example","text":"snappy> SET ISOLATION read committed; 0 rows inserted/updated/deleted snappy> VALUES CURRENT ISOLATION; 1 ---- CS 1 row selected snappy> AUTOCOMMIT off; snappy> insert into airlines VALUES ('AN', 'Another New Airline', 0.20, 0.07, 0.6, 1.7, 20, 10, 5); 1 row inserted/updated/deleted snappy> select * from airlines; A&|AIRLINE_FULL |BASIC_RATE |DISTANCE_DISCOUNT |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT&|ECONOMY_SE&|BUSINESS_S&|FIRSTCLASS& ----------------------------------------------------------------------------------------------------------------------------------------------------------- NA|New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 US|Union Standard Airlines |0.19 |0.05 |0.4 |1.6 |20 |10 |5 AA|Amazonian Airways |0.18 |0.03 |0.5 |1.5 |20 |10 |5 AN|Another New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 4 rows selected snappy> ROLLBACK; snappy> select * from airlines; A&|AIRLINE_FULL |BASIC_RATE |DISTANCE_DISCOUNT |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT&|ECONOMY_SE&|BUSINESS_S&|FIRSTCLASS& ----------------------------------------------------------------------------------------------------------------------------------------------------------- NA|New Airline |0.2 |0.07 |0.6 |1.7 |20 |10 |5 US|Union Standard Airlines |0.19 |0.05 |0.4 |1.6 |20 |10 |5 AA|Amazonian Airways |0.18 |0.03 |0.5 |1.5 |20 |10 |5 3 rows selected","title":"Example"},{"location":"reference/interactive_commands/run/","text":"run \u00b6 Treats the value of the string as a valid file name, and redirects snappy processing to read from that file until it ends or an exit command is executed. Syntax \u00b6 RUN String Description \u00b6 Treats the value of the string as a valid file name, and redirects snappy processing to read from that file until it ends or an Exit command is executed. If the end of the file is reached without snappy exiting, reading continues from the previous input source once the end of the file is reached. Files can contain Run commands. snappy prints out the statements in the file as it executes them. Any changes made to the snappy environment by the file are visible in the environment when processing resumes. Example \u00b6 snappy> run 'ToursDB_schema.sql';","title":"run"},{"location":"reference/interactive_commands/run/#run","text":"Treats the value of the string as a valid file name, and redirects snappy processing to read from that file until it ends or an exit command is executed.","title":"run"},{"location":"reference/interactive_commands/run/#syntax","text":"RUN String","title":"Syntax"},{"location":"reference/interactive_commands/run/#description","text":"Treats the value of the string as a valid file name, and redirects snappy processing to read from that file until it ends or an Exit command is executed. If the end of the file is reached without snappy exiting, reading continues from the previous input source once the end of the file is reached. Files can contain Run commands. snappy prints out the statements in the file as it executes them. Any changes made to the snappy environment by the file are visible in the environment when processing resumes.","title":"Description"},{"location":"reference/interactive_commands/run/#example","text":"snappy> run 'ToursDB_schema.sql';","title":"Example"},{"location":"reference/interactive_commands/set_connection/","text":"set connection \u00b6 Specifies which connection to make current when more than one connection is open. Syntax \u00b6 SET CONNECTION Identifier; Description \u00b6 Allows you to specify which connection to make current when you have more than one connection open. Use the Show Connections command to display open connections. If there is no such connection, an error results and the current connection is unchanged. Example \u00b6 snappy(CONNECTION0)> set connection CONNECTION1; snappy(CONNECTION1)> show connections; CONNECTION0 - jdbc:snappydata:thrift://127.0.0.1[1527] CONNECTION1* - jdbc:snappydata:thrift://127.0.0.1[1527] * = current connection","title":"set connection"},{"location":"reference/interactive_commands/set_connection/#set-connection","text":"Specifies which connection to make current when more than one connection is open.","title":"set connection"},{"location":"reference/interactive_commands/set_connection/#syntax","text":"SET CONNECTION Identifier;","title":"Syntax"},{"location":"reference/interactive_commands/set_connection/#description","text":"Allows you to specify which connection to make current when you have more than one connection open. Use the Show Connections command to display open connections. If there is no such connection, an error results and the current connection is unchanged.","title":"Description"},{"location":"reference/interactive_commands/set_connection/#example","text":"snappy(CONNECTION0)> set connection CONNECTION1; snappy(CONNECTION1)> show connections; CONNECTION0 - jdbc:snappydata:thrift://127.0.0.1[1527] CONNECTION1* - jdbc:snappydata:thrift://127.0.0.1[1527] * = current connection","title":"Example"},{"location":"reference/interactive_commands/show/","text":"SHOW \u00b6 Displays information about active connections and database objects. Syntax \u00b6 SHOW { CONNECTIONS | FUNCTIONS | IMPORTEDKEYS [ IN schemaName | FROM table-Name ] | INDEXES [ IN schemaName | FROM table-Name ] | PROCEDURES [ IN schemaName ] | SCHEMAS | TABLES [ IN schemaName ] | VIEWS [ IN schemaName ] | } The following are covered in this section: SHOW CONNECTIONS SHOW FUNCTIONS SHOW IMPORTEDKEYS SHOW INDEXES SHOW PROCEDURES SHOW SCHEMAS SHOW TABLES SHOW VIEWS Description \u00b6 Displays information about active connections and database objects. **SHOW CONNECTIONS** If there are no connections, the SHOW CONNECTIONS command returns \"No connections available\". Otherwise, the command displays a list of connection names and the URLs used to connect to them. The currently active connection is marked with an * after its name. Example snappy> show connections; CONNECTION0* - jdbc:snappydata:thrift://127.0.0.1[1527] * = current connection **SHOW FUNCTIONS** Displays the details of the default system functions. Currently, UDF functions are not displayed in the list. This will be available in the future releases. Example snappy> show functions; FUNCTION_SCHEM |FUNCTION_NAME |REMARKS ------------------------------------------------------------------------ SNAPPY_HIVE_METASTO&|NUCLEUS_ASCII |org.datanucleus.sto& SNAPPY_HIVE_METASTO&|NUCLEUS_MATCHES |org.datanucleus.sto& SYS |CHECK_TABLE_EX |com.pivotal.gemfire& SYS |GET_CRITICAL_HEAP_PERCENTAGE |com.pivotal.gemfire& SYS |GET_CRITICAL_OFFHEAP_PERCENTA&|com.pivotal.gemfire& SYS |GET_EVICTION_HEAP_PERCENTAGE |com.pivotal.gemfire& SYS |GET_EVICTION_OFFHEAP_PERCENTA&|com.pivotal.gemfire& SYS |GET_IS_NATIVE_NANOTIMER |com.pivotal.gemfire& SYS |GET_NATIVE_NANOTIMER_TYPE |com.pivotal.gemfire& SYS |GET_TABLE_VERSION |com.pivotal.gemfire& SYS |HDFS_LAST_MAJOR_COMPACTION |com.pivotal.gemfire& SYSCS_UTIL |CHECK_TABLE |com.pivotal.gemfire& SYSCS_UTIL |GET_DATABASE_PROPERTY |com.pivotal.gemfire& SYSCS_UTIL |GET_EXPLAIN_CONNECTION |com.pivotal.gemfire& SYSCS_UTIL |GET_RUNTIMESTATISTICS |com.pivotal.gemfire& SYSCS_UTIL |GET_USER_ACCESS |com.pivotal.gemfire& SYSIBM |BLOBCREATELOCATOR |com.pivotal.gemfire& SYSIBM |BLOBGETBYTES |com.pivotal.gemfire& SYSIBM |BLOBGETLENGTH |com.pivotal.gemfire& SYSIBM |BLOBGETPOSITIONFROMBYTES |com.pivotal.gemfire& SYSIBM |BLOBGETPOSITIONFROMLOCATOR |com.pivotal.gemfire& SYSIBM |CLOBCREATELOCATOR |com.pivotal.gemfire& SYSIBM |CLOBGETLENGTH |com.pivotal.gemfire& SYSIBM |CLOBGETPOSITIONFROMLOCATOR |com.pivotal.gemfire& SYSIBM |CLOBGETPOSITIONFROMSTRING |com.pivotal.gemfire& SYSIBM |CLOBGETSUBSTRING |com.pivotal.gemfire& **SHOW IMPORTEDKEYS** Displays all foreign keys in the specified schema or table. If you omit the schema and table clauses, SnappyData displays all foreign keys for all tables in the current schema. Example snappy> show importedkeys in app; PKTABLE_NAME |PKCOLUMN_NAME |PK_NAME |FKTABLE_SCHEM |FKTABLE_NAME |FKCOLUMN_NAME |FK_NAME |KEY_SEQ -------------------------------------------------------------------------------------------------------------------------------- CUSTOMERS |CID |SQL180328162510710 |TRADE |BUYORDERS |CID |BO_CUST_FK |1 SECURITIES |SEC_ID |SEC_PK |TRADE |BUYORDERS |SID |BO_SEC_FK |1 CUSTOMERS |CID |SQL180328162510710 |TRADE |NETWORTH |CID |CUST_NEWT_FK |1 PORTFOLIO |CID |PORTF_PK |TRADE |SELLORDERS |CID |PORTF_FK |1 PORTFOLIO |SID |PORTF_PK |TRADE |SELLORDERS |SID |PORTF_FK |2 CUSTOMERS |CID |SQL180328162510710 |TRADE |PORTFOLIO |CID |CUST_FK |1 SECURITIES |SEC_ID |SEC_PK |TRADE |PORTFOLIO |SID |SEC_FK |1 EMPLOYEES |EID |EMPLOYEES_PK |TRADE |TRADES |EID |EMP_FK |1 CUSTOMERS |CID |SQL180328162510710 |TRADE |TRADES |CID |SQL18032816254&|1 snappy> show importedkeys from app.BUYORDERS; PKTABLE_NAME |PKCOLUMN_NAME |PK_NAME |FKTABLE_SCHEM |FKTABLE_NAME |FKCOLUMN_NAME |FK_NAME |KEY_SEQ ---------------------------------------------------------------------------------------------------------------- CUSTOMERS |CID |SQL180328162510710 |TRADE |BUYORDERS |CID |BO_CUST_FK |1 SECURITIES |SEC_ID |SEC_PK |TRADE |BUYORDERS |SID |BO_SEC_FK |1 2 rows selected **SHOW INDEXES** Displays all the indexes in the database. If IN schemaName is specified, only the indexes in the specified schema are displayed. If FROM table-Name is specified, only the indexes on the specified table are displayed. Example snappy> show indexes in app; TABLE_NAME |INDEX_NAME|COLUMN_NAME |NON_U&|TYPE|ASC&|CARDINA&|PAGES ------------------------------------------------------------------------------------ AIRLINES |AR_IDX1 |AIRLINE |false |3 |A |NULL |NULL CITIES |CT_IDX1 |CITY_ID |false |3 |A |NULL |NULL CITIES |CT_IDX2 |COUNTRY_ISO_CODE |true |3 |A |NULL |NULL COUNTRIES |CR_IDX1 |COUNTRY_ISO_CODE |false |3 |A |NULL |NULL COUNTRIES |CR_IDX1 |COUNTRY |false |3 |A |NULL |NULL FLIGHTAVAILABILITY |FA_IDX1 |FLIGHT_ID |false |3 |A |NULL |NULL FLIGHTAVAILABILITY |FA_IDX1 |SEGMENT_NUMBER |false |3 |A |NULL |NULL FLIGHTAVAILABILITY |FA_IDX1 |FLIGHT_DATE |false |3 |A |NULL |NULL FLIGHTAVAILABILITY |FA_IDX2 |FLIGHT_ID |true |3 |A |NULL |NULL FLIGHTAVAILABILITY |FA_IDX2 |SEGMENT_NUMBER |true |3 |A |NULL |NULL FLIGHTS |FL_IDX1 |FLIGHT_ID |false |3 |A |NULL |NULL FLIGHTS |FL_IDX1 |SEGMENT_NUMBER |false |3 |A |NULL |NULL FLIGHTS |FL_IDX2 |DEST_AIRPORT |true |3 |A |NULL |NULL FLIGHTS |FL_IDX2 |ORIG_AIRPORT |true |3 |A |NULL |NULL MAPS |M_IDX1 |MAP_ID |false |3 |A |NULL |NULL MAPS |M_IDX2 |MAP_NAME |false |3 |A |NULL |NULL 16 rows selected snappy> show indexes from flights; TABLE_NAME |INDEX_NAME|COLUMN_NAME |NON_U&|TYPE|ASC&|CARDINA&|PAGES ------------------------------------------------------------------------------------ FLIGHTS |FL_IDX1 |FLIGHT_ID |false |3 |A |NULL |NULL FLIGHTS |FL_IDX1 |SEGMENT_NUMBER |false |3 |A |NULL |NULL FLIGHTS |FL_IDX2 |DEST_AIRPORT |true |3 |A |NULL |NULL FLIGHTS |FL_IDX2 |ORIG_AIRPORT |true |3 |A |NULL |NULL 4 rows selected **SHOW PROCEDURES** SHOW PROCEDURES displays all the procedures in the database that have been created with the CREATE PROCEDURE statement, as well as system procedures. If IN schemaName is specified, only procedures in the specified schema are displayed. Example snappy> show procedures in syscs_util; PROCEDURE_SCHEM |PROCEDURE_NAME |REMARKS ------------------------------------------------------------------------ SYSCS_UTIL |BACKUP_DATABASE |com.pivotal.gemfire& SYSCS_UTIL |BACKUP_DATABASE_AND_ENABLE_LO&|com.pivotal.gemfire& SYSCS_UTIL |BACKUP_DATABASE_AND_ENABLE_LO&|com.pivotal.gemfire& SYSCS_UTIL |BACKUP_DATABASE_NOWAIT |com.pivotal.gemfire& SYSCS_UTIL |BULK_INSERT |com.pivotal.gemfire& SYSCS_UTIL |CHECKPOINT_DATABASE |com.pivotal.gemfire& SYSCS_UTIL |COMPRESS_TABLE |com.pivotal.gemfire& SYSCS_UTIL |DISABLE_LOG_ARCHIVE_MODE |com.pivotal.gemfire& SYSCS_UTIL |EMPTY_STATEMENT_CACHE |com.pivotal.gemfire& SYSCS_UTIL |EXPORT_QUERY |com.pivotal.gemfire& SYSCS_UTIL |EXPORT_QUERY_LOBS_TO_EXTFILE |com.pivotal.gemfire& SYSCS_UTIL |EXPORT_TABLE |com.pivotal.gemfire& SYSCS_UTIL |EXPORT_TABLE_LOBS_TO_EXTFILE |com.pivotal.gemfire& SYSCS_UTIL |FREEZE_DATABASE |com.pivotal.gemfire& SYSCS_UTIL |IMPORT_DATA |com.pivotal.gemfire& SYSCS_UTIL |IMPORT_DATA_EX |com.pivotal.gemfire& SYSCS_UTIL |IMPORT_DATA_LOBS_FROM_EXTFILE |com.pivotal.gemfire& SYSCS_UTIL |IMPORT_TABLE |com.pivotal.gemfire& SYSCS_UTIL |IMPORT_TABLE_EX |com.pivotal.gemfire& SYSCS_UTIL |IMPORT_TABLE_LOBS_FROM_EXTFILE|com.pivotal.gemfire& SYSCS_UTIL |INPLACE_COMPRESS_TABLE |com.pivotal.gemfire& SYSCS_UTIL |RELOAD_SECURITY_POLICY |com.pivotal.gemfire& SYSCS_UTIL |SET_DATABASE_PROPERTY |com.pivotal.gemfire& SYSCS_UTIL |SET_EXPLAIN_CONNECTION |com.pivotal.gemfire& SYSCS_UTIL |SET_RUNTIMESTATISTICS |com.pivotal.gemfire& SYSCS_UTIL |SET_STATEMENT_STATISTICS |com.pivotal.gemfire& SYSCS_UTIL |SET_STATISTICS_TIMING |com.pivotal.gemfire& SYSCS_UTIL |SET_USER_ACCESS |com.pivotal.gemfire& SYSCS_UTIL |UNFREEZE_DATABASE |com.pivotal.gemfire& 29 rows selected **SHOW SCHEMAS** SHOW SCHEMAS displays all of the schemas in the current connection. Example snappy> create schema sample; snappy> show schemas; TABLE_SCHEM ------------------------------ APP NULLID SAMPLE SQLJ SYS SYSCAT SYSCS_DIAG SYSCS_UTIL SYSFUN SYSIBM SYSPROC SYSSTAT 12 rows selected **SHOW TABLES** SHOW TABLES displays all of the tables in the current schema. If IN schemaName is specified, the tables in the given schema are displayed. Example snappy> show tables in app; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE |REMARKS ------------------------------------------------------------------------ APP |AIRLINES |COLUMN TABLE | APP |CITIES |COLUMN TABLE | APP |COUNTRIES |COLUMN TABLE | APP |FLIGHTAVAILABILITY |EXTERNAL TABLE | APP |FLIGHTS |EXTERNAL TABLE | APP |FLIGHTS_HISTORY |ROW TABLE | APP |MAPS |ROW TABLE | 7 rows selected snappy> **SHOW VIEWS** SHOW VIEWS displays all of the views in the current schema. If IN schemaName is specified, the views in the given schema are displayed. Example snappy> create view v1 as select * from maps; snappy> show views; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE|REMARKS ----------------------------------------------------------------------------------- APP |V1 |VIEW | 1 row selected snappy> show views in APP; TABLE_SCHEM |TABLE_NAME |TABLE_TYPE|REMARKS ----------------------------------------------------------------------------------- APP |V1 |VIEW | APP |V2 |VIEW | 2 rows selected Note SHOW VIEWS do not display the temporary and global temporary views. Related Topics SET CONNECTION CREATE FUNCTION CREATE IMPORTEDKEY CREATE INDEXES CREATE PROCEDURE CREATE SCHEMA CREATE TABLE CREATE VIEW","title":"show"},{"location":"reference/interactive_commands/show/#show","text":"Displays information about active connections and database objects.","title":"SHOW"},{"location":"reference/interactive_commands/show/#syntax","text":"SHOW { CONNECTIONS | FUNCTIONS | IMPORTEDKEYS [ IN schemaName | FROM table-Name ] | INDEXES [ IN schemaName | FROM table-Name ] | PROCEDURES [ IN schemaName ] | SCHEMAS | TABLES [ IN schemaName ] | VIEWS [ IN schemaName ] | } The following are covered in this section: SHOW CONNECTIONS SHOW FUNCTIONS SHOW IMPORTEDKEYS SHOW INDEXES SHOW PROCEDURES SHOW SCHEMAS SHOW TABLES SHOW VIEWS","title":"Syntax"},{"location":"reference/interactive_commands/show/#description","text":"Displays information about active connections and database objects.","title":"Description"},{"location":"reference/misc/del_supported_datatypes/","text":"Data Types \u00b6 The SQL type system determines the compile-time and runtime type of an expression. Each type has a certain range of permissible values that can be assigned to a column or value of that type. The special value NULL, denotes an unassigned or missing value of any of the types (columns that have been assigned as non-nullable using NOT NULL clause or the primary key columns cannot have a NULL value). The supported types are given below: Data Types Supported for Column and Row Tables Data Type Description BIGINT Provides 8-byte integer for long integer values BINARY Binary-encoded strings BLOB Carying-length binary string that can be up to 2,147,483,647 characters long BOOLEAN Logical Boolean values (true/false) BYTE Binary data (\"byte array\") CLOB Text data in random-access chunks DATE Calendar dates (year, month, day) DECIMAL DECIMAL(p) floating point and DECIMAL(p,s) fixed point DOUBLE A double-precision floating point value FLOAT Stores floating point value: that is a number with a fractional part INT Stores integer: a whole number INTEGER Stores signed four-byte integer LONG Stores character strings with a maximum length of 32,700 characters NUMERIC Stores exact numeric of selectable precision REAL Stores single precision floating-point number (4 bytes) SHORT The size of the short type is 2 bytes (16 bits) SMALLINT Stores signed two-byte integer STRING Stores are sequences of characters TIMESTAMP Stores date and time as a combined value TINYINT Stores a very small integer. The signed range is -128 to 127. VARBINARY Stores binary byte strings rather than non-binary character strings VARCHAR Stores character strings of varying length (up to 255 bytes); collation is in code-set order BIGINT \u00b6 Provides 8 bytes storage for long integer values. An attempt to put a BIGINT value into another exact numeric type with smaller size/precision (e.g. INT) fails if the value overflows the maximum allowable by the smaller type. For behavior with other types in expressions, see Numeric type promotion in expressions, Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType BIGINT BINARY \u00b6 Description The BINARY type is similar to the CHAR type, but stores binary byte strings rather than non-binary character strings. It contains no character set, and comparison and sorting are based on the numeric value of the bytes. Field Details public static final DataType BinaryType BLOB \u00b6 Description A binary large object represents an array of raw bytes of varying length. Field Details public static final DataType BLOB BOOLEAN \u00b6 Description Booleans are used to represent true and false values returned by comparison operators and logical functions. The values are, true and false. The display values can be localized. Field Details public static final DataType BOOLEAN BYTE \u00b6 Description The BYTE data type stores any kind of binary data in an undifferentiated byte stream. Binary data typically consists of digitized information, such as spreadsheets, program load modules, digitized voice patterns, and so on. The term simple large object refers to an instance of a TEXT or BYTE data type. No more than 195 columns of the same table can be declared as BYTE and TEXT data types. Field Details public static final DataType ByteType CLOB \u00b6 Description A character large object represents an array of characters of varying length. It is used to store large character-based data such as documents. The length is expressed in number characters, unless you specify the suffix K, M, or G, which uses the multiples of 1024, 1024 1024, or 1024 1024*1024 respectively. Field Details public static final DataType CLOB DATE \u00b6 Description Provides for storage of a date as year-month-day. Supported formats are: yyyy-mm-dd , mm/dd/yyyy , and dd.mm.yyyy The year (yyyy) must always be expressed with four digits, while months (mm) and days (dd) may have either one or two digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Field Details public static final DataType DATE Gets the DateType object. DECIMAL \u00b6 Description Provides an exact decimal value having a specified precision and scale. The precision is the total number of digits both to the left and the right of the decimal point, and the scale is the number of digits in the fraction to the right of the decimal point. A numeric value (e.g. INT, BIGINT, SMALLINT) can be put into a DECIMAL as long as non-fractional precision is not lost else a range exception is thrown (SQLState: \"22003\"). When truncating trailing digits from a DECIMAL, the value is rounded down. For behavior with other types in expressions, see Numeric type promotion in expressions, Scale for decimal arithmetic and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType DECIMAL DOUBLE \u00b6 Description Provides 8-byte storage for numbers using IEEE floating-point notation. Arithmetic operations do not round their resulting values to zero. If the values are too small, you will receive an exception. Numeric floating point constants are limited to a length of 30 characters. For behavior with other types in expressions, see Numeric type promotion in expressions, and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType DOUBLE Gets the DoubleType object. FLOAT \u00b6 Description Alias for a REAL or DOUBLE data type, depending on the specified precision. The default precision is 53 making it equivalent to DOUBLE. A precision of 23 or less makes FLOAT equivalent to REAL while greater than 23 makes it equivalent to DOUBLE. Field Details public static final DataType FLOAT INT \u00b6 Description The INT data type is a synonym for INTEGER. Field Details public static final DataType INT INTEGER \u00b6 Description Integer values are written as a sequence of digits. It provides 4 bytes storage for integer values. INT can be used as a synonym for INTEGER in CREATE TABLE. For behavior with other types in expressions, see Numeric type promotion in expressions, and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType IntegerType LONG \u00b6 Description The long data type is a 64-bit two's complement integer. The signed long has a minimum value of -263 and a maximum value of 263-1. Use this data type when you need a range of values wider than those provided by int. Field Details public static final DataType LongType NUMERIC \u00b6 Description Synonym for DECIMAL data type. The meta-data differences from DECIMAL are listed below. Otherwise, NUMERIC behaves identically to DECIMAL. Field Details public static final DataType NUMERIC REAL \u00b6 Description Provides 4-byte storage for numbers using IEEE floating-point notation. Arithmetic operations do not round their resulting values to zero. If the values are too small, you will receive an exception. Constants always map to DOUBLE \u2013 use an explicit CAST to convert a constant to REAL. For behavior with other types in expressions, see Numeric type promotion in expressions, Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType REAL SHORT \u00b6 Description The short data type is a 16-bit signed two's complement integer. It has a minimum value of -32,768 and a maximum value of 32,767 (inclusive). As with byte, the same guidelines apply: you can use a short to save memory in large arrays, in situations where the memory savings actually matters. Field Details public static final DataType ShortType SMALLINT \u00b6 Description Provides 2 bytes storage for short integer values. For behavior with other types in expressions, see Numeric type promotion in expressions, and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType SMALLINT STRING \u00b6 The string type is used for storing text strings. A text string is a sequence of characters in the Unicode format with the final zero at the end of it. A string constant can be assigned to a string variable. A string constant is a sequence of Unicode characters enclosed in quotes or double quotes: \"This is a string constant\". To include a double quote (\") into a string, the backslash character () must be put before it. The \\ (backslash character) is used to escape characters. Any special character constants can be written in a string, if the backslash character () is typed before them. Field Details public static final DataType StringType Gets the StringType object. TIMESTAMP \u00b6 Description Provides for storage of both DATE and TIME as a combined value. In addition it allows for fractional seconds having up to six digits. Supported formats are: yyyy-MM-dd hh:mm:ss[.nnnnnn] yyyy-MM-dd-hh.mm.ss[.nnnnnn] The year (yyyy) must always be expressed with four digits. Months (MM), days (dd), and hours (hh) may have one or two digits while minutes (mm) and seconds (ss) must have two digits. Microseconds, if present, may have between one and six digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Field Details public static final DataType TimestampType TINYINT \u00b6 Description A very small integer. The signed range is -128 to 127. The unsigned range is 0 to 255. Field Details public static final TINYINT VARBINARY \u00b6 Description The VARBINARY type is similar to the VARCHAR type, but stores binary byte strings rather than non-binary character strings. It contains no character set, and comparison and sorting are based on the numeric value of the bytes. Field Details public static final DataType VARBINARY VARCHAR \u00b6 Description Provides for variable-length strings with a maximum limit for length. If a string value is longer than the maximum length, then any trailing blanks are trimmed to make the length same as the maximum length, while an exception is raised if characters other than spaces are required to be truncated. When mixing CHARs and VARCHARs in expressions, the shorter value is padded with spaces to the length of longer string. The type of a string constant is CHAR, not VARCHAR. To represent a single quotation mark within a string, use two quotation marks: VALUES 'visiting John's place' The length of VARCHAR is an unsigned integer constant. Field Details public static final DataType VARCHAR","title":"Data Types"},{"location":"reference/misc/del_supported_datatypes/#data-types","text":"The SQL type system determines the compile-time and runtime type of an expression. Each type has a certain range of permissible values that can be assigned to a column or value of that type. The special value NULL, denotes an unassigned or missing value of any of the types (columns that have been assigned as non-nullable using NOT NULL clause or the primary key columns cannot have a NULL value). The supported types are given below: Data Types Supported for Column and Row Tables Data Type Description BIGINT Provides 8-byte integer for long integer values BINARY Binary-encoded strings BLOB Carying-length binary string that can be up to 2,147,483,647 characters long BOOLEAN Logical Boolean values (true/false) BYTE Binary data (\"byte array\") CLOB Text data in random-access chunks DATE Calendar dates (year, month, day) DECIMAL DECIMAL(p) floating point and DECIMAL(p,s) fixed point DOUBLE A double-precision floating point value FLOAT Stores floating point value: that is a number with a fractional part INT Stores integer: a whole number INTEGER Stores signed four-byte integer LONG Stores character strings with a maximum length of 32,700 characters NUMERIC Stores exact numeric of selectable precision REAL Stores single precision floating-point number (4 bytes) SHORT The size of the short type is 2 bytes (16 bits) SMALLINT Stores signed two-byte integer STRING Stores are sequences of characters TIMESTAMP Stores date and time as a combined value TINYINT Stores a very small integer. The signed range is -128 to 127. VARBINARY Stores binary byte strings rather than non-binary character strings VARCHAR Stores character strings of varying length (up to 255 bytes); collation is in code-set order","title":"Data Types"},{"location":"reference/misc/del_supported_datatypes/#bigint","text":"Provides 8 bytes storage for long integer values. An attempt to put a BIGINT value into another exact numeric type with smaller size/precision (e.g. INT) fails if the value overflows the maximum allowable by the smaller type. For behavior with other types in expressions, see Numeric type promotion in expressions, Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType BIGINT","title":"BIGINT"},{"location":"reference/misc/del_supported_datatypes/#binary","text":"Description The BINARY type is similar to the CHAR type, but stores binary byte strings rather than non-binary character strings. It contains no character set, and comparison and sorting are based on the numeric value of the bytes. Field Details public static final DataType BinaryType","title":"BINARY"},{"location":"reference/misc/del_supported_datatypes/#blob","text":"Description A binary large object represents an array of raw bytes of varying length. Field Details public static final DataType BLOB","title":"BLOB"},{"location":"reference/misc/del_supported_datatypes/#boolean","text":"Description Booleans are used to represent true and false values returned by comparison operators and logical functions. The values are, true and false. The display values can be localized. Field Details public static final DataType BOOLEAN","title":"BOOLEAN"},{"location":"reference/misc/del_supported_datatypes/#byte","text":"Description The BYTE data type stores any kind of binary data in an undifferentiated byte stream. Binary data typically consists of digitized information, such as spreadsheets, program load modules, digitized voice patterns, and so on. The term simple large object refers to an instance of a TEXT or BYTE data type. No more than 195 columns of the same table can be declared as BYTE and TEXT data types. Field Details public static final DataType ByteType","title":"BYTE"},{"location":"reference/misc/del_supported_datatypes/#clob","text":"Description A character large object represents an array of characters of varying length. It is used to store large character-based data such as documents. The length is expressed in number characters, unless you specify the suffix K, M, or G, which uses the multiples of 1024, 1024 1024, or 1024 1024*1024 respectively. Field Details public static final DataType CLOB","title":"CLOB"},{"location":"reference/misc/del_supported_datatypes/#date","text":"Description Provides for storage of a date as year-month-day. Supported formats are: yyyy-mm-dd , mm/dd/yyyy , and dd.mm.yyyy The year (yyyy) must always be expressed with four digits, while months (mm) and days (dd) may have either one or two digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Field Details public static final DataType DATE Gets the DateType object.","title":"DATE"},{"location":"reference/misc/del_supported_datatypes/#decimal","text":"Description Provides an exact decimal value having a specified precision and scale. The precision is the total number of digits both to the left and the right of the decimal point, and the scale is the number of digits in the fraction to the right of the decimal point. A numeric value (e.g. INT, BIGINT, SMALLINT) can be put into a DECIMAL as long as non-fractional precision is not lost else a range exception is thrown (SQLState: \"22003\"). When truncating trailing digits from a DECIMAL, the value is rounded down. For behavior with other types in expressions, see Numeric type promotion in expressions, Scale for decimal arithmetic and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType DECIMAL","title":"DECIMAL"},{"location":"reference/misc/del_supported_datatypes/#double","text":"Description Provides 8-byte storage for numbers using IEEE floating-point notation. Arithmetic operations do not round their resulting values to zero. If the values are too small, you will receive an exception. Numeric floating point constants are limited to a length of 30 characters. For behavior with other types in expressions, see Numeric type promotion in expressions, and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType DOUBLE Gets the DoubleType object.","title":"DOUBLE"},{"location":"reference/misc/del_supported_datatypes/#float","text":"Description Alias for a REAL or DOUBLE data type, depending on the specified precision. The default precision is 53 making it equivalent to DOUBLE. A precision of 23 or less makes FLOAT equivalent to REAL while greater than 23 makes it equivalent to DOUBLE. Field Details public static final DataType FLOAT","title":"FLOAT"},{"location":"reference/misc/del_supported_datatypes/#int","text":"Description The INT data type is a synonym for INTEGER. Field Details public static final DataType INT","title":"INT"},{"location":"reference/misc/del_supported_datatypes/#integer","text":"Description Integer values are written as a sequence of digits. It provides 4 bytes storage for integer values. INT can be used as a synonym for INTEGER in CREATE TABLE. For behavior with other types in expressions, see Numeric type promotion in expressions, and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType IntegerType","title":"INTEGER"},{"location":"reference/misc/del_supported_datatypes/#long","text":"Description The long data type is a 64-bit two's complement integer. The signed long has a minimum value of -263 and a maximum value of 263-1. Use this data type when you need a range of values wider than those provided by int. Field Details public static final DataType LongType","title":"LONG"},{"location":"reference/misc/del_supported_datatypes/#numeric","text":"Description Synonym for DECIMAL data type. The meta-data differences from DECIMAL are listed below. Otherwise, NUMERIC behaves identically to DECIMAL. Field Details public static final DataType NUMERIC","title":"NUMERIC"},{"location":"reference/misc/del_supported_datatypes/#real","text":"Description Provides 4-byte storage for numbers using IEEE floating-point notation. Arithmetic operations do not round their resulting values to zero. If the values are too small, you will receive an exception. Constants always map to DOUBLE \u2013 use an explicit CAST to convert a constant to REAL. For behavior with other types in expressions, see Numeric type promotion in expressions, Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType REAL","title":"REAL"},{"location":"reference/misc/del_supported_datatypes/#short","text":"Description The short data type is a 16-bit signed two's complement integer. It has a minimum value of -32,768 and a maximum value of 32,767 (inclusive). As with byte, the same guidelines apply: you can use a short to save memory in large arrays, in situations where the memory savings actually matters. Field Details public static final DataType ShortType","title":"SHORT"},{"location":"reference/misc/del_supported_datatypes/#smallint","text":"Description Provides 2 bytes storage for short integer values. For behavior with other types in expressions, see Numeric type promotion in expressions, and Storing values of one numeric data type in columns of another numeric data type. Field Details public static final DataType SMALLINT","title":"SMALLINT"},{"location":"reference/misc/del_supported_datatypes/#string","text":"The string type is used for storing text strings. A text string is a sequence of characters in the Unicode format with the final zero at the end of it. A string constant can be assigned to a string variable. A string constant is a sequence of Unicode characters enclosed in quotes or double quotes: \"This is a string constant\". To include a double quote (\") into a string, the backslash character () must be put before it. The \\ (backslash character) is used to escape characters. Any special character constants can be written in a string, if the backslash character () is typed before them. Field Details public static final DataType StringType Gets the StringType object.","title":"STRING"},{"location":"reference/misc/del_supported_datatypes/#timestamp","text":"Description Provides for storage of both DATE and TIME as a combined value. In addition it allows for fractional seconds having up to six digits. Supported formats are: yyyy-MM-dd hh:mm:ss[.nnnnnn] yyyy-MM-dd-hh.mm.ss[.nnnnnn] The year (yyyy) must always be expressed with four digits. Months (MM), days (dd), and hours (hh) may have one or two digits while minutes (mm) and seconds (ss) must have two digits. Microseconds, if present, may have between one and six digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Field Details public static final DataType TimestampType","title":"TIMESTAMP"},{"location":"reference/misc/del_supported_datatypes/#tinyint","text":"Description A very small integer. The signed range is -128 to 127. The unsigned range is 0 to 255. Field Details public static final TINYINT","title":"TINYINT"},{"location":"reference/misc/del_supported_datatypes/#varbinary","text":"Description The VARBINARY type is similar to the VARCHAR type, but stores binary byte strings rather than non-binary character strings. It contains no character set, and comparison and sorting are based on the numeric value of the bytes. Field Details public static final DataType VARBINARY","title":"VARBINARY"},{"location":"reference/misc/del_supported_datatypes/#varchar","text":"Description Provides for variable-length strings with a maximum limit for length. If a string value is longer than the maximum length, then any trailing blanks are trimmed to make the length same as the maximum length, while an exception is raised if characters other than spaces are required to be truncated. When mixing CHARs and VARCHARs in expressions, the shorter value is padded with spaces to the length of longer string. The type of a string constant is CHAR, not VARCHAR. To represent a single quotation mark within a string, use two quotation marks: VALUES 'visiting John's place' The length of VARCHAR is an unsigned integer constant. Field Details public static final DataType VARCHAR","title":"VARCHAR"},{"location":"reference/misc/passwordless_ssh/","text":"Configuring SSH Login without Password \u00b6 Note Before you begin, ensure that you have not configured SSH login without password. By default, Secure Socket Shell (SSH) requires a password for authentication on a remote server. However, with some changes in the configuration, you can log in to the remote host through the SSH protocol, without having to enter your SSH password multiple times. This is specially helpful when using the cluster start/stop scripts like snappy-start-all.sh to launch the SnappyData cluster spanning multiple hosts. These steps are provided as a guide for setting up passwordless SSH. Check with your system administrator for more details. Check SSH Check if ssh is installed on your Linux-based host(s) using below command. systemctl status sshd Or on systems where systemctl is not available (for example, some versions of Linux Mint), use below command: service ssh status Install and start SSH To install SSH on Ubuntu systems, run apt update && apt install openssh-server On RHEL/CentOS systems, the command is yum -y install openssh-server openssh-clients Then enable and start the SSH service: systemctl enable sshd Or systemctl enable ssh systemctl start sshd Or systemctl start ssh Perform above two steps for all the systems which will be part of the SnappyData cluster. Mac OS X has a built-in SSH client. Generate an RSA key pair Generate an RSA key pair on your local or primary system by running the following command. ssh-keygen -t rsa -f ~/.ssh/id_rsa -N '' This will create two files (a key pair) at ~/.ssh/ path: 1) id_rsa which is the private key and 2) id_rsa.pub - the public key. Copy the Public Key Once the key pair is generated, append the contents of the public key file id_rsa.pub , to the authorized key file ~/.ssh/authorized_keys on all the remote hosts. With this, you can ssh to these remote hosts from your local system, without providing the password. This also enables you to execute cluster start, stop or status scripts from your local system. For the single node setup, you can simply append it by executing cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys on your system. Two-Way Access Optionally, if you want to also do ssh login from remote hosts to your system without providing the password, copy your id_rsa file generated above and place it at ~/.ssh/ on the remote hosts. Make sure you do not already have a id_rsa file present at that location on remote hosts. scp ~/.ssh/id_rsa <remote-host>:~/.ssh/ # You'll be asked for password here. Also, make sure it is not writable for other users. chmod 600 ~/.ssh/id_rsa # On remote host","title":"Configuring SSH Login without Password"},{"location":"reference/misc/passwordless_ssh/#configuring-ssh-login-without-password","text":"Note Before you begin, ensure that you have not configured SSH login without password. By default, Secure Socket Shell (SSH) requires a password for authentication on a remote server. However, with some changes in the configuration, you can log in to the remote host through the SSH protocol, without having to enter your SSH password multiple times. This is specially helpful when using the cluster start/stop scripts like snappy-start-all.sh to launch the SnappyData cluster spanning multiple hosts. These steps are provided as a guide for setting up passwordless SSH. Check with your system administrator for more details. Check SSH Check if ssh is installed on your Linux-based host(s) using below command. systemctl status sshd Or on systems where systemctl is not available (for example, some versions of Linux Mint), use below command: service ssh status Install and start SSH To install SSH on Ubuntu systems, run apt update && apt install openssh-server On RHEL/CentOS systems, the command is yum -y install openssh-server openssh-clients Then enable and start the SSH service: systemctl enable sshd Or systemctl enable ssh systemctl start sshd Or systemctl start ssh Perform above two steps for all the systems which will be part of the SnappyData cluster. Mac OS X has a built-in SSH client. Generate an RSA key pair Generate an RSA key pair on your local or primary system by running the following command. ssh-keygen -t rsa -f ~/.ssh/id_rsa -N '' This will create two files (a key pair) at ~/.ssh/ path: 1) id_rsa which is the private key and 2) id_rsa.pub - the public key. Copy the Public Key Once the key pair is generated, append the contents of the public key file id_rsa.pub , to the authorized key file ~/.ssh/authorized_keys on all the remote hosts. With this, you can ssh to these remote hosts from your local system, without providing the password. This also enables you to execute cluster start, stop or status scripts from your local system. For the single node setup, you can simply append it by executing cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys on your system. Two-Way Access Optionally, if you want to also do ssh login from remote hosts to your system without providing the password, copy your id_rsa file generated above and place it at ~/.ssh/ on the remote hosts. Make sure you do not already have a id_rsa file present at that location on remote hosts. scp ~/.ssh/id_rsa <remote-host>:~/.ssh/ # You'll be asked for password here. Also, make sure it is not writable for other users. chmod 600 ~/.ssh/id_rsa # On remote host","title":"Configuring SSH Login without Password"},{"location":"reference/misc/supported_datatypes/","text":"Data Types \u00b6 The SQL type system determines the compile-time and runtime type of an expression. Each type has a certain range of permissible values that can be assigned to a column or value of that type. The special value NULL, denotes an unassigned or missing value of any of the types (columns that have been assigned as non-nullable using NOT NULL clause or the primary key columns cannot have a NULL value). The supported types are given below. Data Type Supported for Row Tables Supported for Column Tables ARRAY X \u2714 BIGINT \u2714 \u2714 BINARY \u2714 \u2714 BLOB \u2714 \u2714 BOOLEAN \u2714 \u2714 BYTE \u2714 \u2714 CLOB \u2714 \u2714 CHAR \u2714 \u2714 DATE \u2714 \u2714 DECIMAL \u2714 \u2714 DOUBLE \u2714 \u2714 FLOAT \u2714 \u2714 INT \u2714 \u2714 INTEGER \u2714 \u2714 LONG \u2714 \u2714 MAP X \u2714 NUMERIC \u2714 \u2714 REAL \u2714 \u2714 SHORT \u2714 \u2714 SMALLINT \u2714 \u2714 STRING \u2714 \u2714 STRUCT X \u2714 TIMESTAMP \u2714 \u2714 TINYINT \u2714 \u2714 VARCHAR \u2714 \u2714 Attention BINARY, BLOB, CLOB, and FLOAT data types work only if you do not provide the size. ARRAY \u00b6 A column of ARRAY datatype can contain a collection of elements. A column of type Array can store array of Java objects (Object[]), typed arrays, java.util.Collection and scala.collection.Seq. You can use com.pivotal.gemfirexd.snappy.ComplexTypeSerializer class to serialize the array data in order to insert it into column tables. Refer How to store and retrieve complex data types in JDBC programs for a Scala example that shows how to serialize and store an array in a table using JDBC APIs and ComplexTypeSerializer class. Note Supported only for column tables SQL Example # Create a table with column of type of an array of doubles and insert few records CREATE TABLE IF NOT EXISTS Student(rollno Int, name String, marks Array<Double>) USING column; INSERT INTO Student SELECT 1,'John', Array(97.8,85.2,63.9,45.2,75.2,96.5); BIGINT \u00b6 Provides 8 bytes storage for long integer values. An attempt to put a BIGINT value into another exact numeric type with smaller size/precision (e.g. INT) fails if the value overflows the maximum allowable by the smaller type. Equivalent Java type java.lang.Long Minimum value java.lang.Long.MIN_VALUE (-9223372036854775808 ) Maximum value java.lang.Long.MAX_VALUE (9223372036854775807 ) BINARY \u00b6 This is a synonym of BLOB . BLOB \u00b6 A binary large object represents an array of raw bytes of varying length. Equivalent Java type java.lang.Blob Maximum length (also default length) 2 GB - 1 (or 2,147,483,647) { BLOB | BINARY LARGE OBJECT } [ ( length [{ K | M | G }] ) ] The length of the BLOB is expressed in number of bytes by default. The suffixes K, M, and G stand for kilobyte, megabyte and gigabyte, and use the multiples of 1024, 1024*1024, or 1024*1024*1024 respectively. CREATE TABLE blob_data(id INT primary key, data BLOB(10M)); \u2013- search for a blob select length(data) from blob_data where id = 100; BOOLEAN \u00b6 The data type representing Boolean values. This is equivalent to Java's boolean primitive type. BYTE \u00b6 The data type representing Byte values. It is an 8-bit signed integer (equivalent to Java's byte primitive type). Minimum value java.lang.Byte.MIN_VALUE Maximum value java.lang.Byte.MAX_VALUE CHAR \u00b6 Provides for fixed-length strings. If a string value is shorter than the expected length, then spaces are inserted to pad the string to the expected length. If a string value is longer than the expected length, then any trailing blanks are trimmed to make the length same as the expected length, while an exception is raised if characters other than spaces are required to be truncated. For comparison operations, the shorter CHAR string is padded with spaces to the longer value. Similarly when mixing CHARs and VARCHARs in expressions, the shorter value is padded with spaces to the length of the longer string. To represent a single quotation mark within a string, use two quotation marks: VALUES 'going to Chandra's place' The length of CHAR is an unsigned integer constant. Equivalent Java type java.lang.String Maximum length java.lang.Integer.MAX_VALUE (2147483647 ) Default length 1 CHAR[ACTER] [(length)] CLOB \u00b6 A character large object represents an array of characters of varying length. It is used to store large character-based data such as documents. The length is expressed in number characters, unless you specify the suffix K, M, or G, which uses the multiples of 1024, 1024*1024, or 1024*1024*1024 respectively. Equivalent Java type java.sql.Clob Maximum length (also default length) 2 GB - 1 (or 2,147,483,647) { CLOB | CHARACTER LARGE OBJECT } [ ( length [{ K | M | G }] ) ] CREATE TABLE clob_data(id INT primary key, text CLOB(10M)); \u2013- search for a clob select text from clob_data where id = 100; DATE \u00b6 Provides for storage of a date as year-month-day. Supported formats are: yyyy-mm-dd mm/dd/yyyy dd.mm.yyyy The year (yyyy) must always be expressed with four digits, while months (mm) and days (dd) may have either one or two digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Equivalent Java type java.sql.Date VALUES '2010-05-04' VALUES DATE('2001-10-12') The latter example uses the DATE() function described in the section Built-in functions and procedures. DECIMAL \u00b6 Provides an exact decimal value having a specified precision and scale. The precision is the total number of digits both to the left and the right of the decimal point, and the scale is the number of digits in the fraction to the right of the decimal point. A numeric value (e.g. INT, BIGINT, SMALLINT) can be put into a DECIMAL as long as non-fractional precision is not lost else a range exception is thrown (SQLState: \"22003\"). When truncating trailing digits from a DECIMAL, the value is rounded down. Equivalent Java type java.math.BigDecimal Precision min/max 1 to 31 Scale min/max less than or equal to precision Default precision 5 Default scale 0 { DECIMAL | DEC } [(precision [, scale ])] -- this cast loses fractional precision values cast (23.8372 AS decimal(4,1)); -\u2013- results in: 23.8 -- this cast is outside the range values cast (97824 AS decimal(4,1)); \u2013-- throws exception: ERROR 22003: The resulting value is outside the range for the data type DECIMAL/NUMERIC(4,1). DOUBLE \u00b6 Provides 8-byte storage for numbers using IEEE floating-point notation. Arithmetic operations do not round their resulting values to zero. If the values are too small, you will receive an exception. Numeric floating point constants are limited to a length of 30 characters. Equivalent Java type java.lang.Double Note: The maximum/minimum limits are different from those of java.lang.Double as noted below. Minimum value -1.79769E+30 Maximum value 1.79769E+308 Smallest positive value 2.225E-307 Largest negative value -2.225E-307 Default precision 5 Default scale 0 \u2013- examples of valid values values 233.31E3; values 8928E+06; -- this example will throw a range exception (SQLState: \"42820\") values 123456789012345678901234567890123456789e0; FLOAT \u00b6 Alias for a REAL or DOUBLE data type, depending on the specified precision. The default precision is 53 making it equivalent to DOUBLE. A precision of 23 or less makes FLOAT equivalent to REAL while greater than 23 makes it equivalent to DOUBLE. Equivalent Java type java.lang.Double or java.lang.Float depending on precision Minumum/Maximum limits Same as those for FLOAT if the precision is less than 23. Otherwise, same minimum/maximum limits as those for DOUBLE. Default precision 53 FLOAT [(precision)] INT \u00b6 This is a synonym of INTEGER . INTEGER (INT) \u00b6 Provides 4 bytes storage for integer values. INT can be used as a synonym for INTEGER in CREATE TABLE. Equivalent Java type java.lang.Integer Minimum value java.lang.Integer.MIN_VALUE (-2147483648) Maximum value java.lang.Integer.MAX_VALUE (21474836487) LONG \u00b6 The data type representing Long values. It's a 64-bit signed integer (equivalent to Java's long primitive type). Minimum value java.lang.Long.MIN_VALUE Maximum value java.lang.Long.MAX_VALUE MAP \u00b6 A column of MAP datatype can contain a collection of key-value pairs. SQL Examples # Create a table with column of type MAP and insert few records CREATE TABLE IF NOT EXISTS StudentGrades (rollno Integer, name String, Course Map<String, String>) USING column; INSERT INTO StudentGrades SELECT 1,'Jim', Map('English', 'A+'); INSERT INTO StudentGrades SELECT 2,'John', Map('English', 'A', 'Science', 'B'); # Selecting grades for 'English' snappy> select ROLLNO, NAME, course['English'] from StudentGrades; ROLLNO |NAME |COURSE[English] --------------------------- 2 |John |A 1 |Jim |A+ A column of type Map can store java.util.Map or scala.collection.Map . You can use com.pivotal.gemfirexd.snappy.ComplexTypeSerializer class to serialize the map data in order to insert it into column tables. Refer How to store and retrieve complex data types in JDBC programs for a Scala example that shows how to serialize and store an array in a table using JDBC APIs and ComplexTypeSerializer class. Map data can also be stored in a similar way. Note Supported only for column tables NUMERIC \u00b6 Synonym for the DECIMAL data type. NUMERIC [(precision [, scale ])] REAL \u00b6 Provides a 4-byte storage for numbers using IEEE floating-point notation. Equivalent Java type java.lang.Float Minimum value -3.402E+38f Maximum value +3.402E+38f Smallest positive value +1.175E-37f Largest negative value -1.175E-37f SHORT \u00b6 This is a synonym for SMALLINT . SMALLINT (TINYINT) (SHORT) \u00b6 Provides 2 bytes storage for short integer values. Equivalent Java type java.lang.Short Minimum value java.lang.Short.MIN_VALUE (-32768 ) Maximum value java.lang.Short.MAX_VALUE (32767) STRING \u00b6 The data type representing String values. A String encoded in UTF-8 as an Array[Byte], which can be used for comparison search. STRUCT \u00b6 A column of struct datatype can contain a structure with different fields. SQL Examples # Create a table with column of type STRUCT and insert few records. CREATE TABLE IF NOT EXISTS StocksInfo (SYMBOL STRING, INFO STRUCT<TRADING_YEAR: STRING, AVG_DAILY_VOLUME: LONG, HIGHEST_PRICE_IN_YEAR: INT, LOWEST_PRICE_IN_YEAR: INT>) USING COLUMN; INSERT INTO StocksInfo SELECT 'ORD', STRUCT('2018', '400000', '112', '52'); INSERT INTO StocksInfo SELECT 'MSGU', Struct('2018', '500000', '128', '110'); # Select symbols with average daily volume is more than 400000 SELECT SYMBOL FROM StocksInfo WHERE INFO.AVG_DAILY_VOLUME > 400000; SYMBOL ------------------------------------------------------------------------- MSGU A column of type STRUCT can store array of Java objects (Object[]), typed arrays, java.util.Collection, scala.collection.Seq or scala.Product. You can use com.pivotal.gemfirexd.snappy.ComplexTypeSerializer class to serialize the data in order to insert it into column tables. Refer How to store and retrieve complex data types in JDBC programs for a Scala example that shows how to serialize and store an array in a table using JDBC APIs and ComplexTypeSerializer class. TIMESTAMP \u00b6 Provides for storage of both DATE and TIME as a combined value. In addition, it allows for fractional seconds having up to six digits. Supported formats are: yyyy-MM-dd hh:mm:ss[.nnnnnn] yyyy-MM-dd-hh.mm.ss[.nnnnnn] The year (yyyy) must always be expressed with four digits. Months (MM), days (dd), and hours (hh) may have one or two digits while minutes (mm) and seconds (ss) must have two digits. Microseconds, if present, may have between one and six digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Equivalent Java type java.sql.Timestamp VALUES '2000-02-03 12:23:04' VALUES TIMESTAMP(' 2000-02-03 12:23:04.827') VALUES TIMESTAMP('2000-02-03 12:23:04') The latter examples use the TIMESTAMP() function described in the section Built-in functions and procedures. TINYINT \u00b6 This is a synonym for SMALLINT . VARCHAR \u00b6 Provides for variable-length strings with a maximum limit for length. If a string value is longer than the maximum length, then any trailing blanks are trimmed to make the length same as the maximum length, while an exception is raised if characters other than spaces are required to be truncated. When mixing CHARs and VARCHARs in expressions, the shorter value is padded with spaces to the length of the longer string. The type of a string constant is CHAR, not VARCHAR. To represent a single quotation mark within a string, use two quotation marks: VALUES 'going to Chandra''s place' The length of VARCHAR is an unsigned integer constant. Equivalent Java type java.lang.String Maximum length 32672 { VARCHAR | CHAR VARYING | CHARACTER VARYING }(length)","title":"Supported Data Types"},{"location":"reference/misc/supported_datatypes/#data-types","text":"The SQL type system determines the compile-time and runtime type of an expression. Each type has a certain range of permissible values that can be assigned to a column or value of that type. The special value NULL, denotes an unassigned or missing value of any of the types (columns that have been assigned as non-nullable using NOT NULL clause or the primary key columns cannot have a NULL value). The supported types are given below. Data Type Supported for Row Tables Supported for Column Tables ARRAY X \u2714 BIGINT \u2714 \u2714 BINARY \u2714 \u2714 BLOB \u2714 \u2714 BOOLEAN \u2714 \u2714 BYTE \u2714 \u2714 CLOB \u2714 \u2714 CHAR \u2714 \u2714 DATE \u2714 \u2714 DECIMAL \u2714 \u2714 DOUBLE \u2714 \u2714 FLOAT \u2714 \u2714 INT \u2714 \u2714 INTEGER \u2714 \u2714 LONG \u2714 \u2714 MAP X \u2714 NUMERIC \u2714 \u2714 REAL \u2714 \u2714 SHORT \u2714 \u2714 SMALLINT \u2714 \u2714 STRING \u2714 \u2714 STRUCT X \u2714 TIMESTAMP \u2714 \u2714 TINYINT \u2714 \u2714 VARCHAR \u2714 \u2714 Attention BINARY, BLOB, CLOB, and FLOAT data types work only if you do not provide the size.","title":"Data Types"},{"location":"reference/misc/supported_datatypes/#array","text":"A column of ARRAY datatype can contain a collection of elements. A column of type Array can store array of Java objects (Object[]), typed arrays, java.util.Collection and scala.collection.Seq. You can use com.pivotal.gemfirexd.snappy.ComplexTypeSerializer class to serialize the array data in order to insert it into column tables. Refer How to store and retrieve complex data types in JDBC programs for a Scala example that shows how to serialize and store an array in a table using JDBC APIs and ComplexTypeSerializer class. Note Supported only for column tables SQL Example # Create a table with column of type of an array of doubles and insert few records CREATE TABLE IF NOT EXISTS Student(rollno Int, name String, marks Array<Double>) USING column; INSERT INTO Student SELECT 1,'John', Array(97.8,85.2,63.9,45.2,75.2,96.5);","title":"ARRAY"},{"location":"reference/misc/supported_datatypes/#bigint","text":"Provides 8 bytes storage for long integer values. An attempt to put a BIGINT value into another exact numeric type with smaller size/precision (e.g. INT) fails if the value overflows the maximum allowable by the smaller type. Equivalent Java type java.lang.Long Minimum value java.lang.Long.MIN_VALUE (-9223372036854775808 ) Maximum value java.lang.Long.MAX_VALUE (9223372036854775807 )","title":"BIGINT"},{"location":"reference/misc/supported_datatypes/#binary","text":"This is a synonym of BLOB .","title":"BINARY"},{"location":"reference/misc/supported_datatypes/#blob","text":"A binary large object represents an array of raw bytes of varying length. Equivalent Java type java.lang.Blob Maximum length (also default length) 2 GB - 1 (or 2,147,483,647) { BLOB | BINARY LARGE OBJECT } [ ( length [{ K | M | G }] ) ] The length of the BLOB is expressed in number of bytes by default. The suffixes K, M, and G stand for kilobyte, megabyte and gigabyte, and use the multiples of 1024, 1024*1024, or 1024*1024*1024 respectively. CREATE TABLE blob_data(id INT primary key, data BLOB(10M)); \u2013- search for a blob select length(data) from blob_data where id = 100;","title":"BLOB"},{"location":"reference/misc/supported_datatypes/#boolean","text":"The data type representing Boolean values. This is equivalent to Java's boolean primitive type.","title":"BOOLEAN"},{"location":"reference/misc/supported_datatypes/#byte","text":"The data type representing Byte values. It is an 8-bit signed integer (equivalent to Java's byte primitive type). Minimum value java.lang.Byte.MIN_VALUE Maximum value java.lang.Byte.MAX_VALUE","title":"BYTE"},{"location":"reference/misc/supported_datatypes/#char","text":"Provides for fixed-length strings. If a string value is shorter than the expected length, then spaces are inserted to pad the string to the expected length. If a string value is longer than the expected length, then any trailing blanks are trimmed to make the length same as the expected length, while an exception is raised if characters other than spaces are required to be truncated. For comparison operations, the shorter CHAR string is padded with spaces to the longer value. Similarly when mixing CHARs and VARCHARs in expressions, the shorter value is padded with spaces to the length of the longer string. To represent a single quotation mark within a string, use two quotation marks: VALUES 'going to Chandra's place' The length of CHAR is an unsigned integer constant. Equivalent Java type java.lang.String Maximum length java.lang.Integer.MAX_VALUE (2147483647 ) Default length 1 CHAR[ACTER] [(length)]","title":"CHAR"},{"location":"reference/misc/supported_datatypes/#clob","text":"A character large object represents an array of characters of varying length. It is used to store large character-based data such as documents. The length is expressed in number characters, unless you specify the suffix K, M, or G, which uses the multiples of 1024, 1024*1024, or 1024*1024*1024 respectively. Equivalent Java type java.sql.Clob Maximum length (also default length) 2 GB - 1 (or 2,147,483,647) { CLOB | CHARACTER LARGE OBJECT } [ ( length [{ K | M | G }] ) ] CREATE TABLE clob_data(id INT primary key, text CLOB(10M)); \u2013- search for a clob select text from clob_data where id = 100;","title":"CLOB"},{"location":"reference/misc/supported_datatypes/#date","text":"Provides for storage of a date as year-month-day. Supported formats are: yyyy-mm-dd mm/dd/yyyy dd.mm.yyyy The year (yyyy) must always be expressed with four digits, while months (mm) and days (dd) may have either one or two digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Equivalent Java type java.sql.Date VALUES '2010-05-04' VALUES DATE('2001-10-12') The latter example uses the DATE() function described in the section Built-in functions and procedures.","title":"DATE"},{"location":"reference/misc/supported_datatypes/#decimal","text":"Provides an exact decimal value having a specified precision and scale. The precision is the total number of digits both to the left and the right of the decimal point, and the scale is the number of digits in the fraction to the right of the decimal point. A numeric value (e.g. INT, BIGINT, SMALLINT) can be put into a DECIMAL as long as non-fractional precision is not lost else a range exception is thrown (SQLState: \"22003\"). When truncating trailing digits from a DECIMAL, the value is rounded down. Equivalent Java type java.math.BigDecimal Precision min/max 1 to 31 Scale min/max less than or equal to precision Default precision 5 Default scale 0 { DECIMAL | DEC } [(precision [, scale ])] -- this cast loses fractional precision values cast (23.8372 AS decimal(4,1)); -\u2013- results in: 23.8 -- this cast is outside the range values cast (97824 AS decimal(4,1)); \u2013-- throws exception: ERROR 22003: The resulting value is outside the range for the data type DECIMAL/NUMERIC(4,1).","title":"DECIMAL"},{"location":"reference/misc/supported_datatypes/#double","text":"Provides 8-byte storage for numbers using IEEE floating-point notation. Arithmetic operations do not round their resulting values to zero. If the values are too small, you will receive an exception. Numeric floating point constants are limited to a length of 30 characters. Equivalent Java type java.lang.Double Note: The maximum/minimum limits are different from those of java.lang.Double as noted below. Minimum value -1.79769E+30 Maximum value 1.79769E+308 Smallest positive value 2.225E-307 Largest negative value -2.225E-307 Default precision 5 Default scale 0 \u2013- examples of valid values values 233.31E3; values 8928E+06; -- this example will throw a range exception (SQLState: \"42820\") values 123456789012345678901234567890123456789e0;","title":"DOUBLE"},{"location":"reference/misc/supported_datatypes/#float","text":"Alias for a REAL or DOUBLE data type, depending on the specified precision. The default precision is 53 making it equivalent to DOUBLE. A precision of 23 or less makes FLOAT equivalent to REAL while greater than 23 makes it equivalent to DOUBLE. Equivalent Java type java.lang.Double or java.lang.Float depending on precision Minumum/Maximum limits Same as those for FLOAT if the precision is less than 23. Otherwise, same minimum/maximum limits as those for DOUBLE. Default precision 53 FLOAT [(precision)]","title":"FLOAT"},{"location":"reference/misc/supported_datatypes/#int","text":"This is a synonym of INTEGER .","title":"INT"},{"location":"reference/misc/supported_datatypes/#integer-int","text":"Provides 4 bytes storage for integer values. INT can be used as a synonym for INTEGER in CREATE TABLE. Equivalent Java type java.lang.Integer Minimum value java.lang.Integer.MIN_VALUE (-2147483648) Maximum value java.lang.Integer.MAX_VALUE (21474836487)","title":"INTEGER (INT)"},{"location":"reference/misc/supported_datatypes/#long","text":"The data type representing Long values. It's a 64-bit signed integer (equivalent to Java's long primitive type). Minimum value java.lang.Long.MIN_VALUE Maximum value java.lang.Long.MAX_VALUE","title":"LONG"},{"location":"reference/misc/supported_datatypes/#map","text":"A column of MAP datatype can contain a collection of key-value pairs. SQL Examples # Create a table with column of type MAP and insert few records CREATE TABLE IF NOT EXISTS StudentGrades (rollno Integer, name String, Course Map<String, String>) USING column; INSERT INTO StudentGrades SELECT 1,'Jim', Map('English', 'A+'); INSERT INTO StudentGrades SELECT 2,'John', Map('English', 'A', 'Science', 'B'); # Selecting grades for 'English' snappy> select ROLLNO, NAME, course['English'] from StudentGrades; ROLLNO |NAME |COURSE[English] --------------------------- 2 |John |A 1 |Jim |A+ A column of type Map can store java.util.Map or scala.collection.Map . You can use com.pivotal.gemfirexd.snappy.ComplexTypeSerializer class to serialize the map data in order to insert it into column tables. Refer How to store and retrieve complex data types in JDBC programs for a Scala example that shows how to serialize and store an array in a table using JDBC APIs and ComplexTypeSerializer class. Map data can also be stored in a similar way. Note Supported only for column tables","title":"MAP"},{"location":"reference/misc/supported_datatypes/#numeric","text":"Synonym for the DECIMAL data type. NUMERIC [(precision [, scale ])]","title":"NUMERIC"},{"location":"reference/misc/supported_datatypes/#real","text":"Provides a 4-byte storage for numbers using IEEE floating-point notation. Equivalent Java type java.lang.Float Minimum value -3.402E+38f Maximum value +3.402E+38f Smallest positive value +1.175E-37f Largest negative value -1.175E-37f","title":"REAL"},{"location":"reference/misc/supported_datatypes/#short","text":"This is a synonym for SMALLINT .","title":"SHORT"},{"location":"reference/misc/supported_datatypes/#smallint-tinyint-short","text":"Provides 2 bytes storage for short integer values. Equivalent Java type java.lang.Short Minimum value java.lang.Short.MIN_VALUE (-32768 ) Maximum value java.lang.Short.MAX_VALUE (32767)","title":"SMALLINT (TINYINT) (SHORT)"},{"location":"reference/misc/supported_datatypes/#string","text":"The data type representing String values. A String encoded in UTF-8 as an Array[Byte], which can be used for comparison search.","title":"STRING"},{"location":"reference/misc/supported_datatypes/#struct","text":"A column of struct datatype can contain a structure with different fields. SQL Examples # Create a table with column of type STRUCT and insert few records. CREATE TABLE IF NOT EXISTS StocksInfo (SYMBOL STRING, INFO STRUCT<TRADING_YEAR: STRING, AVG_DAILY_VOLUME: LONG, HIGHEST_PRICE_IN_YEAR: INT, LOWEST_PRICE_IN_YEAR: INT>) USING COLUMN; INSERT INTO StocksInfo SELECT 'ORD', STRUCT('2018', '400000', '112', '52'); INSERT INTO StocksInfo SELECT 'MSGU', Struct('2018', '500000', '128', '110'); # Select symbols with average daily volume is more than 400000 SELECT SYMBOL FROM StocksInfo WHERE INFO.AVG_DAILY_VOLUME > 400000; SYMBOL ------------------------------------------------------------------------- MSGU A column of type STRUCT can store array of Java objects (Object[]), typed arrays, java.util.Collection, scala.collection.Seq or scala.Product. You can use com.pivotal.gemfirexd.snappy.ComplexTypeSerializer class to serialize the data in order to insert it into column tables. Refer How to store and retrieve complex data types in JDBC programs for a Scala example that shows how to serialize and store an array in a table using JDBC APIs and ComplexTypeSerializer class.","title":"STRUCT"},{"location":"reference/misc/supported_datatypes/#timestamp","text":"Provides for storage of both DATE and TIME as a combined value. In addition, it allows for fractional seconds having up to six digits. Supported formats are: yyyy-MM-dd hh:mm:ss[.nnnnnn] yyyy-MM-dd-hh.mm.ss[.nnnnnn] The year (yyyy) must always be expressed with four digits. Months (MM), days (dd), and hours (hh) may have one or two digits while minutes (mm) and seconds (ss) must have two digits. Microseconds, if present, may have between one and six digits. DATEs, TIMEs, and TIMESTAMPs must not be mixed with one another in expressions except with an explicit CAST. Equivalent Java type java.sql.Timestamp VALUES '2000-02-03 12:23:04' VALUES TIMESTAMP(' 2000-02-03 12:23:04.827') VALUES TIMESTAMP('2000-02-03 12:23:04') The latter examples use the TIMESTAMP() function described in the section Built-in functions and procedures.","title":"TIMESTAMP"},{"location":"reference/misc/supported_datatypes/#tinyint","text":"This is a synonym for SMALLINT .","title":"TINYINT"},{"location":"reference/misc/supported_datatypes/#varchar","text":"Provides for variable-length strings with a maximum limit for length. If a string value is longer than the maximum length, then any trailing blanks are trimmed to make the length same as the maximum length, while an exception is raised if characters other than spaces are required to be truncated. When mixing CHARs and VARCHARs in expressions, the shorter value is padded with spaces to the length of the longer string. The type of a string constant is CHAR, not VARCHAR. To represent a single quotation mark within a string, use two quotation marks: VALUES 'going to Chandra''s place' The length of VARCHAR is an unsigned integer constant. Equivalent Java type java.lang.String Maximum length 32672 { VARCHAR | CHAR VARYING | CHARACTER VARYING }(length)","title":"VARCHAR"},{"location":"reference/sql_functions/","text":"! \u00b6 ! expr - Logical not. No example/argument for !. % \u00b6 expr1 % expr2 - Returns the remainder after expr1 / expr2 . Examples: > SELECT 2 % 1.8; 0.2 & \u00b6 expr1 & expr2 - Returns the result of bitwise AND of expr1 and expr2 . Examples: > SELECT 3 & 5; 1 * \u00b6 expr1 * expr2 - Returns expr1 * expr2 . Examples: > SELECT 2 * 3; 6 + \u00b6 expr1 + expr2 - Returns expr1 + expr2 . Examples: > SELECT 1 + 2; 3 - \u00b6 expr1 - expr2 - Returns expr1 - expr2 . Examples: > SELECT 2 - 1; 1 / \u00b6 expr1 / expr2 - Returns expr1 / expr2 . It always performs floating point division. Examples: > SELECT 3 / 2; 1.5 > SELECT 2L / 2L; 1.0 < \u00b6 expr1 < expr2 - Returns true if expr1 is less than expr2 . No example/argument for <. <= \u00b6 expr1 <= expr2 - Returns true if expr1 is less than or equal to expr2 . No example/argument for <=. <=> \u00b6 expr1 <=> expr2 - Returns same result as the EQUAL(=) operator for non-null operands, but returns true if both are null, false if one of the them is null. No example/argument for <=>. = \u00b6 expr1 = expr2 - Returns true if expr1 equals expr2 , or false otherwise. No example/argument for =. == \u00b6 expr1 == expr2 - Returns true if expr1 equals expr2 , or false otherwise. No example/argument for ==. > \u00b6 expr1 > expr2 - Returns true if expr1 is greater than expr2 . No example/argument for >. >= \u00b6 expr1 >= expr2 - Returns true if expr1 is greater than or equal to expr2 . No example/argument for >=. ^ \u00b6 expr1 ^ expr2 - Returns the result of bitwise exclusive OR of expr1 and expr2 . Examples: > SELECT 3 ^ 5; 2 abs \u00b6 abs(expr) - Returns the absolute value of the numeric value. Examples: > SELECT abs(-1); 1 absolute_error \u00b6 absolute_error() - Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: > SELECT sum(ArrDelay) ArrivalDelay, absolute_error(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 43.4, Jan acos \u00b6 acos(expr) - Returns the inverse cosine (a.k.a. arccosine) of expr if -1<= expr <=1 or NaN otherwise. Examples: > SELECT acos(1); 0.0 > SELECT acos(2); NaN add_months \u00b6 add_months(start_date, num_months) - Returns the date that is num_months after start_date . Examples: > SELECT add_months('2016-08-31', 1); 2016-09-30 and \u00b6 expr1 and expr2 - Logical AND. No example/argument for and. approx_count_distinct \u00b6 approx_count_distinct(expr[, relativeSD]) - Returns the estimated cardinality by HyperLogLog++. relativeSD defines the maximum estimation error allowed. No example/argument for approx_count_distinct. approx_percentile \u00b6 approx_percentile(col, percentage [, accuracy]) - Returns the approximate percentile value of numeric column col at the given percentage. The value of percentage must be between 0.0 and 1.0. The accuracy parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error of the approximation. When percentage is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column col at the given percentage array. Examples: > SELECT approx_percentile(10.0, array(0.5, 0.4, 0.1), 100); [10.0,10.0,10.0] > SELECT approx_percentile(10.0, 0.5, 100); 10.0 array \u00b6 array(expr, ...) - Returns an array with the given elements. Examples: > SELECT array(1, 2, 3); [1,2,3] array_contains \u00b6 array_contains(array, value) - Returns true if the array contains the value. Examples: > SELECT array_contains(array(1, 2, 3), 2); true ascii \u00b6 ascii(str) - Returns the numeric value of the first character of str . Examples: > SELECT ascii('222'); 50 > SELECT ascii(2); 50 asin \u00b6 asin(expr) - Returns the inverse sine (a.k.a. arcsine) the arc sin of expr if -1<= expr <=1 or NaN otherwise. Examples: > SELECT asin(0); 0.0 > SELECT asin(2); NaN assert_true \u00b6 assert_true(expr) - Throws an exception if expr is not true. Examples: > SELECT assert_true(0 < 1); NULL atan \u00b6 atan(expr) - Returns the inverse tangent (a.k.a. arctangent). Examples: > SELECT atan(0); 0.0 atan2 \u00b6 atan2(expr1, expr2) - Returns the angle in radians between the positive x-axis of a plane and the point given by the coordinates ( expr1 , expr2 ). Examples: > SELECT atan2(0, 0); 0.0 avg \u00b6 avg(expr) - Returns the mean calculated from values of a group. No example/argument for avg. base64 \u00b6 base64(bin) - Converts the argument from a binary bin to a base 64 string. Examples: > SELECT base64('Spark SQL'); U3BhcmsgU1FM bigint \u00b6 bigint(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT bigint('10' as int); 10 bin \u00b6 bin(expr) - Returns the string representation of the long value expr represented in binary. Examples: > SELECT bin(13); 1101 > SELECT bin(-13); 1111111111111111111111111111111111111111111111111111111111110011 > SELECT bin(13.3); 1101 binary \u00b6 binary(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT binary('10' as int); 10 boolean \u00b6 boolean(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT boolean('10' as int); 10 bround \u00b6 bround(expr, d) - Returns expr rounded to d decimal places using HALF_EVEN rounding mode. Examples: > SELECT bround(2.5, 0); 2.0 cbrt \u00b6 cbrt(expr) - Returns the cube root of expr . Examples: > SELECT cbrt(27.0); 3.0 ceil \u00b6 ceil(expr) - Returns the smallest integer not smaller than expr . Examples: > SELECT ceil(-0.1); 0 > SELECT ceil(5); 5 ceiling \u00b6 ceiling(expr) - Returns the smallest integer not smaller than expr . Examples: > SELECT ceiling(-0.1); 0 > SELECT ceiling(5); 5 coalesce \u00b6 coalesce(expr1, expr2, ...) - Returns the first non-null argument if exists. Otherwise, null. Examples: > SELECT coalesce(NULL, 1, NULL); 1 collect_list \u00b6 collect_list(expr) - Collects and returns a list of non-unique elements. No example/argument for collect_list. collect_set \u00b6 collect_set(expr) - Collects and returns a set of unique elements. No example/argument for collect_set. concat \u00b6 concat(str1, str2, ..., strN) - Returns the concatenation of str1, str2, ..., strN. Examples: > SELECT concat('Spark', 'SQL'); SparkSQL concat_ws \u00b6 concat_ws(sep, [str | array(str)]+) - Returns the concatenation of the strings separated by sep . Examples: > SELECT concat_ws(' ', 'Spark', 'SQL'); Spark SQL conv \u00b6 conv(num, from_base, to_base) - Convert num from from_base to to_base . Examples: > SELECT conv('100', 2, 10); 4 > SELECT conv(-10, 16, -10); 16 corr \u00b6 corr(expr1, expr2) - Returns Pearson coefficient of correlation between a set of number pairs. No example/argument for corr. cos \u00b6 cos(expr) - Returns the cosine of expr . Examples: > SELECT cos(0); 1.0 cosh \u00b6 cosh(expr) - Returns the hyperbolic cosine of expr . Examples: > SELECT cosh(0); 1.0 count \u00b6 count(*) - Returns the total number of retrieved rows, including rows containing null. count(expr) - Returns the number of rows for which the supplied expression is non-null. count(DISTINCT expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are unique and non-null. No example/argument for count. covar_pop \u00b6 covar_pop(expr1, expr2) - Returns the population covariance of a set of number pairs. No example/argument for covar_pop. covar_samp \u00b6 covar_samp(expr1, expr2) - Returns the sample covariance of a set of number pairs. No example/argument for covar_samp. crc32 \u00b6 crc32(expr) - Returns a cyclic redundancy check value of the expr as a bigint. Examples: > SELECT crc32('Spark'); 1557323817 cube \u00b6 cume_dist \u00b6 cume_dist() - Computes the position of a value relative to all values in the partition. No example/argument for cume_dist. current_database \u00b6 current_database() - Returns the current database. Examples: > SELECT current_database(); default current_date \u00b6 current_date() - Returns the current date at the start of query evaluation. No example/argument for current_date. current_schema \u00b6 current_schema() - Returns the current database. Examples: > SELECT current_schema(); default current_timestamp \u00b6 current_timestamp() - Returns the current timestamp at the start of query evaluation. No example/argument for current_timestamp. current_user \u00b6 current_user() - Returns the name of the user that owns the session executing the current SQL statement. Examples: > SELECT current_user(); USER1 current_user_ldap_groups \u00b6 current_user_ldap_groups() - Returns all the ldap groups as an ARRAY to which the user who is executing the current SQL statement belongs. Examples: > SELECT array_contains(current_user_ldap_groups(), 'GROUP1'); true date \u00b6 date(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT date('10' as int); 10 date_add \u00b6 date_add(start_date, num_days) - Returns the date that is num_days after start_date . Examples: > SELECT date_add('2016-07-30', 1); 2016-07-31 date_format \u00b6 date_format(timestamp, fmt) - Converts timestamp to a value of string in the format specified by the date format fmt . Examples: > SELECT date_format('2016-04-08', 'y'); 2016 date_sub \u00b6 date_sub(start_date, num_days) - Returns the date that is num_days before start_date . Examples: > SELECT date_sub('2016-07-30', 1); 2016-07-29 datediff \u00b6 datediff(endDate, startDate) - Returns the number of days from startDate to endDate . Examples: > SELECT datediff('2009-07-31', '2009-07-30'); 1 > SELECT datediff('2009-07-30', '2009-07-31'); -1 day \u00b6 day(date) - Returns the day of month of the date/timestamp. Examples: > SELECT day('2009-07-30'); 30 dayofmonth \u00b6 dayofmonth(date) - Returns the day of month of the date/timestamp. Examples: > SELECT dayofmonth('2009-07-30'); 30 dayofyear \u00b6 dayofyear(date) - Returns the day of year of the date/timestamp. Examples: > SELECT dayofyear('2016-04-09'); 100 decimal \u00b6 decimal(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT decimal('10' as int); 10 decode \u00b6 decode(bin, charset) - Decodes the first argument using the second argument character set. Examples: > SELECT decode(encode('abc', 'utf-8'), 'utf-8'); abc degrees \u00b6 degrees(expr) - Converts radians to degrees. Examples: > SELECT degrees(3.141592653589793); 180.0 dense_rank \u00b6 dense_rank() - Computes the rank of a value in a group of values. The result is one plus the previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps in the ranking sequence. No example/argument for dense_rank. double \u00b6 double(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT double('10' as int); 10 dsid \u00b6 dsid() - Returns the unique distributed member ID of executor fetching current row. Examples: > SELECT dsid(); localhost(1831)<v2>:18165 e \u00b6 e() - Returns Euler's number, e. Examples: > SELECT e(); 2.718281828459045 elt \u00b6 elt(n, str1, str2, ...) - Returns the n -th string, e.g., returns str2 when n is 2. Examples: > SELECT elt(1, 'scala', 'java'); scala encode \u00b6 encode(str, charset) - Encodes the first argument using the second argument character set. Examples: > SELECT encode('abc', 'utf-8'); abc exp \u00b6 exp(expr) - Returns e to the power of expr . Examples: > SELECT exp(0); 1.0 explode \u00b6 explode(expr) - Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Examples: > SELECT explode(array(10, 20)); 10 20 expm1 \u00b6 expm1(expr) - Returns exp( expr ) - 1. Examples: > SELECT expm1(0); 0.0 factorial \u00b6 factorial(expr) - Returns the factorial of expr . expr is [0..20]. Otherwise, null. Examples: > SELECT factorial(5); 120 find_in_set \u00b6 find_in_set(str, str_array) - Returns the index (1-based) of the given string ( str ) in the comma-delimited list ( str_array ). Returns 0, if the string was not found or if the given string ( str ) contains a comma. Examples: > SELECT find_in_set('ab','abc,b,ab,c,def'); 3 first \u00b6 first(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for first. first_value \u00b6 first_value(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for first_value. float \u00b6 float(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT float('10' as int); 10 floor \u00b6 floor(expr) - Returns the largest integer not greater than expr . Examples: > SELECT floor(-0.1); -1 > SELECT floor(5); 5 format_number \u00b6 format_number(expr1, expr2) - Formats the number expr1 like '#,###,###.##', rounded to expr2 decimal places. If expr2 is 0, the result has no decimal point or fractional part. This is supposed to function like MySQL's FORMAT. Examples: > SELECT format_number(12332.123456, 4); 12,332.1235 format_string \u00b6 format_string(strfmt, obj, ...) - Returns a formatted string from printf-style format strings. Examples: > SELECT format_string(\"Hello World %d %s\", 100, \"days\"); Hello World 100 days from_unixtime \u00b6 from_unixtime(unix_time, format) - Returns unix_time in the specified format . Examples: > SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss'); 1970-01-01 00:00:00 from_utc_timestamp \u00b6 from_utc_timestamp(timestamp, timezone) - Given a timestamp, which corresponds to a certain time of day in UTC, returns another timestamp that corresponds to the same time of day in the given timezone. Examples: > SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul'); 2016-08-31 09:00:00 get_json_object \u00b6 get_json_object(json_txt, path) - Extracts a json object from path . Examples: > SELECT get_json_object('{\"a\":\"b\"}', '$.a'); b greatest \u00b6 greatest(expr, ...) - Returns the greatest value of all parameters, skipping null values. Examples: > SELECT greatest(10, 9, 2, 4, 3); 10 grouping \u00b6 grouping_id \u00b6 hash \u00b6 hash(expr1, expr2, ...) - Returns a hash value of the arguments. Examples: > SELECT hash('Spark', array(123), 2); -1321691492 hex \u00b6 hex(expr) - Converts expr to hexadecimal. Examples: > SELECT hex(17); 11 > SELECT hex('Spark SQL'); 537061726B2053514C hour \u00b6 hour(timestamp) - Returns the hour component of the string/timestamp. Examples: > SELECT hour('2009-07-30 12:58:59'); 12 hypot \u00b6 hypot(expr1, expr2) - Returns sqrt( expr1 2 + expr2 2). Examples: > SELECT hypot(3, 4); 5.0 if \u00b6 if(expr1, expr2, expr3) - If expr1 evaluates to true, then returns expr2 ; otherwise returns expr3 . Examples: > SELECT if(1 < 2, 'a', 'b'); a ifnull \u00b6 ifnull(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise. Examples: > SELECT ifnull(NULL, array('2')); [\"2\"] in \u00b6 expr1 in(expr2, expr3, ...) - Returns true if expr equals to any valN. No example/argument for in. initcap \u00b6 initcap(str) - Returns str with the first letter of each word in uppercase. All other letters are in lowercase. Words are delimited by white space. Examples: > SELECT initcap('sPark sql'); Spark Sql inline \u00b6 inline(expr) - Explodes an array of structs into a table. Examples: > SELECT inline(array(struct(1, 'a'), struct(2, 'b'))); 1 a 2 b input_file_name \u00b6 input_file_name() - Returns the name of the current file being read if available. No example/argument for input_file_name. instr \u00b6 instr(str, substr) - Returns the (1-based) index of the first occurrence of substr in str . Examples: > SELECT instr('SparkSQL', 'SQL'); 6 int \u00b6 int(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT int('10' as int); 10 isnan \u00b6 isnan(expr) - Returns true if expr is NaN, or false otherwise. Examples: > SELECT isnan(cast('NaN' as double)); true isnotnull \u00b6 isnotnull(expr) - Returns true if expr is not null, or false otherwise. Examples: > SELECT isnotnull(1); true isnull \u00b6 isnull(expr) - Returns true if expr is null, or false otherwise. Examples: > SELECT isnull(1); false java_method \u00b6 java_method(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection. Examples: > SELECT java_method('java.util.UUID', 'randomUUID'); c33fb387-8500-4bfa-81d2-6e0e3e930df2 > SELECT java_method('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2'); a5cf6c42-0c85-418f-af6c-3e4e5b1328f2 json_tuple \u00b6 json_tuple(jsonStr, p1, p2, ..., pn) - Return a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string. Examples: > SELECT json_tuple('{\"a\":1, \"b\":2}', 'a', 'b'); 1 2 kurtosis \u00b6 kurtosis(expr) - Returns the kurtosis value calculated from values of a group. No example/argument for kurtosis. lag \u00b6 lag(input[, offset[, default]]) - Returns the value of input at the offset th row before the current row in the window. The default value of offset is 1 and the default value of default is null. If the value of input at the offset th row is null, null is returned. If there is no such offset row (e.g., when the offset is 1, the first row of the window does not have any previous row), default is returned. No example/argument for lag. last \u00b6 last(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for last. last_day \u00b6 last_day(date) - Returns the last day of the month which the date belongs to. Examples: > SELECT last_day('2009-01-12'); 2009-01-31 last_value \u00b6 last_value(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for last_value. lcase \u00b6 lcase(str) - Returns str with all characters changed to lowercase. Examples: > SELECT lcase('SparkSql'); sparksql lead \u00b6 lead(input[, offset[, default]]) - Returns the value of input at the offset th row after the current row in the window. The default value of offset is 1 and the default value of default is null. If the value of input at the offset th row is null, null is returned. If there is no such an offset row (e.g., when the offset is 1, the last row of the window does not have any subsequent row), default is returned. No example/argument for lead. least \u00b6 least(expr, ...) - Returns the least value of all parameters, skipping null values. Examples: > SELECT least(10, 9, 2, 4, 3); 2 length \u00b6 length(expr) - Returns the length of expr or number of bytes in binary data. Examples: > SELECT length('Spark SQL'); 9 levenshtein \u00b6 levenshtein(str1, str2) - Returns the Levenshtein distance between the two given strings. Examples: > SELECT levenshtein('kitten', 'sitting'); 3 like \u00b6 str like pattern - Returns true if str matches pattern, null if any arguments are null, false otherwise. Arguments: str - a string expression pattern - a string expression. The pattern is a string which is matched literally, with exception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regular expressions) The escape character is '\\'. If an escape character precedes a special symbol or another escape character, the following character is matched literally. It is invalid to escape any other character. Examples: > SELECT '%SystemDrive%\\Users\\John' like '\\%SystemDrive\\%\\\\Users%' true See also: Use RLIKE to match with standard regular expressions. ln \u00b6 ln(expr) - Returns the natural logarithm (base e) of expr . Examples: > SELECT ln(1); 0.0 locate \u00b6 locate(substr, str[, pos]) - Returns the position of the first occurrence of substr in str after position pos . The given pos and return value are 1-based. Examples: > SELECT locate('bar', 'foobarbar', 5); 7 log \u00b6 log(base, expr) - Returns the logarithm of expr with base . Examples: > SELECT log(10, 100); 2.0 log10 \u00b6 log10(expr) - Returns the logarithm of expr with base 10. Examples: > SELECT log10(10); 1.0 log1p \u00b6 log1p(expr) - Returns log(1 + expr ). Examples: > SELECT log1p(0); 0.0 log2 \u00b6 log2(expr) - Returns the logarithm of expr with base 2. Examples: > SELECT log2(2); 1.0 lower \u00b6 lower(str) - Returns str with all characters changed to lowercase. Examples: > SELECT lower('SparkSql'); sparksql lower_bound \u00b6 lower_bound() - Lower value of an estimate interval for a given confidence.calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: > SELECT sum(ArrDelay) ArrivalDelay, lower_bound(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 11101.5, Jan \"``` ### lpad lpad(str, len, pad) - Returns `str`, left-padded with `pad` to a length of `len`. If `str` is longer than `len`, the return value is shortened to `len` characters. Examples: SELECT lpad('hi', 5, '??'); ???hi SELECT lpad('hi', 1, '??'); h ### ltrim ltrim(str) - Removes the leading and trailing space characters from `str`. Examples: SELECT ltrim(' SparkSQL'); SparkSQL ### map map(key0, value0, key1, value1, ...) - Creates a map with the given key/value pairs. Examples: SELECT map(1.0, '2', 3.0, '4'); {1.0:\"2\",3.0:\"4\"} ### map_keys map_keys(map) - Returns an unordered array containing the keys of the map. Examples: SELECT map_keys(map(1, 'a', 2, 'b')); [1,2] ### map_values map_values(map) - Returns an unordered array containing the values of the map. Examples: SELECT map_values(map(1, 'a', 2, 'b')); [\"a\",\"b\"] ### max max(expr) - Returns the maximum value of `expr`. No example/argument for max. ### md5 md5(expr) - Returns an MD5 128-bit checksum as a hex string of `expr`. Examples: SELECT md5('Spark'); 8cde774d6f7333752ed72cacddb05126 ### mean mean(expr) - Returns the mean calculated from values of a group. No example/argument for mean. ### min min(expr) - Returns the minimum value of `expr`. No example/argument for min. ### minute minute(timestamp) - Returns the minute component of the string/timestamp. Examples: SELECT minute('2009-07-30 12:58:59'); 58 ### monotonically_increasing_id monotonically_increasing_id() - Returns monotonically increasing 64-bit integers. The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the lower 33 bits represent the record number within each partition. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records. No example/argument for monotonically_increasing_id. ### month month(date) - Returns the month component of the date/timestamp. Examples: SELECT month('2016-07-30'); 7 ### months_between months_between(timestamp1, timestamp2) - Returns number of months between `timestamp1` and `timestamp2`. Examples: SELECT months_between('1997-02-28 10:30:00', '1996-10-30'); 3.94959677 ### named_struct named_struct(name1, val1, name2, val2, ...) - Creates a struct with the given field names and values. Examples: SELECT named_struct(\"a\", 1, \"b\", 2, \"c\", 3); {\"a\":1,\"b\":2,\"c\":3} ### nanvl nanvl(expr1, expr2) - Returns `expr1` if it's not NaN, or `expr2` otherwise. Examples: SELECT nanvl(cast('NaN' as double), 123); 123.0 ### negative negative(expr) - Returns the negated value of `expr`. Examples: SELECT negative(1); -1 ### next_day next_day(start_date, day_of_week) - Returns the first date which is later than `start_date` and named as indicated. Examples: SELECT next_day('2015-01-14', 'TU'); 2015-01-20 ### not not expr - Logical not. No example/argument for not. ### now now() - Returns the current timestamp at the start of query evaluation. No example/argument for now. ### ntile ntile(n) - Divides the rows for each window partition into `n` buckets ranging from 1 to at most `n`. No example/argument for ntile. ### nullif nullif(expr1, expr2) - Returns null if `expr1` equals to `expr2`, or `expr1` otherwise. Examples: SELECT nullif(2, 2); NULL ### nvl nvl(expr1, expr2) - Returns `expr2` if `expr1` is null, or `expr1` otherwise. Examples: SELECT nvl(NULL, array('2')); [\"2\"] ### nvl2 nvl2(expr1, expr2, expr3) - Returns `expr2` if `expr1` is not null, or `expr3` otherwise. Examples: SELECT nvl2(NULL, 2, 1); 1 ### or expr1 or expr2 - Logical OR. No example/argument for or. ### parse_url parse_url(url, partToExtract[, key]) - Extracts a part from a URL. Examples: SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST') spark.apache.org SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY') query=1 SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query') 1 ### percent_rank percent_rank() - Computes the percentage ranking of a value in a group of values. No example/argument for percent_rank. ### percentile percentile(col, percentage) - Returns the exact percentile value of numeric column `col` at the given percentage. The value of percentage must be between 0.0 and 1.0. percentile(col, array(percentage1 [, percentage2]...)) - Returns the exact percentile value array of numeric column `col` at the given percentage(s). Each value of the percentage array must be between 0.0 and 1.0. No example/argument for percentile. ### percentile_approx percentile_approx(col, percentage [, accuracy]) - Returns the approximate percentile value of numeric column `col` at the given percentage. The value of percentage must be between 0.0 and 1.0. The `accuracy` parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of `accuracy` yields better accuracy, `1.0/accuracy` is the relative error of the approximation. When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column `col` at the given percentage array. Examples: SELECT percentile_approx(10.0, array(0.5, 0.4, 0.1), 100); [10.0,10.0,10.0] SELECT percentile_approx(10.0, 0.5, 100); 10.0 ### pi pi() - Returns pi. Examples: SELECT pi(); 3.141592653589793 ### pmod pmod(expr1, expr2) - Returns the positive value of `expr1` mod `expr2`. Examples: SELECT pmod(10, 3); 1 SELECT pmod(-10, 3); 2 ### posexplode posexplode(expr) - Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Examples: SELECT posexplode(array(10,20)); 0 10 1 20 ### positive positive(expr) - Returns the value of `expr`. No example/argument for positive. ### pow pow(expr1, expr2) - Raises `expr1` to the power of `expr2`. Examples: SELECT pow(2, 3); 8.0 ### power power(expr1, expr2) - Raises `expr1` to the power of `expr2`. Examples: SELECT power(2, 3); 8.0 ### printf printf(strfmt, obj, ...) - Returns a formatted string from printf-style format strings. Examples: SELECT printf(\"Hello World %d %s\", 100, \"days\"); Hello World 100 days ### quarter quarter(date) - Returns the quarter of the year for date, in the range 1 to 4. Examples: SELECT quarter('2016-08-31'); 3 ### radians radians(expr) - Converts degrees to radians. Examples: SELECT radians(180); 3.141592653589793 ### rand rand([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1). Examples: SELECT rand(); 0.9629742951434543 SELECT rand(0); 0.8446490682263027 SELECT rand(null); 0.8446490682263027 ### randn randn([seed]) - Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution. Examples: SELECT randn(); -0.3254147983080288 SELECT randn(0); 1.1164209726833079 SELECT randn(null); 1.1164209726833079 ### rank rank() - Computes the rank of a value in a group of values. The result is one plus the number of rows preceding or equal to the current row in the ordering of the partition. The values will produce gaps in the sequence. No example/argument for rank. ### reflect reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection. Examples: SELECT reflect('java.util.UUID', 'randomUUID'); c33fb387-8500-4bfa-81d2-6e0e3e930df2 SELECT reflect('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2'); a5cf6c42-0c85-418f-af6c-3e4e5b1328f2 ### regexp_extract regexp_extract(str, regexp[, idx]) - Extracts a group that matches `regexp`. Examples: SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1); 100 ### regexp_replace regexp_replace(str, regexp, rep) - Replaces all substrings of `str` that match `regexp` with `rep`. Examples: SELECT regexp_replace('100-200', '(\\d+)', 'num'); num-num ### relative_error relative_error() - Indicates ratio of absolute error to estimate (approx answer). calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: SELECT sum(ArrDelay) ArrivalDelay, relative_error(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 0.3, Jan \"``` repeat \u00b6 repeat(str, n) - Returns the string which repeats the given string value n times. Examples: > SELECT repeat('123', 2); 123123 reverse \u00b6 reverse(str) - Returns the reversed given string. Examples: > SELECT reverse('Spark SQL'); LQS krapS rint \u00b6 rint(expr) - Returns the double value that is closest in value to the argument and is equal to a mathematical integer. Examples: > SELECT rint(12.3456); 12.0 rlike \u00b6 str rlike regexp - Returns true if str matches regexp , or false otherwise. No example/argument for rlike. rollup \u00b6 round \u00b6 round(expr, d) - Returns expr rounded to d decimal places using HALF_UP rounding mode. Examples: > SELECT round(2.5, 0); 3.0 row_number \u00b6 row_number() - Assigns a unique, sequential number to each row, starting with one, according to the ordering of rows within the window partition. No example/argument for row_number. rpad \u00b6 rpad(str, len, pad) - Returns str , right-padded with pad to a length of len . If str is longer than len , the return value is shortened to len characters. Examples: > SELECT rpad('hi', 5, '??'); hi??? > SELECT rpad('hi', 1, '??'); h rtrim \u00b6 rtrim(str) - Removes the trailing space characters from str . Examples: > SELECT rtrim(' SparkSQL '); SparkSQL second \u00b6 second(timestamp) - Returns the second component of the string/timestamp. Examples: > SELECT second('2009-07-30 12:58:59'); 59 sentences \u00b6 sentences(str[, lang, country]) - Splits str into an array of array of words. Examples: > SELECT sentences('Hi there! Good morning.'); [[\"Hi\",\"there\"],[\"Good\",\"morning\"]] sha \u00b6 sha(expr) - Returns a sha1 hash value as a hex string of the expr . Examples: > SELECT sha('Spark'); 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c sha1 \u00b6 sha1(expr) - Returns a sha1 hash value as a hex string of the expr . Examples: > SELECT sha1('Spark'); 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c sha2 \u00b6 sha2(expr, bitLength) - Returns a checksum of SHA-2 family as a hex string of expr . SHA-224, SHA-256, SHA-384, and SHA-512 are supported. Bit length of 0 is equivalent to 256. Examples: > SELECT sha2('Spark', 256); 529bc3b07127ecb7e53a4dcf1991d9152c24537d919178022b2c42657f79a26b shiftleft \u00b6 shiftleft(base, expr) - Bitwise left shift. Examples: > SELECT shiftleft(2, 1); 4 shiftright \u00b6 shiftright(base, expr) - Bitwise (signed) right shift. Examples: > SELECT shiftright(4, 1); 2 shiftrightunsigned \u00b6 shiftrightunsigned(base, expr) - Bitwise unsigned right shift. Examples: > SELECT shiftrightunsigned(4, 1); 2 sign \u00b6 sign(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive. Examples: > SELECT sign(40); 1.0 signum \u00b6 signum(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive. Examples: > SELECT signum(40); 1.0 sin \u00b6 sin(expr) - Returns the sine of expr . Examples: > SELECT sin(0); 0.0 sinh \u00b6 sinh(expr) - Returns the hyperbolic sine of expr . Examples: > SELECT sinh(0); 0.0 size \u00b6 size(expr) - Returns the size of an array or a map. Returns -1 if null. Examples: > SELECT size(array('b', 'd', 'c', 'a')); 4 skewness \u00b6 skewness(expr) - Returns the skewness value calculated from values of a group. No example/argument for skewness. smallint \u00b6 smallint(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT smallint('10' as int); 10 sort_array \u00b6 sort_array(array[, ascendingOrder]) - Sorts the input array in ascending or descending order according to the natural ordering of the array elements. Examples: > SELECT sort_array(array('b', 'd', 'c', 'a'), true); [\"a\",\"b\",\"c\",\"d\"] soundex \u00b6 soundex(str) - Returns Soundex code of the string. Examples: > SELECT soundex('Miller'); M460 space \u00b6 space(n) - Returns a string consisting of n spaces. Examples: > SELECT concat(space(2), '1'); 1 spark_partition_id \u00b6 spark_partition_id() - Returns the current partition id. No example/argument for spark_partition_id. split \u00b6 split(str, regex) - Splits str around occurrences that match regex . Examples: > SELECT split('oneAtwoBthreeC', '[ABC]'); [\"one\",\"two\",\"three\",\"\"] sqrt \u00b6 sqrt(expr) - Returns the square root of expr . Examples: > SELECT sqrt(4); 2.0 stack \u00b6 stack(n, expr1, ..., exprk) - Separates expr1 , ..., exprk into n rows. Examples: > SELECT stack(2, 1, 2, 3); 1 2 3 NULL std \u00b6 std(expr) - Returns the sample standard deviation calculated from values of a group. No example/argument for std. stddev \u00b6 stddev(expr) - Returns the sample standard deviation calculated from values of a group. No example/argument for stddev. stddev_pop \u00b6 stddev_pop(expr) - Returns the population standard deviation calculated from values of a group. No example/argument for stddev_pop. stddev_samp \u00b6 stddev_samp(expr) - Returns the sample standard deviation calculated from values of a group. No example/argument for stddev_samp. str_to_map \u00b6 str_to_map(text[, pairDelim[, keyValueDelim]]) - Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ',' for pairDelim and ':' for keyValueDelim . Examples: > SELECT str_to_map('a:1,b:2,c:3', ',', ':'); map(\"a\":\"1\",\"b\":\"2\",\"c\":\"3\") > SELECT str_to_map('a'); map(\"a\":null) string \u00b6 string(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT string('10' as int); 10 struct \u00b6 struct(col1, col2, col3, ...) - Creates a struct with the given field values. substr \u00b6 substr(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len , or the slice of byte array that starts at pos and is of length len . Examples: > SELECT substr('Spark SQL', 5); k SQL > SELECT substr('Spark SQL', -3); SQL > SELECT substr('Spark SQL', 5, 1); k substring \u00b6 substring(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len , or the slice of byte array that starts at pos and is of length len . Examples: > SELECT substring('Spark SQL', 5); k SQL > SELECT substring('Spark SQL', -3); SQL > SELECT substring('Spark SQL', 5, 1); k substring_index \u00b6 substring_index(str, delim, count) - Returns the substring from str before count occurrences of the delimiter delim . If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. The function substring_index performs a case-sensitive match when searching for delim . Examples: > SELECT substring_index('www.apache.org', '.', 2); www.apache sum \u00b6 sum(expr) - Returns the sum calculated from values of a group. No example/argument for sum. tan \u00b6 tan(expr) - Returns the tangent of expr . Examples: > SELECT tan(0); 0.0 tanh \u00b6 tanh(expr) - Returns the hyperbolic tangent of expr . Examples: > SELECT tanh(0); 0.0 timestamp \u00b6 timestamp(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT timestamp('10' as int); 10 tinyint \u00b6 tinyint(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT tinyint('10' as int); 10 to_date \u00b6 to_date(expr) - Extracts the date part of the date or timestamp expression expr . Examples: > SELECT to_date('2009-07-30 04:17:52'); 2009-07-30 to_unix_timestamp \u00b6 to_unix_timestamp(expr[, pattern]) - Returns the UNIX timestamp of the give time. Examples: > SELECT to_unix_timestamp('2016-04-08', 'yyyy-MM-dd'); 1460041200 to_utc_timestamp \u00b6 to_utc_timestamp(timestamp, timezone) - Given a timestamp, which corresponds to a certain time of day in the given timezone, returns another timestamp that corresponds to the same time of day in UTC. Examples: > SELECT to_utc_timestamp('2016-08-31', 'Asia/Seoul'); 2016-08-30 15:00:00 translate \u00b6 translate(input, from, to) - Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. Examples: > SELECT translate('AaBbCc', 'abc', '123'); A1B2C3 trim \u00b6 trim(str) - Removes the leading and trailing space characters from str . Examples: > SELECT trim(' SparkSQL '); SparkSQL trunc \u00b6 trunc(date, fmt) - Returns date with the time portion of the day truncated to the unit specified by the format model fmt . Examples: > SELECT trunc('2009-02-12', 'MM'); 2009-02-01 > SELECT trunc('2015-10-27', 'YEAR'); 2015-01-01 ucase \u00b6 ucase(str) - Returns str with all characters changed to uppercase. Examples: > SELECT ucase('SparkSql'); SPARKSQL unbase64 \u00b6 unbase64(str) - Converts the argument from a base 64 string str to a binary. Examples: > SELECT unbase64('U3BhcmsgU1FM'); Spark SQL unhex \u00b6 unhex(expr) - Converts hexadecimal expr to binary. Examples: > SELECT decode(unhex('537061726B2053514C'), 'UTF-8'); Spark SQL unix_timestamp \u00b6 unix_timestamp([expr[, pattern]]) - Returns the UNIX timestamp of current or specified time. Examples: > SELECT unix_timestamp(); 1476884637 > SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd'); 1460041200 upper \u00b6 upper(str) - Returns str with all characters changed to uppercase. Examples: > SELECT upper('SparkSql'); SPARKSQL upper_bound \u00b6 upper_bound() - Upper value of an estimate interval for a given confidence.calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: > SELECT sum(ArrDelay) ArrivalDelay, upper_bound(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 11135.5, Jan \"``` ### var_pop var_pop(expr) - Returns the population variance calculated from values of a group. No example/argument for var_pop. ### var_samp var_samp(expr) - Returns the sample variance calculated from values of a group. No example/argument for var_samp. ### variance variance(expr) - Returns the sample variance calculated from values of a group. No example/argument for variance. ### weekofyear weekofyear(date) - Returns the week of the year of the given date. Examples: SELECT weekofyear('2008-02-20'); 8 ### when CASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END - When `expr1` = true, returns `expr2`; when `expr3` = true, return `expr4`; else return `expr5`. No example/argument for when. ### window ### xpath xpath(xml, xpath) - Returns a string array of values within the nodes of xml that match the XPath expression. Examples: SELECT xpath(' b1 b2 b3 c1 c2 ','a/b/text()'); ['b1','b2','b3'] ### xpath_boolean xpath_boolean(xml, xpath) - Returns true if the XPath expression evaluates to true, or if a matching node is found. Examples: SELECT xpath_boolean(' 1 ','a/b'); true ### xpath_double xpath_double(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_double(' 1 2 ', 'sum(a/b)'); 3.0 ### xpath_float xpath_float(xml, xpath) - Returns a float value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_float(' 1 2 ', 'sum(a/b)'); 3.0 ### xpath_int xpath_int(xml, xpath) - Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_int(' 1 2 ', 'sum(a/b)'); 3 ### xpath_long xpath_long(xml, xpath) - Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_long(' 1 2 ', 'sum(a/b)'); 3 ### xpath_number xpath_number(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_number(' 1 2 ', 'sum(a/b)'); 3.0 ### xpath_short xpath_short(xml, xpath) - Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_short(' 1 2 ', 'sum(a/b)'); 3 ### xpath_string xpath_string(xml, xpath) - Returns the text contents of the first xml node that matches the XPath expression. Examples: SELECT xpath_string(' b cc ','a/c'); cc ### year year(date) - Returns the year component of the date/timestamp. Examples: SELECT year('2016-07-30'); 2016 ### | expr1 | expr2 - Returns the result of bitwise OR of `expr1` and `expr2`. Examples: SELECT 3 | 5; 7 ### ~ ~ expr - Returns the result of bitwise NOT of `expr`. Examples: SELECT ~ 0; -1 ```","title":"Index"},{"location":"reference/sql_functions/#_1","text":"! expr - Logical not. No example/argument for !.","title":"!"},{"location":"reference/sql_functions/#_2","text":"expr1 % expr2 - Returns the remainder after expr1 / expr2 . Examples: > SELECT 2 % 1.8; 0.2","title":"%"},{"location":"reference/sql_functions/#_3","text":"expr1 & expr2 - Returns the result of bitwise AND of expr1 and expr2 . Examples: > SELECT 3 & 5; 1","title":"&amp;"},{"location":"reference/sql_functions/#_4","text":"expr1 * expr2 - Returns expr1 * expr2 . Examples: > SELECT 2 * 3; 6","title":"*"},{"location":"reference/sql_functions/#_5","text":"expr1 + expr2 - Returns expr1 + expr2 . Examples: > SELECT 1 + 2; 3","title":"+"},{"location":"reference/sql_functions/#-","text":"expr1 - expr2 - Returns expr1 - expr2 . Examples: > SELECT 2 - 1; 1","title":"-"},{"location":"reference/sql_functions/#_6","text":"expr1 / expr2 - Returns expr1 / expr2 . It always performs floating point division. Examples: > SELECT 3 / 2; 1.5 > SELECT 2L / 2L; 1.0","title":"/"},{"location":"reference/sql_functions/#_7","text":"expr1 < expr2 - Returns true if expr1 is less than expr2 . No example/argument for <.","title":"&lt;"},{"location":"reference/sql_functions/#_8","text":"expr1 <= expr2 - Returns true if expr1 is less than or equal to expr2 . No example/argument for <=.","title":"&lt;="},{"location":"reference/sql_functions/#_9","text":"expr1 <=> expr2 - Returns same result as the EQUAL(=) operator for non-null operands, but returns true if both are null, false if one of the them is null. No example/argument for <=>.","title":"&lt;=&gt;"},{"location":"reference/sql_functions/#_10","text":"expr1 = expr2 - Returns true if expr1 equals expr2 , or false otherwise. No example/argument for =.","title":"="},{"location":"reference/sql_functions/#_11","text":"expr1 == expr2 - Returns true if expr1 equals expr2 , or false otherwise. No example/argument for ==.","title":"=="},{"location":"reference/sql_functions/#_12","text":"expr1 > expr2 - Returns true if expr1 is greater than expr2 . No example/argument for >.","title":"&gt;"},{"location":"reference/sql_functions/#_13","text":"expr1 >= expr2 - Returns true if expr1 is greater than or equal to expr2 . No example/argument for >=.","title":"&gt;="},{"location":"reference/sql_functions/#_14","text":"expr1 ^ expr2 - Returns the result of bitwise exclusive OR of expr1 and expr2 . Examples: > SELECT 3 ^ 5; 2","title":"^"},{"location":"reference/sql_functions/#abs","text":"abs(expr) - Returns the absolute value of the numeric value. Examples: > SELECT abs(-1); 1","title":"abs"},{"location":"reference/sql_functions/#absolute_error","text":"absolute_error() - Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: > SELECT sum(ArrDelay) ArrivalDelay, absolute_error(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 43.4, Jan","title":"absolute_error"},{"location":"reference/sql_functions/#acos","text":"acos(expr) - Returns the inverse cosine (a.k.a. arccosine) of expr if -1<= expr <=1 or NaN otherwise. Examples: > SELECT acos(1); 0.0 > SELECT acos(2); NaN","title":"acos"},{"location":"reference/sql_functions/#add_months","text":"add_months(start_date, num_months) - Returns the date that is num_months after start_date . Examples: > SELECT add_months('2016-08-31', 1); 2016-09-30","title":"add_months"},{"location":"reference/sql_functions/#and","text":"expr1 and expr2 - Logical AND. No example/argument for and.","title":"and"},{"location":"reference/sql_functions/#approx_count_distinct","text":"approx_count_distinct(expr[, relativeSD]) - Returns the estimated cardinality by HyperLogLog++. relativeSD defines the maximum estimation error allowed. No example/argument for approx_count_distinct.","title":"approx_count_distinct"},{"location":"reference/sql_functions/#approx_percentile","text":"approx_percentile(col, percentage [, accuracy]) - Returns the approximate percentile value of numeric column col at the given percentage. The value of percentage must be between 0.0 and 1.0. The accuracy parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error of the approximation. When percentage is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column col at the given percentage array. Examples: > SELECT approx_percentile(10.0, array(0.5, 0.4, 0.1), 100); [10.0,10.0,10.0] > SELECT approx_percentile(10.0, 0.5, 100); 10.0","title":"approx_percentile"},{"location":"reference/sql_functions/#array","text":"array(expr, ...) - Returns an array with the given elements. Examples: > SELECT array(1, 2, 3); [1,2,3]","title":"array"},{"location":"reference/sql_functions/#array_contains","text":"array_contains(array, value) - Returns true if the array contains the value. Examples: > SELECT array_contains(array(1, 2, 3), 2); true","title":"array_contains"},{"location":"reference/sql_functions/#ascii","text":"ascii(str) - Returns the numeric value of the first character of str . Examples: > SELECT ascii('222'); 50 > SELECT ascii(2); 50","title":"ascii"},{"location":"reference/sql_functions/#asin","text":"asin(expr) - Returns the inverse sine (a.k.a. arcsine) the arc sin of expr if -1<= expr <=1 or NaN otherwise. Examples: > SELECT asin(0); 0.0 > SELECT asin(2); NaN","title":"asin"},{"location":"reference/sql_functions/#assert_true","text":"assert_true(expr) - Throws an exception if expr is not true. Examples: > SELECT assert_true(0 < 1); NULL","title":"assert_true"},{"location":"reference/sql_functions/#atan","text":"atan(expr) - Returns the inverse tangent (a.k.a. arctangent). Examples: > SELECT atan(0); 0.0","title":"atan"},{"location":"reference/sql_functions/#atan2","text":"atan2(expr1, expr2) - Returns the angle in radians between the positive x-axis of a plane and the point given by the coordinates ( expr1 , expr2 ). Examples: > SELECT atan2(0, 0); 0.0","title":"atan2"},{"location":"reference/sql_functions/#avg","text":"avg(expr) - Returns the mean calculated from values of a group. No example/argument for avg.","title":"avg"},{"location":"reference/sql_functions/#base64","text":"base64(bin) - Converts the argument from a binary bin to a base 64 string. Examples: > SELECT base64('Spark SQL'); U3BhcmsgU1FM","title":"base64"},{"location":"reference/sql_functions/#bigint","text":"bigint(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT bigint('10' as int); 10","title":"bigint"},{"location":"reference/sql_functions/#bin","text":"bin(expr) - Returns the string representation of the long value expr represented in binary. Examples: > SELECT bin(13); 1101 > SELECT bin(-13); 1111111111111111111111111111111111111111111111111111111111110011 > SELECT bin(13.3); 1101","title":"bin"},{"location":"reference/sql_functions/#binary","text":"binary(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT binary('10' as int); 10","title":"binary"},{"location":"reference/sql_functions/#boolean","text":"boolean(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT boolean('10' as int); 10","title":"boolean"},{"location":"reference/sql_functions/#bround","text":"bround(expr, d) - Returns expr rounded to d decimal places using HALF_EVEN rounding mode. Examples: > SELECT bround(2.5, 0); 2.0","title":"bround"},{"location":"reference/sql_functions/#cbrt","text":"cbrt(expr) - Returns the cube root of expr . Examples: > SELECT cbrt(27.0); 3.0","title":"cbrt"},{"location":"reference/sql_functions/#ceil","text":"ceil(expr) - Returns the smallest integer not smaller than expr . Examples: > SELECT ceil(-0.1); 0 > SELECT ceil(5); 5","title":"ceil"},{"location":"reference/sql_functions/#ceiling","text":"ceiling(expr) - Returns the smallest integer not smaller than expr . Examples: > SELECT ceiling(-0.1); 0 > SELECT ceiling(5); 5","title":"ceiling"},{"location":"reference/sql_functions/#coalesce","text":"coalesce(expr1, expr2, ...) - Returns the first non-null argument if exists. Otherwise, null. Examples: > SELECT coalesce(NULL, 1, NULL); 1","title":"coalesce"},{"location":"reference/sql_functions/#collect_list","text":"collect_list(expr) - Collects and returns a list of non-unique elements. No example/argument for collect_list.","title":"collect_list"},{"location":"reference/sql_functions/#collect_set","text":"collect_set(expr) - Collects and returns a set of unique elements. No example/argument for collect_set.","title":"collect_set"},{"location":"reference/sql_functions/#concat","text":"concat(str1, str2, ..., strN) - Returns the concatenation of str1, str2, ..., strN. Examples: > SELECT concat('Spark', 'SQL'); SparkSQL","title":"concat"},{"location":"reference/sql_functions/#concat_ws","text":"concat_ws(sep, [str | array(str)]+) - Returns the concatenation of the strings separated by sep . Examples: > SELECT concat_ws(' ', 'Spark', 'SQL'); Spark SQL","title":"concat_ws"},{"location":"reference/sql_functions/#conv","text":"conv(num, from_base, to_base) - Convert num from from_base to to_base . Examples: > SELECT conv('100', 2, 10); 4 > SELECT conv(-10, 16, -10); 16","title":"conv"},{"location":"reference/sql_functions/#corr","text":"corr(expr1, expr2) - Returns Pearson coefficient of correlation between a set of number pairs. No example/argument for corr.","title":"corr"},{"location":"reference/sql_functions/#cos","text":"cos(expr) - Returns the cosine of expr . Examples: > SELECT cos(0); 1.0","title":"cos"},{"location":"reference/sql_functions/#cosh","text":"cosh(expr) - Returns the hyperbolic cosine of expr . Examples: > SELECT cosh(0); 1.0","title":"cosh"},{"location":"reference/sql_functions/#count","text":"count(*) - Returns the total number of retrieved rows, including rows containing null. count(expr) - Returns the number of rows for which the supplied expression is non-null. count(DISTINCT expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are unique and non-null. No example/argument for count.","title":"count"},{"location":"reference/sql_functions/#covar_pop","text":"covar_pop(expr1, expr2) - Returns the population covariance of a set of number pairs. No example/argument for covar_pop.","title":"covar_pop"},{"location":"reference/sql_functions/#covar_samp","text":"covar_samp(expr1, expr2) - Returns the sample covariance of a set of number pairs. No example/argument for covar_samp.","title":"covar_samp"},{"location":"reference/sql_functions/#crc32","text":"crc32(expr) - Returns a cyclic redundancy check value of the expr as a bigint. Examples: > SELECT crc32('Spark'); 1557323817","title":"crc32"},{"location":"reference/sql_functions/#cube","text":"","title":"cube"},{"location":"reference/sql_functions/#cume_dist","text":"cume_dist() - Computes the position of a value relative to all values in the partition. No example/argument for cume_dist.","title":"cume_dist"},{"location":"reference/sql_functions/#current_database","text":"current_database() - Returns the current database. Examples: > SELECT current_database(); default","title":"current_database"},{"location":"reference/sql_functions/#current_date","text":"current_date() - Returns the current date at the start of query evaluation. No example/argument for current_date.","title":"current_date"},{"location":"reference/sql_functions/#current_schema","text":"current_schema() - Returns the current database. Examples: > SELECT current_schema(); default","title":"current_schema"},{"location":"reference/sql_functions/#current_timestamp","text":"current_timestamp() - Returns the current timestamp at the start of query evaluation. No example/argument for current_timestamp.","title":"current_timestamp"},{"location":"reference/sql_functions/#current_user","text":"current_user() - Returns the name of the user that owns the session executing the current SQL statement. Examples: > SELECT current_user(); USER1","title":"current_user"},{"location":"reference/sql_functions/#current_user_ldap_groups","text":"current_user_ldap_groups() - Returns all the ldap groups as an ARRAY to which the user who is executing the current SQL statement belongs. Examples: > SELECT array_contains(current_user_ldap_groups(), 'GROUP1'); true","title":"current_user_ldap_groups"},{"location":"reference/sql_functions/#date","text":"date(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT date('10' as int); 10","title":"date"},{"location":"reference/sql_functions/#date_add","text":"date_add(start_date, num_days) - Returns the date that is num_days after start_date . Examples: > SELECT date_add('2016-07-30', 1); 2016-07-31","title":"date_add"},{"location":"reference/sql_functions/#date_format","text":"date_format(timestamp, fmt) - Converts timestamp to a value of string in the format specified by the date format fmt . Examples: > SELECT date_format('2016-04-08', 'y'); 2016","title":"date_format"},{"location":"reference/sql_functions/#date_sub","text":"date_sub(start_date, num_days) - Returns the date that is num_days before start_date . Examples: > SELECT date_sub('2016-07-30', 1); 2016-07-29","title":"date_sub"},{"location":"reference/sql_functions/#datediff","text":"datediff(endDate, startDate) - Returns the number of days from startDate to endDate . Examples: > SELECT datediff('2009-07-31', '2009-07-30'); 1 > SELECT datediff('2009-07-30', '2009-07-31'); -1","title":"datediff"},{"location":"reference/sql_functions/#day","text":"day(date) - Returns the day of month of the date/timestamp. Examples: > SELECT day('2009-07-30'); 30","title":"day"},{"location":"reference/sql_functions/#dayofmonth","text":"dayofmonth(date) - Returns the day of month of the date/timestamp. Examples: > SELECT dayofmonth('2009-07-30'); 30","title":"dayofmonth"},{"location":"reference/sql_functions/#dayofyear","text":"dayofyear(date) - Returns the day of year of the date/timestamp. Examples: > SELECT dayofyear('2016-04-09'); 100","title":"dayofyear"},{"location":"reference/sql_functions/#decimal","text":"decimal(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT decimal('10' as int); 10","title":"decimal"},{"location":"reference/sql_functions/#decode","text":"decode(bin, charset) - Decodes the first argument using the second argument character set. Examples: > SELECT decode(encode('abc', 'utf-8'), 'utf-8'); abc","title":"decode"},{"location":"reference/sql_functions/#degrees","text":"degrees(expr) - Converts radians to degrees. Examples: > SELECT degrees(3.141592653589793); 180.0","title":"degrees"},{"location":"reference/sql_functions/#dense_rank","text":"dense_rank() - Computes the rank of a value in a group of values. The result is one plus the previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps in the ranking sequence. No example/argument for dense_rank.","title":"dense_rank"},{"location":"reference/sql_functions/#double","text":"double(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT double('10' as int); 10","title":"double"},{"location":"reference/sql_functions/#dsid","text":"dsid() - Returns the unique distributed member ID of executor fetching current row. Examples: > SELECT dsid(); localhost(1831)<v2>:18165","title":"dsid"},{"location":"reference/sql_functions/#e","text":"e() - Returns Euler's number, e. Examples: > SELECT e(); 2.718281828459045","title":"e"},{"location":"reference/sql_functions/#elt","text":"elt(n, str1, str2, ...) - Returns the n -th string, e.g., returns str2 when n is 2. Examples: > SELECT elt(1, 'scala', 'java'); scala","title":"elt"},{"location":"reference/sql_functions/#encode","text":"encode(str, charset) - Encodes the first argument using the second argument character set. Examples: > SELECT encode('abc', 'utf-8'); abc","title":"encode"},{"location":"reference/sql_functions/#exp","text":"exp(expr) - Returns e to the power of expr . Examples: > SELECT exp(0); 1.0","title":"exp"},{"location":"reference/sql_functions/#explode","text":"explode(expr) - Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Examples: > SELECT explode(array(10, 20)); 10 20","title":"explode"},{"location":"reference/sql_functions/#expm1","text":"expm1(expr) - Returns exp( expr ) - 1. Examples: > SELECT expm1(0); 0.0","title":"expm1"},{"location":"reference/sql_functions/#factorial","text":"factorial(expr) - Returns the factorial of expr . expr is [0..20]. Otherwise, null. Examples: > SELECT factorial(5); 120","title":"factorial"},{"location":"reference/sql_functions/#find_in_set","text":"find_in_set(str, str_array) - Returns the index (1-based) of the given string ( str ) in the comma-delimited list ( str_array ). Returns 0, if the string was not found or if the given string ( str ) contains a comma. Examples: > SELECT find_in_set('ab','abc,b,ab,c,def'); 3","title":"find_in_set"},{"location":"reference/sql_functions/#first","text":"first(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for first.","title":"first"},{"location":"reference/sql_functions/#first_value","text":"first_value(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for first_value.","title":"first_value"},{"location":"reference/sql_functions/#float","text":"float(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT float('10' as int); 10","title":"float"},{"location":"reference/sql_functions/#floor","text":"floor(expr) - Returns the largest integer not greater than expr . Examples: > SELECT floor(-0.1); -1 > SELECT floor(5); 5","title":"floor"},{"location":"reference/sql_functions/#format_number","text":"format_number(expr1, expr2) - Formats the number expr1 like '#,###,###.##', rounded to expr2 decimal places. If expr2 is 0, the result has no decimal point or fractional part. This is supposed to function like MySQL's FORMAT. Examples: > SELECT format_number(12332.123456, 4); 12,332.1235","title":"format_number"},{"location":"reference/sql_functions/#format_string","text":"format_string(strfmt, obj, ...) - Returns a formatted string from printf-style format strings. Examples: > SELECT format_string(\"Hello World %d %s\", 100, \"days\"); Hello World 100 days","title":"format_string"},{"location":"reference/sql_functions/#from_unixtime","text":"from_unixtime(unix_time, format) - Returns unix_time in the specified format . Examples: > SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss'); 1970-01-01 00:00:00","title":"from_unixtime"},{"location":"reference/sql_functions/#from_utc_timestamp","text":"from_utc_timestamp(timestamp, timezone) - Given a timestamp, which corresponds to a certain time of day in UTC, returns another timestamp that corresponds to the same time of day in the given timezone. Examples: > SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul'); 2016-08-31 09:00:00","title":"from_utc_timestamp"},{"location":"reference/sql_functions/#get_json_object","text":"get_json_object(json_txt, path) - Extracts a json object from path . Examples: > SELECT get_json_object('{\"a\":\"b\"}', '$.a'); b","title":"get_json_object"},{"location":"reference/sql_functions/#greatest","text":"greatest(expr, ...) - Returns the greatest value of all parameters, skipping null values. Examples: > SELECT greatest(10, 9, 2, 4, 3); 10","title":"greatest"},{"location":"reference/sql_functions/#grouping","text":"","title":"grouping"},{"location":"reference/sql_functions/#grouping_id","text":"","title":"grouping_id"},{"location":"reference/sql_functions/#hash","text":"hash(expr1, expr2, ...) - Returns a hash value of the arguments. Examples: > SELECT hash('Spark', array(123), 2); -1321691492","title":"hash"},{"location":"reference/sql_functions/#hex","text":"hex(expr) - Converts expr to hexadecimal. Examples: > SELECT hex(17); 11 > SELECT hex('Spark SQL'); 537061726B2053514C","title":"hex"},{"location":"reference/sql_functions/#hour","text":"hour(timestamp) - Returns the hour component of the string/timestamp. Examples: > SELECT hour('2009-07-30 12:58:59'); 12","title":"hour"},{"location":"reference/sql_functions/#hypot","text":"hypot(expr1, expr2) - Returns sqrt( expr1 2 + expr2 2). Examples: > SELECT hypot(3, 4); 5.0","title":"hypot"},{"location":"reference/sql_functions/#if","text":"if(expr1, expr2, expr3) - If expr1 evaluates to true, then returns expr2 ; otherwise returns expr3 . Examples: > SELECT if(1 < 2, 'a', 'b'); a","title":"if"},{"location":"reference/sql_functions/#ifnull","text":"ifnull(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise. Examples: > SELECT ifnull(NULL, array('2')); [\"2\"]","title":"ifnull"},{"location":"reference/sql_functions/#in","text":"expr1 in(expr2, expr3, ...) - Returns true if expr equals to any valN. No example/argument for in.","title":"in"},{"location":"reference/sql_functions/#initcap","text":"initcap(str) - Returns str with the first letter of each word in uppercase. All other letters are in lowercase. Words are delimited by white space. Examples: > SELECT initcap('sPark sql'); Spark Sql","title":"initcap"},{"location":"reference/sql_functions/#inline","text":"inline(expr) - Explodes an array of structs into a table. Examples: > SELECT inline(array(struct(1, 'a'), struct(2, 'b'))); 1 a 2 b","title":"inline"},{"location":"reference/sql_functions/#input_file_name","text":"input_file_name() - Returns the name of the current file being read if available. No example/argument for input_file_name.","title":"input_file_name"},{"location":"reference/sql_functions/#instr","text":"instr(str, substr) - Returns the (1-based) index of the first occurrence of substr in str . Examples: > SELECT instr('SparkSQL', 'SQL'); 6","title":"instr"},{"location":"reference/sql_functions/#int","text":"int(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT int('10' as int); 10","title":"int"},{"location":"reference/sql_functions/#isnan","text":"isnan(expr) - Returns true if expr is NaN, or false otherwise. Examples: > SELECT isnan(cast('NaN' as double)); true","title":"isnan"},{"location":"reference/sql_functions/#isnotnull","text":"isnotnull(expr) - Returns true if expr is not null, or false otherwise. Examples: > SELECT isnotnull(1); true","title":"isnotnull"},{"location":"reference/sql_functions/#isnull","text":"isnull(expr) - Returns true if expr is null, or false otherwise. Examples: > SELECT isnull(1); false","title":"isnull"},{"location":"reference/sql_functions/#java_method","text":"java_method(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection. Examples: > SELECT java_method('java.util.UUID', 'randomUUID'); c33fb387-8500-4bfa-81d2-6e0e3e930df2 > SELECT java_method('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2'); a5cf6c42-0c85-418f-af6c-3e4e5b1328f2","title":"java_method"},{"location":"reference/sql_functions/#json_tuple","text":"json_tuple(jsonStr, p1, p2, ..., pn) - Return a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string. Examples: > SELECT json_tuple('{\"a\":1, \"b\":2}', 'a', 'b'); 1 2","title":"json_tuple"},{"location":"reference/sql_functions/#kurtosis","text":"kurtosis(expr) - Returns the kurtosis value calculated from values of a group. No example/argument for kurtosis.","title":"kurtosis"},{"location":"reference/sql_functions/#lag","text":"lag(input[, offset[, default]]) - Returns the value of input at the offset th row before the current row in the window. The default value of offset is 1 and the default value of default is null. If the value of input at the offset th row is null, null is returned. If there is no such offset row (e.g., when the offset is 1, the first row of the window does not have any previous row), default is returned. No example/argument for lag.","title":"lag"},{"location":"reference/sql_functions/#last","text":"last(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for last.","title":"last"},{"location":"reference/sql_functions/#last_day","text":"last_day(date) - Returns the last day of the month which the date belongs to. Examples: > SELECT last_day('2009-01-12'); 2009-01-31","title":"last_day"},{"location":"reference/sql_functions/#last_value","text":"last_value(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values. No example/argument for last_value.","title":"last_value"},{"location":"reference/sql_functions/#lcase","text":"lcase(str) - Returns str with all characters changed to lowercase. Examples: > SELECT lcase('SparkSql'); sparksql","title":"lcase"},{"location":"reference/sql_functions/#lead","text":"lead(input[, offset[, default]]) - Returns the value of input at the offset th row after the current row in the window. The default value of offset is 1 and the default value of default is null. If the value of input at the offset th row is null, null is returned. If there is no such an offset row (e.g., when the offset is 1, the last row of the window does not have any subsequent row), default is returned. No example/argument for lead.","title":"lead"},{"location":"reference/sql_functions/#least","text":"least(expr, ...) - Returns the least value of all parameters, skipping null values. Examples: > SELECT least(10, 9, 2, 4, 3); 2","title":"least"},{"location":"reference/sql_functions/#length","text":"length(expr) - Returns the length of expr or number of bytes in binary data. Examples: > SELECT length('Spark SQL'); 9","title":"length"},{"location":"reference/sql_functions/#levenshtein","text":"levenshtein(str1, str2) - Returns the Levenshtein distance between the two given strings. Examples: > SELECT levenshtein('kitten', 'sitting'); 3","title":"levenshtein"},{"location":"reference/sql_functions/#like","text":"str like pattern - Returns true if str matches pattern, null if any arguments are null, false otherwise. Arguments: str - a string expression pattern - a string expression. The pattern is a string which is matched literally, with exception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regular expressions) The escape character is '\\'. If an escape character precedes a special symbol or another escape character, the following character is matched literally. It is invalid to escape any other character. Examples: > SELECT '%SystemDrive%\\Users\\John' like '\\%SystemDrive\\%\\\\Users%' true See also: Use RLIKE to match with standard regular expressions.","title":"like"},{"location":"reference/sql_functions/#ln","text":"ln(expr) - Returns the natural logarithm (base e) of expr . Examples: > SELECT ln(1); 0.0","title":"ln"},{"location":"reference/sql_functions/#locate","text":"locate(substr, str[, pos]) - Returns the position of the first occurrence of substr in str after position pos . The given pos and return value are 1-based. Examples: > SELECT locate('bar', 'foobarbar', 5); 7","title":"locate"},{"location":"reference/sql_functions/#log","text":"log(base, expr) - Returns the logarithm of expr with base . Examples: > SELECT log(10, 100); 2.0","title":"log"},{"location":"reference/sql_functions/#log10","text":"log10(expr) - Returns the logarithm of expr with base 10. Examples: > SELECT log10(10); 1.0","title":"log10"},{"location":"reference/sql_functions/#log1p","text":"log1p(expr) - Returns log(1 + expr ). Examples: > SELECT log1p(0); 0.0","title":"log1p"},{"location":"reference/sql_functions/#log2","text":"log2(expr) - Returns the logarithm of expr with base 2. Examples: > SELECT log2(2); 1.0","title":"log2"},{"location":"reference/sql_functions/#lower","text":"lower(str) - Returns str with all characters changed to lowercase. Examples: > SELECT lower('SparkSql'); sparksql","title":"lower"},{"location":"reference/sql_functions/#lower_bound","text":"lower_bound() - Lower value of an estimate interval for a given confidence.calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: > SELECT sum(ArrDelay) ArrivalDelay, lower_bound(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 11101.5, Jan \"``` ### lpad lpad(str, len, pad) - Returns `str`, left-padded with `pad` to a length of `len`. If `str` is longer than `len`, the return value is shortened to `len` characters. Examples: SELECT lpad('hi', 5, '??'); ???hi SELECT lpad('hi', 1, '??'); h ### ltrim ltrim(str) - Removes the leading and trailing space characters from `str`. Examples: SELECT ltrim(' SparkSQL'); SparkSQL ### map map(key0, value0, key1, value1, ...) - Creates a map with the given key/value pairs. Examples: SELECT map(1.0, '2', 3.0, '4'); {1.0:\"2\",3.0:\"4\"} ### map_keys map_keys(map) - Returns an unordered array containing the keys of the map. Examples: SELECT map_keys(map(1, 'a', 2, 'b')); [1,2] ### map_values map_values(map) - Returns an unordered array containing the values of the map. Examples: SELECT map_values(map(1, 'a', 2, 'b')); [\"a\",\"b\"] ### max max(expr) - Returns the maximum value of `expr`. No example/argument for max. ### md5 md5(expr) - Returns an MD5 128-bit checksum as a hex string of `expr`. Examples: SELECT md5('Spark'); 8cde774d6f7333752ed72cacddb05126 ### mean mean(expr) - Returns the mean calculated from values of a group. No example/argument for mean. ### min min(expr) - Returns the minimum value of `expr`. No example/argument for min. ### minute minute(timestamp) - Returns the minute component of the string/timestamp. Examples: SELECT minute('2009-07-30 12:58:59'); 58 ### monotonically_increasing_id monotonically_increasing_id() - Returns monotonically increasing 64-bit integers. The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the lower 33 bits represent the record number within each partition. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records. No example/argument for monotonically_increasing_id. ### month month(date) - Returns the month component of the date/timestamp. Examples: SELECT month('2016-07-30'); 7 ### months_between months_between(timestamp1, timestamp2) - Returns number of months between `timestamp1` and `timestamp2`. Examples: SELECT months_between('1997-02-28 10:30:00', '1996-10-30'); 3.94959677 ### named_struct named_struct(name1, val1, name2, val2, ...) - Creates a struct with the given field names and values. Examples: SELECT named_struct(\"a\", 1, \"b\", 2, \"c\", 3); {\"a\":1,\"b\":2,\"c\":3} ### nanvl nanvl(expr1, expr2) - Returns `expr1` if it's not NaN, or `expr2` otherwise. Examples: SELECT nanvl(cast('NaN' as double), 123); 123.0 ### negative negative(expr) - Returns the negated value of `expr`. Examples: SELECT negative(1); -1 ### next_day next_day(start_date, day_of_week) - Returns the first date which is later than `start_date` and named as indicated. Examples: SELECT next_day('2015-01-14', 'TU'); 2015-01-20 ### not not expr - Logical not. No example/argument for not. ### now now() - Returns the current timestamp at the start of query evaluation. No example/argument for now. ### ntile ntile(n) - Divides the rows for each window partition into `n` buckets ranging from 1 to at most `n`. No example/argument for ntile. ### nullif nullif(expr1, expr2) - Returns null if `expr1` equals to `expr2`, or `expr1` otherwise. Examples: SELECT nullif(2, 2); NULL ### nvl nvl(expr1, expr2) - Returns `expr2` if `expr1` is null, or `expr1` otherwise. Examples: SELECT nvl(NULL, array('2')); [\"2\"] ### nvl2 nvl2(expr1, expr2, expr3) - Returns `expr2` if `expr1` is not null, or `expr3` otherwise. Examples: SELECT nvl2(NULL, 2, 1); 1 ### or expr1 or expr2 - Logical OR. No example/argument for or. ### parse_url parse_url(url, partToExtract[, key]) - Extracts a part from a URL. Examples: SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST') spark.apache.org SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY') query=1 SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query') 1 ### percent_rank percent_rank() - Computes the percentage ranking of a value in a group of values. No example/argument for percent_rank. ### percentile percentile(col, percentage) - Returns the exact percentile value of numeric column `col` at the given percentage. The value of percentage must be between 0.0 and 1.0. percentile(col, array(percentage1 [, percentage2]...)) - Returns the exact percentile value array of numeric column `col` at the given percentage(s). Each value of the percentage array must be between 0.0 and 1.0. No example/argument for percentile. ### percentile_approx percentile_approx(col, percentage [, accuracy]) - Returns the approximate percentile value of numeric column `col` at the given percentage. The value of percentage must be between 0.0 and 1.0. The `accuracy` parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of `accuracy` yields better accuracy, `1.0/accuracy` is the relative error of the approximation. When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column `col` at the given percentage array. Examples: SELECT percentile_approx(10.0, array(0.5, 0.4, 0.1), 100); [10.0,10.0,10.0] SELECT percentile_approx(10.0, 0.5, 100); 10.0 ### pi pi() - Returns pi. Examples: SELECT pi(); 3.141592653589793 ### pmod pmod(expr1, expr2) - Returns the positive value of `expr1` mod `expr2`. Examples: SELECT pmod(10, 3); 1 SELECT pmod(-10, 3); 2 ### posexplode posexplode(expr) - Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Examples: SELECT posexplode(array(10,20)); 0 10 1 20 ### positive positive(expr) - Returns the value of `expr`. No example/argument for positive. ### pow pow(expr1, expr2) - Raises `expr1` to the power of `expr2`. Examples: SELECT pow(2, 3); 8.0 ### power power(expr1, expr2) - Raises `expr1` to the power of `expr2`. Examples: SELECT power(2, 3); 8.0 ### printf printf(strfmt, obj, ...) - Returns a formatted string from printf-style format strings. Examples: SELECT printf(\"Hello World %d %s\", 100, \"days\"); Hello World 100 days ### quarter quarter(date) - Returns the quarter of the year for date, in the range 1 to 4. Examples: SELECT quarter('2016-08-31'); 3 ### radians radians(expr) - Converts degrees to radians. Examples: SELECT radians(180); 3.141592653589793 ### rand rand([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1). Examples: SELECT rand(); 0.9629742951434543 SELECT rand(0); 0.8446490682263027 SELECT rand(null); 0.8446490682263027 ### randn randn([seed]) - Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution. Examples: SELECT randn(); -0.3254147983080288 SELECT randn(0); 1.1164209726833079 SELECT randn(null); 1.1164209726833079 ### rank rank() - Computes the rank of a value in a group of values. The result is one plus the number of rows preceding or equal to the current row in the ordering of the partition. The values will produce gaps in the sequence. No example/argument for rank. ### reflect reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection. Examples: SELECT reflect('java.util.UUID', 'randomUUID'); c33fb387-8500-4bfa-81d2-6e0e3e930df2 SELECT reflect('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2'); a5cf6c42-0c85-418f-af6c-3e4e5b1328f2 ### regexp_extract regexp_extract(str, regexp[, idx]) - Extracts a group that matches `regexp`. Examples: SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1); 100 ### regexp_replace regexp_replace(str, regexp, rep) - Replaces all substrings of `str` that match `regexp` with `rep`. Examples: SELECT regexp_replace('100-200', '(\\d+)', 'num'); num-num ### relative_error relative_error() - Indicates ratio of absolute error to estimate (approx answer). calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: SELECT sum(ArrDelay) ArrivalDelay, relative_error(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 0.3, Jan \"```","title":"lower_bound"},{"location":"reference/sql_functions/#repeat","text":"repeat(str, n) - Returns the string which repeats the given string value n times. Examples: > SELECT repeat('123', 2); 123123","title":"repeat"},{"location":"reference/sql_functions/#reverse","text":"reverse(str) - Returns the reversed given string. Examples: > SELECT reverse('Spark SQL'); LQS krapS","title":"reverse"},{"location":"reference/sql_functions/#rint","text":"rint(expr) - Returns the double value that is closest in value to the argument and is equal to a mathematical integer. Examples: > SELECT rint(12.3456); 12.0","title":"rint"},{"location":"reference/sql_functions/#rlike","text":"str rlike regexp - Returns true if str matches regexp , or false otherwise. No example/argument for rlike.","title":"rlike"},{"location":"reference/sql_functions/#rollup","text":"","title":"rollup"},{"location":"reference/sql_functions/#round","text":"round(expr, d) - Returns expr rounded to d decimal places using HALF_UP rounding mode. Examples: > SELECT round(2.5, 0); 3.0","title":"round"},{"location":"reference/sql_functions/#row_number","text":"row_number() - Assigns a unique, sequential number to each row, starting with one, according to the ordering of rows within the window partition. No example/argument for row_number.","title":"row_number"},{"location":"reference/sql_functions/#rpad","text":"rpad(str, len, pad) - Returns str , right-padded with pad to a length of len . If str is longer than len , the return value is shortened to len characters. Examples: > SELECT rpad('hi', 5, '??'); hi??? > SELECT rpad('hi', 1, '??'); h","title":"rpad"},{"location":"reference/sql_functions/#rtrim","text":"rtrim(str) - Removes the trailing space characters from str . Examples: > SELECT rtrim(' SparkSQL '); SparkSQL","title":"rtrim"},{"location":"reference/sql_functions/#second","text":"second(timestamp) - Returns the second component of the string/timestamp. Examples: > SELECT second('2009-07-30 12:58:59'); 59","title":"second"},{"location":"reference/sql_functions/#sentences","text":"sentences(str[, lang, country]) - Splits str into an array of array of words. Examples: > SELECT sentences('Hi there! Good morning.'); [[\"Hi\",\"there\"],[\"Good\",\"morning\"]]","title":"sentences"},{"location":"reference/sql_functions/#sha","text":"sha(expr) - Returns a sha1 hash value as a hex string of the expr . Examples: > SELECT sha('Spark'); 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c","title":"sha"},{"location":"reference/sql_functions/#sha1","text":"sha1(expr) - Returns a sha1 hash value as a hex string of the expr . Examples: > SELECT sha1('Spark'); 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c","title":"sha1"},{"location":"reference/sql_functions/#sha2","text":"sha2(expr, bitLength) - Returns a checksum of SHA-2 family as a hex string of expr . SHA-224, SHA-256, SHA-384, and SHA-512 are supported. Bit length of 0 is equivalent to 256. Examples: > SELECT sha2('Spark', 256); 529bc3b07127ecb7e53a4dcf1991d9152c24537d919178022b2c42657f79a26b","title":"sha2"},{"location":"reference/sql_functions/#shiftleft","text":"shiftleft(base, expr) - Bitwise left shift. Examples: > SELECT shiftleft(2, 1); 4","title":"shiftleft"},{"location":"reference/sql_functions/#shiftright","text":"shiftright(base, expr) - Bitwise (signed) right shift. Examples: > SELECT shiftright(4, 1); 2","title":"shiftright"},{"location":"reference/sql_functions/#shiftrightunsigned","text":"shiftrightunsigned(base, expr) - Bitwise unsigned right shift. Examples: > SELECT shiftrightunsigned(4, 1); 2","title":"shiftrightunsigned"},{"location":"reference/sql_functions/#sign","text":"sign(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive. Examples: > SELECT sign(40); 1.0","title":"sign"},{"location":"reference/sql_functions/#signum","text":"signum(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive. Examples: > SELECT signum(40); 1.0","title":"signum"},{"location":"reference/sql_functions/#sin","text":"sin(expr) - Returns the sine of expr . Examples: > SELECT sin(0); 0.0","title":"sin"},{"location":"reference/sql_functions/#sinh","text":"sinh(expr) - Returns the hyperbolic sine of expr . Examples: > SELECT sinh(0); 0.0","title":"sinh"},{"location":"reference/sql_functions/#size","text":"size(expr) - Returns the size of an array or a map. Returns -1 if null. Examples: > SELECT size(array('b', 'd', 'c', 'a')); 4","title":"size"},{"location":"reference/sql_functions/#skewness","text":"skewness(expr) - Returns the skewness value calculated from values of a group. No example/argument for skewness.","title":"skewness"},{"location":"reference/sql_functions/#smallint","text":"smallint(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT smallint('10' as int); 10","title":"smallint"},{"location":"reference/sql_functions/#sort_array","text":"sort_array(array[, ascendingOrder]) - Sorts the input array in ascending or descending order according to the natural ordering of the array elements. Examples: > SELECT sort_array(array('b', 'd', 'c', 'a'), true); [\"a\",\"b\",\"c\",\"d\"]","title":"sort_array"},{"location":"reference/sql_functions/#soundex","text":"soundex(str) - Returns Soundex code of the string. Examples: > SELECT soundex('Miller'); M460","title":"soundex"},{"location":"reference/sql_functions/#space","text":"space(n) - Returns a string consisting of n spaces. Examples: > SELECT concat(space(2), '1'); 1","title":"space"},{"location":"reference/sql_functions/#spark_partition_id","text":"spark_partition_id() - Returns the current partition id. No example/argument for spark_partition_id.","title":"spark_partition_id"},{"location":"reference/sql_functions/#split","text":"split(str, regex) - Splits str around occurrences that match regex . Examples: > SELECT split('oneAtwoBthreeC', '[ABC]'); [\"one\",\"two\",\"three\",\"\"]","title":"split"},{"location":"reference/sql_functions/#sqrt","text":"sqrt(expr) - Returns the square root of expr . Examples: > SELECT sqrt(4); 2.0","title":"sqrt"},{"location":"reference/sql_functions/#stack","text":"stack(n, expr1, ..., exprk) - Separates expr1 , ..., exprk into n rows. Examples: > SELECT stack(2, 1, 2, 3); 1 2 3 NULL","title":"stack"},{"location":"reference/sql_functions/#std","text":"std(expr) - Returns the sample standard deviation calculated from values of a group. No example/argument for std.","title":"std"},{"location":"reference/sql_functions/#stddev","text":"stddev(expr) - Returns the sample standard deviation calculated from values of a group. No example/argument for stddev.","title":"stddev"},{"location":"reference/sql_functions/#stddev_pop","text":"stddev_pop(expr) - Returns the population standard deviation calculated from values of a group. No example/argument for stddev_pop.","title":"stddev_pop"},{"location":"reference/sql_functions/#stddev_samp","text":"stddev_samp(expr) - Returns the sample standard deviation calculated from values of a group. No example/argument for stddev_samp.","title":"stddev_samp"},{"location":"reference/sql_functions/#str_to_map","text":"str_to_map(text[, pairDelim[, keyValueDelim]]) - Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ',' for pairDelim and ':' for keyValueDelim . Examples: > SELECT str_to_map('a:1,b:2,c:3', ',', ':'); map(\"a\":\"1\",\"b\":\"2\",\"c\":\"3\") > SELECT str_to_map('a'); map(\"a\":null)","title":"str_to_map"},{"location":"reference/sql_functions/#string","text":"string(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT string('10' as int); 10","title":"string"},{"location":"reference/sql_functions/#struct","text":"struct(col1, col2, col3, ...) - Creates a struct with the given field values.","title":"struct"},{"location":"reference/sql_functions/#substr","text":"substr(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len , or the slice of byte array that starts at pos and is of length len . Examples: > SELECT substr('Spark SQL', 5); k SQL > SELECT substr('Spark SQL', -3); SQL > SELECT substr('Spark SQL', 5, 1); k","title":"substr"},{"location":"reference/sql_functions/#substring","text":"substring(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len , or the slice of byte array that starts at pos and is of length len . Examples: > SELECT substring('Spark SQL', 5); k SQL > SELECT substring('Spark SQL', -3); SQL > SELECT substring('Spark SQL', 5, 1); k","title":"substring"},{"location":"reference/sql_functions/#substring_index","text":"substring_index(str, delim, count) - Returns the substring from str before count occurrences of the delimiter delim . If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. The function substring_index performs a case-sensitive match when searching for delim . Examples: > SELECT substring_index('www.apache.org', '.', 2); www.apache","title":"substring_index"},{"location":"reference/sql_functions/#sum","text":"sum(expr) - Returns the sum calculated from values of a group. No example/argument for sum.","title":"sum"},{"location":"reference/sql_functions/#tan","text":"tan(expr) - Returns the tangent of expr . Examples: > SELECT tan(0); 0.0","title":"tan"},{"location":"reference/sql_functions/#tanh","text":"tanh(expr) - Returns the hyperbolic tangent of expr . Examples: > SELECT tanh(0); 0.0","title":"tanh"},{"location":"reference/sql_functions/#timestamp","text":"timestamp(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT timestamp('10' as int); 10","title":"timestamp"},{"location":"reference/sql_functions/#tinyint","text":"tinyint(expr AS type) - Casts the value expr to the target data type type . Examples: > SELECT tinyint('10' as int); 10","title":"tinyint"},{"location":"reference/sql_functions/#to_date","text":"to_date(expr) - Extracts the date part of the date or timestamp expression expr . Examples: > SELECT to_date('2009-07-30 04:17:52'); 2009-07-30","title":"to_date"},{"location":"reference/sql_functions/#to_unix_timestamp","text":"to_unix_timestamp(expr[, pattern]) - Returns the UNIX timestamp of the give time. Examples: > SELECT to_unix_timestamp('2016-04-08', 'yyyy-MM-dd'); 1460041200","title":"to_unix_timestamp"},{"location":"reference/sql_functions/#to_utc_timestamp","text":"to_utc_timestamp(timestamp, timezone) - Given a timestamp, which corresponds to a certain time of day in the given timezone, returns another timestamp that corresponds to the same time of day in UTC. Examples: > SELECT to_utc_timestamp('2016-08-31', 'Asia/Seoul'); 2016-08-30 15:00:00","title":"to_utc_timestamp"},{"location":"reference/sql_functions/#translate","text":"translate(input, from, to) - Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. Examples: > SELECT translate('AaBbCc', 'abc', '123'); A1B2C3","title":"translate"},{"location":"reference/sql_functions/#trim","text":"trim(str) - Removes the leading and trailing space characters from str . Examples: > SELECT trim(' SparkSQL '); SparkSQL","title":"trim"},{"location":"reference/sql_functions/#trunc","text":"trunc(date, fmt) - Returns date with the time portion of the day truncated to the unit specified by the format model fmt . Examples: > SELECT trunc('2009-02-12', 'MM'); 2009-02-01 > SELECT trunc('2015-10-27', 'YEAR'); 2015-01-01","title":"trunc"},{"location":"reference/sql_functions/#ucase","text":"ucase(str) - Returns str with all characters changed to uppercase. Examples: > SELECT ucase('SparkSql'); SPARKSQL","title":"ucase"},{"location":"reference/sql_functions/#unbase64","text":"unbase64(str) - Converts the argument from a base 64 string str to a binary. Examples: > SELECT unbase64('U3BhcmsgU1FM'); Spark SQL","title":"unbase64"},{"location":"reference/sql_functions/#unhex","text":"unhex(expr) - Converts hexadecimal expr to binary. Examples: > SELECT decode(unhex('537061726B2053514C'), 'UTF-8'); Spark SQL","title":"unhex"},{"location":"reference/sql_functions/#unix_timestamp","text":"unix_timestamp([expr[, pattern]]) - Returns the UNIX timestamp of current or specified time. Examples: > SELECT unix_timestamp(); 1476884637 > SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd'); 1460041200","title":"unix_timestamp"},{"location":"reference/sql_functions/#upper","text":"upper(str) - Returns str with all characters changed to uppercase. Examples: > SELECT upper('SparkSql'); SPARKSQL","title":"upper"},{"location":"reference/sql_functions/#upper_bound","text":"upper_bound() - Upper value of an estimate interval for a given confidence.calculated using error estimation method (ClosedForm or Bootstrap). [enterprise] Examples: > SELECT sum(ArrDelay) ArrivalDelay, upper_bound(ArrivalDelay), Month_ from airline group by Month_ order by Month_ desc with error 0.10; 1117.6, 11135.5, Jan \"``` ### var_pop var_pop(expr) - Returns the population variance calculated from values of a group. No example/argument for var_pop. ### var_samp var_samp(expr) - Returns the sample variance calculated from values of a group. No example/argument for var_samp. ### variance variance(expr) - Returns the sample variance calculated from values of a group. No example/argument for variance. ### weekofyear weekofyear(date) - Returns the week of the year of the given date. Examples: SELECT weekofyear('2008-02-20'); 8 ### when CASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END - When `expr1` = true, returns `expr2`; when `expr3` = true, return `expr4`; else return `expr5`. No example/argument for when. ### window ### xpath xpath(xml, xpath) - Returns a string array of values within the nodes of xml that match the XPath expression. Examples: SELECT xpath(' b1 b2 b3 c1 c2 ','a/b/text()'); ['b1','b2','b3'] ### xpath_boolean xpath_boolean(xml, xpath) - Returns true if the XPath expression evaluates to true, or if a matching node is found. Examples: SELECT xpath_boolean(' 1 ','a/b'); true ### xpath_double xpath_double(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_double(' 1 2 ', 'sum(a/b)'); 3.0 ### xpath_float xpath_float(xml, xpath) - Returns a float value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_float(' 1 2 ', 'sum(a/b)'); 3.0 ### xpath_int xpath_int(xml, xpath) - Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_int(' 1 2 ', 'sum(a/b)'); 3 ### xpath_long xpath_long(xml, xpath) - Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_long(' 1 2 ', 'sum(a/b)'); 3 ### xpath_number xpath_number(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_number(' 1 2 ', 'sum(a/b)'); 3.0 ### xpath_short xpath_short(xml, xpath) - Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_short(' 1 2 ', 'sum(a/b)'); 3 ### xpath_string xpath_string(xml, xpath) - Returns the text contents of the first xml node that matches the XPath expression. Examples: SELECT xpath_string(' b cc ','a/c'); cc ### year year(date) - Returns the year component of the date/timestamp. Examples: SELECT year('2016-07-30'); 2016 ### | expr1 | expr2 - Returns the result of bitwise OR of `expr1` and `expr2`. Examples: SELECT 3 | 5; 7 ### ~ ~ expr - Returns the result of bitwise NOT of `expr`. Examples: SELECT ~ 0; -1 ```","title":"upper_bound"},{"location":"reference/sql_reference/","text":"SQL Reference Guide \u00b6 This section provides a complete description of the Structured Query Language (SQL) used to manage information in SnappyData. It includes syntax, usage, keywords, and examples of the SQL statements used on SnappyData. The following topics are covered in this section: ALTER TABLE CREATE STATEMENTS CREATE DISKSTORE CREATE EXTERNAL TABLE CREATE FUNCTION CREATE INDEX CREATE SCHEMA CREATE SAMPLE TABLE CREATE STREAM TABLE CREATE TABLE CREATE TEMPORARY TABLE CREATE VIEW DELETE DEPLOY STATEMENTS DEPLOY PACKAGES DEPLOY JARS UNDEPLOY DROP STATEMENTS DROP DISKSTORE DROP FUNCTION DROP INDEX DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE DROP SCHEMA GRANT INSERT LATERAL VIEW PUT INTO REVOKE SELECT SET ISOLATION SET SCHEMA TRUNCATE TABLE UPDATE","title":"Index"},{"location":"reference/sql_reference/#sql-reference-guide","text":"This section provides a complete description of the Structured Query Language (SQL) used to manage information in SnappyData. It includes syntax, usage, keywords, and examples of the SQL statements used on SnappyData. The following topics are covered in this section: ALTER TABLE CREATE STATEMENTS CREATE DISKSTORE CREATE EXTERNAL TABLE CREATE FUNCTION CREATE INDEX CREATE SCHEMA CREATE SAMPLE TABLE CREATE STREAM TABLE CREATE TABLE CREATE TEMPORARY TABLE CREATE VIEW DELETE DEPLOY STATEMENTS DEPLOY PACKAGES DEPLOY JARS UNDEPLOY DROP STATEMENTS DROP DISKSTORE DROP FUNCTION DROP INDEX DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE DROP SCHEMA GRANT INSERT LATERAL VIEW PUT INTO REVOKE SELECT SET ISOLATION SET SCHEMA TRUNCATE TABLE UPDATE","title":"SQL Reference Guide"},{"location":"reference/sql_reference/alter-table/","text":"ALTER TABLE \u00b6 Use the ALTER TABLE statement to add and drop columns in row tables using SnappyData API or SQL. Note ALTER TABLE is not supported on column, temporary and external tables. For row tables, only adding and dropping a column is supported using Snappy APIs or SQL. Syntax \u00b6 SQL ALTER TABLE table-name { ADD COLUMN column-definition | DROP COLUMN column-name } API: snc.alterTable(tableName, isAddColumn, column) Example \u00b6 SQL: -- create a table CREATE TABLE trade.customers ( cid int not null, cust_name varchar(100), addr varchar(100), tid int); -- drop a non-primary key column if the column is not used for table partitioning, and the column has no dependents ALTER TABLE trade.customers DROP COLUMN addr; -- add the column back with a default value ALTER TABLE trade.customers ADD COLUMN addr varchar(100); API: //create a table in Snappy store snc.createTable(\"orders\", \"row\", ordersDF.schema, Map.empty[String, String]) //alter table add/drop specified column, only supported for row tables. // for adding a column isAddColumn should be true snc.alterTable(\"orders\", true, StructField(\"FirstName\", StringType, true)) // for dropping a column isAddColumn should be false snc.alterTable(\"orders\", false, StructField(\"FirstName\", StringType, true))","title":"ALTER TABLE"},{"location":"reference/sql_reference/alter-table/#alter-table","text":"Use the ALTER TABLE statement to add and drop columns in row tables using SnappyData API or SQL. Note ALTER TABLE is not supported on column, temporary and external tables. For row tables, only adding and dropping a column is supported using Snappy APIs or SQL.","title":"ALTER TABLE"},{"location":"reference/sql_reference/alter-table/#syntax","text":"SQL ALTER TABLE table-name { ADD COLUMN column-definition | DROP COLUMN column-name } API: snc.alterTable(tableName, isAddColumn, column)","title":"Syntax"},{"location":"reference/sql_reference/alter-table/#example","text":"SQL: -- create a table CREATE TABLE trade.customers ( cid int not null, cust_name varchar(100), addr varchar(100), tid int); -- drop a non-primary key column if the column is not used for table partitioning, and the column has no dependents ALTER TABLE trade.customers DROP COLUMN addr; -- add the column back with a default value ALTER TABLE trade.customers ADD COLUMN addr varchar(100); API: //create a table in Snappy store snc.createTable(\"orders\", \"row\", ordersDF.schema, Map.empty[String, String]) //alter table add/drop specified column, only supported for row tables. // for adding a column isAddColumn should be true snc.alterTable(\"orders\", true, StructField(\"FirstName\", StringType, true)) // for dropping a column isAddColumn should be false snc.alterTable(\"orders\", false, StructField(\"FirstName\", StringType, true))","title":"Example"},{"location":"reference/sql_reference/create-diskstore/","text":"CREATE DISKSTORE \u00b6 Disk stores provide disk storage for tables that need to overflow or persist. CREATE DISKSTORE diskstore_name [ MAXLOGSIZE max-log-size-in-mb ] [ AUTOCOMPACT boolean-constant ] [ ALLOWFORCECOMPACTION boolean-constant ] [ COMPACTIONTHRESHOLD garbage-threshold ] [ TIMEINTERVAL time-after-which-data-is-flused-to-disk ] [ WRITEBUFFERSIZE buffer-size-in-mb ] [ QUEUESIZE max-row-operations-to-disk ] [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ] Description \u00b6 SnappyData attempts to preallocate oplog files when you execute the CREATE DISKSTORE command. All tables that target the same disk store share that disk store's persistence attributes. A table that does not target a named disk store uses the default disk store for overflow or persistence. By default, SnappyData uses the working directory of the member as the default disk store. MAXLOGSIZE SnappyData records DML statements in an operation log (oplog) files. This option sets the maximum size in megabytes that the oplog can become before SnappyData automatically rolls to a new file. This size is the combined sizes of the oplog files. When SnappyData creates an oplog file, it immediately reserves this amount of file space. SnappyData only truncates the unused space on a clean shutdown (for example, snappy server stop or ./sbin/snappy-stop-all ). The default value is 1 GB. AUTOCOMPACT Set this option to \"true\" (the default) to automatically compact disk files. Set the option to \"false\" if compaction is not needed or if you intend to manually compact disk files using the snappy utility. SnappyData performs compaction by removing \"garbage\" data that DML statements generate in the oplog file. ALLOWFORCECOMPACTION Set this option to \"true\" to enable online compaction of oplog files using the snappy utility. By default, this option is set to \"false\" (disabled). COMPACTIONTHRESHOLD Sets the threshold for the amount of \"garbage\" data that can exist in the oplog before SnappyData initiates automatic compaction. Garbage data is created as DML operations create, update, and delete rows in a table. The threshold is defined as a percentage (an integer from 0\u2013100). The default is 50. When the amount of \"garbage\" data exceeds this percentage, the disk store becomes eligible for auto-compaction if AUTOCOMPACT is enabled. TIMEINTERVAL Sets the number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. TIMEINTERVAL is only used for tables that were created using the asynchronous option in the persistence clause of the CREATE TABLE statement. See CREATE TABLE . The default value is 1000 milliseconds (1 second). WRITEBUFFERSIZE Sets the buffer size in bytes to use when persisting data to disk. The default is 32768 bytes. QUEUESIZE Sets the maximum number of row operations that SnappyData asynchronously queues to disk. After this number of asynchronous operations are queued, additional asynchronous operations block until existing writes are flushed to disk. A single DML operation may affect multiple rows, and each row modification, insertion, and deletion are considered a separate operation. The default QUEUESIZE value is 0, which specifies no limit. dir-name The optional dir-name entry defines a specific host system directory to use for the disk store. You can include one or more dir-name entries using the syntax: [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ] In each entry: dir-name specifies the name of a directory to use for the disk store. The disk store directory is created on each member if necessary. If you do not specify an absolute path, then SnappyData creates or uses the named directory in each member's working directory (or in the value specified by the sys-disk-dir boot property, if defined). If you specify an absolute path, then all parent directories in the path must exist at the time you execute the command. Note SnappyData uses a \"shared nothing\" disk store design, and you cannot use a single disk store directory to store oplog files from multiple SnappyData members. disk-space-in-mb optionally specifies the maximum amount of space, in megabytes, to use for the disk store in that directory. The space used is calculated as the combined sizes of all oplog files in the directory. If you do not specify the disk-space-in-mb value, then SnappyData does not impose a limit on the amount of space used by disk store files in that directory. If you do specify a limit, the size must be large enough to accommodate the disk store oplog files (the MAXLOGSIZE value, or 1 GB by default) and leave enough free space in the directory to avoid low disk space warnings. If you specify a size that cannot accommodate the oplog files and maintain enough free space, SnappyData fails to create the disk store with SQLState error XOZ33: Cannot create oplogs with size {0}MB which is greater than the maximum size {1}MB for store directory ''{2}''. You can specify any number of dir-name entries in a CREATE DISKSTORE statement. The default diskstore does not have an option for multiple directories. It is recommended that you use separate data area from the server working directory. Create a separate diskstore and specify multiple directories. The data is spread evenly among the active disk files in the directories, keeping within any limits you set. Example \u00b6 This example uses the default base directory and parameter values to create a named disk store: snappy> CREATE DISKSTORE STORE1; This example configures disk store parameters and specifies a storage directory: snappy> CREATE DISKSTORE STORE1 MAXLOGSIZE 1024 AUTOCOMPACT TRUE ALLOWFORCECOMPACTION FALSE COMPACTIONTHRESHOLD 80 TIMEINTERVAL 223344 WRITEBUFFERSIZE 19292393 QUEUESIZE 17374 ('dir1' 10240); This example specifies multiple storage directories and directory sizes for oplog files: snappy> CREATE DISKSTORE STORE1 WRITEBUFFERSIZE 19292393 QUEUESIZE 17374 ('dir1' 456 , 'dir2', 'dir3' 532 ); Related Topics DROP DISKSTORE SYSDISKSTORES","title":"CREATE DISKSTORE"},{"location":"reference/sql_reference/create-diskstore/#create-diskstore","text":"Disk stores provide disk storage for tables that need to overflow or persist. CREATE DISKSTORE diskstore_name [ MAXLOGSIZE max-log-size-in-mb ] [ AUTOCOMPACT boolean-constant ] [ ALLOWFORCECOMPACTION boolean-constant ] [ COMPACTIONTHRESHOLD garbage-threshold ] [ TIMEINTERVAL time-after-which-data-is-flused-to-disk ] [ WRITEBUFFERSIZE buffer-size-in-mb ] [ QUEUESIZE max-row-operations-to-disk ] [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]","title":"CREATE DISKSTORE"},{"location":"reference/sql_reference/create-diskstore/#description","text":"SnappyData attempts to preallocate oplog files when you execute the CREATE DISKSTORE command. All tables that target the same disk store share that disk store's persistence attributes. A table that does not target a named disk store uses the default disk store for overflow or persistence. By default, SnappyData uses the working directory of the member as the default disk store. MAXLOGSIZE SnappyData records DML statements in an operation log (oplog) files. This option sets the maximum size in megabytes that the oplog can become before SnappyData automatically rolls to a new file. This size is the combined sizes of the oplog files. When SnappyData creates an oplog file, it immediately reserves this amount of file space. SnappyData only truncates the unused space on a clean shutdown (for example, snappy server stop or ./sbin/snappy-stop-all ). The default value is 1 GB. AUTOCOMPACT Set this option to \"true\" (the default) to automatically compact disk files. Set the option to \"false\" if compaction is not needed or if you intend to manually compact disk files using the snappy utility. SnappyData performs compaction by removing \"garbage\" data that DML statements generate in the oplog file. ALLOWFORCECOMPACTION Set this option to \"true\" to enable online compaction of oplog files using the snappy utility. By default, this option is set to \"false\" (disabled). COMPACTIONTHRESHOLD Sets the threshold for the amount of \"garbage\" data that can exist in the oplog before SnappyData initiates automatic compaction. Garbage data is created as DML operations create, update, and delete rows in a table. The threshold is defined as a percentage (an integer from 0\u2013100). The default is 50. When the amount of \"garbage\" data exceeds this percentage, the disk store becomes eligible for auto-compaction if AUTOCOMPACT is enabled. TIMEINTERVAL Sets the number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. TIMEINTERVAL is only used for tables that were created using the asynchronous option in the persistence clause of the CREATE TABLE statement. See CREATE TABLE . The default value is 1000 milliseconds (1 second). WRITEBUFFERSIZE Sets the buffer size in bytes to use when persisting data to disk. The default is 32768 bytes. QUEUESIZE Sets the maximum number of row operations that SnappyData asynchronously queues to disk. After this number of asynchronous operations are queued, additional asynchronous operations block until existing writes are flushed to disk. A single DML operation may affect multiple rows, and each row modification, insertion, and deletion are considered a separate operation. The default QUEUESIZE value is 0, which specifies no limit. dir-name The optional dir-name entry defines a specific host system directory to use for the disk store. You can include one or more dir-name entries using the syntax: [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ] In each entry: dir-name specifies the name of a directory to use for the disk store. The disk store directory is created on each member if necessary. If you do not specify an absolute path, then SnappyData creates or uses the named directory in each member's working directory (or in the value specified by the sys-disk-dir boot property, if defined). If you specify an absolute path, then all parent directories in the path must exist at the time you execute the command. Note SnappyData uses a \"shared nothing\" disk store design, and you cannot use a single disk store directory to store oplog files from multiple SnappyData members. disk-space-in-mb optionally specifies the maximum amount of space, in megabytes, to use for the disk store in that directory. The space used is calculated as the combined sizes of all oplog files in the directory. If you do not specify the disk-space-in-mb value, then SnappyData does not impose a limit on the amount of space used by disk store files in that directory. If you do specify a limit, the size must be large enough to accommodate the disk store oplog files (the MAXLOGSIZE value, or 1 GB by default) and leave enough free space in the directory to avoid low disk space warnings. If you specify a size that cannot accommodate the oplog files and maintain enough free space, SnappyData fails to create the disk store with SQLState error XOZ33: Cannot create oplogs with size {0}MB which is greater than the maximum size {1}MB for store directory ''{2}''. You can specify any number of dir-name entries in a CREATE DISKSTORE statement. The default diskstore does not have an option for multiple directories. It is recommended that you use separate data area from the server working directory. Create a separate diskstore and specify multiple directories. The data is spread evenly among the active disk files in the directories, keeping within any limits you set.","title":"Description"},{"location":"reference/sql_reference/create-diskstore/#example","text":"This example uses the default base directory and parameter values to create a named disk store: snappy> CREATE DISKSTORE STORE1; This example configures disk store parameters and specifies a storage directory: snappy> CREATE DISKSTORE STORE1 MAXLOGSIZE 1024 AUTOCOMPACT TRUE ALLOWFORCECOMPACTION FALSE COMPACTIONTHRESHOLD 80 TIMEINTERVAL 223344 WRITEBUFFERSIZE 19292393 QUEUESIZE 17374 ('dir1' 10240); This example specifies multiple storage directories and directory sizes for oplog files: snappy> CREATE DISKSTORE STORE1 WRITEBUFFERSIZE 19292393 QUEUESIZE 17374 ('dir1' 456 , 'dir2', 'dir3' 532 ); Related Topics DROP DISKSTORE SYSDISKSTORES","title":"Example"},{"location":"reference/sql_reference/create-external-table/","text":"CREATE EXTERNAL TABLE \u00b6 CREATE EXTERNAL TABLE [IF NOT EXISTS] [schema_name.]table_name [( column-definition [ , column-definition ] * )] USING datasource [OPTIONS (key1 val1, key2 val2, ...)] For more information on column-definition, refer to Column Definition For Column Table . Refer to these sections for more information on Creating Table , Creating Sample Table , Creating Temporary Table and Creating Stream Table . EXTERNAL External tables point to external data sources. SnappyData supports all the data sources supported by Spark. You should use external tables to load data in parallel from any of the external sources. The table definition is persisted in the catalog and visible across all sessions. USING < data source > Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. Note that most of the prominent datastores provide an implementation of 'DataSource' and accessible as a table. For instance, you can use the Cassandra spark package to create external tables pointing to Cassandra tables and directly run queries on them. You can mix any external table and SnappyData managed tables in your queries. Example \u00b6 Create an external table using PARQUET data source on local filesystem snappy> CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData'); Create an external table using CSV data source on local filesystem CREATE EXTERNAL TABLE IF NOT EXISTS CUSTOMER_STAGING USING csv OPTIONS(path '../../quickstart/src/main/resources/customer.csv'); CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 (C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING csv OPTIONS (path '../../quickstart/src/main/resources/customer.csv'); You can also load data from AWS S3 buckets, as given in the example below: CREATE EXTERNAL TABLE NYCTAXI USING parquet OPTIONS(path 's3a://<AWS_SECRET_KEY>:<AWS_SECRET_ID>@<folder>/<data>'); Specifying AWS credentials to access S3 buckets \u00b6 Providing AWS credentials explicitly in the path url may not be advisable. There are alternative ways users can specify these credentials: Users can provide those as properties in conf/leads file at the time of launching the cluster. For example, append these properties with appropriate values for each entry in your conf/leads file. -spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -spark.hadoop.fs.s3a.access.key=<access-key-id> -spark.hadoop.fs.s3a.secret.key=<secret-access-key> One can also provide them as environment variables in conf/spark-env.sh . Simply add these two entries with appropriate values in that file before launching the cluster. export AWS_ACCESS_KEY_ID=<access-key-id> export AWS_SECRET_ACCESS_KEY=<secret-access-key> This option is applicable only if your cluster is running on AWS EC2 instance(s). Also, this may not work if your S3 buckets are created in regions where AWS signature version 2 is not supported. You can attach an IAM role with appropriate permissions to the instance(s) where the cluster is setup. To do this, go to EC2 dashboard page on AWS console, select your EC2 instance and right-click on it. Select Instance Settings and then Attach/Replace IAM Role . Then click on the drop-down list and select appropriate IAM role. If you do not see any IAM role in the list, you need to create one. Refer to AWS documentation for more details. When you attach an IAM role to an instance, a temporary set of credentials are generated by AWS for the instance(s). These are then picked up by the cluster while accessing the S3 buckets. Then provide below configuration in conf/spark-defaults.conf file before launching the cluster. spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.InstanceProfileCredentialsProvider Related Topics DROP EXTERNAL TABLE","title":"CREATE EXTERNAL TABLE"},{"location":"reference/sql_reference/create-external-table/#create-external-table","text":"CREATE EXTERNAL TABLE [IF NOT EXISTS] [schema_name.]table_name [( column-definition [ , column-definition ] * )] USING datasource [OPTIONS (key1 val1, key2 val2, ...)] For more information on column-definition, refer to Column Definition For Column Table . Refer to these sections for more information on Creating Table , Creating Sample Table , Creating Temporary Table and Creating Stream Table . EXTERNAL External tables point to external data sources. SnappyData supports all the data sources supported by Spark. You should use external tables to load data in parallel from any of the external sources. The table definition is persisted in the catalog and visible across all sessions. USING < data source > Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. Note that most of the prominent datastores provide an implementation of 'DataSource' and accessible as a table. For instance, you can use the Cassandra spark package to create external tables pointing to Cassandra tables and directly run queries on them. You can mix any external table and SnappyData managed tables in your queries.","title":"CREATE EXTERNAL TABLE"},{"location":"reference/sql_reference/create-external-table/#example","text":"Create an external table using PARQUET data source on local filesystem snappy> CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData'); Create an external table using CSV data source on local filesystem CREATE EXTERNAL TABLE IF NOT EXISTS CUSTOMER_STAGING USING csv OPTIONS(path '../../quickstart/src/main/resources/customer.csv'); CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 (C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING csv OPTIONS (path '../../quickstart/src/main/resources/customer.csv'); You can also load data from AWS S3 buckets, as given in the example below: CREATE EXTERNAL TABLE NYCTAXI USING parquet OPTIONS(path 's3a://<AWS_SECRET_KEY>:<AWS_SECRET_ID>@<folder>/<data>');","title":"Example"},{"location":"reference/sql_reference/create-external-table/#specifying-aws-credentials-to-access-s3-buckets","text":"Providing AWS credentials explicitly in the path url may not be advisable. There are alternative ways users can specify these credentials: Users can provide those as properties in conf/leads file at the time of launching the cluster. For example, append these properties with appropriate values for each entry in your conf/leads file. -spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -spark.hadoop.fs.s3a.access.key=<access-key-id> -spark.hadoop.fs.s3a.secret.key=<secret-access-key> One can also provide them as environment variables in conf/spark-env.sh . Simply add these two entries with appropriate values in that file before launching the cluster. export AWS_ACCESS_KEY_ID=<access-key-id> export AWS_SECRET_ACCESS_KEY=<secret-access-key> This option is applicable only if your cluster is running on AWS EC2 instance(s). Also, this may not work if your S3 buckets are created in regions where AWS signature version 2 is not supported. You can attach an IAM role with appropriate permissions to the instance(s) where the cluster is setup. To do this, go to EC2 dashboard page on AWS console, select your EC2 instance and right-click on it. Select Instance Settings and then Attach/Replace IAM Role . Then click on the drop-down list and select appropriate IAM role. If you do not see any IAM role in the list, you need to create one. Refer to AWS documentation for more details. When you attach an IAM role to an instance, a temporary set of credentials are generated by AWS for the instance(s). These are then picked up by the cluster while accessing the S3 buckets. Then provide below configuration in conf/spark-defaults.conf file before launching the cluster. spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.InstanceProfileCredentialsProvider Related Topics DROP EXTERNAL TABLE","title":"Specifying AWS credentials to access S3 buckets"},{"location":"reference/sql_reference/create-function/","text":"CREATE FUNCTION \u00b6 CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar' Description \u00b6 Creates a function. Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well. You can extend any one of the interfaces in the package org.apache.spark.sql.api.java . These interfaces can be included in your client application by adding snappy-spark-sql_2.11-2.0.3-2.jar to your classpath. Note For input/output types: The framework always returns the Java types to the UDFs. So, if you are writing scala.math.BigDecimal as an input type or output type, an exception is reported. You can use java.math.BigDecimal in the SCALA code. Return Types to UDF Program Type Mapping SnappyData Type UDF Type STRING java.lang.String INTEGER java.lang.Integer LONG java.lang.Long DOUBLE java.lang.Double DECIMAL java.math.BigDecimal DATE java.sql.Date TIMESTAMP java.sql.Timestamp FLOAT java.lang.Float BOOLEAN java.lang.Boolean SHORT java.lang.Short BYTE java.lang.Byte CHAR java.lang.String VARCHAR java.lang.String Example \u00b6 CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar' You can write a JAVA or SCALA class to write an UDF implementation. Related Topics DROP FUNCTION","title":"CREATE FUNCTION"},{"location":"reference/sql_reference/create-function/#create-function","text":"CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'","title":"CREATE FUNCTION"},{"location":"reference/sql_reference/create-function/#description","text":"Creates a function. Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well. You can extend any one of the interfaces in the package org.apache.spark.sql.api.java . These interfaces can be included in your client application by adding snappy-spark-sql_2.11-2.0.3-2.jar to your classpath. Note For input/output types: The framework always returns the Java types to the UDFs. So, if you are writing scala.math.BigDecimal as an input type or output type, an exception is reported. You can use java.math.BigDecimal in the SCALA code. Return Types to UDF Program Type Mapping SnappyData Type UDF Type STRING java.lang.String INTEGER java.lang.Integer LONG java.lang.Long DOUBLE java.lang.Double DECIMAL java.math.BigDecimal DATE java.sql.Date TIMESTAMP java.sql.Timestamp FLOAT java.lang.Float BOOLEAN java.lang.Boolean SHORT java.lang.Short BYTE java.lang.Byte CHAR java.lang.String VARCHAR java.lang.String","title":"Description"},{"location":"reference/sql_reference/create-function/#example","text":"CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar' You can write a JAVA or SCALA class to write an UDF implementation. Related Topics DROP FUNCTION","title":"Example"},{"location":"reference/sql_reference/create-index/","text":"CREATE INDEX \u00b6 CREATE INDEX index_name ON table-name ( column-name [ , column-name] * ) Description \u00b6 The CREATE INDEX statement creates an index on one or more columns of a table. Indexes can speed up queries that use those columns for filtering data, or can also enforce a unique constraint on the indexed columns. Note CREATE INDEX is currently under development for column tables and does not work if the data is updated. Example \u00b6 Create an index on two columns: CREATE INDEX idx ON FLIGHTS (flight_id, segment_number);","title":"CREATE INDEX"},{"location":"reference/sql_reference/create-index/#create-index","text":"CREATE INDEX index_name ON table-name ( column-name [ , column-name] * )","title":"CREATE INDEX"},{"location":"reference/sql_reference/create-index/#description","text":"The CREATE INDEX statement creates an index on one or more columns of a table. Indexes can speed up queries that use those columns for filtering data, or can also enforce a unique constraint on the indexed columns. Note CREATE INDEX is currently under development for column tables and does not work if the data is updated.","title":"Description"},{"location":"reference/sql_reference/create-index/#example","text":"Create an index on two columns: CREATE INDEX idx ON FLIGHTS (flight_id, segment_number);","title":"Example"},{"location":"reference/sql_reference/create-sample-table/","text":"CREATE SAMPLE TABLE \u00b6 Mode 1 CREATE TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING column_sample OPTIONS ( baseTable 'baseTableName', BUCKETS 'num-partitions', // Default 128. Must be an integer. REDUNDANCY 'num-of-copies' , // Must be an integer EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019, DISKSTORE 'diskstore-name', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event EXPIRE \u2018time-to-live-in-seconds', QCS 'column-name', // column-name [, column-name ] * FRACTION 'population-fraction', //Must be a double STRATARESERVOIRSIZE 'strata-initial-capacity', // Default 50 Must be an integer. ); Mode 2 CREATE SAMPLE TABLE table_name ON base_table_name OPTIONS ( COLOCATE_WITH 'table-name', // Default none BUCKETS 'num-partitions', // Default 128. Must be an integer. REDUNDANCY 'num-redundant-copies' , // Must be an integer EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019, DISKSTORE 'diskstore-name', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event EXPIRE \u2018time-to-live-in-seconds', QCS 'column-name', // column-name [, column-name ] * FRACTION 'population-fraction', //Must be a double STRATARESERVOIRSIZE 'strata-initial-capacity', // Default 50 Must be an integer. ) For more information on column-definition, refer to Column Definition For Column Table . When creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. The sample table also inherits the number of buckets, redundancy and persistence properties from the base table. For sample tables, the overflow property is set to False by default. For column tables the default value is True . Refer to these sections for more information on Creating Table , Creating External Table , Creating Temporary Table and Creating Stream Table . Description \u00b6 QCS : Query Column Set. These columns are used for stratification in stratified sampling. FRACTION : This represents the fraction of the full population (base table) that is managed in the sample. STRATARESERVOIRSIZE : The initial capacity of each stratum. baseTable : Table on which sampling is done. Examples: \u00b6 Mode 1 Example \u00b6 snappy>CREATE TABLE CUSTOMER_SAMPLE ( C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING COLUMN_SAMPLE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', strataReservoirSize '50', baseTable 'CUSTOMER_BASE'); Mode 2 Example \u00b6 snappy>CREATE SAMPLE TABLE CUSTOMER_SAMPLE on CUSTOMER_BASE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', strataReservoirSize '50'); Note Refer to create sample tables in SDE section for more information on creating sample tables on datasets that can be sourced from any source supported in Spark/SnappyData.","title":"CREATE SAMPLE TABLE"},{"location":"reference/sql_reference/create-sample-table/#create-sample-table","text":"Mode 1 CREATE TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING column_sample OPTIONS ( baseTable 'baseTableName', BUCKETS 'num-partitions', // Default 128. Must be an integer. REDUNDANCY 'num-of-copies' , // Must be an integer EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019, DISKSTORE 'diskstore-name', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event EXPIRE \u2018time-to-live-in-seconds', QCS 'column-name', // column-name [, column-name ] * FRACTION 'population-fraction', //Must be a double STRATARESERVOIRSIZE 'strata-initial-capacity', // Default 50 Must be an integer. ); Mode 2 CREATE SAMPLE TABLE table_name ON base_table_name OPTIONS ( COLOCATE_WITH 'table-name', // Default none BUCKETS 'num-partitions', // Default 128. Must be an integer. REDUNDANCY 'num-redundant-copies' , // Must be an integer EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019, DISKSTORE 'diskstore-name', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event EXPIRE \u2018time-to-live-in-seconds', QCS 'column-name', // column-name [, column-name ] * FRACTION 'population-fraction', //Must be a double STRATARESERVOIRSIZE 'strata-initial-capacity', // Default 50 Must be an integer. ) For more information on column-definition, refer to Column Definition For Column Table . When creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. The sample table also inherits the number of buckets, redundancy and persistence properties from the base table. For sample tables, the overflow property is set to False by default. For column tables the default value is True . Refer to these sections for more information on Creating Table , Creating External Table , Creating Temporary Table and Creating Stream Table .","title":"CREATE SAMPLE TABLE"},{"location":"reference/sql_reference/create-sample-table/#description","text":"QCS : Query Column Set. These columns are used for stratification in stratified sampling. FRACTION : This represents the fraction of the full population (base table) that is managed in the sample. STRATARESERVOIRSIZE : The initial capacity of each stratum. baseTable : Table on which sampling is done.","title":"Description"},{"location":"reference/sql_reference/create-sample-table/#examples","text":"","title":"Examples:"},{"location":"reference/sql_reference/create-sample-table/#mode-1-example","text":"snappy>CREATE TABLE CUSTOMER_SAMPLE ( C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING COLUMN_SAMPLE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', strataReservoirSize '50', baseTable 'CUSTOMER_BASE');","title":"Mode 1 Example"},{"location":"reference/sql_reference/create-sample-table/#mode-2-example","text":"snappy>CREATE SAMPLE TABLE CUSTOMER_SAMPLE on CUSTOMER_BASE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', strataReservoirSize '50'); Note Refer to create sample tables in SDE section for more information on creating sample tables on datasets that can be sourced from any source supported in Spark/SnappyData.","title":"Mode 2 Example"},{"location":"reference/sql_reference/create-schema/","text":"CREATE SCHEMA \u00b6 CREATE SCHEMA schema-name; Description \u00b6 This creates a schema with the given name which provides a mechanism to logically group objects by providing a namespace for objects. This can then be used by other CREATE statements as the namespace prefix. For example, CREATE TABLE SCHEMA1.TABLE1 ( ... ) will create a table TABLE1 in the schema SCHEMA1. Note Schema names with trailing underscores are not supported. The CREATE SCHEMA statement is subject to access control when the gemfirexd.sql-authorization property is set to true for the system. Only the system user can create a schema with a name different from the current user name, and only the system user can specify AUTHORIZATION user-name with a user-name other than the current user name. Example \u00b6 Create schema CREATE SCHEMA myschema; Create schema that uses the authorization id ' shared ' as schema-name CREATE SCHEMA AUTHORIZATION shared; Create schema flights and authorize anita to all the objects that use the schema. CREATE SCHEMA flights AUTHORIZATION anita; * Create schema reports and authorize all members of LDAP group finance to all the objects that use the schema. Any member of this LDAP group can GRANT or REVOKE permissions on objects in this schema to other users. CREATE SCHEMA reports AUTHORIZATION ldapgroup:finance;","title":"CREATE SCHEMA"},{"location":"reference/sql_reference/create-schema/#create-schema","text":"CREATE SCHEMA schema-name;","title":"CREATE SCHEMA"},{"location":"reference/sql_reference/create-schema/#description","text":"This creates a schema with the given name which provides a mechanism to logically group objects by providing a namespace for objects. This can then be used by other CREATE statements as the namespace prefix. For example, CREATE TABLE SCHEMA1.TABLE1 ( ... ) will create a table TABLE1 in the schema SCHEMA1. Note Schema names with trailing underscores are not supported. The CREATE SCHEMA statement is subject to access control when the gemfirexd.sql-authorization property is set to true for the system. Only the system user can create a schema with a name different from the current user name, and only the system user can specify AUTHORIZATION user-name with a user-name other than the current user name.","title":"Description"},{"location":"reference/sql_reference/create-schema/#example","text":"Create schema CREATE SCHEMA myschema; Create schema that uses the authorization id ' shared ' as schema-name CREATE SCHEMA AUTHORIZATION shared; Create schema flights and authorize anita to all the objects that use the schema. CREATE SCHEMA flights AUTHORIZATION anita; * Create schema reports and authorize all members of LDAP group finance to all the objects that use the schema. Any member of this LDAP group can GRANT or REVOKE permissions on objects in this schema to other users. CREATE SCHEMA reports AUTHORIZATION ldapgroup:finance;","title":"Example"},{"location":"reference/sql_reference/create-statements/","text":"Create Statements \u00b6 Use Create statements to create functions, indexes, procedures, schemas, tables, and views. CREATE DISKSTORE CREATE FUNCTION CREATE INDEX CREATE SCHEMA CREATE TABLE CREATE EXTERNAL TABLE CREATE SAMPLE TABLE CREATE STREAM TABLE","title":"Create Statements"},{"location":"reference/sql_reference/create-statements/#create-statements","text":"Use Create statements to create functions, indexes, procedures, schemas, tables, and views. CREATE DISKSTORE CREATE FUNCTION CREATE INDEX CREATE SCHEMA CREATE TABLE CREATE EXTERNAL TABLE CREATE SAMPLE TABLE CREATE STREAM TABLE","title":"Create Statements"},{"location":"reference/sql_reference/create-stream-table/","text":"CREATE STREAM TABLE \u00b6 To Create Stream Table: // DDL for creating a stream table CREATE STREAM TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING kafka_stream | file_stream | twitter_stream | socket_stream OPTIONS ( // multiple stream source specific options storagelevel 'cache-data-option', rowConverter 'rowconverter-class-name', subscribe 'comma-seperated-topic-name', kafkaParams 'kafka-related-params', consumerKey 'consumer-key', consumerSecret 'consumer-secret', accessToken 'access-token', accessTokenSecret 'access-token-secret', hostname 'socket-streaming-hostname', port 'socket-streaming-port-number', directory 'file-streaming-directory' ) For more information on column-definition, refer to Column Definition For Column Table . Refer to these sections for more information on Creating Table , Creating Sample Table , Creating External Table and Creating Temporary Table . Description \u00b6 Create a stream table using a stream data source. If a table with the same name already exists in the database, an exception will be thrown. USING <data source> Specify the streaming source to be used for this table. storageLevel Provides different trade-offs between memory usage and CPU efficiency. rowConverter Converts the unstructured streaming data to a set of rows. topics Subscribed Kafka topics. kafkaParams Kafka configuration parameters such as metadata.broker.list , bootstrap.servers etc. directory HDFS directory to monitor for the new file. hostname Hostname to connect to, for receiving data. port Port to connect to, for receiving data. consumerKey Consumer Key (API Key) for your Twitter account. consumerSecret Consumer Secret key for your Twitter account. accessToken Access token for your Twitter account. accessTokenSecret Access token secret for your Twitter account. Note You need to register to https://apps.twitter.com/ to get the consumerKey , consumerSecret , accessToken and accessTokenSecret credentials. Example \u00b6 //create a connection snappy> connect client 'localhost:1527'; // Initialize streaming with batchInterval of 2 seconds snappy> streaming init 2secs; // Create a stream table snappy> create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter'); // Start the streaming snappy> streaming start; //Run ad-hoc queries on the streamTable on current batch of data snappy> select id, text, fullName from streamTable where text like '%snappy%'; // Drop the streamTable snappy> drop table streamTable; // Stop the streaming snappy> streaming stop;","title":"CREATE STREAM TABLE"},{"location":"reference/sql_reference/create-stream-table/#create-stream-table","text":"To Create Stream Table: // DDL for creating a stream table CREATE STREAM TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING kafka_stream | file_stream | twitter_stream | socket_stream OPTIONS ( // multiple stream source specific options storagelevel 'cache-data-option', rowConverter 'rowconverter-class-name', subscribe 'comma-seperated-topic-name', kafkaParams 'kafka-related-params', consumerKey 'consumer-key', consumerSecret 'consumer-secret', accessToken 'access-token', accessTokenSecret 'access-token-secret', hostname 'socket-streaming-hostname', port 'socket-streaming-port-number', directory 'file-streaming-directory' ) For more information on column-definition, refer to Column Definition For Column Table . Refer to these sections for more information on Creating Table , Creating Sample Table , Creating External Table and Creating Temporary Table .","title":"CREATE STREAM TABLE"},{"location":"reference/sql_reference/create-stream-table/#description","text":"Create a stream table using a stream data source. If a table with the same name already exists in the database, an exception will be thrown. USING <data source> Specify the streaming source to be used for this table. storageLevel Provides different trade-offs between memory usage and CPU efficiency. rowConverter Converts the unstructured streaming data to a set of rows. topics Subscribed Kafka topics. kafkaParams Kafka configuration parameters such as metadata.broker.list , bootstrap.servers etc. directory HDFS directory to monitor for the new file. hostname Hostname to connect to, for receiving data. port Port to connect to, for receiving data. consumerKey Consumer Key (API Key) for your Twitter account. consumerSecret Consumer Secret key for your Twitter account. accessToken Access token for your Twitter account. accessTokenSecret Access token secret for your Twitter account. Note You need to register to https://apps.twitter.com/ to get the consumerKey , consumerSecret , accessToken and accessTokenSecret credentials.","title":"Description"},{"location":"reference/sql_reference/create-stream-table/#example","text":"//create a connection snappy> connect client 'localhost:1527'; // Initialize streaming with batchInterval of 2 seconds snappy> streaming init 2secs; // Create a stream table snappy> create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter'); // Start the streaming snappy> streaming start; //Run ad-hoc queries on the streamTable on current batch of data snappy> select id, text, fullName from streamTable where text like '%snappy%'; // Drop the streamTable snappy> drop table streamTable; // Stop the streaming snappy> streaming stop;","title":"Example"},{"location":"reference/sql_reference/create-synonym/","text":"CREATE SYNONYM \u00b6 Provide an alternate name for a table or view. snappy> CREATE SYNONYM [synonym-name] FOR [view-name | table-name]; Description \u00b6 The synonym-Name in the statement represents the synonym name you are giving the target table or view, while the view-Name or table-Name represents the original name of the target table or view. Synonyms for other synonyms can also be created, resulting in nested synonyms. A synonym can be used instead of the original qualified table or view name in SELECT, INSERT, UPDATE, DELETE or LOCK TABLE statements. Synonym for a table or a view can be created for tables or views that doesn't exist, but the target table or view must be present before the synonym can be used. Synonyms share the same namespace as tables or views. Synonyms name cannot be same as the name of an already existing table/view. A synonym can be defined for a table/view that does not exist when you create the synonym. If the table or view doesn't exist, you will receive a warning message (SQLSTATE 01522). The referenced object must be present when the synonym is used in a DML statement. Nested synonym can be created(a synonym for another synonym), but any attempt to create a synonym that results in a circular reference will return an error message (SQLSTATE 42916). Synonyms cannot be defined in system schemas. All schemas starting with 'SYS' are considered system schemas and are reserved by SnappyData. A synonym cannot be defined on a temporary table. Attempting to define a synonym on a temporary table will return an error message (SQLSTATE XCL51). Example \u00b6 snappy> CREATE SYNONYM myairline FOR airlineREF; Related Topics DROP SYNONYM CREATE VIEW","title":"CREATE SYNONYM"},{"location":"reference/sql_reference/create-synonym/#create-synonym","text":"Provide an alternate name for a table or view. snappy> CREATE SYNONYM [synonym-name] FOR [view-name | table-name];","title":"CREATE SYNONYM"},{"location":"reference/sql_reference/create-synonym/#description","text":"The synonym-Name in the statement represents the synonym name you are giving the target table or view, while the view-Name or table-Name represents the original name of the target table or view. Synonyms for other synonyms can also be created, resulting in nested synonyms. A synonym can be used instead of the original qualified table or view name in SELECT, INSERT, UPDATE, DELETE or LOCK TABLE statements. Synonym for a table or a view can be created for tables or views that doesn't exist, but the target table or view must be present before the synonym can be used. Synonyms share the same namespace as tables or views. Synonyms name cannot be same as the name of an already existing table/view. A synonym can be defined for a table/view that does not exist when you create the synonym. If the table or view doesn't exist, you will receive a warning message (SQLSTATE 01522). The referenced object must be present when the synonym is used in a DML statement. Nested synonym can be created(a synonym for another synonym), but any attempt to create a synonym that results in a circular reference will return an error message (SQLSTATE 42916). Synonyms cannot be defined in system schemas. All schemas starting with 'SYS' are considered system schemas and are reserved by SnappyData. A synonym cannot be defined on a temporary table. Attempting to define a synonym on a temporary table will return an error message (SQLSTATE XCL51).","title":"Description"},{"location":"reference/sql_reference/create-synonym/#example","text":"snappy> CREATE SYNONYM myairline FOR airlineREF; Related Topics DROP SYNONYM CREATE VIEW","title":"Example"},{"location":"reference/sql_reference/create-table/","text":"CREATE TABLE \u00b6 Following is the syntax used to create a Row/Column table: CREATE TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING [row | column] // If not specified, a row table is created. OPTIONS ( COLOCATE_WITH 'table-name', // Default none PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables. BUCKETS 'num-partitions', // Default 128. Must be an integer. COMPRESSION 'NONE', //By default COMPRESSION is 'ON'. REDUNDANCY 'num-of-copies' , // Must be an integer. By default, REDUNDANCY is set to 0 (zero). '1' is recommended value. Maximum limit is '3' EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE 'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019, DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set. EXPIRE 'time_to_live_in_seconds', COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table. KEY_COLUMNS 'column_name,..', // Only for column table if putInto support is required COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer > 0 and < 2GB. Only for column table. ) [AS select_statement]; Refer to the following sections for: Creating Sample Table Creating External Table Creating Temporary Table Creating Stream Table . Column Definition \u00b6 The column definition defines the name of a column and its data type. column-definition (for Column Table) column-definition: column-name column-data-type [NOT NULL] column-name: 'unique column name' column-definition (for Row Table) column-definition: column-definition-for-row-table | table-constraint column-definition-for-row-table: column-name column-data-type [ column-constraint ] * [ [ WITH ] DEFAULT { constant-expression | NULL } | [ GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY [ ( START WITH start-value-as-integer [, INCREMENT BY step-value-as-integer ] ) ] ] ] [ column-constraint ] * Refer to the identity section for more information on GENERATED. Refer to the constraint section for more information on table-constraint and column-constraint. column-data-type \u00b6 The following data types are supported: column-data-type: BIGINT | BINARY | BLOB | BOOLEAN | BYTE | CLOB | DATE | DECIMAL | DOUBLE | FLOAT | INT | INTEGER | LONG | NUMERIC | REAL | SHORT | SMALLINT | STRING | TIMESTAMP | TINYINT | VARBINARY | VARCHAR | Column tables can also use ARRAY , MAP and STRUCT types. Decimal and numeric has default precision of 38 and scale of 18. Using \u00b6 You can specify if you want to create a row table or a column table. If this is not specified, a row table is created by default. Options \u00b6 You can specify the following options when you create a table: COLOCATE_WITH PARTITION_BY BUCKETS COMPRESSION REDUNDANCY EVICTION_BY PERSISTENCE DISKSTORE OVERFLOW EXPIRE COLUMN_BATCH_SIZE COLUMN_MAX_DELTA_ROWS Note If options are not specified, then the default values are used to create the table. COLOCATE_WITH The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. PARTITION_BY Use the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning. If not specified, for row table (mentioned further for the case of column table) it is a 'replicated row table'. Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally. The default number of storage partitions (BUCKETS) is 128 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API. BUCKETS The optional BUCKETS attribute specifies the fixed number of \"buckets\" to use for the partitioned row or column tables. Each data server JVM manages one or more buckets. A bucket is a container of data and is the smallest unit of partitioning and migration in the system. For instance, in a cluster of five nodes and a bucket count of 25 would result in 5 buckets on each node. But, if you configured the reverse - 25 nodes and a bucket count of 5, only 5 data servers hosts all the data for this table. If not specified, the number of buckets defaults to 128. See best practices for more information. For row tables, BUCKETS must be created with the PARTITION_BY clause, else an error is reported. COMPRESSION Column tables use compression of data by default. This reduces the total storage footprint for large tables. SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. By default, compression is on for column tables. To disable data compression, you can set the COMPRESSION option to none when you create a table. For example: CREATE TABLE AIRLINE USING column OPTIONS(compression 'none') AS (select * from STAGING_AIRLINE); See best practices for more information. REDUNDANCY Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. It is important to note that redundancy of '1' implies two physical copies of data. By default, REDUNDANCY is set to 0 (zero). A REDUNDANCY value of '1' is recommended. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage. A maximum limit of '3' can be set for REDUNDANCY. See best practices for more information. EVICTION_BY Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store. It is important to note that all tables (expected to host larger data sets) overflow to disk, by default. See best practices for more information. The value for this parameter is set in MB. For column tables, the default eviction setting is LRUHEAPPERCENT and the default action is to overflow to disk. You can also specify the OVERFLOW parameter along with the EVICTION_BY clause. Note EVICTION_BY is not supported for replicated tables. For column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables, no such defaults are set. Row tables allow all the eviction settings. PERSISTENCE When you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member. Note By default, both row and column tables are persistent. The option PERSISTENT has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use PERSISTENCE instead. DISKSTORE The disk directories where you want to persist the table data. By default, SnappyData creates a \"default\" disk store on each member node. You can use this option to control the location where data is stored. For instance, you may decide to use a network file system or specify multiple disk mount points to uniformly scatter the data across disks. For more information, refer to CREATE DISKSTORE . OVERFLOW Use the OVERFLOW clause to specify the action to be taken upon the eviction event. For persistent tables, setting this to 'true' overflows the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' is not allowed except when EVICTION_BY is not set. In such case, the eviction itself is disabled. When you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table). Note The tables are evicted to disk by default, which means table data overflows to a local SnappyStore disk store. EXPIRE Use the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured time_to_live_in_seconds . COLUMN_BATCH_SIZE The default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M). COLUMN_MAX_DELTA_ROWS The maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by COLUMN_BATCH_SIZE (and thus limits the size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The value should be > 0 and < 2GB. The default value is 10000. Note The following corresponding SQLConf properties for COLUMN_BATCH_SIZE and COLUMN_MAX_DELTA_ROWS are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL): snappydata.column.batchSize - Explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit COLUMN_BATCH_SIZE specification, then this is inherited for that table property. snappydata.column.maxDeltaRows - The maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit COLUMN_MAX_DELTA_ROWS specification, then this is inherited for that table property. Tables created using the standard SQL syntax without any of SnappyData specific extensions are created as row-oriented replicated tables. Thus, each data server node in the cluster hosts a consistent replica of the table. All tables are also registered in the Spark catalog and hence visible as DataFrames. For example, create table if not exists Table1 (a int) is equivalent to create table if not exists Table1 (a int) using row . Examples \u00b6 Example: Column Table Partitioned on a Single Column \u00b6 snappy>CREATE TABLE CUSTOMER ( C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY'); Example: Column Table Partitioned with 10 Buckets and Persistence Enabled \u00b6 snappy>CREATE TABLE CUSTOMER ( C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY', PERSISTENCE 'SYNCHRONOUS'); Example: Replicated, Persistent Row Table \u00b6 snappy>CREATE TABLE SUPPLIER ( S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, S_NAME STRING NOT NULL, S_ADDRESS STRING NOT NULL, S_NATIONKEY INTEGER NOT NULL, S_PHONE STRING NOT NULL, S_ACCTBAL DECIMAL(15, 2) NOT NULL, S_COMMENT STRING NOT NULL) USING ROW OPTIONS (PARTITION_BY 'S_SUPPKEY', BUCKETS '10', PERSISTENCE 'ASYNCHRONOUS'); Example: Row Table Partitioned with 10 Buckets and Overflow Enabled \u00b6 snappy>CREATE TABLE SUPPLIER ( S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, S_NAME STRING NOT NULL, S_ADDRESS STRING NOT NULL, S_NATIONKEY INTEGER NOT NULL, S_PHONE STRING NOT NULL, S_ACCTBAL DECIMAL(15, 2) NOT NULL, S_COMMENT STRING NOT NULL) USING ROW OPTIONS (BUCKETS '10', PARTITION_BY 'S_SUPPKEY', PERSISTENCE 'ASYNCHRONOUS', EVICTION_BY 'LRUCOUNT 3', OVERFLOW 'true'); Example: Create Table using Select Query \u00b6 CREATE TABLE CUSTOMER_STAGING USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') AS SELECT * FROM CUSTOMER ; With this alternate form of the CREATE TABLE statement, you specify the column names and/or the column data types with a query. The columns in the query result are used as a model for creating the columns in the new table. If no column names are specified for the new table, then all the columns in the result of the query expression are used to create same-named columns in the new table, of the corresponding data type(s). If one or more column names are specified for the new table, the same number of columns must be present in the result of the query expression; the data types of those columns are used for the corresponding columns of the new table. Note Only the column names and data types from the queried table are used when creating the new table. Additional settings in the queried table, such as partitioning, replication, and persistence, are not duplicated. You can optionally specify partitioning, replication, and persistence configuration settings for the new table and those settings need not match the settings of the queried table. When you are creating a new table in SnappyData from another table, for example an external table, by using CREATE TABLE ... AS SELECT * FROM ... query and you find that the query fails with the message: Syntax error or analysis exception: Table <schemaname.tablename> already exists ... , it's likely due to insufficient memory on one of the servers and that the server is going down. You may also see an entry for that table created when you run show tables command immediately after. This happens because some tasks pertaining to the query may be still running on another server(s). As soon as those tasks are completed, the table gets cleaned up as expected because the query failed. Example: Create Table using Spark DataFrame API \u00b6 For information on using the Apache Spark API, refer to Using the Spark DataFrame API . Example: Create Column Table with PUT INTO \u00b6 snappy> CREATE TABLE COL_TABLE ( PRSN_EVNT_ID BIGINT NOT NULL, VER bigint NOT NULL, CLIENT_ID BIGINT NOT NULL, SRC_TYP_ID BIGINT NOT NULL) USING COLUMN OPTIONS(PARTITION_BY 'PRSN_EVNT_ID,CLIENT_ID', BUCKETS '64', KEY_COLUMNS 'PRSN_EVNT_ID, CLIENT_ID'); Example: Create Table with Eviction Settings \u00b6 Use eviction settings to keep your table within a specified limit, either by removing evicted data completely or by creating an overflow table that persists the evicted data to a disk store. Decide whether to evict based on: Entry count (useful if table row sizes are relatively uniform). Total bytes used. Percentage of JVM heap used. This uses the SnappyData resource manager. When the manager determines that eviction is required, the manager orders the eviction controller to start evicting from all tables where the eviction criterion is set to LRUHEAPPERCENT. Decide what action to take when the limit is reached: Locally destroy the row (partitioned tables only). Overflow the row data to disk. If you want to overflow data to disk (or persist the entire table to disk), configure a named disk store to use for the overflow data. If you do not specify a disk store when creating an overflow table, SnappyData stores the overflow data in the default disk store. Create the table with the required eviction configuration. For example, to evict using LRU entry count and overflow evicted rows to a disk store (OverflowDiskStore): CREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (EVICTION_BY 'LRUCOUNT 2', OVERFLOW 'true', DISKSTORE 'OverflowDiskStore', PERSISTENCE 'async'); To create a table that simply removes evicted data from memory without persisting the evicted data, use the DESTROY eviction action. For example: Default in SnappyData for synchronous is persistence , overflow is true and eviction_by is LRUHEAPPERCENT . CREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (PARTITION_BY 'OrderId', EVICTION_BY 'LRUMEMSIZE 1000'); Example: Create Column Table with COMMENT Clause \u00b6 You can add comments about a column using the COMMENT clause. The COMMENT clause must be used in the column definition as shown in the following example: snappy> create table foobar (a string comment 'column 1', b string not null comment 'column 2') using column; snappy> describe foobar; col_name |data_type |comment ------------------------------ a |string |column 1 b |string |column 2 Constraint (only for Row Tables) \u00b6 A CONSTRAINT clause is an optional part of a CREATE TABLE statement that defines a rule to which table data must conform. There are two types of constraints: Column-level constraints : Refer to a single column in the table and do not specify a column name (except check constraints). They refer to the column that they follow. Table-level constraints : Refer to one or more columns in the table. Table-level constraints specify the names of the columns to which they apply. Table-level CHECK constraints can refer to 0 or more columns in the table. Column and table constraints include: NOT NULL\u2014 Specifies that a column cannot hold NULL values (constraints of this type are not nameable). PRIMARY KEY\u2014 Specifies a column (or multiple columns if specified in a table constraint) that uniquely identifies a row in the table. The identified columns must be defined as NOT NULL. UNIQUE\u2014 Specifies that values in the column must be unique. NULL values are not allowed. FOREIGN KEY\u2014 Specifies that the values in the columns must correspond to values in the referenced primary key or unique columns or that they are NULL. If the foreign key consists of multiple columns and any column is NULL, then the whole key is considered NULL. SnappyData permits the insert no matter what is in the non-null columns. CHECK\u2014 Specifies rules for values in a column, or specifies a wide range of rules for values when included as a table constraint. The CHECK constraint has the same format and restrictions for column and table constraints. Column constraints and table constraints have the same function; the difference is where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition. Column-level constraints (except for check constraints) refer to only one column. If you do not specify a name for a column or table constraint, then SnappyData generates a unique name. Example : The following example demonstrates how to create a table with FOREIGN KEY : snappy> create table trading.customers (cid int not null, cust_name varchar(100), since date, addr varchar(100), tid int, primary key (cid)); snappy> create table trading.networth (cid int not null, cash decimal (30, 20), securities decimal (30, 20), loanlimit int, availloan decimal (30, 20), tid int, constraint netw_pk primary key (cid), constraint cust_newt_fk foreign key (cid) references trading.customers (cid)); snappy> show importedkeys in trading; PKTABLE_NAME |PKCOLUMN_NAME |PK_NAME |FKTABLE_SCHEM |FKTABLE_NAME |FKCOLUMN_NAME |FK_NAME |KEY_SEQ ---------------------------------------------------------------------------------------------------------------------------------------------------------- CUSTOMERS |CID |SQL180403162038220 |TRADING |NETWORTH |CID |CUST_NEWT_FK |1 Identity Columns (only for Row Tables) \u00b6 SnappyData supports both GENERATED ALWAYS and GENERATED BY DEFAULT identity columns only for BIGINT and INTEGER data types. The START WITH and INCREMENT BY clauses are supported only for GENERATED BY DEFAULT identity columns. For a GENERATED ALWAYS identity column, SnappyData increments the default value on every insertion, and stores the incremented value in the column. You cannot insert a value directly into a GENERATED ALWAYS identity column, and you cannot update a value in a GENERATED ALWAYS identity column. Instead, you must either specify the DEFAULT keyword when inserting data into the table or you must leave the identity column out of the insertion column list. Consider a table with the following column definition: create table greetings (i int generated always as identity, ch char(50)) using row; You can insert rows into the table using either the DEFAULT keyword or by omitting the identity column from the INSERT statement: insert into greetings values (DEFAULT, 'hello'); insert into greetings(ch) values ('hi'); The values that SnappyData automatically generates for a GENERATED ALWAYS identity column are unique. For a GENERATED BY DEFAULT identity column, SnappyData increments and uses a default value for an INSERT only when no explicit value is given. To use the generated default value, either specify the DEFAULT keyword when inserting into the identity column or leave the identity column out of the INSERT column list. In contrast to GENERATED ALWAYS identity columns, with a GENERATED BY DEFAULT column you can specify an identity value to use instead of the generated default value. To specify a value, include it in the INSERT statement. For example, consider a table created using the statement: create table greetings (i int generated by default as identity, ch char(50)); The following statement specifies the value \u201c1\u201d for the identity column: insert into greetings values (1, 'hi'); These statements both use generated default values: insert into greetings values (DEFAULT, 'hello'); insert into greetings(ch) values ('bye'); Although the automatically-generated values in a GENERATED BY DEFAULT identity column are unique, a GENERATED BY DEFAULT column does not guarantee unique identity values for all rows in the table. For example, in the above statements, the rows containing \u201chi\u201d and \u201chello\u201d both have an identity value of \u201c1.\u201d This occurs because the generated column starts at \u201c1\u201d and the user-specified value was also \u201c1.\u201d To avoid duplicating identity values (for example, during an import operation), you can use the START WITH clause to specify the first identity value that SnappyData should assign and increment. Or, you can use a primary key or a unique constraint on the GENERATED BY DEFAULT identity column to check for and disallow duplicates. By default, the initial value of a GENERATED BY DEFAULT identity column is 1, and the value is incremented by 1 for each INSERT. Use the optional START WITH clause to specify a new initial value. Use the optional INCREMENT BY clause to change the increment value used during each INSERT. Related Topics DROP TABLE DELETE TABLE SHOW TABLES TRUNCATE TABLE","title":"CREATE TABLE"},{"location":"reference/sql_reference/create-table/#create-table","text":"Following is the syntax used to create a Row/Column table: CREATE TABLE [IF NOT EXISTS] table_name ( column-definition [ , column-definition ] * ) USING [row | column] // If not specified, a row table is created. OPTIONS ( COLOCATE_WITH 'table-name', // Default none PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables. BUCKETS 'num-partitions', // Default 128. Must be an integer. COMPRESSION 'NONE', //By default COMPRESSION is 'ON'. REDUNDANCY 'num-of-copies' , // Must be an integer. By default, REDUNDANCY is set to 0 (zero). '1' is recommended value. Maximum limit is '3' EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT', PERSISTENCE 'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019, DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set. EXPIRE 'time_to_live_in_seconds', COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table. KEY_COLUMNS 'column_name,..', // Only for column table if putInto support is required COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer > 0 and < 2GB. Only for column table. ) [AS select_statement]; Refer to the following sections for: Creating Sample Table Creating External Table Creating Temporary Table Creating Stream Table .","title":"CREATE TABLE"},{"location":"reference/sql_reference/create-table/#column-definition","text":"The column definition defines the name of a column and its data type. column-definition (for Column Table) column-definition: column-name column-data-type [NOT NULL] column-name: 'unique column name' column-definition (for Row Table) column-definition: column-definition-for-row-table | table-constraint column-definition-for-row-table: column-name column-data-type [ column-constraint ] * [ [ WITH ] DEFAULT { constant-expression | NULL } | [ GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY [ ( START WITH start-value-as-integer [, INCREMENT BY step-value-as-integer ] ) ] ] ] [ column-constraint ] * Refer to the identity section for more information on GENERATED. Refer to the constraint section for more information on table-constraint and column-constraint.","title":"Column Definition"},{"location":"reference/sql_reference/create-table/#column-data-type","text":"The following data types are supported: column-data-type: BIGINT | BINARY | BLOB | BOOLEAN | BYTE | CLOB | DATE | DECIMAL | DOUBLE | FLOAT | INT | INTEGER | LONG | NUMERIC | REAL | SHORT | SMALLINT | STRING | TIMESTAMP | TINYINT | VARBINARY | VARCHAR | Column tables can also use ARRAY , MAP and STRUCT types. Decimal and numeric has default precision of 38 and scale of 18.","title":"column-data-type"},{"location":"reference/sql_reference/create-table/#using","text":"You can specify if you want to create a row table or a column table. If this is not specified, a row table is created by default.","title":"Using"},{"location":"reference/sql_reference/create-table/#options","text":"You can specify the following options when you create a table: COLOCATE_WITH PARTITION_BY BUCKETS COMPRESSION REDUNDANCY EVICTION_BY PERSISTENCE DISKSTORE OVERFLOW EXPIRE COLUMN_BATCH_SIZE COLUMN_MAX_DELTA_ROWS Note If options are not specified, then the default values are used to create the table. COLOCATE_WITH The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. PARTITION_BY Use the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning. If not specified, for row table (mentioned further for the case of column table) it is a 'replicated row table'. Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally. The default number of storage partitions (BUCKETS) is 128 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API. BUCKETS The optional BUCKETS attribute specifies the fixed number of \"buckets\" to use for the partitioned row or column tables. Each data server JVM manages one or more buckets. A bucket is a container of data and is the smallest unit of partitioning and migration in the system. For instance, in a cluster of five nodes and a bucket count of 25 would result in 5 buckets on each node. But, if you configured the reverse - 25 nodes and a bucket count of 5, only 5 data servers hosts all the data for this table. If not specified, the number of buckets defaults to 128. See best practices for more information. For row tables, BUCKETS must be created with the PARTITION_BY clause, else an error is reported. COMPRESSION Column tables use compression of data by default. This reduces the total storage footprint for large tables. SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. By default, compression is on for column tables. To disable data compression, you can set the COMPRESSION option to none when you create a table. For example: CREATE TABLE AIRLINE USING column OPTIONS(compression 'none') AS (select * from STAGING_AIRLINE); See best practices for more information. REDUNDANCY Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. It is important to note that redundancy of '1' implies two physical copies of data. By default, REDUNDANCY is set to 0 (zero). A REDUNDANCY value of '1' is recommended. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage. A maximum limit of '3' can be set for REDUNDANCY. See best practices for more information. EVICTION_BY Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store. It is important to note that all tables (expected to host larger data sets) overflow to disk, by default. See best practices for more information. The value for this parameter is set in MB. For column tables, the default eviction setting is LRUHEAPPERCENT and the default action is to overflow to disk. You can also specify the OVERFLOW parameter along with the EVICTION_BY clause. Note EVICTION_BY is not supported for replicated tables. For column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables, no such defaults are set. Row tables allow all the eviction settings. PERSISTENCE When you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member. Note By default, both row and column tables are persistent. The option PERSISTENT has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use PERSISTENCE instead. DISKSTORE The disk directories where you want to persist the table data. By default, SnappyData creates a \"default\" disk store on each member node. You can use this option to control the location where data is stored. For instance, you may decide to use a network file system or specify multiple disk mount points to uniformly scatter the data across disks. For more information, refer to CREATE DISKSTORE . OVERFLOW Use the OVERFLOW clause to specify the action to be taken upon the eviction event. For persistent tables, setting this to 'true' overflows the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' is not allowed except when EVICTION_BY is not set. In such case, the eviction itself is disabled. When you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table). Note The tables are evicted to disk by default, which means table data overflows to a local SnappyStore disk store. EXPIRE Use the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured time_to_live_in_seconds . COLUMN_BATCH_SIZE The default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M). COLUMN_MAX_DELTA_ROWS The maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by COLUMN_BATCH_SIZE (and thus limits the size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The value should be > 0 and < 2GB. The default value is 10000. Note The following corresponding SQLConf properties for COLUMN_BATCH_SIZE and COLUMN_MAX_DELTA_ROWS are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL): snappydata.column.batchSize - Explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit COLUMN_BATCH_SIZE specification, then this is inherited for that table property. snappydata.column.maxDeltaRows - The maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit COLUMN_MAX_DELTA_ROWS specification, then this is inherited for that table property. Tables created using the standard SQL syntax without any of SnappyData specific extensions are created as row-oriented replicated tables. Thus, each data server node in the cluster hosts a consistent replica of the table. All tables are also registered in the Spark catalog and hence visible as DataFrames. For example, create table if not exists Table1 (a int) is equivalent to create table if not exists Table1 (a int) using row .","title":"Options"},{"location":"reference/sql_reference/create-table/#examples","text":"","title":"Examples"},{"location":"reference/sql_reference/create-table/#example-column-table-partitioned-on-a-single-column","text":"snappy>CREATE TABLE CUSTOMER ( C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY');","title":"Example: Column Table Partitioned on a Single Column"},{"location":"reference/sql_reference/create-table/#example-column-table-partitioned-with-10-buckets-and-persistence-enabled","text":"snappy>CREATE TABLE CUSTOMER ( C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, C_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, C_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY', PERSISTENCE 'SYNCHRONOUS');","title":"Example: Column Table Partitioned with 10 Buckets and Persistence Enabled"},{"location":"reference/sql_reference/create-table/#example-replicated-persistent-row-table","text":"snappy>CREATE TABLE SUPPLIER ( S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, S_NAME STRING NOT NULL, S_ADDRESS STRING NOT NULL, S_NATIONKEY INTEGER NOT NULL, S_PHONE STRING NOT NULL, S_ACCTBAL DECIMAL(15, 2) NOT NULL, S_COMMENT STRING NOT NULL) USING ROW OPTIONS (PARTITION_BY 'S_SUPPKEY', BUCKETS '10', PERSISTENCE 'ASYNCHRONOUS');","title":"Example: Replicated, Persistent Row Table"},{"location":"reference/sql_reference/create-table/#example-row-table-partitioned-with-10-buckets-and-overflow-enabled","text":"snappy>CREATE TABLE SUPPLIER ( S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, S_NAME STRING NOT NULL, S_ADDRESS STRING NOT NULL, S_NATIONKEY INTEGER NOT NULL, S_PHONE STRING NOT NULL, S_ACCTBAL DECIMAL(15, 2) NOT NULL, S_COMMENT STRING NOT NULL) USING ROW OPTIONS (BUCKETS '10', PARTITION_BY 'S_SUPPKEY', PERSISTENCE 'ASYNCHRONOUS', EVICTION_BY 'LRUCOUNT 3', OVERFLOW 'true');","title":"Example: Row Table Partitioned with 10 Buckets and Overflow Enabled"},{"location":"reference/sql_reference/create-table/#example-create-table-using-select-query","text":"CREATE TABLE CUSTOMER_STAGING USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') AS SELECT * FROM CUSTOMER ; With this alternate form of the CREATE TABLE statement, you specify the column names and/or the column data types with a query. The columns in the query result are used as a model for creating the columns in the new table. If no column names are specified for the new table, then all the columns in the result of the query expression are used to create same-named columns in the new table, of the corresponding data type(s). If one or more column names are specified for the new table, the same number of columns must be present in the result of the query expression; the data types of those columns are used for the corresponding columns of the new table. Note Only the column names and data types from the queried table are used when creating the new table. Additional settings in the queried table, such as partitioning, replication, and persistence, are not duplicated. You can optionally specify partitioning, replication, and persistence configuration settings for the new table and those settings need not match the settings of the queried table. When you are creating a new table in SnappyData from another table, for example an external table, by using CREATE TABLE ... AS SELECT * FROM ... query and you find that the query fails with the message: Syntax error or analysis exception: Table <schemaname.tablename> already exists ... , it's likely due to insufficient memory on one of the servers and that the server is going down. You may also see an entry for that table created when you run show tables command immediately after. This happens because some tasks pertaining to the query may be still running on another server(s). As soon as those tasks are completed, the table gets cleaned up as expected because the query failed.","title":"Example: Create Table using Select Query"},{"location":"reference/sql_reference/create-table/#example-create-table-using-spark-dataframe-api","text":"For information on using the Apache Spark API, refer to Using the Spark DataFrame API .","title":"Example: Create Table using Spark DataFrame API"},{"location":"reference/sql_reference/create-table/#example-create-column-table-with-put-into","text":"snappy> CREATE TABLE COL_TABLE ( PRSN_EVNT_ID BIGINT NOT NULL, VER bigint NOT NULL, CLIENT_ID BIGINT NOT NULL, SRC_TYP_ID BIGINT NOT NULL) USING COLUMN OPTIONS(PARTITION_BY 'PRSN_EVNT_ID,CLIENT_ID', BUCKETS '64', KEY_COLUMNS 'PRSN_EVNT_ID, CLIENT_ID');","title":"Example: Create Column Table with PUT INTO"},{"location":"reference/sql_reference/create-table/#example-create-table-with-eviction-settings","text":"Use eviction settings to keep your table within a specified limit, either by removing evicted data completely or by creating an overflow table that persists the evicted data to a disk store. Decide whether to evict based on: Entry count (useful if table row sizes are relatively uniform). Total bytes used. Percentage of JVM heap used. This uses the SnappyData resource manager. When the manager determines that eviction is required, the manager orders the eviction controller to start evicting from all tables where the eviction criterion is set to LRUHEAPPERCENT. Decide what action to take when the limit is reached: Locally destroy the row (partitioned tables only). Overflow the row data to disk. If you want to overflow data to disk (or persist the entire table to disk), configure a named disk store to use for the overflow data. If you do not specify a disk store when creating an overflow table, SnappyData stores the overflow data in the default disk store. Create the table with the required eviction configuration. For example, to evict using LRU entry count and overflow evicted rows to a disk store (OverflowDiskStore): CREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (EVICTION_BY 'LRUCOUNT 2', OVERFLOW 'true', DISKSTORE 'OverflowDiskStore', PERSISTENCE 'async'); To create a table that simply removes evicted data from memory without persisting the evicted data, use the DESTROY eviction action. For example: Default in SnappyData for synchronous is persistence , overflow is true and eviction_by is LRUHEAPPERCENT . CREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (PARTITION_BY 'OrderId', EVICTION_BY 'LRUMEMSIZE 1000');","title":"Example: Create Table with Eviction Settings"},{"location":"reference/sql_reference/create-table/#example-create-column-table-with-comment-clause","text":"You can add comments about a column using the COMMENT clause. The COMMENT clause must be used in the column definition as shown in the following example: snappy> create table foobar (a string comment 'column 1', b string not null comment 'column 2') using column; snappy> describe foobar; col_name |data_type |comment ------------------------------ a |string |column 1 b |string |column 2","title":"Example: Create Column Table with COMMENT Clause"},{"location":"reference/sql_reference/create-table/#constraint-only-for-row-tables","text":"A CONSTRAINT clause is an optional part of a CREATE TABLE statement that defines a rule to which table data must conform. There are two types of constraints: Column-level constraints : Refer to a single column in the table and do not specify a column name (except check constraints). They refer to the column that they follow. Table-level constraints : Refer to one or more columns in the table. Table-level constraints specify the names of the columns to which they apply. Table-level CHECK constraints can refer to 0 or more columns in the table. Column and table constraints include: NOT NULL\u2014 Specifies that a column cannot hold NULL values (constraints of this type are not nameable). PRIMARY KEY\u2014 Specifies a column (or multiple columns if specified in a table constraint) that uniquely identifies a row in the table. The identified columns must be defined as NOT NULL. UNIQUE\u2014 Specifies that values in the column must be unique. NULL values are not allowed. FOREIGN KEY\u2014 Specifies that the values in the columns must correspond to values in the referenced primary key or unique columns or that they are NULL. If the foreign key consists of multiple columns and any column is NULL, then the whole key is considered NULL. SnappyData permits the insert no matter what is in the non-null columns. CHECK\u2014 Specifies rules for values in a column, or specifies a wide range of rules for values when included as a table constraint. The CHECK constraint has the same format and restrictions for column and table constraints. Column constraints and table constraints have the same function; the difference is where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition. Column-level constraints (except for check constraints) refer to only one column. If you do not specify a name for a column or table constraint, then SnappyData generates a unique name. Example : The following example demonstrates how to create a table with FOREIGN KEY : snappy> create table trading.customers (cid int not null, cust_name varchar(100), since date, addr varchar(100), tid int, primary key (cid)); snappy> create table trading.networth (cid int not null, cash decimal (30, 20), securities decimal (30, 20), loanlimit int, availloan decimal (30, 20), tid int, constraint netw_pk primary key (cid), constraint cust_newt_fk foreign key (cid) references trading.customers (cid)); snappy> show importedkeys in trading; PKTABLE_NAME |PKCOLUMN_NAME |PK_NAME |FKTABLE_SCHEM |FKTABLE_NAME |FKCOLUMN_NAME |FK_NAME |KEY_SEQ ---------------------------------------------------------------------------------------------------------------------------------------------------------- CUSTOMERS |CID |SQL180403162038220 |TRADING |NETWORTH |CID |CUST_NEWT_FK |1","title":"Constraint (only for Row Tables)"},{"location":"reference/sql_reference/create-table/#identity-columns-only-for-row-tables","text":"SnappyData supports both GENERATED ALWAYS and GENERATED BY DEFAULT identity columns only for BIGINT and INTEGER data types. The START WITH and INCREMENT BY clauses are supported only for GENERATED BY DEFAULT identity columns. For a GENERATED ALWAYS identity column, SnappyData increments the default value on every insertion, and stores the incremented value in the column. You cannot insert a value directly into a GENERATED ALWAYS identity column, and you cannot update a value in a GENERATED ALWAYS identity column. Instead, you must either specify the DEFAULT keyword when inserting data into the table or you must leave the identity column out of the insertion column list. Consider a table with the following column definition: create table greetings (i int generated always as identity, ch char(50)) using row; You can insert rows into the table using either the DEFAULT keyword or by omitting the identity column from the INSERT statement: insert into greetings values (DEFAULT, 'hello'); insert into greetings(ch) values ('hi'); The values that SnappyData automatically generates for a GENERATED ALWAYS identity column are unique. For a GENERATED BY DEFAULT identity column, SnappyData increments and uses a default value for an INSERT only when no explicit value is given. To use the generated default value, either specify the DEFAULT keyword when inserting into the identity column or leave the identity column out of the INSERT column list. In contrast to GENERATED ALWAYS identity columns, with a GENERATED BY DEFAULT column you can specify an identity value to use instead of the generated default value. To specify a value, include it in the INSERT statement. For example, consider a table created using the statement: create table greetings (i int generated by default as identity, ch char(50)); The following statement specifies the value \u201c1\u201d for the identity column: insert into greetings values (1, 'hi'); These statements both use generated default values: insert into greetings values (DEFAULT, 'hello'); insert into greetings(ch) values ('bye'); Although the automatically-generated values in a GENERATED BY DEFAULT identity column are unique, a GENERATED BY DEFAULT column does not guarantee unique identity values for all rows in the table. For example, in the above statements, the rows containing \u201chi\u201d and \u201chello\u201d both have an identity value of \u201c1.\u201d This occurs because the generated column starts at \u201c1\u201d and the user-specified value was also \u201c1.\u201d To avoid duplicating identity values (for example, during an import operation), you can use the START WITH clause to specify the first identity value that SnappyData should assign and increment. Or, you can use a primary key or a unique constraint on the GENERATED BY DEFAULT identity column to check for and disallow duplicates. By default, the initial value of a GENERATED BY DEFAULT identity column is 1, and the value is incremented by 1 for each INSERT. Use the optional START WITH clause to specify a new initial value. Use the optional INCREMENT BY clause to change the increment value used during each INSERT. Related Topics DROP TABLE DELETE TABLE SHOW TABLES TRUNCATE TABLE","title":"Identity Columns (only for Row Tables)"},{"location":"reference/sql_reference/create-temporary-table/","text":"CREATE TEMPORARY TABLE \u00b6 CREATE TEMPORARY TABLE table_name USING datasource [AS select_statement]; For more information on column-definition, refer to Column Definition For Column Table . Refer to these sections for more information on Creating Table , Creating Sample Table , Creating External Table and Creating Stream Table . TEMPORARY Temporary tables are scoped to SQL connection or the Snappy Spark session that creates it. This table does not appear in the system catalog nor visible to other connections or sessions. USING Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. AS Populate the table with input data from the select statement. Examples \u00b6 snappy> CREATE TEMPORARY TABLE STAGING_AIRLINEREF USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData'); snappy> CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF; Note When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results. Ensure that you create temporary tables with a unique name. CREATE GLOBAL TEMPORARY TABLE \u00b6 snappy> CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] USING PARQUET OPTIONS(path 'path-to-parquet'); snappy> CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] AS SELECT [column-name], [column-name] FROM [table-name]; Description \u00b6 Specifies a table definition that is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table. Examples \u00b6 snappy> CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINEREF1 USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData'); snappy> CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINE2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF; Note Temporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.","title":"CREATE TEMPORARY TABLE"},{"location":"reference/sql_reference/create-temporary-table/#create-temporary-table","text":"CREATE TEMPORARY TABLE table_name USING datasource [AS select_statement]; For more information on column-definition, refer to Column Definition For Column Table . Refer to these sections for more information on Creating Table , Creating Sample Table , Creating External Table and Creating Stream Table . TEMPORARY Temporary tables are scoped to SQL connection or the Snappy Spark session that creates it. This table does not appear in the system catalog nor visible to other connections or sessions. USING Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. AS Populate the table with input data from the select statement.","title":"CREATE TEMPORARY TABLE"},{"location":"reference/sql_reference/create-temporary-table/#examples","text":"snappy> CREATE TEMPORARY TABLE STAGING_AIRLINEREF USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData'); snappy> CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF; Note When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results. Ensure that you create temporary tables with a unique name.","title":"Examples"},{"location":"reference/sql_reference/create-temporary-table/#create-global-temporary-table","text":"snappy> CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] USING PARQUET OPTIONS(path 'path-to-parquet'); snappy> CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] AS SELECT [column-name], [column-name] FROM [table-name];","title":"CREATE GLOBAL TEMPORARY TABLE"},{"location":"reference/sql_reference/create-temporary-table/#description","text":"Specifies a table definition that is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table.","title":"Description"},{"location":"reference/sql_reference/create-temporary-table/#examples_1","text":"snappy> CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINEREF1 USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData'); snappy> CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINE2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF; Note Temporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.","title":"Examples"},{"location":"reference/sql_reference/create-view/","text":"CREATE VIEW \u00b6 SYNTAX \u00b6 snappy> CREATE VIEW [view-name] AS SELECT [column_name, column_name] FROM [table_name]; snappy> CREATE VIEW [view-name] (column_name, column_name) AS SELECT column_name, column_name FROM [table_name]; Description \u00b6 The View can be described as a virtual table that contains a set of definitions, built on top of the table(s) or the other view(s), but it does not physically store the data like a table. View is persistent and visible in system catalog and therefore shared between all connections. Examples \u00b6 snappy> CREATE VIEW TRADE.ORDERS1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; snappy> CREATE VIEW TRADE.ORDERS2 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; CREATE TEMPORARY VIEW \u00b6 snappy> CREATE TEMPORARY VIEW [temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name]; snappy> CREATE TEMPORARY VIEW [temporary-view-name] USING PARQUET OPTIONS(PATH 'path-to-parquet'); Description \u00b6 Creates a session-specific temporary view, which is dropped when the session ends. Temporary views have the same restrictions as permanent views, so you cannot perform insert, update, delete, or copy operations on these views. Local temporary views are session-scoped; the view drops automatically when the session ends. Examples \u00b6 snappy> CREATE TEMPORARY VIEW AIRLINEVIEW1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; snappy> CREATE TEMPORARY VIEW AIRLINEVIEW2 USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA'); CREATE GLOBAL TEMPORARY VIEW \u00b6 snappy> CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name]; snappy> CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] USING PARQUET OPTIONS(path 'path-to-parquet'); Description \u00b6 Creates a global temporary view this is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table. The optional GLOBAL keyword allows the view to be shared among all connections but it is not persisted to system catalog so will disappear when lead/driver restarts or fails. Use CREATE VIEW for persistent views. Examples \u00b6 snappy> CREATE GLOBAL TEMPORARY VIEW ORDER AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; snappy> CREATE GLOBAL TEMPORARY VIEW AIRLINEVIEW USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA'); Note Temporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with the former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.","title":"CREATE VIEW"},{"location":"reference/sql_reference/create-view/#create-view","text":"","title":"CREATE VIEW"},{"location":"reference/sql_reference/create-view/#syntax","text":"snappy> CREATE VIEW [view-name] AS SELECT [column_name, column_name] FROM [table_name]; snappy> CREATE VIEW [view-name] (column_name, column_name) AS SELECT column_name, column_name FROM [table_name];","title":"SYNTAX"},{"location":"reference/sql_reference/create-view/#description","text":"The View can be described as a virtual table that contains a set of definitions, built on top of the table(s) or the other view(s), but it does not physically store the data like a table. View is persistent and visible in system catalog and therefore shared between all connections.","title":"Description"},{"location":"reference/sql_reference/create-view/#examples","text":"snappy> CREATE VIEW TRADE.ORDERS1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; snappy> CREATE VIEW TRADE.ORDERS2 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;","title":"Examples "},{"location":"reference/sql_reference/create-view/#create-temporary-view","text":"snappy> CREATE TEMPORARY VIEW [temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name]; snappy> CREATE TEMPORARY VIEW [temporary-view-name] USING PARQUET OPTIONS(PATH 'path-to-parquet');","title":"CREATE TEMPORARY VIEW"},{"location":"reference/sql_reference/create-view/#description_1","text":"Creates a session-specific temporary view, which is dropped when the session ends. Temporary views have the same restrictions as permanent views, so you cannot perform insert, update, delete, or copy operations on these views. Local temporary views are session-scoped; the view drops automatically when the session ends.","title":"Description"},{"location":"reference/sql_reference/create-view/#examples_1","text":"snappy> CREATE TEMPORARY VIEW AIRLINEVIEW1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; snappy> CREATE TEMPORARY VIEW AIRLINEVIEW2 USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA');","title":"Examples"},{"location":"reference/sql_reference/create-view/#create-global-temporary-view","text":"snappy> CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name]; snappy> CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] USING PARQUET OPTIONS(path 'path-to-parquet');","title":"CREATE GLOBAL TEMPORARY VIEW"},{"location":"reference/sql_reference/create-view/#description_2","text":"Creates a global temporary view this is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table. The optional GLOBAL keyword allows the view to be shared among all connections but it is not persisted to system catalog so will disappear when lead/driver restarts or fails. Use CREATE VIEW for persistent views.","title":"Description"},{"location":"reference/sql_reference/create-view/#examples_2","text":"snappy> CREATE GLOBAL TEMPORARY VIEW ORDER AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS; snappy> CREATE GLOBAL TEMPORARY VIEW AIRLINEVIEW USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA'); Note Temporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with the former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.","title":"Examples"},{"location":"reference/sql_reference/delete/","text":"DELETE \u00b6 Delete rows from a table. { DELETE FROM table-name [ ] [ WHERE ] } Description \u00b6 This form is called a searched delete, removes all rows identified by the table name and WHERE clause. Example \u00b6 // Delete rows from the CUSTOMERS table where the CID is equal to 10. DELETE FROM TRADE.CUSTOMERS WHERE CID = 10; // Delete all rows from table T. DELETE FROM T; Related Topics CREATE TABLE DROP TABLE SHOW TABLES TRUNCATE TABLE","title":"DELETE"},{"location":"reference/sql_reference/delete/#delete","text":"Delete rows from a table. { DELETE FROM table-name [ ] [ WHERE ] }","title":"DELETE"},{"location":"reference/sql_reference/delete/#description","text":"This form is called a searched delete, removes all rows identified by the table name and WHERE clause.","title":"Description"},{"location":"reference/sql_reference/delete/#example","text":"// Delete rows from the CUSTOMERS table where the CID is equal to 10. DELETE FROM TRADE.CUSTOMERS WHERE CID = 10; // Delete all rows from table T. DELETE FROM T; Related Topics CREATE TABLE DROP TABLE SHOW TABLES TRUNCATE TABLE","title":"Example"},{"location":"reference/sql_reference/deploy/","text":"DEPLOY \u00b6 Use the DEPLOY statements to deploy packages and dependency jars. You can also undeploy the deployed jars. DEPLOY PACKAGE DEPLOY JAR UNDEPLOY","title":"DEPLOY"},{"location":"reference/sql_reference/deploy/#deploy","text":"Use the DEPLOY statements to deploy packages and dependency jars. You can also undeploy the deployed jars. DEPLOY PACKAGE DEPLOY JAR UNDEPLOY","title":"DEPLOY"},{"location":"reference/sql_reference/deploy_jar/","text":"DEPLOY JARS \u00b6 Deploys a jar in a running system. Syntax \u00b6 deploy jar <unique-alias-name> 'jars' * unique-alias-name - A name to identify the jar. This name can be used to remove the jar from the cluster. You can use alphabets, numbers and underscores to create the name. jars - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData. Description \u00b6 SnappyData provides a method to deploy a jar in a running system through SQL. You can execute the deploy jar command to deploy dependency jars. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system. Example \u00b6 deploy jar SparkDaria 'spark-daria_2.11.8-2.2.0_0.10.0.jar'; Related Topics DEPLOY PACKAGE UNDEPLOY","title":"DEPLOY JAR"},{"location":"reference/sql_reference/deploy_jar/#deploy-jars","text":"Deploys a jar in a running system.","title":"DEPLOY JARS"},{"location":"reference/sql_reference/deploy_jar/#syntax","text":"deploy jar <unique-alias-name> 'jars' * unique-alias-name - A name to identify the jar. This name can be used to remove the jar from the cluster. You can use alphabets, numbers and underscores to create the name. jars - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData.","title":"Syntax"},{"location":"reference/sql_reference/deploy_jar/#description","text":"SnappyData provides a method to deploy a jar in a running system through SQL. You can execute the deploy jar command to deploy dependency jars. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system.","title":"Description"},{"location":"reference/sql_reference/deploy_jar/#example","text":"deploy jar SparkDaria 'spark-daria_2.11.8-2.2.0_0.10.0.jar'; Related Topics DEPLOY PACKAGE UNDEPLOY","title":"Example"},{"location":"reference/sql_reference/deploy_package/","text":"DEPLOY PACKAGES \u00b6 Deploys package in SnappyData. Syntax \u00b6 deploy package <unique-alias-name> \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ] * unique-alias-name - A name to identify a package. This name can be used to remove the package from the cluster. You can use alphabets, numbers, and underscores to create the name. packages - Comma-delimited string of maven packages. repos - Comma-delimited string of remote repositories other than the Maven Central and spark-package repositories. These two repositories are searched by default. The format specified by Maven is: groupId:artifactId:version . path - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the ivy.home property. if even that is not specified, the .ivy2 in the user\u2019s home directory is used. Description \u00b6 Packages can be deployed in SnappyData using the DEPLOY PACKAGE SQL. You can pass the following through this SQL: Name of the package. Repository where the package is located. Path to a local cache of jars. Note SnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise the resolution of the package fails. For resolving the package, Maven Central and Spark packages, located at http://dl.bintray.com/spark-packages, are searched by default. Hence, you must specify the repository only if the package is not there at Maven Central or in the spark-package repository. Tip Use spark-packages.org to search for Spark packages. Most of the popular Spark packages are listed here. Example \u00b6 Deploy packages from a default repository. deploy package spark_deep_learning_0_3_0 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work' deploy package spark_redshift_300 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work' Deploy packages from a non-default repository. Related Topics DEPLOY JAR","title":"DEPLOY PACKAGE"},{"location":"reference/sql_reference/deploy_package/#deploy-packages","text":"Deploys package in SnappyData.","title":"DEPLOY PACKAGES"},{"location":"reference/sql_reference/deploy_package/#syntax","text":"deploy package <unique-alias-name> \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ] * unique-alias-name - A name to identify a package. This name can be used to remove the package from the cluster. You can use alphabets, numbers, and underscores to create the name. packages - Comma-delimited string of maven packages. repos - Comma-delimited string of remote repositories other than the Maven Central and spark-package repositories. These two repositories are searched by default. The format specified by Maven is: groupId:artifactId:version . path - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the ivy.home property. if even that is not specified, the .ivy2 in the user\u2019s home directory is used.","title":"Syntax"},{"location":"reference/sql_reference/deploy_package/#description","text":"Packages can be deployed in SnappyData using the DEPLOY PACKAGE SQL. You can pass the following through this SQL: Name of the package. Repository where the package is located. Path to a local cache of jars. Note SnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise the resolution of the package fails. For resolving the package, Maven Central and Spark packages, located at http://dl.bintray.com/spark-packages, are searched by default. Hence, you must specify the repository only if the package is not there at Maven Central or in the spark-package repository. Tip Use spark-packages.org to search for Spark packages. Most of the popular Spark packages are listed here.","title":"Description"},{"location":"reference/sql_reference/deploy_package/#example","text":"Deploy packages from a default repository. deploy package spark_deep_learning_0_3_0 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work' deploy package spark_redshift_300 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work' Deploy packages from a non-default repository. Related Topics DEPLOY JAR","title":"Example"},{"location":"reference/sql_reference/drop-diskstore/","text":"DROP DISKSTORE \u00b6 Removes a disk store configuration from the SnappyData cluster. DROP DISKSTORE [ IF EXISTS ] store-name IF EXISTS Include the IF EXISTS clause to execute the statement only if the specified disk store exists in SnappyData. store-name User-defined name of the disk store configuration that you want to remove. The available names are stored in the SYSDISKSTORES system table. Example \u00b6 This command removes the disk store \"STORE1\" from the cluster: DROP DISKSTORE store1; Related Topics CREATE DISKSTORE SYSDISKSTORES","title":"DROP DISKSTORE"},{"location":"reference/sql_reference/drop-diskstore/#drop-diskstore","text":"Removes a disk store configuration from the SnappyData cluster. DROP DISKSTORE [ IF EXISTS ] store-name IF EXISTS Include the IF EXISTS clause to execute the statement only if the specified disk store exists in SnappyData. store-name User-defined name of the disk store configuration that you want to remove. The available names are stored in the SYSDISKSTORES system table.","title":"DROP DISKSTORE"},{"location":"reference/sql_reference/drop-diskstore/#example","text":"This command removes the disk store \"STORE1\" from the cluster: DROP DISKSTORE store1; Related Topics CREATE DISKSTORE SYSDISKSTORES","title":"Example"},{"location":"reference/sql_reference/drop-function/","text":"DROP FUNCTION \u00b6 DROP FUNCTION IF EXISTS udf_name Description \u00b6 Drops an existing function. If the function to drop does not exist, an exception is reported. Example \u00b6 DROP FUNCTION IF EXISTS app.strnglen Related Topics CREATE FUNCTION","title":"DROP FUNCTION"},{"location":"reference/sql_reference/drop-function/#drop-function","text":"DROP FUNCTION IF EXISTS udf_name","title":"DROP FUNCTION"},{"location":"reference/sql_reference/drop-function/#description","text":"Drops an existing function. If the function to drop does not exist, an exception is reported.","title":"Description"},{"location":"reference/sql_reference/drop-function/#example","text":"DROP FUNCTION IF EXISTS app.strnglen Related Topics CREATE FUNCTION","title":"Example"},{"location":"reference/sql_reference/drop-index/","text":"DROP INDEX \u00b6 DROP INDEX [ IF EXISTS ] [schema-name.]index-name; Description \u00b6 Drops the index in the given schema (or current schema if none is provided). Include the IF EXISTS clause to execute the statement only if the specified index exists in SnappyData. Example \u00b6 DROP INDEX IF EXISTS app.idx;","title":"DROP INDEX"},{"location":"reference/sql_reference/drop-index/#drop-index","text":"DROP INDEX [ IF EXISTS ] [schema-name.]index-name;","title":"DROP INDEX"},{"location":"reference/sql_reference/drop-index/#description","text":"Drops the index in the given schema (or current schema if none is provided). Include the IF EXISTS clause to execute the statement only if the specified index exists in SnappyData.","title":"Description"},{"location":"reference/sql_reference/drop-index/#example","text":"DROP INDEX IF EXISTS app.idx;","title":"Example"},{"location":"reference/sql_reference/drop-schema/","text":"DROP SCHEMA \u00b6 DROP SCHEMA [schema-name] restrict; Description \u00b6 Permanently removes a schema from the database. Ensure that you delete all the objects that exist in a schema before you drop it. This is an irreversible process. Example \u00b6 drop schema trade restrict;","title":"DROP SCHEMA"},{"location":"reference/sql_reference/drop-schema/#drop-schema","text":"DROP SCHEMA [schema-name] restrict;","title":"DROP SCHEMA"},{"location":"reference/sql_reference/drop-schema/#description","text":"Permanently removes a schema from the database. Ensure that you delete all the objects that exist in a schema before you drop it. This is an irreversible process.","title":"Description"},{"location":"reference/sql_reference/drop-schema/#example","text":"drop schema trade restrict;","title":"Example"},{"location":"reference/sql_reference/drop-statements/","text":"Drop Statements \u00b6 The drop statements are used to drop diskstores, functions, indexes, schemas, and tables. DROP DISKSTORE DROP FUNCTION DROP INDEX DROP SCHEMA DROP TABLE","title":"Drop Statements"},{"location":"reference/sql_reference/drop-statements/#drop-statements","text":"The drop statements are used to drop diskstores, functions, indexes, schemas, and tables. DROP DISKSTORE DROP FUNCTION DROP INDEX DROP SCHEMA DROP TABLE","title":"Drop Statements"},{"location":"reference/sql_reference/drop-synonym/","text":"DROP SYNONYM \u00b6 DROP SYNONYM [ IF EXISTS ] [schema-name.]synonym-name; Description \u00b6 Removes the specified synonym. Include the IF EXISTS clause to execute the statement only if the specified synonym exists in SnappyData. The schema-name. prefix is optional if you are currently using the schema that contains the synonym. Example \u00b6 DROP SYNONYM IF EXISTS app.myairline; Related Topics CREATE SYNONYM","title":"DROP SYNONYM"},{"location":"reference/sql_reference/drop-synonym/#drop-synonym","text":"DROP SYNONYM [ IF EXISTS ] [schema-name.]synonym-name;","title":"DROP SYNONYM"},{"location":"reference/sql_reference/drop-synonym/#description","text":"Removes the specified synonym. Include the IF EXISTS clause to execute the statement only if the specified synonym exists in SnappyData. The schema-name. prefix is optional if you are currently using the schema that contains the synonym.","title":"Description"},{"location":"reference/sql_reference/drop-synonym/#example","text":"DROP SYNONYM IF EXISTS app.myairline; Related Topics CREATE SYNONYM","title":"Example"},{"location":"reference/sql_reference/drop-table/","text":"DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE \u00b6 DROP TABLE [ IF EXISTS ] [schema-name.]table-name Description \u00b6 Removes the specified table. Include the IF EXISTS clause to execute the statement only if the specified table exists in SnappyData. The schema-name. prefix is optional if you are currently using the schema that contains the table. Example \u00b6 DROP TABLE IF EXISTS app.customer Related Topics CREATE TABLE CREATE EXTERNAL TABLE CREATE SAMPLE TABLE DELETE TABLE SHOW TABLES TRUNCATE TABLE","title":"DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE"},{"location":"reference/sql_reference/drop-table/#drop-tableexternal-tablesample-table","text":"DROP TABLE [ IF EXISTS ] [schema-name.]table-name","title":"DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE"},{"location":"reference/sql_reference/drop-table/#description","text":"Removes the specified table. Include the IF EXISTS clause to execute the statement only if the specified table exists in SnappyData. The schema-name. prefix is optional if you are currently using the schema that contains the table.","title":"Description"},{"location":"reference/sql_reference/drop-table/#example","text":"DROP TABLE IF EXISTS app.customer Related Topics CREATE TABLE CREATE EXTERNAL TABLE CREATE SAMPLE TABLE DELETE TABLE SHOW TABLES TRUNCATE TABLE","title":"Example"},{"location":"reference/sql_reference/exec-scala/","text":"exec scala \u00b6 Description \u00b6 exec scala is an SQL feature that you can use to submit Scala code to the SnappyData cluster. This is a SQL construct from any JDBC/ODBC client and one of the ways in which you can submit the Scala code to the SnappyData cluster. You can submit a chunk of Scala code similar to a SQL query, which is submitted to a database on a JDBC/ODBC connection including hive thrift server SQL clients such as beeline. A parallel between a SQL query and a block of Scala code is brought about by using a fixed schema for the Scala code. Since, a select query's result set metadata is fixed, whereas there is no meaning of a fixed schema for a chunk of Scala code. Therefore, SnappyData provides a fixed schema to all the Scala code. This is elaborated in the following sections. How Does it Work? \u00b6 All the code from exec scala is executed using the Scala REPL on the SnappyData Lead node. When Spark Dataframes are invoked this would automatically result in workload distribution across all the SnappyData servers. The Lead node manages a pool of REPL based interpreters. The user SQL activity is delegated to one of the interpreters from this pool. The pool is lazily created. Any connection (JDBC or ODBC) results in the creation of a SnappySession within the CDB cluster. Moreover, the session remains associated with the connection until it is closed or dereferenced. The first time exec scala is executed, an interpreter from the pool gets associated with the connection. This allows the user to manage any adhoc private state on the server side. For example, any variables, objects, or even classes created will be isolated from other users. The functioning of the interpreter is same as that of the interactive Spark-shell only with one difference. As commands are interpreted any output generated will be cached in a buffer. And, when the command is done, the cached output will be available in the client side ResultSet object. Important Information about exec scala \u00b6 Arbitrary Scala code can be executed through exec scala. A SnappySession object and the SparkContext object can be accessed through the Symbol name snappysession and sc in the exec scala code. If you are having a loop inside your Scala code, then the output of each loop is not returned after the execution of each loop. The entire execution happens first, and after that, the output is fetched at once. This is unlike the spark-shell. In a spark-shell or Scala shell, the output of each loop execution is printed on the console. If you run rogue code such as System.exit or a divide by 0 scenario, the lead node is bound to crash. The system does not check the code before executing it. In a non-secured system, anyone can run exec scala . In a secured system only the dbowner or database owner can run exec scala by default. However, a database owner can grant and revoke privileges to and from users and LDAP groups. Multiple sessions can run exec scala . These sessions can be concurrent too. All the sessions are isolated from each other, that is any variables or/and types defined in one session that is visible only in the same session. By default, the output of this SQL is a single column resultset of type varchar . Essentially, whatever the interpreter writes on the console is brought back to the caller/user. The user can specify the name of the dataframe instead as a return result instead of the default behavior. The symbol should be of a valid existing dataframe symbol in the context of the session. Syntax \u00b6 exec scala [options (returnDF \u2018dfName\u2019)] <Scala_code> exec and scala are the keywords to identify this SQL type. options is an optional part of the syntax. If it is present, then after the keyword options , you can specify the allowed options inside parentheses. Currently, only one optional parameter, that is returnDF , can be specified with the execution. For this option, you can provide the name of any actual symbol in the Scala code, which is of type DataFrame. Through the returnDF option, you can request the system to return the result of the specific dataframe, which got created as the result of the Scala code execution. By default, the exec scala just returns the output of each interpreted line, which the interpreter prints on the Console after executing each line. Examples \u00b6 Examples I \u00b6 Following are some examples to demonstrate the usage of exec scala . You can run these examples using Snappy shell . A simple Scala code to define a value x and print it. exec scala val x = 5 /* This is a test program */ // Just see the value of x println(x); C0 ------------------------------------------------------------------------------------------------------ x: Int = 5 5 Create table using the available snappy session, which can be accessed through the variable \u2018snappysession\u2019. It then inserts a couple of rows, obtains a dataframe object, and then uses the df.show command. snappy> exec scala snappysession.sql(\"create table t1(c1 int not null)\") /* This is a data frame test */ // Check the collect output snappysession.sql(\"insert into t1 values (1), (2)\") val df = snappysession.table(\"t1\") val cnt = df.count df.show; C0 -------------------------------------------------------------------------------------------------- res4: org.apache.spark.sql.DataFrame = [] res7: org.apache.spark.sql.DataFrame = [count: int] df: org.apache.spark.sql.DataFrame = [c1: int] cnt: Long = 2 +---+ | c1| +---+ | 2| | 1| +---+ Note The variable snappysession is not declared anywhere; however, the above code uses it. The snappysession symbol name is for the object SnappySession , which represents this database connection. Similarly, sc is there to access the singleton SparkContext present on the lead node. Examples II \u00b6 Executing scala code snippet via JDBC or ODBC connection on to the SnappyData cluster. // Note this is an SQL command... this is the text you will send using a JDBC or ODBC connection. val prepDataCommand = \"\"\" exec scala val dataDF = snappysession.read.option(\"header\", \"true\").csv(\"../path/to/customers.csv\") // Variable 'snappysession' is injected to your program automatically. // Refers to a SnappySession instance. val newDF = dataDF.withColumn(\"promotion\", \"$20\") newDF.createOrReplaceTempView(\"customers\") //OR, store in in-memory table newDF.write.format(\"column\").saveAsTable(\"customers\") \"\"\" // Acquire JDBC connection and execute Spark program Class.forName(\"io.snappydata.jdbc.ClientDriver\") val conn = DriverManager.getConnection(\"jdbc:snappydata://localhost:1527/\") conn.createStatement().executeSQL(prepDataCommand) Returning a specific Dataframe using keyword returnDF . Note the use of the option returnDF here. Through this option, you can request the system to return the result of the specific dataframe you want, which got created as the result of the Scala code execution. val getDataFromCDB = \"\"\" exec scala options(returnDF 'someDF') val someDF = snappysession.table(\"customers\").filter(..... ) \"\"\" ResultSet rs = conn.createStatement().executeSQL(getDataFromCDB) //Use JDBC ResultSet API to fetch result. Data types will be mapped automatically Declaring a case class and then creating a dataset using it. Also creating another dataset and then getting a dataframe on it. Note the use of the option returnDF here. Through this option, you can request the system to return the result of the specific dataframe you want, which got created as the result of the Scala code execution. Here both ds1 and ds2 are created. However, the caller wants the output of the ds2 and hence specified the symbol ds2 in the options. By default, the exec scala returns the output of each interpreted line, which the interpreter prints on the console after executing each line. exec scala options(returnDF 'ds2') case class ClassData(a: String, b: Int) val sqlContext= new org.apache.spark.sql.SQLContext(sc) import sqlContext.implicits._ val ds1 = Seq((\"a\", 1), (\"b\", 2), (\"c\", 3)).toDF(\"a\", \"b\").as[ClassData] var rdd = sc.parallelize(Seq((\"a\", 1), (\"b\", 2), (\"c\", 3)), 1) val ds2 = rdd.toDF(\"a\", \"b\").as[ClassData]; a |b --------------------------------------------------------------------------- a |1 b |2 c |3 3 rows selected Note The use of the sc symbol here in this example. This is the global SparkContext present in the lead node. Securing the Usage of exec scala SQL \u00b6 The ability to run Scala code directly on a running cluster can be dangerous. This is because there are no checks on what code you can run. The submitted Scala code is executed on the lead node and has the potential to bring it down. It becomes essential to secure the use of this functionality. By default, in a secure cluster, only the database owner is allowed to run Scala code through exec scala SQL or even through snappy-scala shell. The database owner is the user who brings up the SnappyData cluster. If different credentials are used for different components of the SnappyData cluster, then the credentials with which the lead node is started becomes the database owner for this purpose. Ideally, every node should start with the same superuser credentials. The superuser or the database owner, in turn, can grant the privilege of executing Scala code to other users and even to entire LDAP groups. Similarly, it is only the superuser who can revoke this privilege from any user or LDAP group. You can use the following DDL for this purpose: GRANT grant privilege exec scala to <user(s) and or ldap_group(s)> Examples grant privilege exec scala to user1 revoke privilege exec scala from user1 grant privilege exec scala to user1,user2 grant privilege exec scala to LDAPGROUP:group1 revoke privilege exec scala from user2,LDAPGROUP:group1 Known Issues and Limitations \u00b6 An interpreter cannot serialize a dependent closure properly. A dependent closure is the closure that refers to some other class, function, or variable that is defined outside the closure. Thus, the following example fails with closure serialization error. exec scala def multiply(number: Int, factor: Int): Int = { number * factor } val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(multiply(_, 2)).collect() The execution of the last line fails as the closure cannot be serialized due to this issue. This is referring to the function multiply that is defined outside the closure. Similarly, even the following example fails: val x = 5 val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(_ * x).collect() This is because the closure is referring to x , which is defined outside the closure. There are no issues if the closure has no dependency on any external variables. Exceptions \u00b6 Following are some examples of exceptions that can occur in the code that is executed through exec scala . Attention Whenever an exception occurs in the code that is executed via exec scala, then the error code, as part of SQLEexception, is SQLState=38000 . Hence, if you get an SQLException with an error code other than SQLState=38000 , the exception is originated from another part of code and not from the snippet that is passed via exec scala. Examples \u00b6 Example 1 execstr = \"exe scala snappysession.sql(\\\"create table testtab (col1 int)\\\").show\" java.sql.SQLException: (SQLState=42X01 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-9) Syntax error: Invalid input \"exe \", expected select, insert, put, update, delete, ctes, dmlOperation, putValuesOperation or ddl (line 1, column 1): exe scala snappysession.sql(\"create table testtab (col1 int)\").show ^;. Example 2 execstr = \"exec scala snappysession.sql(\\\"create table testtab (col1 int)\\\").show\" java.sql.SQLException: (SQLState=38000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-9) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(16536)<v1>:30591, caused by java.lang.RuntimeException: Got error while interpreting line snappysession.sql(\"create table testtab (col1 int)\").show and interpreter output = org.apache.spark.sql.AnalysisException: Table app.testtab already exists.; Example 3 execstr = \"exec scala snappysession.sql(\\\"create ta testtab (col1 int)\\\").show\" java.sql.SQLException: (SQLState=38000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-9) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(16536)<v1>:30591, caused by java.lang.RuntimeException: Got error while interpreting line snappysession.sql(\"create ta testtab (col1 int)\").show and interpreter output = org.apache.spark.sql.ParseException: Invalid input \"ta \", expected TABLE, EXTERNAL, OR, globalOrTemporary, VIEW, SCHEMA, POLICY, STREAM, GLOBAL, UNIQUE, INDEX, TEMPORARY or FUNCTION (line 1, column 8): create ta testtab (col1 int) ^;","title":"exec scala"},{"location":"reference/sql_reference/exec-scala/#exec-scala","text":"","title":"exec scala"},{"location":"reference/sql_reference/exec-scala/#description","text":"exec scala is an SQL feature that you can use to submit Scala code to the SnappyData cluster. This is a SQL construct from any JDBC/ODBC client and one of the ways in which you can submit the Scala code to the SnappyData cluster. You can submit a chunk of Scala code similar to a SQL query, which is submitted to a database on a JDBC/ODBC connection including hive thrift server SQL clients such as beeline. A parallel between a SQL query and a block of Scala code is brought about by using a fixed schema for the Scala code. Since, a select query's result set metadata is fixed, whereas there is no meaning of a fixed schema for a chunk of Scala code. Therefore, SnappyData provides a fixed schema to all the Scala code. This is elaborated in the following sections.","title":"Description"},{"location":"reference/sql_reference/exec-scala/#how-does-it-work","text":"All the code from exec scala is executed using the Scala REPL on the SnappyData Lead node. When Spark Dataframes are invoked this would automatically result in workload distribution across all the SnappyData servers. The Lead node manages a pool of REPL based interpreters. The user SQL activity is delegated to one of the interpreters from this pool. The pool is lazily created. Any connection (JDBC or ODBC) results in the creation of a SnappySession within the CDB cluster. Moreover, the session remains associated with the connection until it is closed or dereferenced. The first time exec scala is executed, an interpreter from the pool gets associated with the connection. This allows the user to manage any adhoc private state on the server side. For example, any variables, objects, or even classes created will be isolated from other users. The functioning of the interpreter is same as that of the interactive Spark-shell only with one difference. As commands are interpreted any output generated will be cached in a buffer. And, when the command is done, the cached output will be available in the client side ResultSet object.","title":"How Does it Work?"},{"location":"reference/sql_reference/exec-scala/#important-information-about-exec-scala","text":"Arbitrary Scala code can be executed through exec scala. A SnappySession object and the SparkContext object can be accessed through the Symbol name snappysession and sc in the exec scala code. If you are having a loop inside your Scala code, then the output of each loop is not returned after the execution of each loop. The entire execution happens first, and after that, the output is fetched at once. This is unlike the spark-shell. In a spark-shell or Scala shell, the output of each loop execution is printed on the console. If you run rogue code such as System.exit or a divide by 0 scenario, the lead node is bound to crash. The system does not check the code before executing it. In a non-secured system, anyone can run exec scala . In a secured system only the dbowner or database owner can run exec scala by default. However, a database owner can grant and revoke privileges to and from users and LDAP groups. Multiple sessions can run exec scala . These sessions can be concurrent too. All the sessions are isolated from each other, that is any variables or/and types defined in one session that is visible only in the same session. By default, the output of this SQL is a single column resultset of type varchar . Essentially, whatever the interpreter writes on the console is brought back to the caller/user. The user can specify the name of the dataframe instead as a return result instead of the default behavior. The symbol should be of a valid existing dataframe symbol in the context of the session.","title":"Important Information about exec scala"},{"location":"reference/sql_reference/exec-scala/#syntax","text":"exec scala [options (returnDF \u2018dfName\u2019)] <Scala_code> exec and scala are the keywords to identify this SQL type. options is an optional part of the syntax. If it is present, then after the keyword options , you can specify the allowed options inside parentheses. Currently, only one optional parameter, that is returnDF , can be specified with the execution. For this option, you can provide the name of any actual symbol in the Scala code, which is of type DataFrame. Through the returnDF option, you can request the system to return the result of the specific dataframe, which got created as the result of the Scala code execution. By default, the exec scala just returns the output of each interpreted line, which the interpreter prints on the Console after executing each line.","title":"Syntax"},{"location":"reference/sql_reference/exec-scala/#examples","text":"","title":"Examples"},{"location":"reference/sql_reference/exec-scala/#examples-i","text":"Following are some examples to demonstrate the usage of exec scala . You can run these examples using Snappy shell . A simple Scala code to define a value x and print it. exec scala val x = 5 /* This is a test program */ // Just see the value of x println(x); C0 ------------------------------------------------------------------------------------------------------ x: Int = 5 5 Create table using the available snappy session, which can be accessed through the variable \u2018snappysession\u2019. It then inserts a couple of rows, obtains a dataframe object, and then uses the df.show command. snappy> exec scala snappysession.sql(\"create table t1(c1 int not null)\") /* This is a data frame test */ // Check the collect output snappysession.sql(\"insert into t1 values (1), (2)\") val df = snappysession.table(\"t1\") val cnt = df.count df.show; C0 -------------------------------------------------------------------------------------------------- res4: org.apache.spark.sql.DataFrame = [] res7: org.apache.spark.sql.DataFrame = [count: int] df: org.apache.spark.sql.DataFrame = [c1: int] cnt: Long = 2 +---+ | c1| +---+ | 2| | 1| +---+ Note The variable snappysession is not declared anywhere; however, the above code uses it. The snappysession symbol name is for the object SnappySession , which represents this database connection. Similarly, sc is there to access the singleton SparkContext present on the lead node.","title":"Examples I"},{"location":"reference/sql_reference/exec-scala/#examples-ii","text":"Executing scala code snippet via JDBC or ODBC connection on to the SnappyData cluster. // Note this is an SQL command... this is the text you will send using a JDBC or ODBC connection. val prepDataCommand = \"\"\" exec scala val dataDF = snappysession.read.option(\"header\", \"true\").csv(\"../path/to/customers.csv\") // Variable 'snappysession' is injected to your program automatically. // Refers to a SnappySession instance. val newDF = dataDF.withColumn(\"promotion\", \"$20\") newDF.createOrReplaceTempView(\"customers\") //OR, store in in-memory table newDF.write.format(\"column\").saveAsTable(\"customers\") \"\"\" // Acquire JDBC connection and execute Spark program Class.forName(\"io.snappydata.jdbc.ClientDriver\") val conn = DriverManager.getConnection(\"jdbc:snappydata://localhost:1527/\") conn.createStatement().executeSQL(prepDataCommand) Returning a specific Dataframe using keyword returnDF . Note the use of the option returnDF here. Through this option, you can request the system to return the result of the specific dataframe you want, which got created as the result of the Scala code execution. val getDataFromCDB = \"\"\" exec scala options(returnDF 'someDF') val someDF = snappysession.table(\"customers\").filter(..... ) \"\"\" ResultSet rs = conn.createStatement().executeSQL(getDataFromCDB) //Use JDBC ResultSet API to fetch result. Data types will be mapped automatically Declaring a case class and then creating a dataset using it. Also creating another dataset and then getting a dataframe on it. Note the use of the option returnDF here. Through this option, you can request the system to return the result of the specific dataframe you want, which got created as the result of the Scala code execution. Here both ds1 and ds2 are created. However, the caller wants the output of the ds2 and hence specified the symbol ds2 in the options. By default, the exec scala returns the output of each interpreted line, which the interpreter prints on the console after executing each line. exec scala options(returnDF 'ds2') case class ClassData(a: String, b: Int) val sqlContext= new org.apache.spark.sql.SQLContext(sc) import sqlContext.implicits._ val ds1 = Seq((\"a\", 1), (\"b\", 2), (\"c\", 3)).toDF(\"a\", \"b\").as[ClassData] var rdd = sc.parallelize(Seq((\"a\", 1), (\"b\", 2), (\"c\", 3)), 1) val ds2 = rdd.toDF(\"a\", \"b\").as[ClassData]; a |b --------------------------------------------------------------------------- a |1 b |2 c |3 3 rows selected Note The use of the sc symbol here in this example. This is the global SparkContext present in the lead node.","title":"Examples II"},{"location":"reference/sql_reference/exec-scala/#securing-the-usage-of-exec-scala-sql","text":"The ability to run Scala code directly on a running cluster can be dangerous. This is because there are no checks on what code you can run. The submitted Scala code is executed on the lead node and has the potential to bring it down. It becomes essential to secure the use of this functionality. By default, in a secure cluster, only the database owner is allowed to run Scala code through exec scala SQL or even through snappy-scala shell. The database owner is the user who brings up the SnappyData cluster. If different credentials are used for different components of the SnappyData cluster, then the credentials with which the lead node is started becomes the database owner for this purpose. Ideally, every node should start with the same superuser credentials. The superuser or the database owner, in turn, can grant the privilege of executing Scala code to other users and even to entire LDAP groups. Similarly, it is only the superuser who can revoke this privilege from any user or LDAP group. You can use the following DDL for this purpose: GRANT grant privilege exec scala to <user(s) and or ldap_group(s)> Examples grant privilege exec scala to user1 revoke privilege exec scala from user1 grant privilege exec scala to user1,user2 grant privilege exec scala to LDAPGROUP:group1 revoke privilege exec scala from user2,LDAPGROUP:group1","title":"Securing the Usage of exec scala SQL"},{"location":"reference/sql_reference/exec-scala/#known-issues-and-limitations","text":"An interpreter cannot serialize a dependent closure properly. A dependent closure is the closure that refers to some other class, function, or variable that is defined outside the closure. Thus, the following example fails with closure serialization error. exec scala def multiply(number: Int, factor: Int): Int = { number * factor } val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(multiply(_, 2)).collect() The execution of the last line fails as the closure cannot be serialized due to this issue. This is referring to the function multiply that is defined outside the closure. Similarly, even the following example fails: val x = 5 val data = Array(1, 2, 3, 4, 5) val numbersRdd = sc.parallelize(data, 1) val collectedNumbers = numbersRdd.map(_ * x).collect() This is because the closure is referring to x , which is defined outside the closure. There are no issues if the closure has no dependency on any external variables.","title":"Known Issues and Limitations"},{"location":"reference/sql_reference/exec-scala/#exceptions","text":"Following are some examples of exceptions that can occur in the code that is executed through exec scala . Attention Whenever an exception occurs in the code that is executed via exec scala, then the error code, as part of SQLEexception, is SQLState=38000 . Hence, if you get an SQLException with an error code other than SQLState=38000 , the exception is originated from another part of code and not from the snippet that is passed via exec scala.","title":"Exceptions"},{"location":"reference/sql_reference/exec-scala/#examples_1","text":"Example 1 execstr = \"exe scala snappysession.sql(\\\"create table testtab (col1 int)\\\").show\" java.sql.SQLException: (SQLState=42X01 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-9) Syntax error: Invalid input \"exe \", expected select, insert, put, update, delete, ctes, dmlOperation, putValuesOperation or ddl (line 1, column 1): exe scala snappysession.sql(\"create table testtab (col1 int)\").show ^;. Example 2 execstr = \"exec scala snappysession.sql(\\\"create table testtab (col1 int)\\\").show\" java.sql.SQLException: (SQLState=38000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-9) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(16536)<v1>:30591, caused by java.lang.RuntimeException: Got error while interpreting line snappysession.sql(\"create table testtab (col1 int)\").show and interpreter output = org.apache.spark.sql.AnalysisException: Table app.testtab already exists.; Example 3 execstr = \"exec scala snappysession.sql(\\\"create ta testtab (col1 int)\\\").show\" java.sql.SQLException: (SQLState=38000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-9) The exception 'com.pivotal.gemfirexd.internal.engine.jdbc.GemFireXDRuntimeException: myID: 127.0.0.1(16536)<v1>:30591, caused by java.lang.RuntimeException: Got error while interpreting line snappysession.sql(\"create ta testtab (col1 int)\").show and interpreter output = org.apache.spark.sql.ParseException: Invalid input \"ta \", expected TABLE, EXTERNAL, OR, globalOrTemporary, VIEW, SCHEMA, POLICY, STREAM, GLOBAL, UNIQUE, INDEX, TEMPORARY or FUNCTION (line 1, column 8): create ta testtab (col1 int) ^;","title":"Examples"},{"location":"reference/sql_reference/explain/","text":"EXPLAIN \u00b6 Syntax \u00b6 snappy> explain select <query> Description \u00b6 Provides information about how your query is executed. Example \u00b6 snappy> explain select avg(arrdelay) from airline; plan -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- == Physical Plan == CollectAggregate +- *SnappyHashAggregate(ByteBufferHashMap used = false; keys=[], modes=Partial, functions=[partial_avg(cast(arrdelay#750 as bigint))]) +- *Partitioned Scan ColumnFormatRelation[app.airline], Requested Columns = [arrdelay#750] partitionColumns = [] numBuckets = 8 numPartitions = 2","title":"EXPLAIN"},{"location":"reference/sql_reference/explain/#explain","text":"","title":"EXPLAIN"},{"location":"reference/sql_reference/explain/#syntax","text":"snappy> explain select <query>","title":"Syntax"},{"location":"reference/sql_reference/explain/#description","text":"Provides information about how your query is executed.","title":"Description"},{"location":"reference/sql_reference/explain/#example","text":"snappy> explain select avg(arrdelay) from airline; plan -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- == Physical Plan == CollectAggregate +- *SnappyHashAggregate(ByteBufferHashMap used = false; keys=[], modes=Partial, functions=[partial_avg(cast(arrdelay#750 as bigint))]) +- *Partitioned Scan ColumnFormatRelation[app.airline], Requested Columns = [arrdelay#750] partitionColumns = [] numBuckets = 8 numPartitions = 2","title":"Example"},{"location":"reference/sql_reference/grant-all/","text":"GRANT ALL \u00b6 Syntax \u00b6 GRANT ALL ON <external-table> to <user>; Description \u00b6 This syntax is used to grant the permissions on external tables for other users. Example \u00b6 Here is an example SQL to grant privileges to individual users on external tables: GRANT ALL ON EXT_T1 TO samb,bob;","title":"GRANT ALL"},{"location":"reference/sql_reference/grant-all/#grant-all","text":"","title":"GRANT ALL"},{"location":"reference/sql_reference/grant-all/#syntax","text":"GRANT ALL ON <external-table> to <user>;","title":"Syntax"},{"location":"reference/sql_reference/grant-all/#description","text":"This syntax is used to grant the permissions on external tables for other users.","title":"Description"},{"location":"reference/sql_reference/grant-all/#example","text":"Here is an example SQL to grant privileges to individual users on external tables: GRANT ALL ON EXT_T1 TO samb,bob;","title":"Example"},{"location":"reference/sql_reference/grant/","text":"GRANT \u00b6 Syntax \u00b6 The syntax for the GRANT statement differs if you are granting privileges to a table or to a routine. Syntax for Tables \u00b6 GRANT privilege-type ON [ TABLE ] { table-name | view-name } TO grantees Syntax for Routines \u00b6 GRANT EXECUTE ON { FUNCTION | PROCEDURE } routine-designator TO grantees Description \u00b6 The GRANT statement enables permissions for a specific user or all users to perform actions on SQL objects. The following types of permissions can be granted: Perform DML operations on a specific table. Insert/Delete rows from a table. Select/Update data on a table or subset of columns in a table. Create a foreign key reference to the named table or to a subset of columns from a table. Run a specified function or procedure. privilege-type \u00b6 ALL PRIVILEGES | privilege-list Use the ALL PRIVILEGES privilege type to grant all of the permissions to the user for the specified table. You can also grant one or more table privileges by specifying a privilege-list. privilege-list \u00b6 table-privilege {, table-privilege }* table-privilege \u00b6 ALTER | DELETE | INSERT | REFERENCES [column-list] | SELECT [column-list] | UPDATE [ column-list ] Use the ALTER privilege to grant permission to the command on the specified table. Use the DELETE privilege type to grant permission to delete rows from the specified table. Use the INSERT privilege type to grant permission to insert rows into the specified table. Use the SELECT privilege type to grant permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is valid on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table. Use the UPDATE privilege type to grant permission to use the UPDATE statement on the specified table. If a column list is specified, the permission applies only to the specified columns. To update a row using a statement that includes a WHERE clause, you must have SELECT permission on the columns in the row that you want to update. column-list \u00b6 ( column-identifier {, column-identifier }* ) grantees \u00b6 { authorization ID | PUBLIC } [,{ authorization ID | PUBLIC } ] * You can grant privileges for specific users or for all users. Use the keyword PUBLIC to specify all users. When PUBLIC is specified, the privileges affect all current and future users. The privileges granted to PUBLIC and to individual users are independent privileges. For example, a SELECT privilege on table t is granted to both PUBLIC and to the authorization ID harry. The SELECT privilege is later revoked from the authorization ID harry, but Harry can access the table t through the PUBLIC privilege. routine-designator \u00b6 { function-name | procedure-name } Example \u00b6 To grant the SELECT privilege on table \"t\" to the authorization IDs \"sam\" and \"bob:\" GRANT SELECT ON TABLE t TO sam,bob; To grant the UPDATE privileges on table \"t\" to the authorization IDs \"john\" and \"smith:\" GRANT UPDATE ON TABLE t TO john,smith; To grant ALTER TABLE privileges on table \"t\" to the authorization ID \"adam:\" GRANT ALTER ON TABLE t TO adam; To grant the SELECT privilege on table \"test.sample\" to all users: GRANT SELECT ON TABLE test.sample to PUBLIC;","title":"GRANT"},{"location":"reference/sql_reference/grant/#grant","text":"","title":"GRANT"},{"location":"reference/sql_reference/grant/#syntax","text":"The syntax for the GRANT statement differs if you are granting privileges to a table or to a routine.","title":"Syntax"},{"location":"reference/sql_reference/grant/#syntax-for-tables","text":"GRANT privilege-type ON [ TABLE ] { table-name | view-name } TO grantees","title":"Syntax for Tables"},{"location":"reference/sql_reference/grant/#syntax-for-routines","text":"GRANT EXECUTE ON { FUNCTION | PROCEDURE } routine-designator TO grantees","title":"Syntax for Routines"},{"location":"reference/sql_reference/grant/#description","text":"The GRANT statement enables permissions for a specific user or all users to perform actions on SQL objects. The following types of permissions can be granted: Perform DML operations on a specific table. Insert/Delete rows from a table. Select/Update data on a table or subset of columns in a table. Create a foreign key reference to the named table or to a subset of columns from a table. Run a specified function or procedure.","title":"Description"},{"location":"reference/sql_reference/grant/#privilege-type","text":"ALL PRIVILEGES | privilege-list Use the ALL PRIVILEGES privilege type to grant all of the permissions to the user for the specified table. You can also grant one or more table privileges by specifying a privilege-list.","title":"privilege-type"},{"location":"reference/sql_reference/grant/#privilege-list","text":"table-privilege {, table-privilege }*","title":"privilege-list"},{"location":"reference/sql_reference/grant/#table-privilege","text":"ALTER | DELETE | INSERT | REFERENCES [column-list] | SELECT [column-list] | UPDATE [ column-list ] Use the ALTER privilege to grant permission to the command on the specified table. Use the DELETE privilege type to grant permission to delete rows from the specified table. Use the INSERT privilege type to grant permission to insert rows into the specified table. Use the SELECT privilege type to grant permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is valid on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table. Use the UPDATE privilege type to grant permission to use the UPDATE statement on the specified table. If a column list is specified, the permission applies only to the specified columns. To update a row using a statement that includes a WHERE clause, you must have SELECT permission on the columns in the row that you want to update.","title":"table-privilege"},{"location":"reference/sql_reference/grant/#column-list","text":"( column-identifier {, column-identifier }* )","title":"column-list"},{"location":"reference/sql_reference/grant/#grantees","text":"{ authorization ID | PUBLIC } [,{ authorization ID | PUBLIC } ] * You can grant privileges for specific users or for all users. Use the keyword PUBLIC to specify all users. When PUBLIC is specified, the privileges affect all current and future users. The privileges granted to PUBLIC and to individual users are independent privileges. For example, a SELECT privilege on table t is granted to both PUBLIC and to the authorization ID harry. The SELECT privilege is later revoked from the authorization ID harry, but Harry can access the table t through the PUBLIC privilege.","title":"grantees"},{"location":"reference/sql_reference/grant/#routine-designator","text":"{ function-name | procedure-name }","title":"routine-designator"},{"location":"reference/sql_reference/grant/#example","text":"To grant the SELECT privilege on table \"t\" to the authorization IDs \"sam\" and \"bob:\" GRANT SELECT ON TABLE t TO sam,bob; To grant the UPDATE privileges on table \"t\" to the authorization IDs \"john\" and \"smith:\" GRANT UPDATE ON TABLE t TO john,smith; To grant ALTER TABLE privileges on table \"t\" to the authorization ID \"adam:\" GRANT ALTER ON TABLE t TO adam; To grant the SELECT privilege on table \"test.sample\" to all users: GRANT SELECT ON TABLE test.sample to PUBLIC;","title":"Example"},{"location":"reference/sql_reference/insert/","text":"INSERT \u00b6 An INSERT statement creates a row or rows and stores them in the named table. The number of values assigned in an INSERT statement must be the same as the number of specified or implied columns. INSERT INTO table-name [ ( simple-column-name [ , simple-column-name ]* ) ] Query Description \u00b6 The query can be: a VALUES list a multiple-row VALUES expression Note SnappyData does not support an INSERT with a subselect query if any subselect query requires aggregation. Single-row and multiple-row lists can include the keyword DEFAULT. Specifying DEFAULT for a column inserts the column's default value into the column. Another way to insert the default value into the column is to omit the column from the column list and only insert values into other columns in the table. For more information, refer to SELECT . Example \u00b6 --create table trade.customers CREATE TABLE TRADE.CUSTOMERS (CID INT, CUST_NAME VARCHAR(100), SINCE DATE, ADDR VARCHAR(100)); INSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData'); -- Insert a new customer into the CUSTOMERS table, -- but do not assign value to 'SINCE' column INSERT INTO TRADE.CUSTOMERS(CID, CUST_NAME, ADDR) VALUES (2, 'USER 2', 'SnappyData'); -- Insert two new customers using one statement -- into the CUSTOMER table as in the previous example, -- but do not assign value to 'SINCE' field of the new customer. INSERT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR) VALUES (3, 'User 3' , 'SnappyData'), (4, 'User 4' , 'SnappyData'); -- Insert the DEFAULT value for the SINCE column INSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', DEFAULT, 'SnappyData'); -- Insert into another table using a select statement. INSERT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1';","title":"INSERT"},{"location":"reference/sql_reference/insert/#insert","text":"An INSERT statement creates a row or rows and stores them in the named table. The number of values assigned in an INSERT statement must be the same as the number of specified or implied columns. INSERT INTO table-name [ ( simple-column-name [ , simple-column-name ]* ) ] Query","title":"INSERT"},{"location":"reference/sql_reference/insert/#description","text":"The query can be: a VALUES list a multiple-row VALUES expression Note SnappyData does not support an INSERT with a subselect query if any subselect query requires aggregation. Single-row and multiple-row lists can include the keyword DEFAULT. Specifying DEFAULT for a column inserts the column's default value into the column. Another way to insert the default value into the column is to omit the column from the column list and only insert values into other columns in the table. For more information, refer to SELECT .","title":"Description"},{"location":"reference/sql_reference/insert/#example","text":"--create table trade.customers CREATE TABLE TRADE.CUSTOMERS (CID INT, CUST_NAME VARCHAR(100), SINCE DATE, ADDR VARCHAR(100)); INSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData'); -- Insert a new customer into the CUSTOMERS table, -- but do not assign value to 'SINCE' column INSERT INTO TRADE.CUSTOMERS(CID, CUST_NAME, ADDR) VALUES (2, 'USER 2', 'SnappyData'); -- Insert two new customers using one statement -- into the CUSTOMER table as in the previous example, -- but do not assign value to 'SINCE' field of the new customer. INSERT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR) VALUES (3, 'User 3' , 'SnappyData'), (4, 'User 4' , 'SnappyData'); -- Insert the DEFAULT value for the SINCE column INSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', DEFAULT, 'SnappyData'); -- Insert into another table using a select statement. INSERT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1';","title":"Example"},{"location":"reference/sql_reference/lateral-view/","text":"LATERAL VIEW \u00b6 Support for hive compatible LATERAL VIEW. It works with a table generating function like explode() and for each output row, joins it with the base table to create a view. Refer to this document for details. Syntax \u00b6 FROM baseTable (lateralView)* lateralView: LATERAL VIEW function([expressions]) tableAlias [AS columnAlias (',' columnAlias)*] Example \u00b6 \"id\": 1 \"purpose\": \"business\" \"type\": sales \"contact\" : [{ \"phone\" : \"555-1234\", \"email\" : \"jsmth@company.com\" }, { \"phone\" : \"666-1234\", \"email\" : \"smithj@company.com\" } ] SELECT id, part.phone, part.email FROM json LATERAL VIEW explode(parts) partTable AS part 1, \"555-1234\", \"jsmith@company.com\" 1, \"666-1234\", \"smithj@company.com\"","title":"LATERAL VIEW"},{"location":"reference/sql_reference/lateral-view/#lateral-view","text":"Support for hive compatible LATERAL VIEW. It works with a table generating function like explode() and for each output row, joins it with the base table to create a view. Refer to this document for details.","title":"LATERAL VIEW"},{"location":"reference/sql_reference/lateral-view/#syntax","text":"FROM baseTable (lateralView)* lateralView: LATERAL VIEW function([expressions]) tableAlias [AS columnAlias (',' columnAlias)*]","title":"Syntax"},{"location":"reference/sql_reference/lateral-view/#example","text":"\"id\": 1 \"purpose\": \"business\" \"type\": sales \"contact\" : [{ \"phone\" : \"555-1234\", \"email\" : \"jsmth@company.com\" }, { \"phone\" : \"666-1234\", \"email\" : \"smithj@company.com\" } ] SELECT id, part.phone, part.email FROM json LATERAL VIEW explode(parts) partTable AS part 1, \"555-1234\", \"jsmith@company.com\" 1, \"666-1234\", \"smithj@company.com\"","title":"Example"},{"location":"reference/sql_reference/list/","text":"LIST \u00b6 Sytax \u00b6 snappy> list packages; Or snappy> list jars; Description \u00b6 Lists all the packages or jars that are installed in the system. Hence, you can use either one of those commands. Example \u00b6 snappy> list jars; alias |coordinate |isPackage ------------------------------------------------------------------------- CASSCONN |datastax:spark-cassandra-connector:2.0.1-s_2.11|true MYJAR |b.jar |false","title":"LIST"},{"location":"reference/sql_reference/list/#list","text":"","title":"LIST"},{"location":"reference/sql_reference/list/#sytax","text":"snappy> list packages; Or snappy> list jars;","title":"Sytax"},{"location":"reference/sql_reference/list/#description","text":"Lists all the packages or jars that are installed in the system. Hence, you can use either one of those commands.","title":"Description"},{"location":"reference/sql_reference/list/#example","text":"snappy> list jars; alias |coordinate |isPackage ------------------------------------------------------------------------- CASSCONN |datastax:spark-cassandra-connector:2.0.1-s_2.11|true MYJAR |b.jar |false","title":"Example"},{"location":"reference/sql_reference/pivot/","text":"PIVOT \u00b6 Description \u00b6 SnappyData parser support for PIVOT clause. It deviates from Spark 2.4 support in that it only allows literals in the value IN list rather than named expressions. On the contrary, it supports explicit GROUP BY columns with PIVOT instead of always doing implicit detection (which may be different from what user needs in some cases). Example \u00b6 Both the following examples are identical. Spark 2.4 does not support second variant: ``` select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) ORDER BY year DESC Compared to blog example the **IN** clause only supports constants and not aliases. select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) GROUP BY year ORDER BY year DESC ```","title":"PIVOT"},{"location":"reference/sql_reference/pivot/#pivot","text":"","title":"PIVOT"},{"location":"reference/sql_reference/pivot/#description","text":"SnappyData parser support for PIVOT clause. It deviates from Spark 2.4 support in that it only allows literals in the value IN list rather than named expressions. On the contrary, it supports explicit GROUP BY columns with PIVOT instead of always doing implicit detection (which may be different from what user needs in some cases).","title":"Description"},{"location":"reference/sql_reference/pivot/#example","text":"Both the following examples are identical. Spark 2.4 does not support second variant: ``` select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) ORDER BY year DESC Compared to blog example the **IN** clause only supports constants and not aliases. select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) GROUP BY year ORDER BY year DESC ```","title":"Example"},{"location":"reference/sql_reference/put-into/","text":"PUT INTO \u00b6 PUT INTO operates like a standard INSERT statement. Note Insert/PUT INTO with partial column specification is not supported in SnappyData. For Column Tables \u00b6 If you need the putInto() functionality for column tables, you must specify key_columns while defining the column table. PUT INTO updates the row if present, else, it inserts the row. As column tables do not have the primary key functionality, you must specify key_columns when creating the table. These columns are used to identify a row uniquely. PUT INTO is available by SQL as well APIs. Syntax \u00b6 PUT INTO <TABLENAME> SELECT V1, V2, V3,......,Vn; PUT INTO <schema name>.<table name2> SELECT * from <schema name>.<table name1>; PUT INTO <schema name>.<table name2> SELECT * from <schema name>.<table name1> WHERE <column name>='<Value>' PUT INTO <schema name>.<table name2> SELECT from <schema name>.<table name1> WHERE <column name>='<Value>' PUT INTO <schema name>.<table name> VALUES (V1, V2,... ,Vn); Examples \u00b6 For SQL // Insert into another table using a select statement for column tables with key columns PUT INTO TRADE.CUSTOMERS SELECT '1','2','hello'; PUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS; PUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS WHERE C_NAME='User 1'; PUT INTO TRADE.NEWCUSTOMERS SELECT from CUSTOMERS WHERE C_NAME='User 1'; PUT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData', 1); PUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID) VALUES (1, 'User 1' , 'SnappyData', 1); For API putInto API is available from the DataFrameWriter extension. import org.apache.spark.sql.snappy._ dataFrame.write.putInto(\"col_table\") For Row Tables \u00b6 PUT INTO uses a syntax similar to the INSERT statement, but SnappyData does not check the existing primary key values before executing the PUT INTO command. If a row with the same primary key exists in the table, PUT INTO overwrites the older row value. If no rows with the same primary key exist, PUT INTO operates like a standard INSERT. This behavior ensures that only the last primary key value inserted or updated remains in the system, which preserves the primary key constraint. Removing the primary key check speeds execution when importing bulk data. The PUT INTO statement is similar to the \"UPSERT\" command or capability provided by other RDBMS to relax primary key checks. By default, the PUT INTO statement ignores only primary key constraints. Syntax \u00b6 PUT INTO <schema name>.<table name> VALUES (V1, V2,... ,Vn); Example \u00b6 PUT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData', 1); When specifying columns with table, columns should not have any CONSTRAINT , as explained in the following example: PUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID) VALUES (1, 'User 1' , 'SnappyData', 1), (2, 'User 2' , 'SnappyData', 1); PUT into another table using a select statement PUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS; PUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1'","title":"PUT INTO"},{"location":"reference/sql_reference/put-into/#put-into","text":"PUT INTO operates like a standard INSERT statement. Note Insert/PUT INTO with partial column specification is not supported in SnappyData.","title":"PUT INTO"},{"location":"reference/sql_reference/put-into/#for-column-tables","text":"If you need the putInto() functionality for column tables, you must specify key_columns while defining the column table. PUT INTO updates the row if present, else, it inserts the row. As column tables do not have the primary key functionality, you must specify key_columns when creating the table. These columns are used to identify a row uniquely. PUT INTO is available by SQL as well APIs.","title":"For Column Tables"},{"location":"reference/sql_reference/put-into/#syntax","text":"PUT INTO <TABLENAME> SELECT V1, V2, V3,......,Vn; PUT INTO <schema name>.<table name2> SELECT * from <schema name>.<table name1>; PUT INTO <schema name>.<table name2> SELECT * from <schema name>.<table name1> WHERE <column name>='<Value>' PUT INTO <schema name>.<table name2> SELECT from <schema name>.<table name1> WHERE <column name>='<Value>' PUT INTO <schema name>.<table name> VALUES (V1, V2,... ,Vn);","title":"Syntax"},{"location":"reference/sql_reference/put-into/#examples","text":"For SQL // Insert into another table using a select statement for column tables with key columns PUT INTO TRADE.CUSTOMERS SELECT '1','2','hello'; PUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS; PUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS WHERE C_NAME='User 1'; PUT INTO TRADE.NEWCUSTOMERS SELECT from CUSTOMERS WHERE C_NAME='User 1'; PUT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData', 1); PUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID) VALUES (1, 'User 1' , 'SnappyData', 1); For API putInto API is available from the DataFrameWriter extension. import org.apache.spark.sql.snappy._ dataFrame.write.putInto(\"col_table\")","title":"Examples"},{"location":"reference/sql_reference/put-into/#for-row-tables","text":"PUT INTO uses a syntax similar to the INSERT statement, but SnappyData does not check the existing primary key values before executing the PUT INTO command. If a row with the same primary key exists in the table, PUT INTO overwrites the older row value. If no rows with the same primary key exist, PUT INTO operates like a standard INSERT. This behavior ensures that only the last primary key value inserted or updated remains in the system, which preserves the primary key constraint. Removing the primary key check speeds execution when importing bulk data. The PUT INTO statement is similar to the \"UPSERT\" command or capability provided by other RDBMS to relax primary key checks. By default, the PUT INTO statement ignores only primary key constraints.","title":"For Row Tables"},{"location":"reference/sql_reference/put-into/#syntax_1","text":"PUT INTO <schema name>.<table name> VALUES (V1, V2,... ,Vn);","title":"Syntax"},{"location":"reference/sql_reference/put-into/#example","text":"PUT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData', 1); When specifying columns with table, columns should not have any CONSTRAINT , as explained in the following example: PUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID) VALUES (1, 'User 1' , 'SnappyData', 1), (2, 'User 2' , 'SnappyData', 1); PUT into another table using a select statement PUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS; PUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1'","title":"Example"},{"location":"reference/sql_reference/revoke/","text":"REVOKE \u00b6 Syntax \u00b6 The syntax used for the REVOKE statement differs depending on whether you revoke privileges for a table or for a routine. REVOKE privilege-type ON [ TABLE ] { table-name | view-name } FROM grantees If you do not specify a column list, the statement revokes the privilege for all of the columns in the table. REVOKE EXECUTE ON { FUNCTION | PROCEDURE } routine-designator FROM grantees RESTRICT You must use the RESTRICT clause on REVOKE statements for routines. The RESTRICT clause specifies that the EXECUTE privilege cannot be revoked if the specified routine is used in a view or constraint, and the privilege is being revoked from the owner of the view or constraint. Description \u00b6 The REVOKE statement removes permissions from a specific user or from all users to perform actions on database objects. The following types of permissions can be revoked: DML operations permissions on a specific table. Insert/Delete data to/from a specific table. Select/Update data permissions on a table or a subset of columns in a table. Create a foreign key reference to the named table or to a subset of columns from a table. Execute permission on a specified routine (function or procedure). You can revoke privileges from an object if you are the owner of the object or the distributed member owner. privilege-type \u00b6 Use the ALL PRIVILEGES privilege type to revoke all of the permissions from the user for the specified table. You can also revoke one or more table privileges by specifying a privilege-list. Use the DELETE privilege type to revoke permission to delete rows from the specified table. Use the INSERT privilege type to revoke permission to insert rows into the specified table. Use the REFERENCES privilege type to revoke permission to create a foreign key reference to the specified table. If a column list is specified with the REFERENCES privilege, the permission is revoked on only the foreign key reference to the specified columns. Use the SELECT privilege type to revoke permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is revoked on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table. Use the UPDATE privilege type to revoke permission to use the UPDATE statement on the specified table. If a column list is specified, the permission is revoked only on the specified columns. grantees \u00b6 You can revoke the privileges from specific users or from all users. Use the keyword PUBLIC to specify all users. The privileges revoked from PUBLIC and from individual users are independent privileges. For example, consider the case where the SELECT privilege on table \"t\" is granted to both PUBLIC and to the authorization ID \"harry.\" If the SELECT privilege is later revoked from the authorization ID \"harry,\" harry can still access table \"t\" using the PUBLIC privilege. Note The privileges of the owner (distributed member) of an object cannot be revoked. routine-designator \u00b6 { qualified-name [ signature ] } Cascading Object Dependencies \u00b6 For views and constraints, if the privilege on which the object depends on is revoked, the object is automatically dropped. SnappyData does not try to determine if you have other privileges that can replace the privileges that are being revoked. Table-Level Privilege Limitations \u00b6 All of the table-level privilege types for a specified grantee and table ID are stored in one row in the SYSTABLEPERMS system table. For example, when user2 is granted the SELECT and DELETE privileges on table user1.t1, a row is added to the SYSTABLEPERMS table. The GRANTEE field contains user2 and the TABLEID contains user1.t1. The SELECTPRIV and DELETEPRIV fields are set to Y. The remaining privilege type fields are set to N. When a grantee creates an object that relies on one of the privilege types, the engine tracks the dependency of the object on the specific row in the SYSTABLEPERMS table. For example, user2 creates the view v1 by using the statement SELECT * FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1). The dependency manager knows only that the view is dependent on a privilege type in that specific row, but does not track exactly which privilege type the view is dependent on. When a REVOKE statement for a table-level privilege is issued for a grantee and table ID, all of the objects that are dependent on the grantee and table ID are dropped. For example, if user1 revokes the DELETE privilege on table t1 from user2, the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the DELETE privilege for GRANTEE(user2), TABLEID(user1.t1). Column-Level Privilege Limitations \u00b6 Only one type of privilege for a specified grantee and table ID are stored in one row in the SYSCOLPERMS system table. For example, when user2 is granted the SELECT privilege on table user1.t1 for columns c12 and c13, a row is added to the SYSCOLPERMS. The GRANTEE field contains user2, the TABLEID contains user1.t1, the TYPE field contains S, and the COLUMNS field contains c12, c13. When a grantee creates an object that relies on the privilege type and the subset of columns in a table ID, the engine tracks the dependency of the object on the specific row in the SYSCOLPERMS table. For example, user2 creates the view v1 by using the statement SELECT c11 FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S). The dependency manager knows that the view is dependent on the SELECT privilege type, but does not track exactly which columns the view is dependent on. When a REVOKE statement for a column-level privilege is issued for a grantee, table ID, and type, all of the objects that are dependent on the grantee, table ID, and type are dropped. For example, if user1 revokes the SELECT privilege on column c12 on table user1.t1 from user2, the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the column c12 for GRANTEE(user2), TABLEID(user1.t1), TYPE(S). Examples \u00b6 To revoke the SELECT privilege on table t from the authorization IDs maria and harry: REVOKE SELECT ON TABLE t FROM sam,bob; To revoke the UPDATE privileges on table t from the authorization IDs john and smith: REVOKE UPDATE ON TABLE t FROM adam,richard; To revoke the SELECT privilege on table s.v from all users: REVOKE SELECT ON TABLE test.sample FROM PUBLIC; To revoke the UPDATE privilege on columns c1 and c2 of table s.v from all users: REVOKE UPDATE (c1,c2) ON TABLE test.sample FROM PUBLIC; To revoke the EXECUTE privilege on procedure p from the authorization ID george: REVOKE EXECUTE ON PROCEDURE p FROM richard RESTRICT;","title":"REVOKE"},{"location":"reference/sql_reference/revoke/#revoke","text":"","title":"REVOKE"},{"location":"reference/sql_reference/revoke/#syntax","text":"The syntax used for the REVOKE statement differs depending on whether you revoke privileges for a table or for a routine. REVOKE privilege-type ON [ TABLE ] { table-name | view-name } FROM grantees If you do not specify a column list, the statement revokes the privilege for all of the columns in the table. REVOKE EXECUTE ON { FUNCTION | PROCEDURE } routine-designator FROM grantees RESTRICT You must use the RESTRICT clause on REVOKE statements for routines. The RESTRICT clause specifies that the EXECUTE privilege cannot be revoked if the specified routine is used in a view or constraint, and the privilege is being revoked from the owner of the view or constraint.","title":"Syntax"},{"location":"reference/sql_reference/revoke/#description","text":"The REVOKE statement removes permissions from a specific user or from all users to perform actions on database objects. The following types of permissions can be revoked: DML operations permissions on a specific table. Insert/Delete data to/from a specific table. Select/Update data permissions on a table or a subset of columns in a table. Create a foreign key reference to the named table or to a subset of columns from a table. Execute permission on a specified routine (function or procedure). You can revoke privileges from an object if you are the owner of the object or the distributed member owner.","title":"Description"},{"location":"reference/sql_reference/revoke/#privilege-type","text":"Use the ALL PRIVILEGES privilege type to revoke all of the permissions from the user for the specified table. You can also revoke one or more table privileges by specifying a privilege-list. Use the DELETE privilege type to revoke permission to delete rows from the specified table. Use the INSERT privilege type to revoke permission to insert rows into the specified table. Use the REFERENCES privilege type to revoke permission to create a foreign key reference to the specified table. If a column list is specified with the REFERENCES privilege, the permission is revoked on only the foreign key reference to the specified columns. Use the SELECT privilege type to revoke permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is revoked on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table. Use the UPDATE privilege type to revoke permission to use the UPDATE statement on the specified table. If a column list is specified, the permission is revoked only on the specified columns.","title":"privilege-type"},{"location":"reference/sql_reference/revoke/#grantees","text":"You can revoke the privileges from specific users or from all users. Use the keyword PUBLIC to specify all users. The privileges revoked from PUBLIC and from individual users are independent privileges. For example, consider the case where the SELECT privilege on table \"t\" is granted to both PUBLIC and to the authorization ID \"harry.\" If the SELECT privilege is later revoked from the authorization ID \"harry,\" harry can still access table \"t\" using the PUBLIC privilege. Note The privileges of the owner (distributed member) of an object cannot be revoked.","title":"grantees"},{"location":"reference/sql_reference/revoke/#routine-designator","text":"{ qualified-name [ signature ] }","title":"routine-designator"},{"location":"reference/sql_reference/revoke/#cascading-object-dependencies","text":"For views and constraints, if the privilege on which the object depends on is revoked, the object is automatically dropped. SnappyData does not try to determine if you have other privileges that can replace the privileges that are being revoked.","title":"Cascading Object Dependencies"},{"location":"reference/sql_reference/revoke/#table-level-privilege-limitations","text":"All of the table-level privilege types for a specified grantee and table ID are stored in one row in the SYSTABLEPERMS system table. For example, when user2 is granted the SELECT and DELETE privileges on table user1.t1, a row is added to the SYSTABLEPERMS table. The GRANTEE field contains user2 and the TABLEID contains user1.t1. The SELECTPRIV and DELETEPRIV fields are set to Y. The remaining privilege type fields are set to N. When a grantee creates an object that relies on one of the privilege types, the engine tracks the dependency of the object on the specific row in the SYSTABLEPERMS table. For example, user2 creates the view v1 by using the statement SELECT * FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1). The dependency manager knows only that the view is dependent on a privilege type in that specific row, but does not track exactly which privilege type the view is dependent on. When a REVOKE statement for a table-level privilege is issued for a grantee and table ID, all of the objects that are dependent on the grantee and table ID are dropped. For example, if user1 revokes the DELETE privilege on table t1 from user2, the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the DELETE privilege for GRANTEE(user2), TABLEID(user1.t1).","title":"Table-Level Privilege Limitations"},{"location":"reference/sql_reference/revoke/#column-level-privilege-limitations","text":"Only one type of privilege for a specified grantee and table ID are stored in one row in the SYSCOLPERMS system table. For example, when user2 is granted the SELECT privilege on table user1.t1 for columns c12 and c13, a row is added to the SYSCOLPERMS. The GRANTEE field contains user2, the TABLEID contains user1.t1, the TYPE field contains S, and the COLUMNS field contains c12, c13. When a grantee creates an object that relies on the privilege type and the subset of columns in a table ID, the engine tracks the dependency of the object on the specific row in the SYSCOLPERMS table. For example, user2 creates the view v1 by using the statement SELECT c11 FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S). The dependency manager knows that the view is dependent on the SELECT privilege type, but does not track exactly which columns the view is dependent on. When a REVOKE statement for a column-level privilege is issued for a grantee, table ID, and type, all of the objects that are dependent on the grantee, table ID, and type are dropped. For example, if user1 revokes the SELECT privilege on column c12 on table user1.t1 from user2, the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the column c12 for GRANTEE(user2), TABLEID(user1.t1), TYPE(S).","title":"Column-Level Privilege Limitations"},{"location":"reference/sql_reference/revoke/#examples","text":"To revoke the SELECT privilege on table t from the authorization IDs maria and harry: REVOKE SELECT ON TABLE t FROM sam,bob; To revoke the UPDATE privileges on table t from the authorization IDs john and smith: REVOKE UPDATE ON TABLE t FROM adam,richard; To revoke the SELECT privilege on table s.v from all users: REVOKE SELECT ON TABLE test.sample FROM PUBLIC; To revoke the UPDATE privilege on columns c1 and c2 of table s.v from all users: REVOKE UPDATE (c1,c2) ON TABLE test.sample FROM PUBLIC; To revoke the EXECUTE privilege on procedure p from the authorization ID george: REVOKE EXECUTE ON PROCEDURE p FROM richard RESTRICT;","title":"Examples"},{"location":"reference/sql_reference/revoke_all/","text":"REVOKE ALL \u00b6 Syntax \u00b6 REVOKE ALL ON <external-table> FROM <user>; Description \u00b6 The REVOKE ALL statement removes permissions from a specific user on external Tables. Examples \u00b6 Here is an example SQL to revoke permissions to individual users on external tables: REVOKE ALL ON EXT_T1 FROM samb,bob;","title":"REVOKE ALL"},{"location":"reference/sql_reference/revoke_all/#revoke-all","text":"","title":"REVOKE ALL"},{"location":"reference/sql_reference/revoke_all/#syntax","text":"REVOKE ALL ON <external-table> FROM <user>;","title":"Syntax"},{"location":"reference/sql_reference/revoke_all/#description","text":"The REVOKE ALL statement removes permissions from a specific user on external Tables.","title":"Description"},{"location":"reference/sql_reference/revoke_all/#examples","text":"Here is an example SQL to revoke permissions to individual users on external tables: REVOKE ALL ON EXT_T1 FROM samb,bob;","title":"Examples"},{"location":"reference/sql_reference/select/","text":"SELECT \u00b6 SELECT [DISTINCT] named_expression[, named_expression, ...] FROM relation[, relation, ...] [WHERE boolean_expression] [aggregation [HAVING boolean_expression]] [ORDER BY sort_expressions] [CLUSTER BY expressions] [DISTRIBUTE BY expressions] [SORT BY sort_expressions] [WINDOW named_window[, WINDOW named_window, ...]] [LIMIT num_rows] [PIVOT_expressions] named_expression: : expression [AS alias] relation: | join_relation | (table_name|query) [sample] [AS alias] expressions: : expression[, expression, ...] sort_expressions: : expression [ASC|DESC][, expression [ASC|DESC], ...] For information on executing queries on Approximate Query Processing (AQP), refer to AQP . Description \u00b6 Output data from one or more relations. A relation here refers to any source of input data. It could be the contents of an existing table (or view), the joined result of two existing tables, or a subquery (the result of another select statement). DISTINCT Select all matching rows from the relation then remove duplicate results. WHERE Filter rows by a predicate. HAVING Filter grouped result by a predicate. ORDER BY Impose total ordering on a set of expressions. Default sort direction is ascending. This may not be used with SORT BY, CLUSTER BY, or DISTRIBUTE BY. DISTRIBUTE BY Repartition rows in the relation based on a set of expressions. Rows with the same expression values will be hashed to the same worker. This may not be used with ORDER BY or CLUSTER BY. SORT BY Impose ordering on a set of expressions within each partition. Default sort direction is ascending. This may not be used with ORDER BY or CLUSTER BY. CLUSTER BY Repartition rows in the relation based on a set of expressions and sort the rows in ascending order based on the expressions. In other words, this is a shorthand for DISTRIBUTE BY and SORT BY where all expressions are sorted in ascending order. This may not be used with ORDER BY, DISTRIBUTE BY, or SORT BY. WINDOW Assign an identifier to a window specification. LIMIT Limit the number of rows returned. PIVOT The support for PIVOT clause in SnappyData parser deviates from Spark 2.4 support in the following two aspects: It only allows literals in the value IN list rather than named expressions. On the contrary, SnappyData supports explicit GROUP BY columns with PIVOT instead of always doing implicit detection. Only SnappyData (and not Spark 2.4) supports the explicit GROUP BY clause in the following example. select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) GROUP BY year ORDER BY year DESC The IN clause only supports constants and not aliases. select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) GROUP BY year ORDER BY year DESC Example \u00b6 SELECT * FROM boxes SELECT width, length FROM boxes WHERE height=3 SELECT DISTINCT width, length FROM boxes WHERE height=3 LIMIT 2 SELECT * FROM VALUES (1, 2, 3) AS (width, length, height) SELECT * FROM VALUES (1, 2, 3), (2, 3, 4) AS (width, length, height) SELECT * FROM boxes ORDER BY width SELECT * FROM boxes DISTRIBUTE BY width SORT BY width SELECT * FROM boxes CLUSTER BY length JOINS \u00b6 join_relation: | relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...])) : relation NATURAL join_type JOIN relation join_type: | INNER | (LEFT|RIGHT) SEMI | (LEFT|RIGHT|FULL) [OUTER] : [LEFT] ANTI INNER JOIN Select all rows from both relations where there is match. OUTER JOIN Select all rows from both relations, filling with null values on the side that does not have a match. SEMI JOIN Select only rows from the side of the SEMI JOIN where there is a match. If one row matches multiple rows, only the first match is returned. LEFT ANTI JOIN Select only rows from the left side that match no rows on the right side. Example : SELECT * FROM boxes INNER JOIN rectangles ON boxes.width = rectangles.width SELECT * FROM boxes FULL OUTER JOIN rectangles USING (width, length) SELECT * FROM boxes NATURAL JOIN rectangles AGGREGATION \u00b6 aggregation: : GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))] Group by a set of expressions using one or more aggregate functions. Common built-in aggregate functions include count, avg, min, max, and sum. ROLLUP Create a grouping set at each hierarchical level of the specified expressions. For instance, GROUP BY a, b, c WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (a), ()). The total number of grouping sets will be N + 1, where N is the number of group expressions. CUBE Create a grouping set for each possible combination of a set of the specified expressions. For instance, GROUP BY a, b, c WITH CUBE is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ()). The total number of grouping sets will be 2^N, where N is the number of group expressions. GROUPING SETS Perform a group by for each subset of the group expressions specified in the grouping sets. For instance, GROUP BY x, y GROUPING SETS (x, y) is equivalent to the result of GROUP BY x unioned with that of GROUP BY y. Example : SELECT height, COUNT(*) AS num_rows FROM boxes GROUP BY height SELECT width, AVG(length) AS average_length FROM boxes GROUP BY width SELECT width, length, height FROM boxes GROUP BY width, length, height WITH ROLLUP SELECT width, length, avg(height) FROM boxes GROUP BY width, length GROUPING SETS (width, length) Window Functions \u00b6 window_expression: : expression OVER window_spec named_window: : window_identifier AS window_spec window_spec: | window_identifier : ((PARTITION|DISTRIBUTE) BY expressions [(ORDER|SORT) BY sort_expressions] [window_frame]) window_frame: | (RANGE|ROWS) frame_bound : (RANGE|ROWS) BETWEEN frame_bound AND frame_bound frame_bound: | CURRENT ROW | UNBOUNDED (PRECEDING|FOLLOWING) : expression (PRECEDING|FOLLOWING) Compute a result over a range of input rows. A windowed expression is specified using the OVER keyword, which is followed by either an identifier to the window (defined using the WINDOW keyword) or the specification of a window. PARTITION BY Specify which rows will be in the same partition, aliased by DISTRIBUTE BY. ORDER BY Specify how rows within a window partition are ordered, aliased by SORT BY. RANGE bound Express the size of the window in terms of a value range for the expression. ROWS bound Express the size of the window in terms of the number of rows before and/or after the current row. CURRENT ROW Use the current row as a bound. UNBOUNDED Use negative infinity as the lower bound or infinity as the upper bound. PRECEDING If used with a RANGE bound, this defines the lower bound of the value range. If used with a ROWS bound, this determines the number of rows before the current row to keep in the window. FOLLOWING If used with a RANGE bound, this defines the upper bound of the value range. If used with a ROWS bound, this determines the number of rows after the current row to keep in the window.","title":"SELECT"},{"location":"reference/sql_reference/select/#select","text":"SELECT [DISTINCT] named_expression[, named_expression, ...] FROM relation[, relation, ...] [WHERE boolean_expression] [aggregation [HAVING boolean_expression]] [ORDER BY sort_expressions] [CLUSTER BY expressions] [DISTRIBUTE BY expressions] [SORT BY sort_expressions] [WINDOW named_window[, WINDOW named_window, ...]] [LIMIT num_rows] [PIVOT_expressions] named_expression: : expression [AS alias] relation: | join_relation | (table_name|query) [sample] [AS alias] expressions: : expression[, expression, ...] sort_expressions: : expression [ASC|DESC][, expression [ASC|DESC], ...] For information on executing queries on Approximate Query Processing (AQP), refer to AQP .","title":"SELECT"},{"location":"reference/sql_reference/select/#description","text":"Output data from one or more relations. A relation here refers to any source of input data. It could be the contents of an existing table (or view), the joined result of two existing tables, or a subquery (the result of another select statement). DISTINCT Select all matching rows from the relation then remove duplicate results. WHERE Filter rows by a predicate. HAVING Filter grouped result by a predicate. ORDER BY Impose total ordering on a set of expressions. Default sort direction is ascending. This may not be used with SORT BY, CLUSTER BY, or DISTRIBUTE BY. DISTRIBUTE BY Repartition rows in the relation based on a set of expressions. Rows with the same expression values will be hashed to the same worker. This may not be used with ORDER BY or CLUSTER BY. SORT BY Impose ordering on a set of expressions within each partition. Default sort direction is ascending. This may not be used with ORDER BY or CLUSTER BY. CLUSTER BY Repartition rows in the relation based on a set of expressions and sort the rows in ascending order based on the expressions. In other words, this is a shorthand for DISTRIBUTE BY and SORT BY where all expressions are sorted in ascending order. This may not be used with ORDER BY, DISTRIBUTE BY, or SORT BY. WINDOW Assign an identifier to a window specification. LIMIT Limit the number of rows returned. PIVOT The support for PIVOT clause in SnappyData parser deviates from Spark 2.4 support in the following two aspects: It only allows literals in the value IN list rather than named expressions. On the contrary, SnappyData supports explicit GROUP BY columns with PIVOT instead of always doing implicit detection. Only SnappyData (and not Spark 2.4) supports the explicit GROUP BY clause in the following example. select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) GROUP BY year ORDER BY year DESC The IN clause only supports constants and not aliases. select * from ( select year(day) year, month(day) month, temp from dayAvgTemp ) PIVOT ( CAST(avg(temp) AS DECIMAL(5, 2)) FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) ) GROUP BY year ORDER BY year DESC","title":"Description"},{"location":"reference/sql_reference/select/#example","text":"SELECT * FROM boxes SELECT width, length FROM boxes WHERE height=3 SELECT DISTINCT width, length FROM boxes WHERE height=3 LIMIT 2 SELECT * FROM VALUES (1, 2, 3) AS (width, length, height) SELECT * FROM VALUES (1, 2, 3), (2, 3, 4) AS (width, length, height) SELECT * FROM boxes ORDER BY width SELECT * FROM boxes DISTRIBUTE BY width SORT BY width SELECT * FROM boxes CLUSTER BY length","title":"Example"},{"location":"reference/sql_reference/select/#joins","text":"join_relation: | relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...])) : relation NATURAL join_type JOIN relation join_type: | INNER | (LEFT|RIGHT) SEMI | (LEFT|RIGHT|FULL) [OUTER] : [LEFT] ANTI INNER JOIN Select all rows from both relations where there is match. OUTER JOIN Select all rows from both relations, filling with null values on the side that does not have a match. SEMI JOIN Select only rows from the side of the SEMI JOIN where there is a match. If one row matches multiple rows, only the first match is returned. LEFT ANTI JOIN Select only rows from the left side that match no rows on the right side. Example : SELECT * FROM boxes INNER JOIN rectangles ON boxes.width = rectangles.width SELECT * FROM boxes FULL OUTER JOIN rectangles USING (width, length) SELECT * FROM boxes NATURAL JOIN rectangles","title":"JOINS"},{"location":"reference/sql_reference/select/#aggregation","text":"aggregation: : GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))] Group by a set of expressions using one or more aggregate functions. Common built-in aggregate functions include count, avg, min, max, and sum. ROLLUP Create a grouping set at each hierarchical level of the specified expressions. For instance, GROUP BY a, b, c WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (a), ()). The total number of grouping sets will be N + 1, where N is the number of group expressions. CUBE Create a grouping set for each possible combination of a set of the specified expressions. For instance, GROUP BY a, b, c WITH CUBE is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ()). The total number of grouping sets will be 2^N, where N is the number of group expressions. GROUPING SETS Perform a group by for each subset of the group expressions specified in the grouping sets. For instance, GROUP BY x, y GROUPING SETS (x, y) is equivalent to the result of GROUP BY x unioned with that of GROUP BY y. Example : SELECT height, COUNT(*) AS num_rows FROM boxes GROUP BY height SELECT width, AVG(length) AS average_length FROM boxes GROUP BY width SELECT width, length, height FROM boxes GROUP BY width, length, height WITH ROLLUP SELECT width, length, avg(height) FROM boxes GROUP BY width, length GROUPING SETS (width, length)","title":"AGGREGATION"},{"location":"reference/sql_reference/select/#window-functions","text":"window_expression: : expression OVER window_spec named_window: : window_identifier AS window_spec window_spec: | window_identifier : ((PARTITION|DISTRIBUTE) BY expressions [(ORDER|SORT) BY sort_expressions] [window_frame]) window_frame: | (RANGE|ROWS) frame_bound : (RANGE|ROWS) BETWEEN frame_bound AND frame_bound frame_bound: | CURRENT ROW | UNBOUNDED (PRECEDING|FOLLOWING) : expression (PRECEDING|FOLLOWING) Compute a result over a range of input rows. A windowed expression is specified using the OVER keyword, which is followed by either an identifier to the window (defined using the WINDOW keyword) or the specification of a window. PARTITION BY Specify which rows will be in the same partition, aliased by DISTRIBUTE BY. ORDER BY Specify how rows within a window partition are ordered, aliased by SORT BY. RANGE bound Express the size of the window in terms of a value range for the expression. ROWS bound Express the size of the window in terms of the number of rows before and/or after the current row. CURRENT ROW Use the current row as a bound. UNBOUNDED Use negative infinity as the lower bound or infinity as the upper bound. PRECEDING If used with a RANGE bound, this defines the lower bound of the value range. If used with a ROWS bound, this determines the number of rows before the current row to keep in the window. FOLLOWING If used with a RANGE bound, this defines the upper bound of the value range. If used with a ROWS bound, this determines the number of rows after the current row to keep in the window.","title":"Window Functions"},{"location":"reference/sql_reference/set-isolation/","text":"SET ISOLATION \u00b6 Change the transaction isolation level for the connection. Syntax \u00b6 SET [ CURRENT ] ISOLATION [ = ] { CS | READ COMMITTED RS | REPEATABLE READ RESET } Description \u00b6 The supported isolation levels in SnappyData are NONE, READ COMMITTED and REPEATABLE READ. Isolation level NONE indicates no transactional behavior. This is a special constant that indicates that transactions are not supported. The RESET clause corresponds to the NONE isolation level. For more information, see Overview of SnappyData Distributed Transactions . This statement behaves identically to the JDBC java.sql.Connection.setTransactionIsolation method and commits the current transaction if isolation level has changed. Example \u00b6 snappy> set ISOLATION READ COMMITTED; snappy> VALUES CURRENT ISOLATION; 1 ---- CS 1 row selected","title":"SET ISOLATION"},{"location":"reference/sql_reference/set-isolation/#set-isolation","text":"Change the transaction isolation level for the connection.","title":"SET ISOLATION"},{"location":"reference/sql_reference/set-isolation/#syntax","text":"SET [ CURRENT ] ISOLATION [ = ] { CS | READ COMMITTED RS | REPEATABLE READ RESET }","title":"Syntax"},{"location":"reference/sql_reference/set-isolation/#description","text":"The supported isolation levels in SnappyData are NONE, READ COMMITTED and REPEATABLE READ. Isolation level NONE indicates no transactional behavior. This is a special constant that indicates that transactions are not supported. The RESET clause corresponds to the NONE isolation level. For more information, see Overview of SnappyData Distributed Transactions . This statement behaves identically to the JDBC java.sql.Connection.setTransactionIsolation method and commits the current transaction if isolation level has changed.","title":"Description"},{"location":"reference/sql_reference/set-isolation/#example","text":"snappy> set ISOLATION READ COMMITTED; snappy> VALUES CURRENT ISOLATION; 1 ---- CS 1 row selected","title":"Example"},{"location":"reference/sql_reference/set-schema/","text":"SET SCHEMA \u00b6 Set or change the default schema for a connection's session. SET SCHEMA schema-name Description \u00b6 The SET SCHEMA statement sets or changes the default schema for a connection's session to the provided schema. This is then used as the schema for all statements issued from the connection that does not explicitly specify a schema name. The default schema is APP. Example \u00b6 -- below are equivalent assuming a TRADE schema SET SCHEMA TRADE; SET SCHEMA trade;","title":"SET SCHEMA"},{"location":"reference/sql_reference/set-schema/#set-schema","text":"Set or change the default schema for a connection's session. SET SCHEMA schema-name","title":"SET SCHEMA"},{"location":"reference/sql_reference/set-schema/#description","text":"The SET SCHEMA statement sets or changes the default schema for a connection's session to the provided schema. This is then used as the schema for all statements issued from the connection that does not explicitly specify a schema name. The default schema is APP.","title":"Description"},{"location":"reference/sql_reference/set-schema/#example","text":"-- below are equivalent assuming a TRADE schema SET SCHEMA TRADE; SET SCHEMA trade;","title":"Example"},{"location":"reference/sql_reference/truncate-table/","text":"TRUNCATE TABLE \u00b6 Remove all content from a table and return it to its initial, empty state. TRUNCATE TABLE clears all in-memory data for the specified table as well as any data that was persisted to SnappyData disk stores. TRUNCATE TABLE table-name Description \u00b6 To truncate a table, you must be the table's owner. You cannot use this command to truncate system tables. Example \u00b6 To truncate the \"flights\" table in the current schema: TRUNCATE TABLE flights; Related Topics CREATE TABLE DROP TABLE DELETE TABLE SHOW TABLES","title":"TRUNCATE TABLE"},{"location":"reference/sql_reference/truncate-table/#truncate-table","text":"Remove all content from a table and return it to its initial, empty state. TRUNCATE TABLE clears all in-memory data for the specified table as well as any data that was persisted to SnappyData disk stores. TRUNCATE TABLE table-name","title":"TRUNCATE TABLE"},{"location":"reference/sql_reference/truncate-table/#description","text":"To truncate a table, you must be the table's owner. You cannot use this command to truncate system tables.","title":"Description"},{"location":"reference/sql_reference/truncate-table/#example","text":"To truncate the \"flights\" table in the current schema: TRUNCATE TABLE flights; Related Topics CREATE TABLE DROP TABLE DELETE TABLE SHOW TABLES","title":"Example"},{"location":"reference/sql_reference/undeploy/","text":"UNDEPLOY \u00b6 Removes the jars that are directly installed and the jars that are associated with a package, from the system. Syntax \u00b6 undeploy <unique-alias-name>; * jar-name - Name of the jar that must be removed. Description \u00b6 The command removes the jars that are directly installed and the jars that are associated with a package, from the system. Example \u00b6 undeploy spark_deep_learning_0_3_0; Related Topics DEPLOY PACKAGE DEPLOY JAR","title":"UNDEPLOY"},{"location":"reference/sql_reference/undeploy/#undeploy","text":"Removes the jars that are directly installed and the jars that are associated with a package, from the system.","title":"UNDEPLOY"},{"location":"reference/sql_reference/undeploy/#syntax","text":"undeploy <unique-alias-name>; * jar-name - Name of the jar that must be removed.","title":"Syntax"},{"location":"reference/sql_reference/undeploy/#description","text":"The command removes the jars that are directly installed and the jars that are associated with a package, from the system.","title":"Description"},{"location":"reference/sql_reference/undeploy/#example","text":"undeploy spark_deep_learning_0_3_0; Related Topics DEPLOY PACKAGE DEPLOY JAR","title":"Example"},{"location":"reference/sql_reference/update/","text":"UPDATE \u00b6 Update the value of one or more columns. UPDATE table-name SET column-name = value [, column-name = value ]* [ WHERE predicate ] value: expression | DEFAULT Description \u00b6 This form of the UPDATE statement is called a searched update. It updates the value of one or more columns for all rows of the table for which the WHERE clause evaluates to TRUE. Specifying DEFAULT for the update value sets the value of the column to the default defined for that table. The UPDATE statement returns the number of rows that were updated. Note Updates on partitioning columns and primary key columns are not supported. Delete/Update with a subquery does not work with row tables, if the row table does not have a primary key. An exception to this rule is, if the subquery contains another row table and a simple where clause. Implicit conversion of string to numeric value is not performed in an UPDATE statement when the string type expression is used as part of a binary arithmetic expression. For example, the following SQL fails with AnalysisException. Here age is an int type column: update users set age = age + '2' As a workaround, you can apply an explicit cast, if the string is a number. For example: update users set age = age + cast ('2' as int) Note It is important to note that when you cast a non-numeric string, it results in a NULL value which is reflected in the table as a result of the update. This behavior of casting is inherited from Apache Spark, as Spark performs a fail-safe casting. For example, the following statement populates age column with NULL values: update users set age = age + cast ('abc' as int) Assigning a non-matching type expression to a column also fails with AnalysisException . However, the assignment is allowed in the following cases, even if the data type does not match: assigning null value assigning narrower decimal to a wider decimal assigning narrower numeric type to wider numeric type as long as precision is not compromised assigning of narrower numeric types to decimal type assigning expression of any data type to a string type column For example, the following statement fails with AnalysisException : update users set age = '45' Here also, the workaround is to cast the expression explicitly. For example, the following statement will pass: update users set age = cast ('45' as int) Example \u00b6 // Change the ADDRESS and SINCE fields to null for all customers with ID greater than 10. UPDATE TRADE.CUSTOMERS SET ADDR=NULL, SINCE=NULL WHERE CID > 10; // Set the ADDR of all customers to 'SnappyData' where the current address is NULL. UPDATE TRADE.CUSTOMERS SET ADDR = 'Snappydata' WHERE ADDR IS NULL; // Increase the QTY field by 10 for all rows of SELLORDERS table. UPDATE TRADE.SELLORDERS SET QTY = QTY+10; // Change the STATUS field of all orders of SELLORDERS table to DEFAULT ( 'open') , for customer ID = 10 UPDATE TRADE.SELLORDERS SET STATUS = DEFAULT WHERE CID = 10; // Use multiple tables in UPDATE statement with JOIN UPDATE TRADE.SELLORDERS SET A.TID = B.TID FROM TRADE.SELLORDERS A JOIN TRADE.CUSTOMERS B ON A.CID = B.CID;","title":"UPDATE"},{"location":"reference/sql_reference/update/#update","text":"Update the value of one or more columns. UPDATE table-name SET column-name = value [, column-name = value ]* [ WHERE predicate ] value: expression | DEFAULT","title":"UPDATE"},{"location":"reference/sql_reference/update/#description","text":"This form of the UPDATE statement is called a searched update. It updates the value of one or more columns for all rows of the table for which the WHERE clause evaluates to TRUE. Specifying DEFAULT for the update value sets the value of the column to the default defined for that table. The UPDATE statement returns the number of rows that were updated. Note Updates on partitioning columns and primary key columns are not supported. Delete/Update with a subquery does not work with row tables, if the row table does not have a primary key. An exception to this rule is, if the subquery contains another row table and a simple where clause. Implicit conversion of string to numeric value is not performed in an UPDATE statement when the string type expression is used as part of a binary arithmetic expression. For example, the following SQL fails with AnalysisException. Here age is an int type column: update users set age = age + '2' As a workaround, you can apply an explicit cast, if the string is a number. For example: update users set age = age + cast ('2' as int) Note It is important to note that when you cast a non-numeric string, it results in a NULL value which is reflected in the table as a result of the update. This behavior of casting is inherited from Apache Spark, as Spark performs a fail-safe casting. For example, the following statement populates age column with NULL values: update users set age = age + cast ('abc' as int) Assigning a non-matching type expression to a column also fails with AnalysisException . However, the assignment is allowed in the following cases, even if the data type does not match: assigning null value assigning narrower decimal to a wider decimal assigning narrower numeric type to wider numeric type as long as precision is not compromised assigning of narrower numeric types to decimal type assigning expression of any data type to a string type column For example, the following statement fails with AnalysisException : update users set age = '45' Here also, the workaround is to cast the expression explicitly. For example, the following statement will pass: update users set age = cast ('45' as int)","title":"Description"},{"location":"reference/sql_reference/update/#example","text":"// Change the ADDRESS and SINCE fields to null for all customers with ID greater than 10. UPDATE TRADE.CUSTOMERS SET ADDR=NULL, SINCE=NULL WHERE CID > 10; // Set the ADDR of all customers to 'SnappyData' where the current address is NULL. UPDATE TRADE.CUSTOMERS SET ADDR = 'Snappydata' WHERE ADDR IS NULL; // Increase the QTY field by 10 for all rows of SELLORDERS table. UPDATE TRADE.SELLORDERS SET QTY = QTY+10; // Change the STATUS field of all orders of SELLORDERS table to DEFAULT ( 'open') , for customer ID = 10 UPDATE TRADE.SELLORDERS SET STATUS = DEFAULT WHERE CID = 10; // Use multiple tables in UPDATE statement with JOIN UPDATE TRADE.SELLORDERS SET A.TID = B.TID FROM TRADE.SELLORDERS A JOIN TRADE.CUSTOMERS B ON A.CID = B.CID;","title":"Example"},{"location":"reference/system_tables/","text":"System Tables \u00b6 Tables that SnappyData uses to manage the distributed system and its data. All system tables are part of the SYS schema. The following system tables are available: MEMBERS SYSDISKSTORES SYSDISKSTOREIDS SYSTABLES","title":"System Tables"},{"location":"reference/system_tables/#system-tables","text":"Tables that SnappyData uses to manage the distributed system and its data. All system tables are part of the SYS schema. The following system tables are available: MEMBERS SYSDISKSTORES SYSDISKSTOREIDS SYSTABLES","title":"System Tables"},{"location":"reference/system_tables/members/","text":"MEMBERS \u00b6 A SnappyData virtual table that contains information about each distributed system member. Note SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table. Column Name Type Length Nullable Contents ID VARCHAR 128 No The unique ID of the member. This ID has the format: hostname(process_id);member_number;:udp_port/tcp_port For example: 10.0.1.31(66878);v0;:41715/63386 KIND VARCHAR 24 No Specifies the type of SnappyData member process: * datastore\u2014A member that hosts data. * peer\u2014A member that does not host data. * locator\u2014Provides discovery services for a cluster. Member types can also be qualified with additional keywords * normal\u2014The member can communicate with other members in a cluster. * loner\u2014The member is standalone and cannot communicate with other members. Loners use no locators for discovery. * admin\u2014The member also acts as a JMX manager node. HOSTDATA BOOLEAN Yes A value of \u20181\u2019 indicates that this member is a data store and can host data. Otherwise, the member is a peer client with no hosted data. ISELDER BOOLEAN No Is this the eldest member of the distributed system. Typically, this is the member who first joins the cluster. IPADDRESS VARCHAR 64 Yes The fully-qualified hostname/IP address of the member. HOST VARCHAR 128 Yes The fully-qualified hostname of the member. PID INTEGER 10 No The member process ID. PORT INTEGER 10 No The member UDP port. ROLES VARCHAR 128 No Not used. NETSERVERS VARCHAR 32672 No Host and port information for Network Servers that are running on SnappyData members. LOCATOR VARCHAR 32672 No Host and port information for locator members. SERVERGROUPS VARCHAR 32672 No A comma-separated list of server groups of which this member is a part. Note : SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table. SYSTEMPROPS CLOB 2147483647 No A list of all system properties used to start this member. This includes properties such as the classpath, JVM version, and so forth. GEMFIREPROPS CLOB 2147483647 No The names and values of GemFire core system properties that the member uses. BOOTPROPS CLOB 2147483647 No All of the SnappyData boot properties names and values that a member uses. Example snappy> select * from SYS.MEMBERS; ID |KIND |STATUS |HOSTDATA|ISELDER|IPADDRESS |HOST |PID |PORT |ROLES|NETSERVERS |THRIFTSERVERS |LOCATOR |SERVERGROUPS|MANAGERINFO |SYSTEMPROPS |GEMFIREPROPS |BOOTPROPS ---------------------------------------------------------------------------------------------------------------- 127.0.0.1(5687)<v1>:47719 |datastore |RUNNING|true |false |/127.0.0.1|localhost|5687 |47719 | |localhost/127.0.0.1[1528]|localhost/127.0.0.1[1528]|NULL | |Managed Node | --- System Pr&| --- GemFire P&| --- GemFireXD& 127.0.0.1(5877)<v2>:10769 |primary lead |RUNNING|false |false |/127.0.0.1|localhost|5877 |10769 | | | |NULL | |Managed Node | --- System Pr&| --- GemFire P&| --- GemFireXD& 127.0.0.1(5548)<ec><v0>:21415|locator |RUNNING|false |true |/127.0.0.1|localhost|5548 |21415 | |localhost/127.0.0.1[1527]|localhost/127.0.0.1[1527]|127.0.0.1[10334]| |Manager Node: Not Running| --- System Pr&| --- GemFire P&| --- GemFireXD& 3 rows selected","title":"MEMBERS"},{"location":"reference/system_tables/members/#members","text":"A SnappyData virtual table that contains information about each distributed system member. Note SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table. Column Name Type Length Nullable Contents ID VARCHAR 128 No The unique ID of the member. This ID has the format: hostname(process_id);member_number;:udp_port/tcp_port For example: 10.0.1.31(66878);v0;:41715/63386 KIND VARCHAR 24 No Specifies the type of SnappyData member process: * datastore\u2014A member that hosts data. * peer\u2014A member that does not host data. * locator\u2014Provides discovery services for a cluster. Member types can also be qualified with additional keywords * normal\u2014The member can communicate with other members in a cluster. * loner\u2014The member is standalone and cannot communicate with other members. Loners use no locators for discovery. * admin\u2014The member also acts as a JMX manager node. HOSTDATA BOOLEAN Yes A value of \u20181\u2019 indicates that this member is a data store and can host data. Otherwise, the member is a peer client with no hosted data. ISELDER BOOLEAN No Is this the eldest member of the distributed system. Typically, this is the member who first joins the cluster. IPADDRESS VARCHAR 64 Yes The fully-qualified hostname/IP address of the member. HOST VARCHAR 128 Yes The fully-qualified hostname of the member. PID INTEGER 10 No The member process ID. PORT INTEGER 10 No The member UDP port. ROLES VARCHAR 128 No Not used. NETSERVERS VARCHAR 32672 No Host and port information for Network Servers that are running on SnappyData members. LOCATOR VARCHAR 32672 No Host and port information for locator members. SERVERGROUPS VARCHAR 32672 No A comma-separated list of server groups of which this member is a part. Note : SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table. SYSTEMPROPS CLOB 2147483647 No A list of all system properties used to start this member. This includes properties such as the classpath, JVM version, and so forth. GEMFIREPROPS CLOB 2147483647 No The names and values of GemFire core system properties that the member uses. BOOTPROPS CLOB 2147483647 No All of the SnappyData boot properties names and values that a member uses. Example snappy> select * from SYS.MEMBERS; ID |KIND |STATUS |HOSTDATA|ISELDER|IPADDRESS |HOST |PID |PORT |ROLES|NETSERVERS |THRIFTSERVERS |LOCATOR |SERVERGROUPS|MANAGERINFO |SYSTEMPROPS |GEMFIREPROPS |BOOTPROPS ---------------------------------------------------------------------------------------------------------------- 127.0.0.1(5687)<v1>:47719 |datastore |RUNNING|true |false |/127.0.0.1|localhost|5687 |47719 | |localhost/127.0.0.1[1528]|localhost/127.0.0.1[1528]|NULL | |Managed Node | --- System Pr&| --- GemFire P&| --- GemFireXD& 127.0.0.1(5877)<v2>:10769 |primary lead |RUNNING|false |false |/127.0.0.1|localhost|5877 |10769 | | | |NULL | |Managed Node | --- System Pr&| --- GemFire P&| --- GemFireXD& 127.0.0.1(5548)<ec><v0>:21415|locator |RUNNING|false |true |/127.0.0.1|localhost|5548 |21415 | |localhost/127.0.0.1[1527]|localhost/127.0.0.1[1527]|127.0.0.1[10334]| |Manager Node: Not Running| --- System Pr&| --- GemFire P&| --- GemFireXD& 3 rows selected","title":"MEMBERS"},{"location":"reference/system_tables/sysdiskstoreids/","text":"SYSDISKSTOREIDS \u00b6 Contains information about all disk stores IDs created in the SnappyData distributed system. Column Name Type Length Nullable Contents MEMBERID VARCHAR 128 false The ID of the cluster member. NAME VARCHAR 128 false The diskstore name. Two inbuilt diskstores one for DataDictionary is named GFXD-DD-DISKSTORE while the default data diskstore is named GFXD-DEFAULT-DISKSTORE . ID CHAR 36 false The unique diskstore ID. This is what appears in log files or output of snappy-status-all.sh script if a member is waiting for another node to start to sync its data (if it determines the other node may have more recent data). DIRS VARCHAR 32672 false Comma-separated list of directories used for the diskstore. These are the ones provided in CREATE DISKSTORE or else if no explicit directory was provided, then the working directory of the node. Example snappy> create diskstore d1 ('D1'); snappy> select * from sys.diskstoreids; MEMBERID |NAME |ID |DIRS ----------------------------------------------------------------------------------------------------------------------------------- 127.0.0.1(7794)<v2>:20960 |GFXD-DEFAULT-DISKSTORE |a47f63ca-f128-4102-bcd6-51549fa&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE1 |88d7685c-a190-4711-b2ca-ec27ece&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |D1-SNAPPY-DELTA |7432070a-05e3-4303-b731-91abbe3&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |GFXD-DEFAULT-DISKSTORE |d1024174-8ec8-47ac-8df8-4b7f63b&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE1-SNAPPY-DELTA |e252410f-1440-4b8e-9b84-343276f&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |D1 |56a1f778-ba8c-41bd-8556-27c35fa&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE2-SNAPPY-DELTA |fc3a72c0-f70a-4fed-891f-f3521d4&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE2 |4b089853-3fb1-4fd7-ba81-e3e6fe3&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |GFXD-DD-DISKSTORE |72350926-f5fa-415f-9124-24e0944&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |SNAPPY-INTERNAL-DELTA |6a20b9db-e5c6-4081-baa8-77da8b1&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7494)<ec><v0>:35874 |GFXD-DD-DISKSTORE |da99f2a1-56fa-47a6-a31b-ea12e8a&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7494)<ec><v0>:35874 |GFXD-DEFAULT-DISKSTORE |5412d64a-dfd2-46dc-b3ba-09c2079&|/build-artifacts/scala-2.11/snappy& 12 rows selected","title":"SYSDISKSTOREIDS"},{"location":"reference/system_tables/sysdiskstoreids/#sysdiskstoreids","text":"Contains information about all disk stores IDs created in the SnappyData distributed system. Column Name Type Length Nullable Contents MEMBERID VARCHAR 128 false The ID of the cluster member. NAME VARCHAR 128 false The diskstore name. Two inbuilt diskstores one for DataDictionary is named GFXD-DD-DISKSTORE while the default data diskstore is named GFXD-DEFAULT-DISKSTORE . ID CHAR 36 false The unique diskstore ID. This is what appears in log files or output of snappy-status-all.sh script if a member is waiting for another node to start to sync its data (if it determines the other node may have more recent data). DIRS VARCHAR 32672 false Comma-separated list of directories used for the diskstore. These are the ones provided in CREATE DISKSTORE or else if no explicit directory was provided, then the working directory of the node. Example snappy> create diskstore d1 ('D1'); snappy> select * from sys.diskstoreids; MEMBERID |NAME |ID |DIRS ----------------------------------------------------------------------------------------------------------------------------------- 127.0.0.1(7794)<v2>:20960 |GFXD-DEFAULT-DISKSTORE |a47f63ca-f128-4102-bcd6-51549fa&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE1 |88d7685c-a190-4711-b2ca-ec27ece&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |D1-SNAPPY-DELTA |7432070a-05e3-4303-b731-91abbe3&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |GFXD-DEFAULT-DISKSTORE |d1024174-8ec8-47ac-8df8-4b7f63b&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE1-SNAPPY-DELTA |e252410f-1440-4b8e-9b84-343276f&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |D1 |56a1f778-ba8c-41bd-8556-27c35fa&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE2-SNAPPY-DELTA |fc3a72c0-f70a-4fed-891f-f3521d4&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |STORE2 |4b089853-3fb1-4fd7-ba81-e3e6fe3&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |GFXD-DD-DISKSTORE |72350926-f5fa-415f-9124-24e0944&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7638)<v1>:24353 |SNAPPY-INTERNAL-DELTA |6a20b9db-e5c6-4081-baa8-77da8b1&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7494)<ec><v0>:35874 |GFXD-DD-DISKSTORE |da99f2a1-56fa-47a6-a31b-ea12e8a&|/build-artifacts/scala-2.11/snappy& 127.0.0.1(7494)<ec><v0>:35874 |GFXD-DEFAULT-DISKSTORE |5412d64a-dfd2-46dc-b3ba-09c2079&|/build-artifacts/scala-2.11/snappy& 12 rows selected","title":"SYSDISKSTOREIDS"},{"location":"reference/system_tables/sysdiskstores/","text":"SYSDISKSTORES \u00b6 Contains information about all disk stores created in the SnappyData distributed system. See CREATE DISKSTORE . Column Name Type Length Nullable Contents NAME VARCHAR 128 No The unique identifier of the disk store. MAXLOGSIZE BIGINT 10 No The maximum size, in megabytes, of a single oplog file in the disk store. AUTOCOMPACT CHAR 6 No Specifies whether SnappyData automatically compacts log files in this disk store. ALLOWFORCECOMPACTION CHAR 6 No Specifies whether the disk store permits online compaction of log files using the snappy utility. COMPACTIONTHRESHOLD INTEGER 10 No The threshold after which an oplog file is eligible for compaction. Specified as a percentage value from 0\u2013100. TIMEINTERVAL BIGINT 10 No The maximum number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. WRITEBUFFERSIZE INTEGER 10 No The size of the buffer SnappyData uses to store operations when writing to the disk store. QUEUESIZE INTEGER 10 No The maximum number of row operations that SnappyData can asynchronously queue for writing to the disk store. DIR_PATH_SIZE VARCHAR 32672 No The directory names that hold disk store oplog files, and the maximum size in megabytes that each directory can store. Example snappy> select * from sys.SYSDISKSTORES; NAME |MAXLOGSIZE |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL |WRITEBUFFERSIZE|QUEUESIZE |DIR_PATH_SIZE ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- STORE2 |456 |true |false |50 |1000 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& STORE1-SNAPPY-DELTA |50 |true |false |80 |223344 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& STORE2-SNAPPY-DELTA |50 |true |false |50 |1000 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& GFXD-DEFAULT-DISKSTORE |1024 |true |false |50 |1000 |32768 |0 |/build-artifacts/scala-2.11/snappy/& GFXD-DD-DISKSTORE |10 |true |false |50 |1000 |32768 |0 |/build-artifacts/scala-2.11/snappy/& SNAPPY-INTERNAL-DELTA |50 |true |false |50 |1000 |32768 |0 |/build-artifacts/scala-2.11/snappy/& STORE1 |1024 |true |false |80 |223344 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& 7 rows selected snappy> select * from SYS.SYSDISKSTORES where NAME = 'STORE1'; NAME |MAXLOGSIZE |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL |WRITEBUFFERSIZE|QUEUESIZE |DIR_PATH_SIZE ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- STORE1 |1024 |true |false |80 |223344 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& 1 row selected snappy> select * from sys.sysdiskstores where DIR_PATH_SIZE like '%mytest%'; NAME |MAXLOGSIZE |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL |WRITEBUFFERSIZE|QUEUESIZE |DIR_PATH_SIZE ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ TEST |1024 |true |false |50 |1000 |32768 |0 |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest TEST-SNAPPY-DELTA|50 |true |false |50 |1000 |32768 |0 |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest/snappy-internal-delta 2 rows selected Related Topics DROP DISKSTORE CREATE DISKSTORE","title":"SYSDISKSTORES"},{"location":"reference/system_tables/sysdiskstores/#sysdiskstores","text":"Contains information about all disk stores created in the SnappyData distributed system. See CREATE DISKSTORE . Column Name Type Length Nullable Contents NAME VARCHAR 128 No The unique identifier of the disk store. MAXLOGSIZE BIGINT 10 No The maximum size, in megabytes, of a single oplog file in the disk store. AUTOCOMPACT CHAR 6 No Specifies whether SnappyData automatically compacts log files in this disk store. ALLOWFORCECOMPACTION CHAR 6 No Specifies whether the disk store permits online compaction of log files using the snappy utility. COMPACTIONTHRESHOLD INTEGER 10 No The threshold after which an oplog file is eligible for compaction. Specified as a percentage value from 0\u2013100. TIMEINTERVAL BIGINT 10 No The maximum number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. WRITEBUFFERSIZE INTEGER 10 No The size of the buffer SnappyData uses to store operations when writing to the disk store. QUEUESIZE INTEGER 10 No The maximum number of row operations that SnappyData can asynchronously queue for writing to the disk store. DIR_PATH_SIZE VARCHAR 32672 No The directory names that hold disk store oplog files, and the maximum size in megabytes that each directory can store. Example snappy> select * from sys.SYSDISKSTORES; NAME |MAXLOGSIZE |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL |WRITEBUFFERSIZE|QUEUESIZE |DIR_PATH_SIZE ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- STORE2 |456 |true |false |50 |1000 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& STORE1-SNAPPY-DELTA |50 |true |false |80 |223344 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& STORE2-SNAPPY-DELTA |50 |true |false |50 |1000 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& GFXD-DEFAULT-DISKSTORE |1024 |true |false |50 |1000 |32768 |0 |/build-artifacts/scala-2.11/snappy/& GFXD-DD-DISKSTORE |10 |true |false |50 |1000 |32768 |0 |/build-artifacts/scala-2.11/snappy/& SNAPPY-INTERNAL-DELTA |50 |true |false |50 |1000 |32768 |0 |/build-artifacts/scala-2.11/snappy/& STORE1 |1024 |true |false |80 |223344 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& 7 rows selected snappy> select * from SYS.SYSDISKSTORES where NAME = 'STORE1'; NAME |MAXLOGSIZE |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL |WRITEBUFFERSIZE|QUEUESIZE |DIR_PATH_SIZE ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- STORE1 |1024 |true |false |80 |223344 |19292393 |17374 |/build-artifacts/scala-2.11/snappy/& 1 row selected snappy> select * from sys.sysdiskstores where DIR_PATH_SIZE like '%mytest%'; NAME |MAXLOGSIZE |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL |WRITEBUFFERSIZE|QUEUESIZE |DIR_PATH_SIZE ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ TEST |1024 |true |false |50 |1000 |32768 |0 |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest TEST-SNAPPY-DELTA|50 |true |false |50 |1000 |32768 |0 |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest/snappy-internal-delta 2 rows selected Related Topics DROP DISKSTORE CREATE DISKSTORE","title":"SYSDISKSTORES"},{"location":"reference/system_tables/systables/","text":"SYSTABLES \u00b6 Describes the tables and views in the distributed system. Column Name Type Length Nullable Contents TABLEID CHAR 36 No Unique identifier for table or view TABLENAME VARCHAR 128 No Table or view name TABLETYPE CHAR 1 No 'S' (system table), 'T' (user table), 'A' (synonym), or 'V' (view) SCHEMAID CHAR 36 No Schema ID for the table or view TABLESCHEMANAME VARCHAR 128 No The table schema LOCKGRANULARITY CHAR 1 No Lock granularity for the table: 'T' (table level locking) or 'R' (row level locking, the default) SERVERGROUPS VARCHAR 128 No The server groups assigned to the table DATAPOLICY VARCHAR 24 No Table partitioning and replication status PARTITIONATTRS LONG VARCHAR 32,700 Yes For partitioned tables, displays the additional partitioning attributes assigned with the CREATE TABLE statement, such as colocation, buckets, and redundancy values RESOLVER LONG VARCHAR 32,700 Yes The partitioning resolver (contains the partitioning clause). EXPIRATIONATTRS LONG VARCHAR 32,700 Yes Row expiration settings EVICTIONATTRS LONG VARCHAR 32,700 Yes Row eviction settings DISKATTRS LONG VARCHAR 32,700 Yes Table persistence settings LOADER VARCHAR 128 Yes Not available for this release WRITER VARCHAR 128 Yes Not available for this release LISTENERS LONG VARCHAR 32,700 Yes Not available for this release ASYNCLISTENERS VARCHAR 256 Yes Not available for this release GATEWAYENABLED BOOLEAN 1 No Not available for this release GATEWAYSENDERS VARCHAR 256 Yes Not available for this release Example snappy> select * from SYS.SYSTABLES; TABLEID |TABLENAME |TABLETYPE|SCHEMAID |TABLESCHEMANAME |LOCKGRANULARITY|SERVERGROUPS|DATAPOLICY |PARTITIONATTRS|RESOLVER|EXPIRATIONATTRS|EVICTIONATTRS|DISKATTRS |LOADER|WRITER|LISTENERS|ASYNCLISTENERS|GATEWAYENABLED|GATEWAYSENDERS|OFFHEAPENABLED -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- df2180fd-0162-84af-a660-0000f2ef3af8|TBLS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 0073400e-00b6-fdfc-71ce-000b0a763800|GATEWAYSENDERS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 80000000-00d1-15f7-ab70-000a0a0b1500|SYSSTATEMENTS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false a6c0c1fe-0162-84af-a660-0000f2ef3af8|PARTITION_KEY_VALS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 140d4147-0162-84af-a660-0000f2ef3af8|SORT_COLS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 9fc7c266-0162-84af-a660-0000f2ef3af8|FUNCS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false f33d40c7-0162-84af-a660-0000f2ef3af8|SYSXPLAIN_RESULTSETS |C |c013800d-00fb-2644-07ec-000000134f30|SYSSTAT |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 80000039-00d0-fd77-3ed8-000a0a0b1900|SYSKEYS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false e03f4017-0115-382c-08df-ffffe275b270|SYSROLES |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 80000000-00d3-e222-873f-000a0a0b1900|SYSFILES |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 2057c01b-0103-0e39-b8e7-00000010f010|SYSROUTINEPERMS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 65548115-0162-84af-a660-0000f2ef3af8|SDS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 96a2414f-0162-84af-a660-0000f2ef3af8|SD_PARAMS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false","title":"SYSTABLES"},{"location":"reference/system_tables/systables/#systables","text":"Describes the tables and views in the distributed system. Column Name Type Length Nullable Contents TABLEID CHAR 36 No Unique identifier for table or view TABLENAME VARCHAR 128 No Table or view name TABLETYPE CHAR 1 No 'S' (system table), 'T' (user table), 'A' (synonym), or 'V' (view) SCHEMAID CHAR 36 No Schema ID for the table or view TABLESCHEMANAME VARCHAR 128 No The table schema LOCKGRANULARITY CHAR 1 No Lock granularity for the table: 'T' (table level locking) or 'R' (row level locking, the default) SERVERGROUPS VARCHAR 128 No The server groups assigned to the table DATAPOLICY VARCHAR 24 No Table partitioning and replication status PARTITIONATTRS LONG VARCHAR 32,700 Yes For partitioned tables, displays the additional partitioning attributes assigned with the CREATE TABLE statement, such as colocation, buckets, and redundancy values RESOLVER LONG VARCHAR 32,700 Yes The partitioning resolver (contains the partitioning clause). EXPIRATIONATTRS LONG VARCHAR 32,700 Yes Row expiration settings EVICTIONATTRS LONG VARCHAR 32,700 Yes Row eviction settings DISKATTRS LONG VARCHAR 32,700 Yes Table persistence settings LOADER VARCHAR 128 Yes Not available for this release WRITER VARCHAR 128 Yes Not available for this release LISTENERS LONG VARCHAR 32,700 Yes Not available for this release ASYNCLISTENERS VARCHAR 256 Yes Not available for this release GATEWAYENABLED BOOLEAN 1 No Not available for this release GATEWAYSENDERS VARCHAR 256 Yes Not available for this release Example snappy> select * from SYS.SYSTABLES; TABLEID |TABLENAME |TABLETYPE|SCHEMAID |TABLESCHEMANAME |LOCKGRANULARITY|SERVERGROUPS|DATAPOLICY |PARTITIONATTRS|RESOLVER|EXPIRATIONATTRS|EVICTIONATTRS|DISKATTRS |LOADER|WRITER|LISTENERS|ASYNCLISTENERS|GATEWAYENABLED|GATEWAYSENDERS|OFFHEAPENABLED -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- df2180fd-0162-84af-a660-0000f2ef3af8|TBLS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 0073400e-00b6-fdfc-71ce-000b0a763800|GATEWAYSENDERS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 80000000-00d1-15f7-ab70-000a0a0b1500|SYSSTATEMENTS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false a6c0c1fe-0162-84af-a660-0000f2ef3af8|PARTITION_KEY_VALS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 140d4147-0162-84af-a660-0000f2ef3af8|SORT_COLS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 9fc7c266-0162-84af-a660-0000f2ef3af8|FUNCS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false f33d40c7-0162-84af-a660-0000f2ef3af8|SYSXPLAIN_RESULTSETS |C |c013800d-00fb-2644-07ec-000000134f30|SYSSTAT |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 80000039-00d0-fd77-3ed8-000a0a0b1900|SYSKEYS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false e03f4017-0115-382c-08df-ffffe275b270|SYSROLES |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 80000000-00d3-e222-873f-000a0a0b1900|SYSFILES |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 2057c01b-0103-0e39-b8e7-00000010f010|SYSROUTINEPERMS |S |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS |R | |NORMAL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |NULL |false |NULL |false 65548115-0162-84af-a660-0000f2ef3af8|SDS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false 96a2414f-0162-84af-a660-0000f2ef3af8|SD_PARAMS |T |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R | |PERSISTENT_REPLICATE|NULL |NULL |NULL |NULL |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL |NULL |NULL |NULL |false |NULL |false","title":"SYSTABLES"},{"location":"reference/system_tables/systablestats/","text":"SYSTABLESTATS \u00b6 Creates a virtual table that displays statistics for all the tables that exist in distributed system. Column Name Type Length Nullable Contents TABLE VARCHAR 512 NO Name of the table IS_COLUMN_TABLE BOOLEAN NO Displays True for column tables and False for row tables IS_REPLICATED_TABLE BOOLEAN NO Displays True for replicated tables and False for partitioned tables ROW_COUNT BIGINT 10 NO Total number of rows in the table SIZE_IN_MEMORY BIGINT 10 NO Heap memory used by data table to store its data/records. TOTAL_SIZE BIGINT 10 NO Collective physical memory and disk overflow space used by the data table to store its data/record BUCKETS BIGINT 10 NO Number of buckets Example snappy> select * from SYS.TABLESTATS; TABLE |IS_COLUMN_TABLE|IS_REPLICATED_TABLE|ROW_COUNT|SIZE_IN_MEMORY|TOTAL_SIZE|BUCKETS ----------------------------------------------------------------------------------------------- APP.CUSTOMERS |false |true |0 |1672 |1672 |1 APP.ABCD |true |false |7 |36704 |36704 |10 TRADE.ORDERS |false |false |1 |199968 |199968 |113 APP.EMPLOYEE |true |false |10 |37280 |37280 |10 4 rows selected","title":"SYSTABLESTATS"},{"location":"reference/system_tables/systablestats/#systablestats","text":"Creates a virtual table that displays statistics for all the tables that exist in distributed system. Column Name Type Length Nullable Contents TABLE VARCHAR 512 NO Name of the table IS_COLUMN_TABLE BOOLEAN NO Displays True for column tables and False for row tables IS_REPLICATED_TABLE BOOLEAN NO Displays True for replicated tables and False for partitioned tables ROW_COUNT BIGINT 10 NO Total number of rows in the table SIZE_IN_MEMORY BIGINT 10 NO Heap memory used by data table to store its data/records. TOTAL_SIZE BIGINT 10 NO Collective physical memory and disk overflow space used by the data table to store its data/record BUCKETS BIGINT 10 NO Number of buckets Example snappy> select * from SYS.TABLESTATS; TABLE |IS_COLUMN_TABLE|IS_REPLICATED_TABLE|ROW_COUNT|SIZE_IN_MEMORY|TOTAL_SIZE|BUCKETS ----------------------------------------------------------------------------------------------- APP.CUSTOMERS |false |true |0 |1672 |1672 |1 APP.ABCD |true |false |7 |36704 |36704 |10 TRADE.ORDERS |false |false |1 |199968 |199968 |113 APP.EMPLOYEE |true |false |10 |37280 |37280 |10 4 rows selected","title":"SYSTABLESTATS"},{"location":"release_notes/known_issues_1.1.0/","text":"Known Issues for SnappyData 1.1.0 release \u00b6 The following key issues have been registered as bugs in the SnappyData bug tracking system: BUG ID Title Description Workaround SNAP-1375 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-327.13.1.el7.x86\\_64 - Java version: 1.8.0\\_121 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-1422 Catalog in smart connector inconsistent with servers Catalog in smart connector inconsistent with servers|When a table is queried from spark-shell (or from an application that uses smart connector mode) the table metadata is cached on the smart connector side. If this table is dropped from SnappyData embedded cluster (by using snappy-shell, or JDBC application, or a Snappy job), the metadata on the smart connector side stays cached even though catalog has changed (table is dropped). In such cases, the user may see unexpected errors like \"org.apache.spark.sql.AnalysisException: Table `SNAPPYTABLE` already exists\" in the smart connector app side for example for `DataFrameWriter.saveAsTable()` API if the same table name that was dropped is used in `saveAsTable()` 1. User may either create a new SnappySession in such scenarios OR 2. Invalidate the cache on the Smart Connector mode, for example by calling `snappy.sessionCatalog.invalidateAll()` SNAP-1634 Creating a temporary table with the same name as an existing table in any schema should not be allowed When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results. Ensure that you create temporary tables with a unique name. SNAP-1753 TPCH Q19 execution performance degraded in 0.9 A disjunctive query (that is, query with two or more predicates joined by an OR clause) with common filter predicates may report performance issues. To resolve this, the query should be rewritten in the following manner to achieve better performance: select sum(l_extendedprice) from LINEITEM, PART where ( p_partkey = l_partkey and p_size between 1 and 5 and l_shipinstruct = 'DELIVER IN PERSON' ) or ( p_partkey = l_partkey and p_brand = 'Brand#?' and l_shipinstruct = 'DELIVER IN PERSON' ) select sum(l_extendedprice) from LINEITEM, PART where ( p_partkey = l_partkey and l_shipinstruct = 'DELIVER IN PERSON') and ( p_size between 1 and 5 or p_brand = 'Brand#3') SNAP-1911 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-327.13.1.el7.x86\\_64 - Java version: 1.8.0\\_131 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-1999 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-327.13.1.el7.x86\\_64 - Java version: 1.8.0\\_131 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-2017 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-514.10.2.el7.x86\\_64 - Java version: 1.8.0\\_144 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-2436 Data mismatch in queries running on servers coming up after a failure Data mismatch is observed in queries which are running when some servers are coming up after a failure. Also, the tables on which the queries are running must have set their redundancy to one or more for the issue to be observed. This issue happens due to Spark retry mechanism with SnappyData tables. To avoid this issue, you can stop all the queries when one or more servers are coming up. If that is not feasible, you should configure the lead node with `spark.task.maxFailures = 0`; SNAP-2381 Data inconsistency due to concurrent putInto/update operations Concurrent putInto/update operations and inserts in column tables with overlapping keys may cause data inconsistency. This problem is not seen when all the concurrent operations deal with different sets of rows. You can either ensure serialized mutable operations on column tables or these should be working on a distinct set of key columns. SNAP-2457 Inconsistent results during further transformation when using snappySession.sql() from jobs, Zeppelin etc. When using snappySession.sql() from jobs, Zeppelin etc, if a further transformation is applied on the DataFrame, it may give incorrect results due to plan caching. If you are using SnappyJobs and using snappySession.sql(\"sql string\") you must ensure that further transformation is not done. For example: val df1 = snappySession.sql(\"sql string\") val df2 = df1.repartition(12) // modifying df1 df2.collect() The above operation will give inconsistent results, if you are using df2 further in your code. To avoid this problem, you can use snappySession.sqlUncached(\"sql string\"). For example: val df1 = snappySession.sqlUncached(\"sql string\") val df2 = df1.repartition(12) // modifying df1 df2.collect()","title":"Known Issues for SnappyData 1.1.0 release"},{"location":"release_notes/known_issues_1.1.0/#known-issues-for-snappydata-110-release","text":"The following key issues have been registered as bugs in the SnappyData bug tracking system: BUG ID Title Description Workaround SNAP-1375 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-327.13.1.el7.x86\\_64 - Java version: 1.8.0\\_121 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-1422 Catalog in smart connector inconsistent with servers Catalog in smart connector inconsistent with servers|When a table is queried from spark-shell (or from an application that uses smart connector mode) the table metadata is cached on the smart connector side. If this table is dropped from SnappyData embedded cluster (by using snappy-shell, or JDBC application, or a Snappy job), the metadata on the smart connector side stays cached even though catalog has changed (table is dropped). In such cases, the user may see unexpected errors like \"org.apache.spark.sql.AnalysisException: Table `SNAPPYTABLE` already exists\" in the smart connector app side for example for `DataFrameWriter.saveAsTable()` API if the same table name that was dropped is used in `saveAsTable()` 1. User may either create a new SnappySession in such scenarios OR 2. Invalidate the cache on the Smart Connector mode, for example by calling `snappy.sessionCatalog.invalidateAll()` SNAP-1634 Creating a temporary table with the same name as an existing table in any schema should not be allowed When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results. Ensure that you create temporary tables with a unique name. SNAP-1753 TPCH Q19 execution performance degraded in 0.9 A disjunctive query (that is, query with two or more predicates joined by an OR clause) with common filter predicates may report performance issues. To resolve this, the query should be rewritten in the following manner to achieve better performance: select sum(l_extendedprice) from LINEITEM, PART where ( p_partkey = l_partkey and p_size between 1 and 5 and l_shipinstruct = 'DELIVER IN PERSON' ) or ( p_partkey = l_partkey and p_brand = 'Brand#?' and l_shipinstruct = 'DELIVER IN PERSON' ) select sum(l_extendedprice) from LINEITEM, PART where ( p_partkey = l_partkey and l_shipinstruct = 'DELIVER IN PERSON') and ( p_size between 1 and 5 or p_brand = 'Brand#3') SNAP-1911 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-327.13.1.el7.x86\\_64 - Java version: 1.8.0\\_131 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-1999 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-327.13.1.el7.x86\\_64 - Java version: 1.8.0\\_131 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-2017 JVM crash reported This was reported on: - RHEL kernel version: 3.10.0-514.10.2.el7.x86\\_64 - Java version: 1.8.0\\_144 To resolve this, use: - RHEL kernel version: 3.10.0-693.2.2.el7.x86\\_64 - Java version: 1.8.0\\_144 SNAP-2436 Data mismatch in queries running on servers coming up after a failure Data mismatch is observed in queries which are running when some servers are coming up after a failure. Also, the tables on which the queries are running must have set their redundancy to one or more for the issue to be observed. This issue happens due to Spark retry mechanism with SnappyData tables. To avoid this issue, you can stop all the queries when one or more servers are coming up. If that is not feasible, you should configure the lead node with `spark.task.maxFailures = 0`; SNAP-2381 Data inconsistency due to concurrent putInto/update operations Concurrent putInto/update operations and inserts in column tables with overlapping keys may cause data inconsistency. This problem is not seen when all the concurrent operations deal with different sets of rows. You can either ensure serialized mutable operations on column tables or these should be working on a distinct set of key columns. SNAP-2457 Inconsistent results during further transformation when using snappySession.sql() from jobs, Zeppelin etc. When using snappySession.sql() from jobs, Zeppelin etc, if a further transformation is applied on the DataFrame, it may give incorrect results due to plan caching. If you are using SnappyJobs and using snappySession.sql(\"sql string\") you must ensure that further transformation is not done. For example: val df1 = snappySession.sql(\"sql string\") val df2 = df1.repartition(12) // modifying df1 df2.collect() The above operation will give inconsistent results, if you are using df2 further in your code. To avoid this problem, you can use snappySession.sqlUncached(\"sql string\"). For example: val df1 = snappySession.sqlUncached(\"sql string\") val df2 = df1.repartition(12) // modifying df1 df2.collect()","title":"Known Issues for SnappyData 1.1.0 release"},{"location":"release_notes/release_notes/","text":"Overview \u00b6 SnappyData\u2122 is a memory-optimized database based on Apache Spark. It delivers very high throughput, low latency, and high concurrency for unified analytic workloads that may combine streaming, interactive analytics, and artificial intelligence in a single, easy to manage distributed cluster. In previous releases there were two editions namely, the Community Edition which was a fully functional core OSS distribution that was under the Apache Source License v2.0, and the Enterprise Edition which was sold by TIBCO Software under the name TIBCO ComputeDB\u2122 that included everything offered in the OSS version along with additional capabilities that are closed source and only available as part of a licensed subscription. The SnappyData team is pleased to announce the availability of version 1.3.1 of the platform. Starting with release 1.3.0, all the platform's private modules have been made open-source apart from the streaming GemFire connector (which depends on non-OSS Pivotal GemFire product jars). These include Approximate Query Processing (AQP) and the JDBC connector repositories which also include the off-heap storage support for column tables and the security modules. In addition, the ODBC driver has also been made open-source. With this, the entire code base of the platform (apart from the GemFire connector) has been made open source and there is no longer an Enterprise edition distributed by TIBCO. You can find details of the release artifacts towards the end of this page. The full set of documentation for SnappyData 1.3.1 including installation guide, user guide and reference guide can be found here . Release notes for the previous 1.3.0 release can be found here . The following table summarizes the high-level features available in the SnappyData platform: Feature Available Mutable Row and Column Store X Compatibility with Spark X Shared Nothing Persistence and HA X REST API for Spark Job Submission X Fault Tolerance for Driver X Access to the system using JDBC Driver X CLI for backup, restore, and export data X Spark console extensions X System Performance/behavior statistics X Support for transactions in Row tables X Support for indexing in Row tables X Support for snapshot transactions in Column tables X Online compaction of column block data X Transparent disk overflow of large query results X Support for external Hive meta store X SQL extensions for stream processing X SnappyData sink for structured stream processing X Structured Streaming user interface X Runtime deployment of packages and jars X Scala code execution from SQL (EXEC SCALA) X Out of the box support for cloud storage X Support for Hadoop 3.2 X SnappyData Interpreter for Apache Zeppelin X Synopsis Data Engine for Approximate Querying X Support for Synopsis Data Engine from TIBCO Spotfire\u00ae X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X GemFire connector New Features \u00b6 SnappyData 1.3.1 release includes the following new features over the previous 1.3.1 release: Support for Log4J 2.x (2.17.2) which is used by default Following up with the exposure of Log4Shell and related vulnerabilities, SnappyData Platform has moved to the latest Log4J 2.x (2.17.2) from the previous Log4J 1.x. Patches were ported for the Spark components (where support for Log4J 2.x will land only with the 3.3.0 release), while other components were updated to use Log4J/SLF4J. The Spark connector component supports both Log4J 2.x and Log4J 1.x to allow compatibility with upstream Spark releases which the SnappyData's Spark distribution only uses Log4J 2.x. Stability and Security Improvements \u00b6 SnappyData 1.3.1 includes the following changes to improve stability and security of the platform: Use timed Process.waitFor instead of loop on exitValue() which is unreliable. Fixed a race condition in old entries cleaner thread deleting in-use snapshot entries. Allow retry in startup failure even for data nodes. In some rare cases region initialization may fail due to colocated region still being initialized, so retry region initialization. Fixed UDF names lookups to do exact regex match in the CSV list in the meta-data region. Apart from Log4J, following dependencies were updated to address known security issues: Jetty upgraded to 9.4.44.v20210927 jackson-mapper-asl and jackson-core-asl upgraded to 1.9.14-atlassian-6 jackson and jackson-databind upgraded to 2.13.1 Kafka upgraded to 2.2.2 SPARK-34110 : Upgrade Zookeeper to 3.6.2 SPARK-37901 : Upgrade Netty to 4.1.73 gcs-hadoop-connector upgraded to hadoop3-2.1.2 Ported patches for the following issues from Apache Geode: GEODE-1252 : Modify bits field atomically GEODE-2802 : Tombstone version vector to contain only the members that generate the tombstone GEODE-5278 : Unexpected CommitConflictException caused by faulty region synchronization GEODE-4083 : Fix infinite loop caused by thread race changing version GEODE-3796 : Changes are made to validate region version after the region is initialized GEODE-6058 : recordVersion should allow update higher local version if for non-persistent region GEODE-6013 : Use expected initial image requester's rvv information GEODE-2159 : Add serialVersionUIDs to exception classes not having them GEODE-5559 : Improve runtime of RegionVersionHolder.canonicalExceptions GEODE-5612 : Fix RVVExceptionB.writeReceived() GEODE-7085 : Ensure that the bitset stays within BIT_SET_WIDTH and is flushed in all code paths GFE-50415: Wait for membership change in persistence advisor can hang if the event was missed GEODE-5111 : Set offline members to null only when done waiting for them Merged patches for the following Spark issues: SPARK-6305 : Migrate from log4j1 to log4j2 Followups SPARK-37684, SPARK-37774 to upgrade log4j to 2.17.x SPARK-37791 : Use log4j2 in examples SPARK-37794 : Remove internal log4j bridge api usage SPARK-37746 : log4j2-defaults.properties is not working since log4j 2 is always initialized by default SPARK-37792 : Fix the check of custom configuration in SparkShellLoggingFilter SPARK-37795 : Add a scalastyle rule to ban org.apache.log4j imports SPARK-37805 : Refactor TestUtils#configTestLog4j method to use log4j2 api SPARK-37889 : Replace Log4j2 MarkerFilter with RegexFilter SPARK-26267 : Retry when detecting incorrect offsets from Kafka SPARK-37729 : Fix SparkSession.setLogLevel that is not working in Spark Shell SPARK-37887 : Fix the check of repl log level SPARK-37790 : Upgrade SLF4J to 1.7.32 SPARK-22324 : Upgrade Arrow to 0.8.0 SPARK-25598 : Remove flume connector in Spark SPARK-37693 : Fix ChildProcAppHandleSuite failed in Jenkins maven test Resolved Issues \u00b6 SnappyData 1.3.1 resolves the following major issues apart from the patches noted in the previous section: SDSNAP-825 : Update can leave a dangling snapshot lock on the table Known Issues \u00b6 The known issues noted in 1.3.0 release notes still apply in 1.3.1 release. Key Item Description Workaround SNAP-1422 Catalog in Smart connector inconsistent with servers. Catalog in Smart connector is inconsistent with servers when a table is queried from spark-shell (or from an application that uses Smart connector mode) the table metadata is cached on the Smart connector side. If this table is dropped from the SnappyData Embedded cluster (by using snappy-shell, or JDBC application, or a Snappy job), the metadata on the Smart connector side stays cached even though the catalog has changed (table is dropped). In such cases, the user may see unexpected errors such as org.apache.spark.sql.AnalysisException: Table \"SNAPPYTABLE\" already exists in the Smart connector app side, for example, for DataFrameWriter.saveAsTable() API if the same table name that was dropped is used in saveAsTable(). User may either create a new SnappySession in such scenarios OR Invalidate the cache on the Smart Connector mode. For example, by calling snappy.sessionCatalog.invalidateAll() . SNAP-1153 Creating a temporary table with the same name as an existing table in any schema should not be allowed. When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return the wrong results. Ensure that you create temporary tables with a unique name. SNAP-2910 DataFrame API behavior in Spark, Snappy. Saving a Dataset using Spark's JDBC provider with SnappyData JDBC driver into SnappyData row/column tables fails. Use row or column provider in the Embedded or Smart connector. For Spark versions not supported by Smart connector, use the SnappyData JDBC Extension Connector . SNAP-3148 Unicode escape character \\u does not work for insert into table values() syntax. Escape character \\u is used to indicate that code following \\u is for a Unicode character but this does not work with insert into table values () syntax that is allowed for column and row tables. As a workaround, instead of insert into table values ('\\u...', ...) syntax, use insert into table select '\\u...', ... syntax. User can also directly insert the Unicode character instead of using an escape sequence. For example: create table region (val string, description string) using column The following insert query will insert a string value '\\u7ca5' instead of a Unicode char: insert into region values ('\\u7ca5', 'unicode2') However, following insert statement will insert the appropriate Unicode char: insert into region select '\\u7ca5', 'unicode2' The following query that directly inserts a Unicode char instead of using escape char also works: insert into region values ('\u7ca4','unicode2') SNAP-3146 UDF execution from Smart Connector. A UDF, once executed from the smart connector side, continues to remain accessible from the same SnappySession on the Smart connector side, even if it is deleted from the embedded side. Drop the UDF from the Smart connector side or use a new SnappySession. SNAP-3293 Cache optimized plan for plan caching instead of the physical plan. Currently, SnappyData caches the physical plan of the query for plan caching. Evaluating the physical plan may lead to an extra sampling job for some type of queries like view creations. Because of this, you may notice an extra job submitted while running the CREATE VIEW DDL if the view query contains some operations which require a sampling job. This may impact the performance of the CREATE VIEW query. SNAP-3298 Credentials set in Hadoop configuration in the Spark context can be set only once without restarting the cluster. The credentials that are embedded in a FileSystem object. The object is cached in FileSystem cache. The cached object does not get refreshed when there is a configuration (credentials) change. Hence, it uses the initially set credentials even if you have set new credentials. Run the org.apache.hadoop.fs.FileSystem.closeAll() command on snappy-scala shell or using EXEC SCALA SQL or in a job. This clears the cache. Ensure that there are no queries running on the cluster when you are executing the command. After this you can set the new credentials. Description of Download Artifacts \u00b6 The following table describes the download artifacts included in SnappyData 1.3.1 release: Artifact Name Description snappydata-1.3.1-bin.tar.gz Full product binary (includes Hadoop 3.2.0). snappydata-jdbc_2.11-1.3.1.jar JDBC client driver and push down JDBC data source for Spark. Compatible with Java 8, Java 11 and higher. snappydata-spark-connector_2.11-1.3.1.jar The single jar needed in Smart Connector mode; an alternative to --packages option. Compatible with Spark versions 2.1.1, 2.1.2 and 2.1.3. snappydata-odbc_1.3.0_win64.zip 32-bit and 64-bit ODBC client drivers for Windows 64-bit platform. snappydata-zeppelin_2.11-0.8.2.1.jar The Zeppelin interpreter jar for SnappyData compatible with Apache Zeppelin 0.8.2. The standard jdbc interpreter is now recommended instead of this. See How to Use Apache Zeppelin with SnappyData . snappydata-1.3.1.sha256 The SHA256 checksums of the product artifacts. On Linux verify using sha256sum --check snappydata-1.3.1.sha256 . snappydata-1.3.1.sha256.asc PGP signature for snappydata-1.3.1.sha256 in ASCII format. Get the public key using gpg --keyserver hkps://keyserver.ubuntu.com --recv-keys A7994CE77A24E5511A68727D8CED09EB8184C4D6 . Then verify using gpg --verify snappydata-1.3.1.sha256.asc which should show a good signature using that key having build@snappydata.io as the email.","title":"Release Notes"},{"location":"release_notes/release_notes/#overview","text":"SnappyData\u2122 is a memory-optimized database based on Apache Spark. It delivers very high throughput, low latency, and high concurrency for unified analytic workloads that may combine streaming, interactive analytics, and artificial intelligence in a single, easy to manage distributed cluster. In previous releases there were two editions namely, the Community Edition which was a fully functional core OSS distribution that was under the Apache Source License v2.0, and the Enterprise Edition which was sold by TIBCO Software under the name TIBCO ComputeDB\u2122 that included everything offered in the OSS version along with additional capabilities that are closed source and only available as part of a licensed subscription. The SnappyData team is pleased to announce the availability of version 1.3.1 of the platform. Starting with release 1.3.0, all the platform's private modules have been made open-source apart from the streaming GemFire connector (which depends on non-OSS Pivotal GemFire product jars). These include Approximate Query Processing (AQP) and the JDBC connector repositories which also include the off-heap storage support for column tables and the security modules. In addition, the ODBC driver has also been made open-source. With this, the entire code base of the platform (apart from the GemFire connector) has been made open source and there is no longer an Enterprise edition distributed by TIBCO. You can find details of the release artifacts towards the end of this page. The full set of documentation for SnappyData 1.3.1 including installation guide, user guide and reference guide can be found here . Release notes for the previous 1.3.0 release can be found here . The following table summarizes the high-level features available in the SnappyData platform: Feature Available Mutable Row and Column Store X Compatibility with Spark X Shared Nothing Persistence and HA X REST API for Spark Job Submission X Fault Tolerance for Driver X Access to the system using JDBC Driver X CLI for backup, restore, and export data X Spark console extensions X System Performance/behavior statistics X Support for transactions in Row tables X Support for indexing in Row tables X Support for snapshot transactions in Column tables X Online compaction of column block data X Transparent disk overflow of large query results X Support for external Hive meta store X SQL extensions for stream processing X SnappyData sink for structured stream processing X Structured Streaming user interface X Runtime deployment of packages and jars X Scala code execution from SQL (EXEC SCALA) X Out of the box support for cloud storage X Support for Hadoop 3.2 X SnappyData Interpreter for Apache Zeppelin X Synopsis Data Engine for Approximate Querying X Support for Synopsis Data Engine from TIBCO Spotfire\u00ae X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X GemFire connector","title":"Overview"},{"location":"release_notes/release_notes/#new-features","text":"SnappyData 1.3.1 release includes the following new features over the previous 1.3.1 release: Support for Log4J 2.x (2.17.2) which is used by default Following up with the exposure of Log4Shell and related vulnerabilities, SnappyData Platform has moved to the latest Log4J 2.x (2.17.2) from the previous Log4J 1.x. Patches were ported for the Spark components (where support for Log4J 2.x will land only with the 3.3.0 release), while other components were updated to use Log4J/SLF4J. The Spark connector component supports both Log4J 2.x and Log4J 1.x to allow compatibility with upstream Spark releases which the SnappyData's Spark distribution only uses Log4J 2.x.","title":"New Features"},{"location":"release_notes/release_notes/#stability-and-security-improvements","text":"SnappyData 1.3.1 includes the following changes to improve stability and security of the platform: Use timed Process.waitFor instead of loop on exitValue() which is unreliable. Fixed a race condition in old entries cleaner thread deleting in-use snapshot entries. Allow retry in startup failure even for data nodes. In some rare cases region initialization may fail due to colocated region still being initialized, so retry region initialization. Fixed UDF names lookups to do exact regex match in the CSV list in the meta-data region. Apart from Log4J, following dependencies were updated to address known security issues: Jetty upgraded to 9.4.44.v20210927 jackson-mapper-asl and jackson-core-asl upgraded to 1.9.14-atlassian-6 jackson and jackson-databind upgraded to 2.13.1 Kafka upgraded to 2.2.2 SPARK-34110 : Upgrade Zookeeper to 3.6.2 SPARK-37901 : Upgrade Netty to 4.1.73 gcs-hadoop-connector upgraded to hadoop3-2.1.2 Ported patches for the following issues from Apache Geode: GEODE-1252 : Modify bits field atomically GEODE-2802 : Tombstone version vector to contain only the members that generate the tombstone GEODE-5278 : Unexpected CommitConflictException caused by faulty region synchronization GEODE-4083 : Fix infinite loop caused by thread race changing version GEODE-3796 : Changes are made to validate region version after the region is initialized GEODE-6058 : recordVersion should allow update higher local version if for non-persistent region GEODE-6013 : Use expected initial image requester's rvv information GEODE-2159 : Add serialVersionUIDs to exception classes not having them GEODE-5559 : Improve runtime of RegionVersionHolder.canonicalExceptions GEODE-5612 : Fix RVVExceptionB.writeReceived() GEODE-7085 : Ensure that the bitset stays within BIT_SET_WIDTH and is flushed in all code paths GFE-50415: Wait for membership change in persistence advisor can hang if the event was missed GEODE-5111 : Set offline members to null only when done waiting for them Merged patches for the following Spark issues: SPARK-6305 : Migrate from log4j1 to log4j2 Followups SPARK-37684, SPARK-37774 to upgrade log4j to 2.17.x SPARK-37791 : Use log4j2 in examples SPARK-37794 : Remove internal log4j bridge api usage SPARK-37746 : log4j2-defaults.properties is not working since log4j 2 is always initialized by default SPARK-37792 : Fix the check of custom configuration in SparkShellLoggingFilter SPARK-37795 : Add a scalastyle rule to ban org.apache.log4j imports SPARK-37805 : Refactor TestUtils#configTestLog4j method to use log4j2 api SPARK-37889 : Replace Log4j2 MarkerFilter with RegexFilter SPARK-26267 : Retry when detecting incorrect offsets from Kafka SPARK-37729 : Fix SparkSession.setLogLevel that is not working in Spark Shell SPARK-37887 : Fix the check of repl log level SPARK-37790 : Upgrade SLF4J to 1.7.32 SPARK-22324 : Upgrade Arrow to 0.8.0 SPARK-25598 : Remove flume connector in Spark SPARK-37693 : Fix ChildProcAppHandleSuite failed in Jenkins maven test","title":"Stability and Security Improvements"},{"location":"release_notes/release_notes/#resolved-issues","text":"SnappyData 1.3.1 resolves the following major issues apart from the patches noted in the previous section: SDSNAP-825 : Update can leave a dangling snapshot lock on the table","title":"Resolved Issues"},{"location":"release_notes/release_notes/#known-issues","text":"The known issues noted in 1.3.0 release notes still apply in 1.3.1 release. Key Item Description Workaround SNAP-1422 Catalog in Smart connector inconsistent with servers. Catalog in Smart connector is inconsistent with servers when a table is queried from spark-shell (or from an application that uses Smart connector mode) the table metadata is cached on the Smart connector side. If this table is dropped from the SnappyData Embedded cluster (by using snappy-shell, or JDBC application, or a Snappy job), the metadata on the Smart connector side stays cached even though the catalog has changed (table is dropped). In such cases, the user may see unexpected errors such as org.apache.spark.sql.AnalysisException: Table \"SNAPPYTABLE\" already exists in the Smart connector app side, for example, for DataFrameWriter.saveAsTable() API if the same table name that was dropped is used in saveAsTable(). User may either create a new SnappySession in such scenarios OR Invalidate the cache on the Smart Connector mode. For example, by calling snappy.sessionCatalog.invalidateAll() . SNAP-1153 Creating a temporary table with the same name as an existing table in any schema should not be allowed. When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return the wrong results. Ensure that you create temporary tables with a unique name. SNAP-2910 DataFrame API behavior in Spark, Snappy. Saving a Dataset using Spark's JDBC provider with SnappyData JDBC driver into SnappyData row/column tables fails. Use row or column provider in the Embedded or Smart connector. For Spark versions not supported by Smart connector, use the SnappyData JDBC Extension Connector . SNAP-3148 Unicode escape character \\u does not work for insert into table values() syntax. Escape character \\u is used to indicate that code following \\u is for a Unicode character but this does not work with insert into table values () syntax that is allowed for column and row tables. As a workaround, instead of insert into table values ('\\u...', ...) syntax, use insert into table select '\\u...', ... syntax. User can also directly insert the Unicode character instead of using an escape sequence. For example: create table region (val string, description string) using column The following insert query will insert a string value '\\u7ca5' instead of a Unicode char: insert into region values ('\\u7ca5', 'unicode2') However, following insert statement will insert the appropriate Unicode char: insert into region select '\\u7ca5', 'unicode2' The following query that directly inserts a Unicode char instead of using escape char also works: insert into region values ('\u7ca4','unicode2') SNAP-3146 UDF execution from Smart Connector. A UDF, once executed from the smart connector side, continues to remain accessible from the same SnappySession on the Smart connector side, even if it is deleted from the embedded side. Drop the UDF from the Smart connector side or use a new SnappySession. SNAP-3293 Cache optimized plan for plan caching instead of the physical plan. Currently, SnappyData caches the physical plan of the query for plan caching. Evaluating the physical plan may lead to an extra sampling job for some type of queries like view creations. Because of this, you may notice an extra job submitted while running the CREATE VIEW DDL if the view query contains some operations which require a sampling job. This may impact the performance of the CREATE VIEW query. SNAP-3298 Credentials set in Hadoop configuration in the Spark context can be set only once without restarting the cluster. The credentials that are embedded in a FileSystem object. The object is cached in FileSystem cache. The cached object does not get refreshed when there is a configuration (credentials) change. Hence, it uses the initially set credentials even if you have set new credentials. Run the org.apache.hadoop.fs.FileSystem.closeAll() command on snappy-scala shell or using EXEC SCALA SQL or in a job. This clears the cache. Ensure that there are no queries running on the cluster when you are executing the command. After this you can set the new credentials.","title":"Known Issues"},{"location":"release_notes/release_notes/#description-of-download-artifacts","text":"The following table describes the download artifacts included in SnappyData 1.3.1 release: Artifact Name Description snappydata-1.3.1-bin.tar.gz Full product binary (includes Hadoop 3.2.0). snappydata-jdbc_2.11-1.3.1.jar JDBC client driver and push down JDBC data source for Spark. Compatible with Java 8, Java 11 and higher. snappydata-spark-connector_2.11-1.3.1.jar The single jar needed in Smart Connector mode; an alternative to --packages option. Compatible with Spark versions 2.1.1, 2.1.2 and 2.1.3. snappydata-odbc_1.3.0_win64.zip 32-bit and 64-bit ODBC client drivers for Windows 64-bit platform. snappydata-zeppelin_2.11-0.8.2.1.jar The Zeppelin interpreter jar for SnappyData compatible with Apache Zeppelin 0.8.2. The standard jdbc interpreter is now recommended instead of this. See How to Use Apache Zeppelin with SnappyData . snappydata-1.3.1.sha256 The SHA256 checksums of the product artifacts. On Linux verify using sha256sum --check snappydata-1.3.1.sha256 . snappydata-1.3.1.sha256.asc PGP signature for snappydata-1.3.1.sha256 in ASCII format. Get the public key using gpg --keyserver hkps://keyserver.ubuntu.com --recv-keys A7994CE77A24E5511A68727D8CED09EB8184C4D6 . Then verify using gpg --verify snappydata-1.3.1.sha256.asc which should show a good signature using that key having build@snappydata.io as the email.","title":"Description of Download Artifacts"},{"location":"release_notes/release_notes_1.0-rc/","text":"Release Notes \u00b6 The SnappyData team is pleased to announce the availability of SnappyData version 1.0.0-RC1. New Features \u00b6 Fully compatible with Apache Spark 2.1.1 Mutability support for column store (SNAP-1389): UPDATE and DELETE operations are now supported on column tables. ALTER TABLE support for row table (SNAP-1326). Security Support (available in enterprise edition ): This release introduces cluster security with authentication and authorisation based on LDAP mechanism. Will be extended to other mechanisms in future (SNAP-1656, SNAP-1813). Support for setting scheduler pools using the set command. Multi-node cluster now boots up quickly as background start of server processes is enabled by default. Pulse Console: SnappyData Pulse has been enhanced to be more useful to both developers and operations personnel (SNAP-1890, SNAP-1792). Improvements include Ability to sort members list based on members type. Added new UI view named SnappyData Member Details Page which includes, among other things, latest logs. Added members Heap and Off-Heap memory usage details along with their storage and execution splits. Users can specify streaming batch interval when submitting a stream job via conf/snappy-job.sh (SNAP-1948). Row tables now support LONG, SHORT, TINYINT and BYTE datatypes (SNAP-1722). The history file for snappy shell has been renamed from .gfxd.history to .snappy.history. You may copy your existing ~/.gfxd.history to ~/.snappy.history to be able to access your historical snappy shell commands. Performance Enhancements \u00b6 Performance enhancements with dictionary decoder when dictionary is large. (SNAP-1877) Different sessions end up creating different code due to indeterminate statsPredicate ordering. Now using a consistent sort order so that generated code is identical across sessions for the same query. Reduced the size of generated code. Indexed cursors in decoders to improve heavily filtered queries (SNAP-1936) Performance improvements in Smart Connector mode, specially with queries on tables with wide schema (SNAP-1363, SNAP-1699) Several other performance improvements. Select bug fixes and performance related fixes \u00b6 Some of these are included below. For the complete list, see ReleaseNotes.txt . Fixed data inconsistency issues when a new node is joining the cluster and at the same time write operations are going on. (SNAP-1756) The product internally does retries on redundant copy of partitions on the event of a node failure (SNAP-1377, SNAP-902) Fixed the wrong status of locators on restarts. After cluster restart, snappy-status-all.sh used to show locators in waiting state even when the actual status changed to running (SNAP-1893) Fixed the SnappyData Pulse freezing when loading data sets (SNAP-1426) More accurate accounting of execution and storage memory (SNAP-1688, SNAP-1798) Corrected case-sensitivity handling for query API calls (SNAP-1714)","title":"Release Notes"},{"location":"release_notes/release_notes_1.0-rc/#release-notes","text":"The SnappyData team is pleased to announce the availability of SnappyData version 1.0.0-RC1.","title":"Release Notes"},{"location":"release_notes/release_notes_1.0-rc/#new-features","text":"Fully compatible with Apache Spark 2.1.1 Mutability support for column store (SNAP-1389): UPDATE and DELETE operations are now supported on column tables. ALTER TABLE support for row table (SNAP-1326). Security Support (available in enterprise edition ): This release introduces cluster security with authentication and authorisation based on LDAP mechanism. Will be extended to other mechanisms in future (SNAP-1656, SNAP-1813). Support for setting scheduler pools using the set command. Multi-node cluster now boots up quickly as background start of server processes is enabled by default. Pulse Console: SnappyData Pulse has been enhanced to be more useful to both developers and operations personnel (SNAP-1890, SNAP-1792). Improvements include Ability to sort members list based on members type. Added new UI view named SnappyData Member Details Page which includes, among other things, latest logs. Added members Heap and Off-Heap memory usage details along with their storage and execution splits. Users can specify streaming batch interval when submitting a stream job via conf/snappy-job.sh (SNAP-1948). Row tables now support LONG, SHORT, TINYINT and BYTE datatypes (SNAP-1722). The history file for snappy shell has been renamed from .gfxd.history to .snappy.history. You may copy your existing ~/.gfxd.history to ~/.snappy.history to be able to access your historical snappy shell commands.","title":"New Features"},{"location":"release_notes/release_notes_1.0-rc/#performance-enhancements","text":"Performance enhancements with dictionary decoder when dictionary is large. (SNAP-1877) Different sessions end up creating different code due to indeterminate statsPredicate ordering. Now using a consistent sort order so that generated code is identical across sessions for the same query. Reduced the size of generated code. Indexed cursors in decoders to improve heavily filtered queries (SNAP-1936) Performance improvements in Smart Connector mode, specially with queries on tables with wide schema (SNAP-1363, SNAP-1699) Several other performance improvements.","title":"Performance Enhancements"},{"location":"release_notes/release_notes_1.0-rc/#select-bug-fixes-and-performance-related-fixes","text":"Some of these are included below. For the complete list, see ReleaseNotes.txt . Fixed data inconsistency issues when a new node is joining the cluster and at the same time write operations are going on. (SNAP-1756) The product internally does retries on redundant copy of partitions on the event of a node failure (SNAP-1377, SNAP-902) Fixed the wrong status of locators on restarts. After cluster restart, snappy-status-all.sh used to show locators in waiting state even when the actual status changed to running (SNAP-1893) Fixed the SnappyData Pulse freezing when loading data sets (SNAP-1426) More accurate accounting of execution and storage memory (SNAP-1688, SNAP-1798) Corrected case-sensitivity handling for query API calls (SNAP-1714)","title":"Select bug fixes and performance related fixes"},{"location":"release_notes/release_notes_1.0.1/","text":"Release Notes \u00b6 The SnappyData team is pleased to announce the availability of version 1.0.1 of the platform. Download the Enterprise Edition here . New Features \u00b6 putInto and deleteFrom bulk operations support for column tables (SNAP-2092, SNAP-2093, SNAP-2094): Ability to specify \"key columns\" in the table DDL to use for putInto and deleteFrom APIs \"PUT INTO\" SQL or putInto API extension to overwrite existing rows and insert non-existing ones \"DELETE FROM\" SQL or deleteFrom API extension to delete a set of matching rows UPDATE SQL now supports using expressions with column references of another table in RHS of SET Improvements in cluster restart with off-line, failed nodes or with corrupt meta-data (SNAP-2096) New admin command \"unblock\" to allow the initialization of a table even if it is waiting for offline members No discard of data (unlike revoke) with the new approach and initialize with the latest online working copy (SNAP-2143) Parallel recovery of data regions to break any cyclic dependencies between the nodes, and allow reporting on all off-line nodes that may have more recent copy of data Many bug-fixes related to startup issues due to meta-data inconsistencies: Incorrect ConflictingPersistentDataExeption at restart due to disk-store meta-data corruption (SNAP-2097, SNAP-2098) Metadata corruption issues causing GII to fail (SNAP-2140) Compression of column batches in disk storage and over the network (SNAP-1743) Support for LZ4 and SNAPPY compression codecs in persistent storage and transport for column table data Smart connector uses compression when pulling from remote hosts while using uncompressed for same host for best performance New SOURCEPATH and COMPRESSION columns in SYS.HIVETABLES virtual table Support for temporary, global temporary and persistent VIEWs (SNAP-2072): CREATE VIEW, CREATE TEMPORARY VIEW and CREATE GLOBAL TEMPORARY VIEW DDLs SQL from JDBC/ODBC for VIEWs routed to Spark execution engine External connectors (like cassandra) used from smart connector no longer require any jars in the snappydata cluster (SNAP-2072) External tables display in dashboard and snappy command-line (SNAP-2086) Auto-configuration of SPARK_PUBLIC_DNS, hostname-for-clients etc in AWS environment (SNAP-2116) Out-of-the-box support for AWS URLs/APIs in the product GRANT/REVOKE SQL support in SnappySession.sql() earlier only allowed from JDBC/ODBC (SNAP-2042) LATERAL VIEW support in SnappySession.sql() (SNAP-1283) FETCH FIRST syntax as an alternative to LIMIT to support some SQL tools that use former Addition of IndexStats in for local row table index lookup and range scans SYS.DISKSTOREIDS virtual table to disk-store IDs being used in the cluster by all members (SNAP-2113) Show ARRAY/MAP/STRUCT complex types in JDBC/ODBC metadata as part of SNAP-2141 Performance Enhancements \u00b6 Major performance improvements in smart connector mode (SNAP-2101, SNAP-2084) Minimized buffer copying especially when connector executors are colocated with store data nodes Intelligent connector query handling to use key lookups into column table rather than full scan for cases of heavily filtered scan queries Reduced round-trips for operations (transactions, bucket pruning) Allow using SnappyUnifiedMemoryManager with smart connector (SNAP-2084) New memory and disk iterator to minimize faultins and do serial disk reads across concurrent iterators (SNAP-2102): New iterator to substantially reduce faultins in case all data is not in memory Cross-iterator serial disk reads per diskstore to minimize random reads from disk New remote iterator that substantially reduces the memory overhead and caches only current batch Startup performance improvements to cut down on locator/server/lead start and restart times (SNAP-338) Improve performance of reads of variable length data for some queries (SNAP-2118) Use colocated joins with VIEWs when possible (SNAP-2204) Separate disk store for delta buffer regions to substantially improve column table compaction (SNAP-2121) Projection push-down to scan layer for non-deterministic expressions like spark_partition_id() (SNAP-2036) Parser performance improvements (by ~50%) code-generation cache is larger by default and configurable (SNAP-2120) Select bug fixes and performance related fixes \u00b6 A sample of bug fixes done as part of this release are noted below. For a more comprehensive list, see ReleaseNotes.txt . Now only overflow-to-disk is allowed as eviction action for tables (SNAP-1501): Only overflow-to-disk is allowed as a valid eviction action and cannot be explicitly specified (LOCAL_DESTROY removed due to possible data inconsistencies) OVERFLOW=false property can be used to disable eviction which is true by default Memory accounting fixes: Incorrect initial memory accounting causing insert failure even with memory available (SNAP-2084) Zero usage shown in UI on restart (SNAP-2180) Disable embedded Zeppelin interpreter in a secure cluster which can bypass security (SNAP-2191) JSON conversion for complex types ARRAY/STRUCT/MAP (SNAP-2056) Fix import of JSON data (SNAP-2087) CREATE TABLE ... AS SELECT * fails without explicit schema (SNAP-2047) Selects missing results or failing during node failures (SNAP-889, SNAP-1547) Incorrect results with joins/anti-joins for some cases having more than one match for a key (SNAP-2212) Fixes and improvements to server and lead status in both the launcher status and SYS.MEMBERS table (SNAP-1960, SNAP-2060, SNAP-1645) Fix global temporary tables/views with SnappySession (SNAP-2072) Fix updates on complex types (SNAP-2141) Column table scan fixes related to null value reads (SNAP-2088) Incorrect reads of column table statistics rows in some cases (SNAP-2124) Disable tokenization for external tables and session flag to disable it and plan caching (SNAP-2114, SNAP-2124) Table meta-data issues with squirrel client and otherwise (SNAP-2083) Case-sensitive column names now work correctly with all operations (GITHUB-900 and other related fixes) Allow for hyphens in schema names Deadlock in transactional operations with GII (SNAP-1950) Couple of fixes in UPDATE SQL: Failure due to rollover during update operation (SNAP-2192) Subquery updates shown as ResultSet rather than update count (SNAP-2156) Correct mismatch between executor and DistributedMember names in some configurations that caused Remote routing of partitions (SNAP-2122) Fixes ported from Apache Geode (GEODE-2109, GEODE-2240) Fixes to all failures in snappy-spark test suite which includes both product and test changes Fixes related to ODBC driver calls to thrift server More comprehensive python API testing (SNAP-2044)","title":"Release Notes"},{"location":"release_notes/release_notes_1.0.1/#release-notes","text":"The SnappyData team is pleased to announce the availability of version 1.0.1 of the platform. Download the Enterprise Edition here .","title":"Release Notes"},{"location":"release_notes/release_notes_1.0.1/#new-features","text":"putInto and deleteFrom bulk operations support for column tables (SNAP-2092, SNAP-2093, SNAP-2094): Ability to specify \"key columns\" in the table DDL to use for putInto and deleteFrom APIs \"PUT INTO\" SQL or putInto API extension to overwrite existing rows and insert non-existing ones \"DELETE FROM\" SQL or deleteFrom API extension to delete a set of matching rows UPDATE SQL now supports using expressions with column references of another table in RHS of SET Improvements in cluster restart with off-line, failed nodes or with corrupt meta-data (SNAP-2096) New admin command \"unblock\" to allow the initialization of a table even if it is waiting for offline members No discard of data (unlike revoke) with the new approach and initialize with the latest online working copy (SNAP-2143) Parallel recovery of data regions to break any cyclic dependencies between the nodes, and allow reporting on all off-line nodes that may have more recent copy of data Many bug-fixes related to startup issues due to meta-data inconsistencies: Incorrect ConflictingPersistentDataExeption at restart due to disk-store meta-data corruption (SNAP-2097, SNAP-2098) Metadata corruption issues causing GII to fail (SNAP-2140) Compression of column batches in disk storage and over the network (SNAP-1743) Support for LZ4 and SNAPPY compression codecs in persistent storage and transport for column table data Smart connector uses compression when pulling from remote hosts while using uncompressed for same host for best performance New SOURCEPATH and COMPRESSION columns in SYS.HIVETABLES virtual table Support for temporary, global temporary and persistent VIEWs (SNAP-2072): CREATE VIEW, CREATE TEMPORARY VIEW and CREATE GLOBAL TEMPORARY VIEW DDLs SQL from JDBC/ODBC for VIEWs routed to Spark execution engine External connectors (like cassandra) used from smart connector no longer require any jars in the snappydata cluster (SNAP-2072) External tables display in dashboard and snappy command-line (SNAP-2086) Auto-configuration of SPARK_PUBLIC_DNS, hostname-for-clients etc in AWS environment (SNAP-2116) Out-of-the-box support for AWS URLs/APIs in the product GRANT/REVOKE SQL support in SnappySession.sql() earlier only allowed from JDBC/ODBC (SNAP-2042) LATERAL VIEW support in SnappySession.sql() (SNAP-1283) FETCH FIRST syntax as an alternative to LIMIT to support some SQL tools that use former Addition of IndexStats in for local row table index lookup and range scans SYS.DISKSTOREIDS virtual table to disk-store IDs being used in the cluster by all members (SNAP-2113) Show ARRAY/MAP/STRUCT complex types in JDBC/ODBC metadata as part of SNAP-2141","title":"New Features"},{"location":"release_notes/release_notes_1.0.1/#performance-enhancements","text":"Major performance improvements in smart connector mode (SNAP-2101, SNAP-2084) Minimized buffer copying especially when connector executors are colocated with store data nodes Intelligent connector query handling to use key lookups into column table rather than full scan for cases of heavily filtered scan queries Reduced round-trips for operations (transactions, bucket pruning) Allow using SnappyUnifiedMemoryManager with smart connector (SNAP-2084) New memory and disk iterator to minimize faultins and do serial disk reads across concurrent iterators (SNAP-2102): New iterator to substantially reduce faultins in case all data is not in memory Cross-iterator serial disk reads per diskstore to minimize random reads from disk New remote iterator that substantially reduces the memory overhead and caches only current batch Startup performance improvements to cut down on locator/server/lead start and restart times (SNAP-338) Improve performance of reads of variable length data for some queries (SNAP-2118) Use colocated joins with VIEWs when possible (SNAP-2204) Separate disk store for delta buffer regions to substantially improve column table compaction (SNAP-2121) Projection push-down to scan layer for non-deterministic expressions like spark_partition_id() (SNAP-2036) Parser performance improvements (by ~50%) code-generation cache is larger by default and configurable (SNAP-2120)","title":"Performance Enhancements"},{"location":"release_notes/release_notes_1.0.1/#select-bug-fixes-and-performance-related-fixes","text":"A sample of bug fixes done as part of this release are noted below. For a more comprehensive list, see ReleaseNotes.txt . Now only overflow-to-disk is allowed as eviction action for tables (SNAP-1501): Only overflow-to-disk is allowed as a valid eviction action and cannot be explicitly specified (LOCAL_DESTROY removed due to possible data inconsistencies) OVERFLOW=false property can be used to disable eviction which is true by default Memory accounting fixes: Incorrect initial memory accounting causing insert failure even with memory available (SNAP-2084) Zero usage shown in UI on restart (SNAP-2180) Disable embedded Zeppelin interpreter in a secure cluster which can bypass security (SNAP-2191) JSON conversion for complex types ARRAY/STRUCT/MAP (SNAP-2056) Fix import of JSON data (SNAP-2087) CREATE TABLE ... AS SELECT * fails without explicit schema (SNAP-2047) Selects missing results or failing during node failures (SNAP-889, SNAP-1547) Incorrect results with joins/anti-joins for some cases having more than one match for a key (SNAP-2212) Fixes and improvements to server and lead status in both the launcher status and SYS.MEMBERS table (SNAP-1960, SNAP-2060, SNAP-1645) Fix global temporary tables/views with SnappySession (SNAP-2072) Fix updates on complex types (SNAP-2141) Column table scan fixes related to null value reads (SNAP-2088) Incorrect reads of column table statistics rows in some cases (SNAP-2124) Disable tokenization for external tables and session flag to disable it and plan caching (SNAP-2114, SNAP-2124) Table meta-data issues with squirrel client and otherwise (SNAP-2083) Case-sensitive column names now work correctly with all operations (GITHUB-900 and other related fixes) Allow for hyphens in schema names Deadlock in transactional operations with GII (SNAP-1950) Couple of fixes in UPDATE SQL: Failure due to rollover during update operation (SNAP-2192) Subquery updates shown as ResultSet rather than update count (SNAP-2156) Correct mismatch between executor and DistributedMember names in some configurations that caused Remote routing of partitions (SNAP-2122) Fixes ported from Apache Geode (GEODE-2109, GEODE-2240) Fixes to all failures in snappy-spark test suite which includes both product and test changes Fixes related to ODBC driver calls to thrift server More comprehensive python API testing (SNAP-2044)","title":"Select bug fixes and performance related fixes"},{"location":"release_notes/release_notes_1.0.2.1/","text":"Release Notes \u00b6 The SnappyData team is pleased to announce the availability of version 1.0.2.1 of the platform. You can find the release artifacts of its Community Edition towards the end of this page. You can also download the Enterprise Edition here . The following table summarizes the features available in Enterprise and OSS (Community) editions. Feature Community Enterprise Mutable Row & Column Store X X Compatibility with Spark X X Shared Nothing Persistence and HA X X REST API for Spark Job Submission X X Fault Tolerance for Driver X X Access to the system using JDBC Driver X X CLI for backup, restore, and export data X X Spark console extensions X X System Perf/Behavior statistics X X Support for transactions in Row tables X X Support for indexing in Row Tables X X SQL extensions for stream processing X X Runtime deployment of packages and jars X X Synopsis Data Engine for Approximate Querying X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X GemFire/Apache Geode connector X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X New Features \u00b6 SnappyData 1.0.2.1 version includes the following new features: Support Spark's HiveServer2 in SnappyData cluster. Enables starting an embedded Spark HiveServer2 on leads in embedded mode. Provided a default Structured Streaming Sink implementation for SnappyData column and row tables. A Sink property can enable conflation of events with the same key columns. Added a -agent JVM argument in the launch commands to kill the JVM as soon as Out-of-Memory(OOM) occurs. This is important because the VM sometimes used to crash in unexpected ways later as a side effect of this corrupting internal metadata which later gave restart troubles. Handling Out-of-Memory Error Allow NONE as a valid policy for server-auth-provider . Essentially, the cluster can now be configured only for user authentication, and mutual peer to peer authentication of cluster members can be disabled by specifying this property as NONE. Add support for query hints to force a join type. This may be useful for cases where the result is known to be small, for example, but plan rules cannot determine it. Allow deleteFrom API to work as long as the dataframe contains key columns. Performance Enhancements \u00b6 The following performance enhancements are included in SnappyData 1.0.2.1 version: Avoid shuffle when join key columns are a superset of child partitioning. Added a pooled version of SnappyData JDBC driver for Spark to connect to SnappyData cluster as JDBC data source. Connecting with JDBC Client Pool Driver Added caching for hive catalog lookups. Meta-data queries with large number of tables take quite long because of nested loop joins between SYSTABLES and HIVETABLES for most meta-data queries. Even if the table numbers were in hundreds, it used to take much time. [SNAP-2657] Select Fixes and Performance Related Fixes \u00b6 The following defect fixes are included in SnappyData 1.0.2.1 version: Reset the pool at the end of collect to avoid spillover of low latency pool setting to the latter operations that may not use the CachedDataFrame execution paths. [SNAP-2659] Fixed: Column added using 'ALTER TABLE ... ADD COLUMN ...' through SnappyData shell does not reflect in spark-shell. [SNAP-2491] Fixed the occasional failures in serialization using CachedDataFrame , if the node is just starting/stopping. Also, fixed a hang in shutdown for cases where hive client close is trying to boot up the node again, waiting on the locks that are taken during the shutdown. Lead and Lag window functions were failing due to incorrect analysis error. [SNAP-2566] Fixed the validate-disk-store tool. It was not getting initialized with registered types. This was required to deserialize byte arrays being read from persisted files. Fix schema in ResultSet metadata. It used to show the default schema APP always. Sometimes a false unique constraint violation happened due to removed or destroyed AbstractRegionEntry. Now an attempt is made to remove it from the index and another try is made to put the new value against the index key. [SNAP-2627] Fix for memory leak in oldEntrieMap leading to LowMemoryException and OutOfMemoryException . [SNAP-2654] Description of Download Artifacts \u00b6 The following table describes the download artifacts included in SnappyData 1.0.2.1 version: Artifact Name Description snappydata-1.0.2.1-bin.tar.gz Full product binary (includes Hadoop 2.7) snappydata-1.0.2.1-without-hadoop-bin.tar.gz Product without the Hadoop dependency JARs snappydata-jdbc_2.11-1.0.2.2.jar Client (JDBC) JAR snappydata-zeppelin_2.11-0.7.3.4.jar The Zeppelin interpreter jar for SnappyData, compatible with Apache Zeppelin 0.7.3 snappydata-ec2-0.8.2.tar.gz Script to Launch SnappyData cluster on AWS EC2 instances","title":"Release Notes"},{"location":"release_notes/release_notes_1.0.2.1/#release-notes","text":"The SnappyData team is pleased to announce the availability of version 1.0.2.1 of the platform. You can find the release artifacts of its Community Edition towards the end of this page. You can also download the Enterprise Edition here . The following table summarizes the features available in Enterprise and OSS (Community) editions. Feature Community Enterprise Mutable Row & Column Store X X Compatibility with Spark X X Shared Nothing Persistence and HA X X REST API for Spark Job Submission X X Fault Tolerance for Driver X X Access to the system using JDBC Driver X X CLI for backup, restore, and export data X X Spark console extensions X X System Perf/Behavior statistics X X Support for transactions in Row tables X X Support for indexing in Row Tables X X SQL extensions for stream processing X X Runtime deployment of packages and jars X X Synopsis Data Engine for Approximate Querying X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X GemFire/Apache Geode connector X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X","title":"Release Notes"},{"location":"release_notes/release_notes_1.0.2.1/#new-features","text":"SnappyData 1.0.2.1 version includes the following new features: Support Spark's HiveServer2 in SnappyData cluster. Enables starting an embedded Spark HiveServer2 on leads in embedded mode. Provided a default Structured Streaming Sink implementation for SnappyData column and row tables. A Sink property can enable conflation of events with the same key columns. Added a -agent JVM argument in the launch commands to kill the JVM as soon as Out-of-Memory(OOM) occurs. This is important because the VM sometimes used to crash in unexpected ways later as a side effect of this corrupting internal metadata which later gave restart troubles. Handling Out-of-Memory Error Allow NONE as a valid policy for server-auth-provider . Essentially, the cluster can now be configured only for user authentication, and mutual peer to peer authentication of cluster members can be disabled by specifying this property as NONE. Add support for query hints to force a join type. This may be useful for cases where the result is known to be small, for example, but plan rules cannot determine it. Allow deleteFrom API to work as long as the dataframe contains key columns.","title":"New Features"},{"location":"release_notes/release_notes_1.0.2.1/#performance-enhancements","text":"The following performance enhancements are included in SnappyData 1.0.2.1 version: Avoid shuffle when join key columns are a superset of child partitioning. Added a pooled version of SnappyData JDBC driver for Spark to connect to SnappyData cluster as JDBC data source. Connecting with JDBC Client Pool Driver Added caching for hive catalog lookups. Meta-data queries with large number of tables take quite long because of nested loop joins between SYSTABLES and HIVETABLES for most meta-data queries. Even if the table numbers were in hundreds, it used to take much time. [SNAP-2657]","title":"Performance Enhancements"},{"location":"release_notes/release_notes_1.0.2.1/#select-fixes-and-performance-related-fixes","text":"The following defect fixes are included in SnappyData 1.0.2.1 version: Reset the pool at the end of collect to avoid spillover of low latency pool setting to the latter operations that may not use the CachedDataFrame execution paths. [SNAP-2659] Fixed: Column added using 'ALTER TABLE ... ADD COLUMN ...' through SnappyData shell does not reflect in spark-shell. [SNAP-2491] Fixed the occasional failures in serialization using CachedDataFrame , if the node is just starting/stopping. Also, fixed a hang in shutdown for cases where hive client close is trying to boot up the node again, waiting on the locks that are taken during the shutdown. Lead and Lag window functions were failing due to incorrect analysis error. [SNAP-2566] Fixed the validate-disk-store tool. It was not getting initialized with registered types. This was required to deserialize byte arrays being read from persisted files. Fix schema in ResultSet metadata. It used to show the default schema APP always. Sometimes a false unique constraint violation happened due to removed or destroyed AbstractRegionEntry. Now an attempt is made to remove it from the index and another try is made to put the new value against the index key. [SNAP-2627] Fix for memory leak in oldEntrieMap leading to LowMemoryException and OutOfMemoryException . [SNAP-2654]","title":"Select Fixes and Performance Related Fixes"},{"location":"release_notes/release_notes_1.0.2.1/#description-of-download-artifacts","text":"The following table describes the download artifacts included in SnappyData 1.0.2.1 version: Artifact Name Description snappydata-1.0.2.1-bin.tar.gz Full product binary (includes Hadoop 2.7) snappydata-1.0.2.1-without-hadoop-bin.tar.gz Product without the Hadoop dependency JARs snappydata-jdbc_2.11-1.0.2.2.jar Client (JDBC) JAR snappydata-zeppelin_2.11-0.7.3.4.jar The Zeppelin interpreter jar for SnappyData, compatible with Apache Zeppelin 0.7.3 snappydata-ec2-0.8.2.tar.gz Script to Launch SnappyData cluster on AWS EC2 instances","title":"Description of Download Artifacts"},{"location":"release_notes/release_notes_1.0.2/","text":"Release Notes \u00b6 The SnappyData team is pleased to announce the availability of version 1.0.2 of the platform. You can find the release artifacts of its Community Edition towards the end of this page. You can also download the Enterprise Edition here . The following table summarizes the features available in Enterprise and OSS (Community) editions. Feature Community Enterprise Mutable Row & Column Store X X Compatibility with Spark X X Shared Nothing Persistence and HA X X REST API for Spark Job Submission X X Fault Tolerance for Driver X X Access to the system using JDBC Driver X X CLI for backup, restore, and export data X X Spark console extensions X X System Perf/Behavior statistics X X Support for transactions in Row tables X X Support for indexing in Row Tables X X SQL extensions for stream processing X X Runtime deployment of packages and jars X X Synopsis Data Engine for Approximate Querying X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X GemFire/Apache Geode connector X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X New Features \u00b6 The following new features are included in SnappyData 1.0.2 version: Introduced an API in snappy session catalog to get Primary Key of Row tables or Key Columns of Column Tables, as DataFrame. Introduced an API in snappy session catalog to get table type as String. Added support for arbitrary size view definition. It use to fail when view text size went beyond 32k. Support for displaying VIEWTEXT for views in SYS.HIVETABLES. For example: Select viewtext from sys.hivetables where tablename = \u2018view_name\u201d will give the text with which the view was created. Added Row level Security feature. Admins can define multiple security policies on tables for different users or LDAP groups. Refer Row Level Security Auto refresh of UI page. Now the SnappyData UI page gets updated automatically and frequently. Users need not refresh or reload. Refer SnappyData Pulse Richer user interface. Added graphs for memory, CPU consumption etc. for last 15 minutes. The user has the ability to see how the cluster health has been for the last 15 minutes instead of just current state. Total CPU core count capacity of the cluster is now displayed on the UI. Refer SnappyData Pulse Bucket count of tables are also displayed now on the user interface. Support deployment of packages and jars as DDL command. Refer Deploy Added support for reading maven dependencies using --packages option in our job server scripts. Refer Deploying Packages in SnappyData . Changes to procedure sys.repair_catalog to execute it on the server (earlier this was run on lead by sending a message to it). This will be useful to repair catalog even when lead is down. Refer Catalog Repair Added support for PreparedStatement.getMetadata() JDBC API . This is on an experimental basis. Added support for execution of some DDL commands viz CREATE/DROP DISKSTORE, GRANT, REVOKE. CALL procedures from snappy session as well. Quote table names in all store DDL/DML/query strings to allow for special characters and keywords in table names. Spark application with same name cannot be submitted to SnappyData. This has been done so that individual apps can be killed by its name when required. Users are not allowed to create tables in their own schema based on system property - snappydata.RESTRICT_TABLE_CREATION . In some cases it may be required to control use of cluster resources in which case the table creation is done only by authorized owners of schema. Schema can be owned by an LDAP group also and not necessarily by a single user. Support for deploying SnappyData on Kubernetes using Helm charts. Refer Kubernetes Disk Store Validate tool enhancement. Validation of disk store can find out all the inconsistencies at once. BINARY data type is same as BLOB data type. Performance Enhancements \u00b6 The following performance enhancements are included in SnappyData 1.0.2 version: Fixed concurrent query performance issue by resolving the incorrect output partition choice. Due to numBucket check, all the partition pruned queries were converted to hash partition with one partition. This was causing an exchange node to be introduced. (SNAP-2421) Fixed SnappyData UI becoming unresponsive on LowMemoryException.(SNAP-2071) Cleaning up tokenization handling and fixes. Main change is addition of the following two separate classes for tokenization: ParamLiteral TokenLiteral Both classes extend a common trait TokenizedLiteral . Tokenization will always happen independently of plan caching, unless it is explicitly turned off. (SNAP-1932) Procedure for smart connector iteration and fixes. Includes fixes for perf issues as noted for all iterators (disk iterator, smart connector and remote iterator). (SNAP-2243) Select Fixes and Performance Related Fixes \u00b6 The following defect fixes are included in SnappyData 1.0.2 version: Fixed incorrect server status shown on the UI. Sometimes due to a race condition for the same member two entries were shown up on the UI. (SNAP-2433) Fixed missing SQL tab on SnappyData UI in local mode. (SNAP-2470) Fixed few issues related to wrong results for Row tables due to plan caching. (SNAP-2463 - Incorrect pushing down of OR and AND clause filter combination in push down query, SNAP-2351 - re-evaluation of filter was not happening due to plan caching, SNAP-2451, SNAP-2457) Skip batch, if the stats row is missing while scanning column values from disk. This was already handled for in-memory batches and the same has been added for on-disk batches. (SNAP-2364) Fixes in UI to forbid unauthorized users to view any tab. (ENT-21) Fixes in SnappyData parser to create inlined table. (SNAP-2302), \u2018()\u2019 as optional in some function like \u2018current_date()\u2019, \u2018current_timestamp()\u2019 etc. (SNAP-2303) Consider the current schema name also as part of Caching Key for plan caching. So same query on same table but from different schema should not clash with each other. (SNAP-2438) Fix for COLUMN table shown as ROW table on dashboard after LME in data server. (SNAP-2382) Fixed off-heap size for Partitioned Regions, showed on UI. (SNAP-2186) Fixed failure when query on view does not fallback to Spark plan in case Code Generation fails. (SNAP-2363) Fix invalid decompress call on stats row.(SNAP-2348). Use to fail in run time while scanning column tables.(SNAP-2348) Fixed negative bucket size with eviction. (GITHUB-982) Fixed the issue of incorrect LowMemoryException, even if a lot of memory was left. (SNAP-2356) Handled int overflow case in memory accounting. Due to this ExecutionMemoryPool released more memory than it has throws AssertionError (SNAP-2312) Fixed the pooled connection not being returned to the pool after authorization check failure which led to unusable cluster. (SNAP-2255) Fixed different results of nearly identical queries, due to join order. Its due to EXCHANGE hash ordering being different from table partitioning. It will happen for the specific case when query join order is different from partitioning of one of the tables while the other table being joined is partitioned differently. (SNAP-2225) Corrected row count updated/inserted in a column table via putInto. (SNAP-2220) Fixed the OOM issue due to hive queries. This was a memory leak. Due to this the system became very slow after sometime even if idle. (SNAP-2248) Fixed the issue of incomplete plan and query string info in UI due to plan caching changes. Corrected the logic of existence join. Sensitive information, like user password, LDAP password etc, which are passed as properties to the cluster are masked on the UI now. Schema with boolean columns sometimes returned incorrect null values. Fixed. (SNAP-2436) Fixed the scenario where break in colocation chain of buckets due to crash led to disk store metadata going bad causing restart failure. Wrong entry count on restart, if region got closed on a server due to DiskAccessException leading to a feeling of loss of data. Do not let the region close in case of LME. This has been done by not letting non IOException get wrapped in DiskAccessException. (SNAP-2375) Fix to avoid hang or delay in stop when stop is issued and the component has gone into reconnect cycle. (SNAP-2380) Handle joining of new servers better. Avoid ConflictingPersistentDataException when a new server starts before any of the old server start. SNAP-2236 ODBC driver bug fix. Added EmbedDatabaseMetaData.getTableSchemas . Change the order in which backup is taken. Internal DD diskstore of backup is taken first followed by rest of the disk stores. This helps in stream apps which want to store offset of replayable source in snappydata. They can create the offset table backed up by the internal DD store instead of default or custom disk store. Description of Download Artifacts \u00b6 The following table describes the download artifacts included in SnappyData 1.0.2 version: Artifact Name Description snappydata-1.0.2-bin.tar.gz Full product binary (includes Hadoop 2.7) snappydata-1.0.2-without-hadoop-bin.tar.gz Product without the Hadoop dependency JARs snappydata-client-1.6.2.jar Client (JDBC) JAR snappydata-zeppelin_2.11-0.7.3.2.jar The Zeppelin interpreter jar for SnappyData, compatible with Apache Zeppelin 0.7.3 snappydata-ec2-0.8.2.tar.gz Script to Launch SnappyData cluster on AWS EC2 instances","title":"Release Notes"},{"location":"release_notes/release_notes_1.0.2/#release-notes","text":"The SnappyData team is pleased to announce the availability of version 1.0.2 of the platform. You can find the release artifacts of its Community Edition towards the end of this page. You can also download the Enterprise Edition here . The following table summarizes the features available in Enterprise and OSS (Community) editions. Feature Community Enterprise Mutable Row & Column Store X X Compatibility with Spark X X Shared Nothing Persistence and HA X X REST API for Spark Job Submission X X Fault Tolerance for Driver X X Access to the system using JDBC Driver X X CLI for backup, restore, and export data X X Spark console extensions X X System Perf/Behavior statistics X X Support for transactions in Row tables X X Support for indexing in Row Tables X X SQL extensions for stream processing X X Runtime deployment of packages and jars X X Synopsis Data Engine for Approximate Querying X ODBC Driver with High Concurrency X Off-heap data storage for column tables X CDC Stream receiver for SQL Server into SnappyData X GemFire/Apache Geode connector X Row Level Security X Use encrypted password instead of clear text password X Restrict Table, View, Function creation even in user\u2019s own schema X LDAP security interface X","title":"Release Notes"},{"location":"release_notes/release_notes_1.0.2/#new-features","text":"The following new features are included in SnappyData 1.0.2 version: Introduced an API in snappy session catalog to get Primary Key of Row tables or Key Columns of Column Tables, as DataFrame. Introduced an API in snappy session catalog to get table type as String. Added support for arbitrary size view definition. It use to fail when view text size went beyond 32k. Support for displaying VIEWTEXT for views in SYS.HIVETABLES. For example: Select viewtext from sys.hivetables where tablename = \u2018view_name\u201d will give the text with which the view was created. Added Row level Security feature. Admins can define multiple security policies on tables for different users or LDAP groups. Refer Row Level Security Auto refresh of UI page. Now the SnappyData UI page gets updated automatically and frequently. Users need not refresh or reload. Refer SnappyData Pulse Richer user interface. Added graphs for memory, CPU consumption etc. for last 15 minutes. The user has the ability to see how the cluster health has been for the last 15 minutes instead of just current state. Total CPU core count capacity of the cluster is now displayed on the UI. Refer SnappyData Pulse Bucket count of tables are also displayed now on the user interface. Support deployment of packages and jars as DDL command. Refer Deploy Added support for reading maven dependencies using --packages option in our job server scripts. Refer Deploying Packages in SnappyData . Changes to procedure sys.repair_catalog to execute it on the server (earlier this was run on lead by sending a message to it). This will be useful to repair catalog even when lead is down. Refer Catalog Repair Added support for PreparedStatement.getMetadata() JDBC API . This is on an experimental basis. Added support for execution of some DDL commands viz CREATE/DROP DISKSTORE, GRANT, REVOKE. CALL procedures from snappy session as well. Quote table names in all store DDL/DML/query strings to allow for special characters and keywords in table names. Spark application with same name cannot be submitted to SnappyData. This has been done so that individual apps can be killed by its name when required. Users are not allowed to create tables in their own schema based on system property - snappydata.RESTRICT_TABLE_CREATION . In some cases it may be required to control use of cluster resources in which case the table creation is done only by authorized owners of schema. Schema can be owned by an LDAP group also and not necessarily by a single user. Support for deploying SnappyData on Kubernetes using Helm charts. Refer Kubernetes Disk Store Validate tool enhancement. Validation of disk store can find out all the inconsistencies at once. BINARY data type is same as BLOB data type.","title":"New Features"},{"location":"release_notes/release_notes_1.0.2/#performance-enhancements","text":"The following performance enhancements are included in SnappyData 1.0.2 version: Fixed concurrent query performance issue by resolving the incorrect output partition choice. Due to numBucket check, all the partition pruned queries were converted to hash partition with one partition. This was causing an exchange node to be introduced. (SNAP-2421) Fixed SnappyData UI becoming unresponsive on LowMemoryException.(SNAP-2071) Cleaning up tokenization handling and fixes. Main change is addition of the following two separate classes for tokenization: ParamLiteral TokenLiteral Both classes extend a common trait TokenizedLiteral . Tokenization will always happen independently of plan caching, unless it is explicitly turned off. (SNAP-1932) Procedure for smart connector iteration and fixes. Includes fixes for perf issues as noted for all iterators (disk iterator, smart connector and remote iterator). (SNAP-2243)","title":"Performance Enhancements"},{"location":"release_notes/release_notes_1.0.2/#select-fixes-and-performance-related-fixes","text":"The following defect fixes are included in SnappyData 1.0.2 version: Fixed incorrect server status shown on the UI. Sometimes due to a race condition for the same member two entries were shown up on the UI. (SNAP-2433) Fixed missing SQL tab on SnappyData UI in local mode. (SNAP-2470) Fixed few issues related to wrong results for Row tables due to plan caching. (SNAP-2463 - Incorrect pushing down of OR and AND clause filter combination in push down query, SNAP-2351 - re-evaluation of filter was not happening due to plan caching, SNAP-2451, SNAP-2457) Skip batch, if the stats row is missing while scanning column values from disk. This was already handled for in-memory batches and the same has been added for on-disk batches. (SNAP-2364) Fixes in UI to forbid unauthorized users to view any tab. (ENT-21) Fixes in SnappyData parser to create inlined table. (SNAP-2302), \u2018()\u2019 as optional in some function like \u2018current_date()\u2019, \u2018current_timestamp()\u2019 etc. (SNAP-2303) Consider the current schema name also as part of Caching Key for plan caching. So same query on same table but from different schema should not clash with each other. (SNAP-2438) Fix for COLUMN table shown as ROW table on dashboard after LME in data server. (SNAP-2382) Fixed off-heap size for Partitioned Regions, showed on UI. (SNAP-2186) Fixed failure when query on view does not fallback to Spark plan in case Code Generation fails. (SNAP-2363) Fix invalid decompress call on stats row.(SNAP-2348). Use to fail in run time while scanning column tables.(SNAP-2348) Fixed negative bucket size with eviction. (GITHUB-982) Fixed the issue of incorrect LowMemoryException, even if a lot of memory was left. (SNAP-2356) Handled int overflow case in memory accounting. Due to this ExecutionMemoryPool released more memory than it has throws AssertionError (SNAP-2312) Fixed the pooled connection not being returned to the pool after authorization check failure which led to unusable cluster. (SNAP-2255) Fixed different results of nearly identical queries, due to join order. Its due to EXCHANGE hash ordering being different from table partitioning. It will happen for the specific case when query join order is different from partitioning of one of the tables while the other table being joined is partitioned differently. (SNAP-2225) Corrected row count updated/inserted in a column table via putInto. (SNAP-2220) Fixed the OOM issue due to hive queries. This was a memory leak. Due to this the system became very slow after sometime even if idle. (SNAP-2248) Fixed the issue of incomplete plan and query string info in UI due to plan caching changes. Corrected the logic of existence join. Sensitive information, like user password, LDAP password etc, which are passed as properties to the cluster are masked on the UI now. Schema with boolean columns sometimes returned incorrect null values. Fixed. (SNAP-2436) Fixed the scenario where break in colocation chain of buckets due to crash led to disk store metadata going bad causing restart failure. Wrong entry count on restart, if region got closed on a server due to DiskAccessException leading to a feeling of loss of data. Do not let the region close in case of LME. This has been done by not letting non IOException get wrapped in DiskAccessException. (SNAP-2375) Fix to avoid hang or delay in stop when stop is issued and the component has gone into reconnect cycle. (SNAP-2380) Handle joining of new servers better. Avoid ConflictingPersistentDataException when a new server starts before any of the old server start. SNAP-2236 ODBC driver bug fix. Added EmbedDatabaseMetaData.getTableSchemas . Change the order in which backup is taken. Internal DD diskstore of backup is taken first followed by rest of the disk stores. This helps in stream apps which want to store offset of replayable source in snappydata. They can create the offset table backed up by the internal DD store instead of default or custom disk store.","title":"Select Fixes and Performance Related Fixes"},{"location":"release_notes/release_notes_1.0.2/#description-of-download-artifacts","text":"The following table describes the download artifacts included in SnappyData 1.0.2 version: Artifact Name Description snappydata-1.0.2-bin.tar.gz Full product binary (includes Hadoop 2.7) snappydata-1.0.2-without-hadoop-bin.tar.gz Product without the Hadoop dependency JARs snappydata-client-1.6.2.jar Client (JDBC) JAR snappydata-zeppelin_2.11-0.7.3.2.jar The Zeppelin interpreter jar for SnappyData, compatible with Apache Zeppelin 0.7.3 snappydata-ec2-0.8.2.tar.gz Script to Launch SnappyData cluster on AWS EC2 instances","title":"Description of Download Artifacts"},{"location":"release_notes/release_notes_1.0/","text":"Release Notes \u00b6 The SnappyData team is pleased to announce the availability of SnappyData Release 1.0.0 (GA) of the platform. New Features/Fixed Issues \u00b6 Fully compatible with Apache Spark 2.1.1 Mutability support for column store (SNAP-1389): UPDATE and DELETE operations are now supported on column tables. ALTER TABLE support for row table (SNAP-1326). Security Support (available in the Enterprise Edition ): This release introduces cluster security with authentication and authorisation based on LDAP mechanism. It will be extended to other mechanisms in future releases. (SNAP-1656, SNAP-1813). DEB and RPM installers (distProduct target in source build). Support for setting scheduler pools using the set command. Multi-node cluster now boots up quickly as background start of server processes is enabled by default. Pulse Console: SnappyData Pulse has been enhanced to be more useful to both developers and operations personnel (SNAP-1890, SNAP-1792). Improvements include Ability to sort members list based on members type. Added new UI view named SnappyData Member Details Page which includes, among other things, latest logs. Added members Heap and Off-Heap memory usage details along with their storage and execution splits. Users can specify streaming batch interval when submitting a stream job via conf/snappy-job.sh (SNAP-1948). Row tables now support LONG, SHORT, TINYINT and BYTE datatypes (SNAP-1722). The history file for snappy shell has been renamed from .gfxd.history to .snappy.history. You may copy your existing ~/.gfxd.history to ~/.snappy.history to be able to access your historical snappy shell commands. Performance Enhancements \u00b6 Performance enhancements with dictionary decoder when dictionary is large. (SNAP-1877) Using a consistent sort for pushed down predicates so that different sessions do not end up creating different generated code. Reduced the size of generated code. Indexed cursors in decoders to improve heavily filtered queries (SNAP-1936) Performance improvements in Smart Connector mode, specially with queries on tables with wide schema (SNAP-1363, SNAP-1699) Several other performance improvements. Select bug fixes and performance related fixes \u00b6 There have been numerous bug fixes done as part of this release. Some of these are included below. For a more comprehensive list, see ReleaseNotes . Issue Description SNAP-1756 Fixed data inconsistency issues when a new node is joining the cluster and at the same time write operations are going on. SNAP -1377 SNAP -902 The product internally does retries on redundant copy of partitions on the event of a node failure SNAP -1893 Fixed the wrong status of locators on restarts. After cluster restart, snappy-status-all.sh used to show locators in waiting state even when the actual status changed to running SNAP -1426 Fixed the SnappyData Pulse freezing when loading data sets SNAP -1688 SNAP-1798 More accurate accounting of execution and storage memory SNAP -1714 Corrected case-sensitivity handling for query API calls","title":"Release Notes"},{"location":"release_notes/release_notes_1.0/#release-notes","text":"The SnappyData team is pleased to announce the availability of SnappyData Release 1.0.0 (GA) of the platform.","title":"Release Notes"},{"location":"release_notes/release_notes_1.0/#new-featuresfixed-issues","text":"Fully compatible with Apache Spark 2.1.1 Mutability support for column store (SNAP-1389): UPDATE and DELETE operations are now supported on column tables. ALTER TABLE support for row table (SNAP-1326). Security Support (available in the Enterprise Edition ): This release introduces cluster security with authentication and authorisation based on LDAP mechanism. It will be extended to other mechanisms in future releases. (SNAP-1656, SNAP-1813). DEB and RPM installers (distProduct target in source build). Support for setting scheduler pools using the set command. Multi-node cluster now boots up quickly as background start of server processes is enabled by default. Pulse Console: SnappyData Pulse has been enhanced to be more useful to both developers and operations personnel (SNAP-1890, SNAP-1792). Improvements include Ability to sort members list based on members type. Added new UI view named SnappyData Member Details Page which includes, among other things, latest logs. Added members Heap and Off-Heap memory usage details along with their storage and execution splits. Users can specify streaming batch interval when submitting a stream job via conf/snappy-job.sh (SNAP-1948). Row tables now support LONG, SHORT, TINYINT and BYTE datatypes (SNAP-1722). The history file for snappy shell has been renamed from .gfxd.history to .snappy.history. You may copy your existing ~/.gfxd.history to ~/.snappy.history to be able to access your historical snappy shell commands.","title":"New Features/Fixed Issues"},{"location":"release_notes/release_notes_1.0/#performance-enhancements","text":"Performance enhancements with dictionary decoder when dictionary is large. (SNAP-1877) Using a consistent sort for pushed down predicates so that different sessions do not end up creating different generated code. Reduced the size of generated code. Indexed cursors in decoders to improve heavily filtered queries (SNAP-1936) Performance improvements in Smart Connector mode, specially with queries on tables with wide schema (SNAP-1363, SNAP-1699) Several other performance improvements.","title":"Performance Enhancements"},{"location":"release_notes/release_notes_1.0/#select-bug-fixes-and-performance-related-fixes","text":"There have been numerous bug fixes done as part of this release. Some of these are included below. For a more comprehensive list, see ReleaseNotes . Issue Description SNAP-1756 Fixed data inconsistency issues when a new node is joining the cluster and at the same time write operations are going on. SNAP -1377 SNAP -902 The product internally does retries on redundant copy of partitions on the event of a node failure SNAP -1893 Fixed the wrong status of locators on restarts. After cluster restart, snappy-status-all.sh used to show locators in waiting state even when the actual status changed to running SNAP -1426 Fixed the SnappyData Pulse freezing when loading data sets SNAP -1688 SNAP-1798 More accurate accounting of execution and storage memory SNAP -1714 Corrected case-sensitivity handling for query API calls","title":"Select bug fixes and performance related fixes"},{"location":"sde/","text":"Overview of Synopsis Data Engine (SDE) \u00b6 Note This feature is not supported in the Smart Connector mode. The following topics are covered in this section: Key Concepts Working with Stratified Samples Running Queries More Examples Sample Selection High-level Accuracy Contracts (HAC) Sketching The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large datasets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time. For instance, in exploratory analytics, a data analyst might be slicing and dicing large datasets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering), while the engineer continues to slice and dice the datasets without any interruptions. When accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters. While in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records. Unlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a prior known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query. How does it work? \u00b6 The following diagram provides a simplified view of how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can, however, be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample. Using SDE \u00b6 In the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.","title":"Overview of Synopsis Data Engine (SDE)"},{"location":"sde/#overview-of-synopsis-data-engine-sde","text":"Note This feature is not supported in the Smart Connector mode. The following topics are covered in this section: Key Concepts Working with Stratified Samples Running Queries More Examples Sample Selection High-level Accuracy Contracts (HAC) Sketching The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large datasets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time. For instance, in exploratory analytics, a data analyst might be slicing and dicing large datasets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering), while the engineer continues to slice and dice the datasets without any interruptions. When accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters. While in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records. Unlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a prior known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.","title":"Overview of Synopsis Data Engine (SDE)"},{"location":"sde/#how-does-it-work","text":"The following diagram provides a simplified view of how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can, however, be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample.","title":"How does it work?"},{"location":"sde/#using-sde","text":"In the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.","title":"Using SDE"},{"location":"sde/hac_contracts/","text":"High-level Accuracy Contracts (HAC) \u00b6 SnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts. When an error constraint is not met, the action to be taken is defined in the behavior clause. Behavior Clause \u00b6 Approximate Query Processing (AQP) has HAC support using the following behavior clause. <do_nothing> \u00b6 The AQP engine returns the estimate as is. <local_omit> \u00b6 For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\". <strict> \u00b6 If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception. <run_on_full_table> \u00b6 If any of the single output row exceeds the specified error, then the full query is re-executed on the base table. <partial_run_on_base_table> \u00b6 If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups. This result is then merged (without any duplicates) with the result derived from the sample table. In the following example, any one of the above behavior clause can be applied. SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ with error <fraction> [CONFIDENCE <fraction>] [BEHAVIOR <behavior>] Error Functions \u00b6 In addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection. The following four methods are available to be used in query projection when running approximate queries: absolute_error(column alias) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) relative_error(column alias) : Indicates ratio of absolute error to estimate. lower_bound(column alias) : Lower value of an estimate interval for a given confidence. upper_bound(column alias) : Upper value of an estimate interval for a given confidence. Confidence is the probability that the value of a parameter falls within a specified range of values. For example: SELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr), UniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9 * The absolute_error and relative_error function values returns 0 if query is executed on the base table. * lower_bound and upper_bound values returns null if query is executed on the base table. * The values are seen in case behavior is set to <run_on_full_table> or <partial_run_on_base_table> In addition to using SQL syntax in the queries, you can use data frame API as well. For example, if you have a data frame for the airline table, then the below query can equivalently also be written as : select AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95 snc.table(basetable).groupBy(\"Year_\").agg( avg(\"ArrDelay\").alias(\"arrivalDelay), relative_error(\"arrivalDelay\"), absolute_error(\"arrivalDelay\"), col(\"Year_\")).withError(0.10, .95).sort(col(\"Year_\").asc) Reserved Keywords \u00b6 Keywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword sample_ is reserved for SnappyData. If the aggregate function is aliased in the query as sample_<any string> , then what you get is true answers on the sample table, and not the estimates of the base table. select count() rowCount, count() as sample_count from airline with error 0.1 rowCount returns estimate of a number of rows in airline table. sample_count returns a number of rows (true answer) in sample table of airline table.","title":"High-level Accuracy Contracts (HAC)"},{"location":"sde/hac_contracts/#high-level-accuracy-contracts-hac","text":"SnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts. When an error constraint is not met, the action to be taken is defined in the behavior clause.","title":"High-level Accuracy Contracts (HAC)"},{"location":"sde/hac_contracts/#behavior-clause","text":"Approximate Query Processing (AQP) has HAC support using the following behavior clause.","title":"Behavior Clause"},{"location":"sde/hac_contracts/#do_nothing","text":"The AQP engine returns the estimate as is.","title":"&lt;do_nothing&gt;"},{"location":"sde/hac_contracts/#local_omit","text":"For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\".","title":"&lt;local_omit&gt;"},{"location":"sde/hac_contracts/#strict","text":"If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception.","title":"&lt;strict&gt;"},{"location":"sde/hac_contracts/#run_on_full_table","text":"If any of the single output row exceeds the specified error, then the full query is re-executed on the base table.","title":"&lt;run_on_full_table&gt;"},{"location":"sde/hac_contracts/#partial_run_on_base_table","text":"If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups. This result is then merged (without any duplicates) with the result derived from the sample table. In the following example, any one of the above behavior clause can be applied. SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ with error <fraction> [CONFIDENCE <fraction>] [BEHAVIOR <behavior>]","title":"&lt;partial_run_on_base_table&gt;"},{"location":"sde/hac_contracts/#error-functions","text":"In addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection. The following four methods are available to be used in query projection when running approximate queries: absolute_error(column alias) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) relative_error(column alias) : Indicates ratio of absolute error to estimate. lower_bound(column alias) : Lower value of an estimate interval for a given confidence. upper_bound(column alias) : Upper value of an estimate interval for a given confidence. Confidence is the probability that the value of a parameter falls within a specified range of values. For example: SELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr), UniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9 * The absolute_error and relative_error function values returns 0 if query is executed on the base table. * lower_bound and upper_bound values returns null if query is executed on the base table. * The values are seen in case behavior is set to <run_on_full_table> or <partial_run_on_base_table> In addition to using SQL syntax in the queries, you can use data frame API as well. For example, if you have a data frame for the airline table, then the below query can equivalently also be written as : select AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95 snc.table(basetable).groupBy(\"Year_\").agg( avg(\"ArrDelay\").alias(\"arrivalDelay), relative_error(\"arrivalDelay\"), absolute_error(\"arrivalDelay\"), col(\"Year_\")).withError(0.10, .95).sort(col(\"Year_\").asc)","title":"Error Functions"},{"location":"sde/hac_contracts/#reserved-keywords","text":"Keywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword sample_ is reserved for SnappyData. If the aggregate function is aliased in the query as sample_<any string> , then what you get is true answers on the sample table, and not the estimates of the base table. select count() rowCount, count() as sample_count from airline with error 0.1 rowCount returns estimate of a number of rows in airline table. sample_count returns a number of rows (true answer) in sample table of airline table.","title":"Reserved Keywords"},{"location":"sde/key_concepts/","text":"Key Concepts \u00b6 SnappyData AQP relies on two methods for approximations - Stratified Sampling and Sketching . A brief introduction to these concepts is provided below. Stratified Sampling \u00b6 Sampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying. Take this simple example table that manages AdImpressions. If random sample is created that is a third of the original size two records is picked in random. This is depicted in the following figure: If a query is executed, like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample. But, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries. Stratified sampling, on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata have enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer. To understand these concepts in further detail, refer to the handbook . It explains different sampling strategies, error estimation mechanisms, and various types of data synopses. Online Sampling \u00b6 AQP also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark DataFrame APIs to create a uniform random sample over static RDDs. For online sampling, AQP first does reservoir sampling for each stratum in a write-optimized store before flushing it into a read-optimized store for stratified samples. There is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, SnappyData can ensure having enough samples over each 5-minute time window, while still ensuring that all GEOs have good representation in the sample. Sketching \u00b6 While stratified sampling ensures that data dimensions with low representation are captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, other data structures like a Count-min-sketch are relied on to capture data frequencies in a stream. This is a data structure that requires that it captures how often an element is seen in a stream for the top-N such elements. While a Count-min-sketch is well described, AQP extends this with support for providing top-K estimates over time series data.","title":"Key Concepts"},{"location":"sde/key_concepts/#key-concepts","text":"SnappyData AQP relies on two methods for approximations - Stratified Sampling and Sketching . A brief introduction to these concepts is provided below.","title":"Key Concepts"},{"location":"sde/key_concepts/#stratified-sampling","text":"Sampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying. Take this simple example table that manages AdImpressions. If random sample is created that is a third of the original size two records is picked in random. This is depicted in the following figure: If a query is executed, like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample. But, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries. Stratified sampling, on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata have enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer. To understand these concepts in further detail, refer to the handbook . It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.","title":"Stratified Sampling"},{"location":"sde/key_concepts/#online-sampling","text":"AQP also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark DataFrame APIs to create a uniform random sample over static RDDs. For online sampling, AQP first does reservoir sampling for each stratum in a write-optimized store before flushing it into a read-optimized store for stratified samples. There is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, SnappyData can ensure having enough samples over each 5-minute time window, while still ensuring that all GEOs have good representation in the sample.","title":"Online Sampling"},{"location":"sde/key_concepts/#sketching","text":"While stratified sampling ensures that data dimensions with low representation are captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, other data structures like a Count-min-sketch are relied on to capture data frequencies in a stream. This is a data structure that requires that it captures how often an element is seen in a stream for the top-N such elements. While a Count-min-sketch is well described, AQP extends this with support for providing top-K estimates over time series data.","title":"Sketching"},{"location":"sde/more_examples/","text":"More Examples \u00b6 Example 1 \u00b6 Create a sample table with qcs 'medallion' CREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI OPTIONS (buckets '8', qcs 'medallion', fraction '0.01', strataReservoirSize '50'); SQL Query: select medallion,avg(trip_distance) as avgTripDist, absolute_error(avgTripDist),relative_error(avgTripDist), lower_bound(avgTripDist),upper_bound(avgTripDist) from nyctaxi group by medallion order by medallion desc limit 100 with error; //These built-in error functions is explained in a section below. DataFrame API Query: snc.table(basetable).groupBy(\"medallion\").agg( avg(\"trip_distance\").alias(\"avgTripDist\"), absolute_error(\"avgTripDist\"), relative_error(\"avgTripDist\"), lower_bound(\"avgTripDist\"), upper_bound(\"avgTripDist\")).withError(.6, .90, \"do_nothing\").sort(col(\"medallion\").desc).limit(100) Example 2 \u00b6 Create additional sample table with qcs 'hack_license' CREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS (buckets '8', qcs 'hack_license', fraction '0.01', strataReservoirSize '50'); SQL Query: select hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error // the engine will automitically use the HackLicense sample for a more accurate answer to this query. DataFrame API Query: snc.table(basetable).groupBy(\"hack_license\").count().withError(.6,.90,\"do_nothing\").sort(col(\"count\").desc).limit(10) Example 3 \u00b6 Create a sample table using function \"hour(pickup_datetime) as QCS: # Sample Table create sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50'); SQL Query: select sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime) with error DataFrame API Query: snc.table(basetable).groupBy(hour(col(\"pickup_datetime\"))).agg(Map(\"trip_time_in_secs\" -> \"sum\")).withError(0.6,0.90,\"do_nothing\").limit(10) Example 4 \u00b6 If you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns. # Sample Table create sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50'); SQL Query: Select hack_license, sum(trip_distance) as daily_trips from nyctaxi where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license order by daily_trips desc with error DataFrame API Query: snc.table(basetable).groupBy(\"hack_license\",\"pickup_datetime\").agg(Map(\"trip_distance\" -> \"sum\")).alias(\"daily_trips\"). filter(year(col(\"pickup_datetime\")).equalTo(2013) and month(col(\"pickup_datetime\")).equalTo(9)).withError(0.6,0.90,\"do_nothing\").sort(col(\"sum(trip_distance)\").desc).limit(10)","title":"More Examples"},{"location":"sde/more_examples/#more-examples","text":"","title":"More Examples"},{"location":"sde/more_examples/#example-1","text":"Create a sample table with qcs 'medallion' CREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI OPTIONS (buckets '8', qcs 'medallion', fraction '0.01', strataReservoirSize '50'); SQL Query: select medallion,avg(trip_distance) as avgTripDist, absolute_error(avgTripDist),relative_error(avgTripDist), lower_bound(avgTripDist),upper_bound(avgTripDist) from nyctaxi group by medallion order by medallion desc limit 100 with error; //These built-in error functions is explained in a section below. DataFrame API Query: snc.table(basetable).groupBy(\"medallion\").agg( avg(\"trip_distance\").alias(\"avgTripDist\"), absolute_error(\"avgTripDist\"), relative_error(\"avgTripDist\"), lower_bound(\"avgTripDist\"), upper_bound(\"avgTripDist\")).withError(.6, .90, \"do_nothing\").sort(col(\"medallion\").desc).limit(100)","title":"Example 1"},{"location":"sde/more_examples/#example-2","text":"Create additional sample table with qcs 'hack_license' CREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS (buckets '8', qcs 'hack_license', fraction '0.01', strataReservoirSize '50'); SQL Query: select hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error // the engine will automitically use the HackLicense sample for a more accurate answer to this query. DataFrame API Query: snc.table(basetable).groupBy(\"hack_license\").count().withError(.6,.90,\"do_nothing\").sort(col(\"count\").desc).limit(10)","title":"Example 2"},{"location":"sde/more_examples/#example-3","text":"Create a sample table using function \"hour(pickup_datetime) as QCS: # Sample Table create sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50'); SQL Query: select sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime) with error DataFrame API Query: snc.table(basetable).groupBy(hour(col(\"pickup_datetime\"))).agg(Map(\"trip_time_in_secs\" -> \"sum\")).withError(0.6,0.90,\"do_nothing\").limit(10)","title":"Example 3"},{"location":"sde/more_examples/#example-4","text":"If you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns. # Sample Table create sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50'); SQL Query: Select hack_license, sum(trip_distance) as daily_trips from nyctaxi where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license order by daily_trips desc with error DataFrame API Query: snc.table(basetable).groupBy(\"hack_license\",\"pickup_datetime\").agg(Map(\"trip_distance\" -> \"sum\")).alias(\"daily_trips\"). filter(year(col(\"pickup_datetime\")).equalTo(2013) and month(col(\"pickup_datetime\")).equalTo(9)).withError(0.6,0.90,\"do_nothing\").sort(col(\"sum(trip_distance)\").desc).limit(10)","title":"Example 4"},{"location":"sde/running_queries/","text":"Running Queries \u00b6 Queries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause. Here is the syntax: SELECT ... FROM .. WHERE .. GROUP BY ...<br> WITH ERROR `<fraction> `[CONFIDENCE` <fraction>`] [BEHAVIOR `<string>]` WITH ERROR - this is a mandatory clause. The values are 0 < value(double) < 1 . CONFIDENCE - this is optional clause. The values are confidence 0 < value(double) < 1 . The default value is 0.95 BEHAVIOR - this is an optional clause. The values are do_nothing , local_omit , strict , run_on_full_table , partial_run_on_base_table . The default value is run_on_full_table These 'behavior' options are fully described in the section below. Here are some examples: SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 // tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95. SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error // tolerate any error in the answer. Just give me a quick response. SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019 // tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95. // If the error for any row is greater than 10% omit the answer. i.e. the row is omitted. Using the Spark DataFrame API \u00b6 The Spark DataFrame API is extended with support for approximate queries. Here is 'withError' API on DataFrames. def withError(error: Double, confidence: Double = Constant.DEFAULT_CONFIDENCE, behavior: String = \"DO_NOTHING\"): DataFrame Query examples using the DataFrame API snc.table(baseTable).agg(Map(\"ArrDelay\" -> \"sum\")).orderBy( desc(\"Month_\")).withError(0.10) snc.table(baseTable).agg(Map(\"ArrDelay\" -> \"sum\")).orderBy( desc(\"Month_\")).withError(0.10, 0.95, 'local_omit\u2019) Supporting BI Tools or Existing Apps \u00b6 To allow BI tools and existing Apps that say might be generating SQL, AQP also supports specifying these options through your SQL connection or using the Snappy SQLContext. snContext.sql(s\"spark.sql.aqp.error=$error\") snContext.sql(s\"spark.sql.aqp.confidence=$confidence\") snContext.sql(s\"set spark.sql.aqp.behavior=$behavior\") These settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above. Applications or tools using JDBC/ODBC can set the following properties. When using Apache Zeppelin JDBC interpreter or the Snappy SQL you can set the values as follows: set spark.sql.aqp.error=$error; set spark.sql.aqp.confidence=$confidence; set spark.sql.aqp.behavior=$behavior; Setting AQP specific properties as a connection level property using JDBC: Properties prop = new Properties(); prop.setProperty(\"spark.sql.aqp.error\",\"0.3\"); prop.setProperty(\"spark.sql.aqp.behavior\",\"local_omit\");","title":"Running Queries"},{"location":"sde/running_queries/#running-queries","text":"Queries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause. Here is the syntax: SELECT ... FROM .. WHERE .. GROUP BY ...<br> WITH ERROR `<fraction> `[CONFIDENCE` <fraction>`] [BEHAVIOR `<string>]` WITH ERROR - this is a mandatory clause. The values are 0 < value(double) < 1 . CONFIDENCE - this is optional clause. The values are confidence 0 < value(double) < 1 . The default value is 0.95 BEHAVIOR - this is an optional clause. The values are do_nothing , local_omit , strict , run_on_full_table , partial_run_on_base_table . The default value is run_on_full_table These 'behavior' options are fully described in the section below. Here are some examples: SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 // tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95. SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error // tolerate any error in the answer. Just give me a quick response. SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019 // tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95. // If the error for any row is greater than 10% omit the answer. i.e. the row is omitted.","title":"Running Queries"},{"location":"sde/running_queries/#using-the-spark-dataframe-api","text":"The Spark DataFrame API is extended with support for approximate queries. Here is 'withError' API on DataFrames. def withError(error: Double, confidence: Double = Constant.DEFAULT_CONFIDENCE, behavior: String = \"DO_NOTHING\"): DataFrame Query examples using the DataFrame API snc.table(baseTable).agg(Map(\"ArrDelay\" -> \"sum\")).orderBy( desc(\"Month_\")).withError(0.10) snc.table(baseTable).agg(Map(\"ArrDelay\" -> \"sum\")).orderBy( desc(\"Month_\")).withError(0.10, 0.95, 'local_omit\u2019)","title":"Using the Spark DataFrame API"},{"location":"sde/running_queries/#supporting-bi-tools-or-existing-apps","text":"To allow BI tools and existing Apps that say might be generating SQL, AQP also supports specifying these options through your SQL connection or using the Snappy SQLContext. snContext.sql(s\"spark.sql.aqp.error=$error\") snContext.sql(s\"spark.sql.aqp.confidence=$confidence\") snContext.sql(s\"set spark.sql.aqp.behavior=$behavior\") These settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above. Applications or tools using JDBC/ODBC can set the following properties. When using Apache Zeppelin JDBC interpreter or the Snappy SQL you can set the values as follows: set spark.sql.aqp.error=$error; set spark.sql.aqp.confidence=$confidence; set spark.sql.aqp.behavior=$behavior; Setting AQP specific properties as a connection level property using JDBC: Properties prop = new Properties(); prop.setProperty(\"spark.sql.aqp.error\",\"0.3\"); prop.setProperty(\"spark.sql.aqp.behavior\",\"local_omit\");","title":"Supporting BI Tools or Existing Apps"},{"location":"sde/sample_selection/","text":"Sample Selection \u00b6 Sample selection logic selects most appropriate sample, based on this relatively simple logic in the current version: If the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else, If query QCS (columns involved in Where/GroupBy/Having matched the sample QCS, then, select that sample If exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used If superset of sample QCS is not available, a sample where the sample QCS is a subset of query QCS is used When multiple stratified samples with a subset of QCSs match, a sample with most matching columns is used. The largest size of the sample gets selected if multiple such samples are available.","title":"Sample Selection"},{"location":"sde/sample_selection/#sample-selection","text":"Sample selection logic selects most appropriate sample, based on this relatively simple logic in the current version: If the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else, If query QCS (columns involved in Where/GroupBy/Having matched the sample QCS, then, select that sample If exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used If superset of sample QCS is not available, a sample where the sample QCS is a subset of query QCS is used When multiple stratified samples with a subset of QCSs match, a sample with most matching columns is used. The largest size of the sample gets selected if multiple such samples are available.","title":"Sample Selection"},{"location":"sde/sketching/","text":"Sketching \u00b6 Synopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A BloomFilter is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a Count-Min-Sketch which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API. Creating TopK tables \u00b6 TopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query. TopK queries aim to retrieve, from a potentially very large result set, only the k (k >= 1) best answers. SQL API for creating a TopK table in SnappyData \u00b6 snsc.sql(\"create topK table MostPopularTweets on tweetStreamTable \" + \"options(key 'hashtag', frequencyCol 'retweets')\") The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables. Scala API for creating a TopK table \u00b6 val topKOptionMap = Map( \"epoch\" -> System.currentTimeMillis().toString, \"timeInterval\" -> \"1000ms\", \"size\" -> \"40\", \"frequencyCol\" -> \"retweets\" ) val schema = StructType(List(StructField(\"HashTag\", StringType))) snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"), \"HashTag\", schema, topKOptionMap) The code above shows how to do the same thing using the SnappyData Scala API. Querying the TopK table \u00b6 select * from topkTweets order by EstimatedValue desc The example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most re-tweets. Approximate TopK analytics for time series data \u00b6 Time is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the time stamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most re-tweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system. Here is an example of a time-based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData AQP module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals. select hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' order by EstimatedValue desc If time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp. SQL API for creating a TopK table in SnappyData specifying timestampColumn \u00b6 In the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet. snsc.sql(\"create topK table MostPopularTweets on tweetStreamTable \" + \"options(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' )\") The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest re-tweets value in the base table. This works for both static tables and streaming tables Scala API for creating a TopK table \u00b6 val topKOptionMap = Map( \"epoch\" -> System.currentTimeMillis().toString, \"timeInterval\" -> \"1000ms\", \"size\" -> \"40\", \"frequencyCol\" -> \"retweets\", \"timeSeriesColumn\" -> \"tweetTime\" ) val schema = StructType(List(StructField(\"HashTag\", StringType))) snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"), \"HashTag\", schema, topKOptionMap) The code above shows how to do the same thing using the SnappyData Scala API. It is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the timeInterval attribute when creating the TopK table.","title":"Sketching"},{"location":"sde/sketching/#sketching","text":"Synopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A BloomFilter is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a Count-Min-Sketch which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.","title":"Sketching"},{"location":"sde/sketching/#creating-topk-tables","text":"TopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query. TopK queries aim to retrieve, from a potentially very large result set, only the k (k >= 1) best answers.","title":"Creating TopK tables"},{"location":"sde/sketching/#sql-api-for-creating-a-topk-table-in-snappydata","text":"snsc.sql(\"create topK table MostPopularTweets on tweetStreamTable \" + \"options(key 'hashtag', frequencyCol 'retweets')\") The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.","title":"SQL API for creating a TopK table in SnappyData"},{"location":"sde/sketching/#scala-api-for-creating-a-topk-table","text":"val topKOptionMap = Map( \"epoch\" -> System.currentTimeMillis().toString, \"timeInterval\" -> \"1000ms\", \"size\" -> \"40\", \"frequencyCol\" -> \"retweets\" ) val schema = StructType(List(StructField(\"HashTag\", StringType))) snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"), \"HashTag\", schema, topKOptionMap) The code above shows how to do the same thing using the SnappyData Scala API.","title":"Scala API for creating a TopK table"},{"location":"sde/sketching/#querying-the-topk-table","text":"select * from topkTweets order by EstimatedValue desc The example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most re-tweets.","title":"Querying the TopK table"},{"location":"sde/sketching/#approximate-topk-analytics-for-time-series-data","text":"Time is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the time stamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most re-tweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system. Here is an example of a time-based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData AQP module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals. select hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' order by EstimatedValue desc If time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.","title":"Approximate TopK analytics for time series data"},{"location":"sde/sketching/#sql-api-for-creating-a-topk-table-in-snappydata-specifying-timestampcolumn","text":"In the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet. snsc.sql(\"create topK table MostPopularTweets on tweetStreamTable \" + \"options(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' )\") The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest re-tweets value in the base table. This works for both static tables and streaming tables","title":"SQL API for creating a TopK table in SnappyData specifying timestampColumn"},{"location":"sde/sketching/#scala-api-for-creating-a-topk-table_1","text":"val topKOptionMap = Map( \"epoch\" -> System.currentTimeMillis().toString, \"timeInterval\" -> \"1000ms\", \"size\" -> \"40\", \"frequencyCol\" -> \"retweets\", \"timeSeriesColumn\" -> \"tweetTime\" ) val schema = StructType(List(StructField(\"HashTag\", StringType))) snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"), \"HashTag\", schema, topKOptionMap) The code above shows how to do the same thing using the SnappyData Scala API. It is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the timeInterval attribute when creating the TopK table.","title":"Scala API for creating a TopK table"},{"location":"sde/working_with_stratified_samples/","text":"Working with Stratified Samples \u00b6 Create Sample Tables \u00b6 You can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark DataFrames, or sourced from an external data source such as S3 or HDFS. Creation of sample table will implicitly sample the data from the base table. Here is an SQL based example to create a sample on tables locally available in the SnappyData cluster. CREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01'); CREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE OPTIONS (qcs 'hack_license', fraction '0.01'); Often your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. In this example below, a sample table is created for an S3 (external) dataset: CREATE EXTERNAL TABLE TAXIFARE USING parquet OPTIONS(path 's3a://<AWS_SECRET_ACCESS_KEY>:<AWS_ACCESS_KEY_ID>@computedb-test-data/nyctaxifaredata_cleaned'); //Next, create the sample sourced from this table .. CREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE options (qcs 'hack_license', fraction '0.01'); When creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. For sample tables, the overflow property is set to False by default. (For row and column tables the default value is True ). For example: CREATE TABLE BASETABLENAME <column details> USING COLUMN OPTIONS (partition_by '<column_name_a>', Buckets '7', Redundancy '1') CREATE TABLE SAMPLETABLENAME <column details> USING COLUMN_SAMPLE OPTIONS (qcs '<column_name_b>',fraction '0.05', strataReservoirSize '50', baseTable 'baseTableName') Note After a sample table is created from a base table, any changes to the base table, (for example update and delete operations) is not automatically applied to the sample table. Howerver, if the data is populated in the main table using any of the following ways: dataFrame.write Using JDBC API batch insert Insert into table values select * from x Then the data is also sampled and put into sample table(s) if exists. For successful creation of sample tables, the number of buckets in the sample table should be more than the number of nodes in the cluster. QCS (Query Column Set) and Sample Selection \u00b6 For stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction). QCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like hour (pickup_datetime) . The parameter fraction represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes, you can get pretty accurate answers with a very small fraction. With most data sets that follow a normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. AQP always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. For instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers (a lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement. One can create multiple sample tables using different sample QCS and sample fraction for a given base table. Here are some general guidelines to use when creating samples: Note that samples are only applicable when running aggregation queries. For point lookups or selective queries, the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store. Start by identifying the most common columns used in GroupBy/Where and Having clauses. Then, identify a subset of these columns where the cardinality is not too large. For instance, in the example above 'hack_license' is picked (one license per driver) as the strata and 1% of the records associated with each driver is sampled. Avoid using unique columns or timestamps for your QCS. For instance, in the example above, 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is a possibility that each record in the dataset has a different timesstamp. Instead, when dealing with time series the 'hour' function is used to capture data for each hour. When the accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample. Note The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.","title":"Working with Stratified Samples"},{"location":"sde/working_with_stratified_samples/#working-with-stratified-samples","text":"","title":"Working with Stratified Samples"},{"location":"sde/working_with_stratified_samples/#create-sample-tables","text":"You can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark DataFrames, or sourced from an external data source such as S3 or HDFS. Creation of sample table will implicitly sample the data from the base table. Here is an SQL based example to create a sample on tables locally available in the SnappyData cluster. CREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01'); CREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE OPTIONS (qcs 'hack_license', fraction '0.01'); Often your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. In this example below, a sample table is created for an S3 (external) dataset: CREATE EXTERNAL TABLE TAXIFARE USING parquet OPTIONS(path 's3a://<AWS_SECRET_ACCESS_KEY>:<AWS_ACCESS_KEY_ID>@computedb-test-data/nyctaxifaredata_cleaned'); //Next, create the sample sourced from this table .. CREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE options (qcs 'hack_license', fraction '0.01'); When creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. For sample tables, the overflow property is set to False by default. (For row and column tables the default value is True ). For example: CREATE TABLE BASETABLENAME <column details> USING COLUMN OPTIONS (partition_by '<column_name_a>', Buckets '7', Redundancy '1') CREATE TABLE SAMPLETABLENAME <column details> USING COLUMN_SAMPLE OPTIONS (qcs '<column_name_b>',fraction '0.05', strataReservoirSize '50', baseTable 'baseTableName') Note After a sample table is created from a base table, any changes to the base table, (for example update and delete operations) is not automatically applied to the sample table. Howerver, if the data is populated in the main table using any of the following ways: dataFrame.write Using JDBC API batch insert Insert into table values select * from x Then the data is also sampled and put into sample table(s) if exists. For successful creation of sample tables, the number of buckets in the sample table should be more than the number of nodes in the cluster.","title":"Create Sample Tables"},{"location":"sde/working_with_stratified_samples/#qcs-query-column-set-and-sample-selection","text":"For stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction). QCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like hour (pickup_datetime) . The parameter fraction represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes, you can get pretty accurate answers with a very small fraction. With most data sets that follow a normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. AQP always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. For instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers (a lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement. One can create multiple sample tables using different sample QCS and sample fraction for a given base table. Here are some general guidelines to use when creating samples: Note that samples are only applicable when running aggregation queries. For point lookups or selective queries, the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store. Start by identifying the most common columns used in GroupBy/Where and Having clauses. Then, identify a subset of these columns where the cardinality is not too large. For instance, in the example above 'hack_license' is picked (one license per driver) as the strata and 1% of the records associated with each driver is sampled. Avoid using unique columns or timestamps for your QCS. For instance, in the example above, 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is a possibility that each record in the dataset has a different timesstamp. Instead, when dealing with time series the 'hour' function is used to capture data for each hour. When the accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample. Note The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.","title":"QCS (Query Column Set) and Sample Selection"},{"location":"security/","text":"Managing Security \u00b6 In the current release, SnappyData only supports LDAP authentication which allows users to authenticate against an existing LDAP directory service in your organization. LDAP (lightweight directory access protocol) provides an open directory access protocol that runs over TCP/IP. This feature provides a secure way for users to use their existing login credentials (usernames and passwords) to access the cluster and data. SnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system. Note Currently, only LDAP based authentication and authorization is supported The user launching the cluster becomes the admin user of the cluster. All members of the cluster (leads, locators, and servers) must be started by the same user Refer to User Names for Authentication, Authorization, and Membership for more information on how user names are treated by each system. Launching the Cluster in Secure Mode Specifying Encrypted Passwords in Conf Files or in Client Connections Authentication - Connecting to a Secure Cluster Authorization Implementing Row Level Security Configuring Network Encryption and Authentication using SSL Securing SnappyData Monitoring Console Connection User Names for Authentication, Authorization, and Membership","title":"Managing Security"},{"location":"security/#managing-security","text":"In the current release, SnappyData only supports LDAP authentication which allows users to authenticate against an existing LDAP directory service in your organization. LDAP (lightweight directory access protocol) provides an open directory access protocol that runs over TCP/IP. This feature provides a secure way for users to use their existing login credentials (usernames and passwords) to access the cluster and data. SnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system. Note Currently, only LDAP based authentication and authorization is supported The user launching the cluster becomes the admin user of the cluster. All members of the cluster (leads, locators, and servers) must be started by the same user Refer to User Names for Authentication, Authorization, and Membership for more information on how user names are treated by each system. Launching the Cluster in Secure Mode Specifying Encrypted Passwords in Conf Files or in Client Connections Authentication - Connecting to a Secure Cluster Authorization Implementing Row Level Security Configuring Network Encryption and Authentication using SSL Securing SnappyData Monitoring Console Connection User Names for Authentication, Authorization, and Membership","title":"Managing Security"},{"location":"security/authentication_connecting_to_a_secure_cluster/","text":"Authentication - Connecting to a Secure Cluster \u00b6 Authentication is the process of verifying someone's identity. When a user tries to log in, that request is forwarded to the specified LDAP directory to verify if the credentials are correct. There are a few different ways to connect to a secure cluster using either JDBC (Thin) Client, Smart Connector Mode and Snappy Jobs. Accessing a secure cluster requires users to provide their user credentials. Using JDBC (Thin) Client \u00b6 When using the JDBC client, provide the user credentials using connection properties 'user' and 'password'. Example: JDBC Client val props = new Properties() props.setProperty(\"user\", username); props.setProperty(\"password\", password); val url: String = s\"jdbc:snappydata://localhost:1527/\" val conn = DriverManager.getConnection(url, props) Example: Snappy shell connect client 'localhost:1527;user=user1;password=user123'; For more information, refer How to connect using JDBC driver . Using ODBC Driver \u00b6 You can also connect to the SnappyData Cluster using SnappyData ODBC Driver using the following command: Driver=SnappyData ODBC Driver;server=<ServerHost>;port=<ServerPort>;user=<userName>;password=<password> For more information refer to, How to Connect using ODBC Driver . Using Smart Connector Mode \u00b6 In Smart Connector mode, provide the user credentials as Spark configuration properties named spark.snappydata.store.user and spark.snappydata.store.password . Example In the below example, these properties are set in the SparkConf which is used to create SnappyContext in your job. val conf = new SparkConf() .setAppName(\"My Spark Application with SnappyData\") .setMaster(s\"spark://$hostName:7077\") .set(\"spark.executor.cores\", TestUtils.defaultCores.toString) .set(\"spark.executor.extraClassPath\", System.getenv(\"SNAPPY_HOME\") + \"/jars/*\" ) .set(\"snappydata.connection\", snappydataLocatorURL) .set(\"spark.snappydata.store.user\", username) .set(\"spark.snappydata.store.password\", password) val sc = SparkContext.getOrCreate(conf) val snc = SnappyContext(sc) Example The below example demonstrates how to connect to the cluster via Spark shell using the --conf option to specify the properties. $./bin/spark-shell --master local[*] --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.store.user=user1 --conf spark.snappydata.store.password=user123 Example : Alternatively, you can specify the user credentials in the Spark conf file. Spark reads these properties when you lauch the spark-shell or invoke spark-submit. To do so, specify the user credentials in the spark-defaults.conf file, located in the conf directory. In this file, you can specify: spark.snappydata.store.user <username> spark.snappydata.store.password <password> Using Snappy Jobs \u00b6 When submitting Snappy jobs using snappy-job.sh , provide user credentials through a configuration file using the option --passfile . For example, a sample configuration file is provided below: $ cat /home/user1/snappy/job.config -u user1:password In the below example, the above configuration file is passed when submitting a job. $./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.CreateAndLoadAirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\ --passfile /home/user1/snappy/job.config Note When checking the status of a job using snappyjob.sh status --jobid , provide user credentials through a configuration file using the option --passfile Only trusted users should be allowed to submit jobs, as an untrusted user may be able to do harm through jobs by invoking internal APIs which can bypass the authorization checks. Currently, SparkJobServer UI may not be accessible when security is enabled, but you can use the snappy-job.sh script to access any information required using commands like status , listcontexts , etc. Execute ./bin/snappy-job.sh for more details. The configuration file should be in a secure location with read access only to an authorized user. These user credentials are passed to the Snappy instance available in the job, and it will be used to authorize the operations in the job.","title":"Authentication - Connecting to a Secure Cluster"},{"location":"security/authentication_connecting_to_a_secure_cluster/#authentication-connecting-to-a-secure-cluster","text":"Authentication is the process of verifying someone's identity. When a user tries to log in, that request is forwarded to the specified LDAP directory to verify if the credentials are correct. There are a few different ways to connect to a secure cluster using either JDBC (Thin) Client, Smart Connector Mode and Snappy Jobs. Accessing a secure cluster requires users to provide their user credentials.","title":"Authentication - Connecting to a Secure Cluster"},{"location":"security/authentication_connecting_to_a_secure_cluster/#using-jdbc-thin-client","text":"When using the JDBC client, provide the user credentials using connection properties 'user' and 'password'. Example: JDBC Client val props = new Properties() props.setProperty(\"user\", username); props.setProperty(\"password\", password); val url: String = s\"jdbc:snappydata://localhost:1527/\" val conn = DriverManager.getConnection(url, props) Example: Snappy shell connect client 'localhost:1527;user=user1;password=user123'; For more information, refer How to connect using JDBC driver .","title":"Using JDBC (Thin) Client"},{"location":"security/authentication_connecting_to_a_secure_cluster/#using-odbc-driver","text":"You can also connect to the SnappyData Cluster using SnappyData ODBC Driver using the following command: Driver=SnappyData ODBC Driver;server=<ServerHost>;port=<ServerPort>;user=<userName>;password=<password> For more information refer to, How to Connect using ODBC Driver .","title":"Using ODBC Driver"},{"location":"security/authentication_connecting_to_a_secure_cluster/#using-smart-connector-mode","text":"In Smart Connector mode, provide the user credentials as Spark configuration properties named spark.snappydata.store.user and spark.snappydata.store.password . Example In the below example, these properties are set in the SparkConf which is used to create SnappyContext in your job. val conf = new SparkConf() .setAppName(\"My Spark Application with SnappyData\") .setMaster(s\"spark://$hostName:7077\") .set(\"spark.executor.cores\", TestUtils.defaultCores.toString) .set(\"spark.executor.extraClassPath\", System.getenv(\"SNAPPY_HOME\") + \"/jars/*\" ) .set(\"snappydata.connection\", snappydataLocatorURL) .set(\"spark.snappydata.store.user\", username) .set(\"spark.snappydata.store.password\", password) val sc = SparkContext.getOrCreate(conf) val snc = SnappyContext(sc) Example The below example demonstrates how to connect to the cluster via Spark shell using the --conf option to specify the properties. $./bin/spark-shell --master local[*] --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.store.user=user1 --conf spark.snappydata.store.password=user123 Example : Alternatively, you can specify the user credentials in the Spark conf file. Spark reads these properties when you lauch the spark-shell or invoke spark-submit. To do so, specify the user credentials in the spark-defaults.conf file, located in the conf directory. In this file, you can specify: spark.snappydata.store.user <username> spark.snappydata.store.password <password>","title":"Using Smart Connector Mode"},{"location":"security/authentication_connecting_to_a_secure_cluster/#using-snappy-jobs","text":"When submitting Snappy jobs using snappy-job.sh , provide user credentials through a configuration file using the option --passfile . For example, a sample configuration file is provided below: $ cat /home/user1/snappy/job.config -u user1:password In the below example, the above configuration file is passed when submitting a job. $./bin/snappy-job.sh submit \\ --lead localhost:8090 \\ --app-name airlineApp \\ --class io.snappydata.examples.CreateAndLoadAirlineDataJob \\ --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\ --passfile /home/user1/snappy/job.config Note When checking the status of a job using snappyjob.sh status --jobid , provide user credentials through a configuration file using the option --passfile Only trusted users should be allowed to submit jobs, as an untrusted user may be able to do harm through jobs by invoking internal APIs which can bypass the authorization checks. Currently, SparkJobServer UI may not be accessible when security is enabled, but you can use the snappy-job.sh script to access any information required using commands like status , listcontexts , etc. Execute ./bin/snappy-job.sh for more details. The configuration file should be in a secure location with read access only to an authorized user. These user credentials are passed to the Snappy instance available in the job, and it will be used to authorize the operations in the job.","title":"Using Snappy Jobs"},{"location":"security/authorization/","text":"Authorization \u00b6 Authorization is the process of determining what access permissions the authenticated user has. Users are authorized to perform tasks based on their role assignments. SnappyData also supports LDAP group authorization. The administrator can manage user permissions in a secure cluster using the GRANT and REVOKE SQL statements which allow you to set permission for the user for specific database objects or for specific SQL actions. The GRANT statement is used to grant specific permissions to users. The REVOKE statement is used to revoke permissions. Note A user requiring INSERT , UPDATE or DELETE permissions may also require explicit SELECT permission on a table Only administrators can execute built-in procedures (like INSTALL-JAR) Adding Restrictions in Default Schema \u00b6 Users in SnappyData cluster have their own schema by default when they log into the cluster. They have full access within this schema. But in some cases, cluster administrators may need to ensure controlled use of the cluster resources by its users and may need to enforce restrictions on them. This can be achieved by setting the system property snappydata.RESTRICT_TABLE_CREATION to true in conf files at the time of starting the cluster. This forbids the users to create tables in their default schema. Users also cannot execute queries on tables in the schema. Administrators, however, can explicitly grant permissions to these users on their respective default schemas using GRANT command. The default value of the property is false. You need to prefix -J-D to the property name while specifying it in the conf files (locators, leads, and servers). $ cat conf/servers localhost -auth-provider=LDAP -J-Dsnappydata.RESTRICT_TABLE_CREATION=true -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ \\ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-pw=user123 LDAP Groups in SnappyData Authorization \u00b6 SnappyData extends the SQL GRANT statement to support LDAP Group names as Grantees. Here is an example SQL to grant privileges to individual users on column/row tables: GRANT SELECT ON TABLE t TO sam,bob; Here is an example SQL to grant privileges to individual users on external tables: GRANT ALL ON EXT_T1 TO samb,bob; You can also grant privileges to LDAP groups using the following syntax: GRANT SELECT ON Table t TO ldapGroup:<groupName>, bob; GRANT INSERT ON Table t TO ldapGroup:<groupName>, bob; You can also grant privileges to LDAP groups using the following syntax on external tables: GRANT ALL ON EXT_T1 TO ldapGroup:<groupName>, bob; SnappyData fetches the current list of members for the LDAP Group and grants each member privileges individually (stored in SnappyData). Similarly, when a REVOKE SQL statement is executed SnappyData removes the privileges individually for all members that make up a group. To support changes to Group membership within the LDAP Server, there is an additional System procedure to refresh the privileges recorded in SnappyData. CALL SYS.REFRESH_LDAP_GROUP('<GROUP NAME>'); This step has to be performed manually by admin when relevant LDAP groups change on the server. To optimize searching for groups in the LDAP server the following optional properties can be specified. These are similar to the current ones used for authentication: gemfirexd.auth-ldap-search-base and gemfirexd.auth-ldap-search-filter . The support for LDAP groups requires using LDAP as also the authentication mechanism. gemfirexd.group-ldap-search-base // base to identify objects of type group gemfirexd.group-ldap-search-filter // any additional search filter for groups gemfirexd.group-ldap-member-attributes //attributes specifying the list of members If no gemfirexd.group-ldap-search-base property has been provided then the one used for authentication gemfirexd.auth-ldap-search-base is used. If no search filter is specified then SnappyData uses the standard objectClass groupOfMembers (rfc2307) or groupOfNames with attribute as member, or objectClass groupOfUniqueMembers with attribute as uniqueMember. To be precise, the default search filter is: (&(|(objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfMembers) (objectClass=groupOfUniqueNames))(|(cn=%GROUP%)(name=%GROUP%))) The token \"%GROUP%\" is replaced by the actual group name in the search pattern. A custom search filter should use the same as a placeholder, for the group name. The default member attribute list is member, uniqueMember. The LDAP group resolution is recursive, meaning a group can refer to another group (see example below). There is no detection for broken LDAP group definitions having a cycle of group references and such a situation leads to a failure in GRANT or REFRESH_LDAP_GROUP with StackOverflowError. An LDAP group entry can look like below: dn: cn=group1,ou=group,dc=example,dc=com objectClass: groupOfNames cn: group1 gidNumber: 1001 member: uid=user1,ou=group,dc=example,dc=com member: uid=user2,ou=group,dc=example,dc=com member: cn=group11,ou=group,dc=example,dc=com Note There is NO multi-group support for users yet, so if a user has been granted access by two LDAP groups only the first one will take effect. If a user belongs to LDAP group as well as granted permissions separately as a user, then the latter is given precedence. So even if LDAP group permission is later revoked (or user is removed from LDAP group), the user will continue to have permissions unless explicitly revoked as a user. LDAPGROUP is now a reserved word, so cannot be used for a user name.","title":"Authorization"},{"location":"security/authorization/#authorization","text":"Authorization is the process of determining what access permissions the authenticated user has. Users are authorized to perform tasks based on their role assignments. SnappyData also supports LDAP group authorization. The administrator can manage user permissions in a secure cluster using the GRANT and REVOKE SQL statements which allow you to set permission for the user for specific database objects or for specific SQL actions. The GRANT statement is used to grant specific permissions to users. The REVOKE statement is used to revoke permissions. Note A user requiring INSERT , UPDATE or DELETE permissions may also require explicit SELECT permission on a table Only administrators can execute built-in procedures (like INSTALL-JAR)","title":"Authorization"},{"location":"security/authorization/#adding-restrictions-in-default-schema","text":"Users in SnappyData cluster have their own schema by default when they log into the cluster. They have full access within this schema. But in some cases, cluster administrators may need to ensure controlled use of the cluster resources by its users and may need to enforce restrictions on them. This can be achieved by setting the system property snappydata.RESTRICT_TABLE_CREATION to true in conf files at the time of starting the cluster. This forbids the users to create tables in their default schema. Users also cannot execute queries on tables in the schema. Administrators, however, can explicitly grant permissions to these users on their respective default schemas using GRANT command. The default value of the property is false. You need to prefix -J-D to the property name while specifying it in the conf files (locators, leads, and servers). $ cat conf/servers localhost -auth-provider=LDAP -J-Dsnappydata.RESTRICT_TABLE_CREATION=true -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ \\ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-pw=user123","title":"Adding Restrictions in Default Schema"},{"location":"security/authorization/#ldap-groups-in-snappydata-authorization","text":"SnappyData extends the SQL GRANT statement to support LDAP Group names as Grantees. Here is an example SQL to grant privileges to individual users on column/row tables: GRANT SELECT ON TABLE t TO sam,bob; Here is an example SQL to grant privileges to individual users on external tables: GRANT ALL ON EXT_T1 TO samb,bob; You can also grant privileges to LDAP groups using the following syntax: GRANT SELECT ON Table t TO ldapGroup:<groupName>, bob; GRANT INSERT ON Table t TO ldapGroup:<groupName>, bob; You can also grant privileges to LDAP groups using the following syntax on external tables: GRANT ALL ON EXT_T1 TO ldapGroup:<groupName>, bob; SnappyData fetches the current list of members for the LDAP Group and grants each member privileges individually (stored in SnappyData). Similarly, when a REVOKE SQL statement is executed SnappyData removes the privileges individually for all members that make up a group. To support changes to Group membership within the LDAP Server, there is an additional System procedure to refresh the privileges recorded in SnappyData. CALL SYS.REFRESH_LDAP_GROUP('<GROUP NAME>'); This step has to be performed manually by admin when relevant LDAP groups change on the server. To optimize searching for groups in the LDAP server the following optional properties can be specified. These are similar to the current ones used for authentication: gemfirexd.auth-ldap-search-base and gemfirexd.auth-ldap-search-filter . The support for LDAP groups requires using LDAP as also the authentication mechanism. gemfirexd.group-ldap-search-base // base to identify objects of type group gemfirexd.group-ldap-search-filter // any additional search filter for groups gemfirexd.group-ldap-member-attributes //attributes specifying the list of members If no gemfirexd.group-ldap-search-base property has been provided then the one used for authentication gemfirexd.auth-ldap-search-base is used. If no search filter is specified then SnappyData uses the standard objectClass groupOfMembers (rfc2307) or groupOfNames with attribute as member, or objectClass groupOfUniqueMembers with attribute as uniqueMember. To be precise, the default search filter is: (&(|(objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfMembers) (objectClass=groupOfUniqueNames))(|(cn=%GROUP%)(name=%GROUP%))) The token \"%GROUP%\" is replaced by the actual group name in the search pattern. A custom search filter should use the same as a placeholder, for the group name. The default member attribute list is member, uniqueMember. The LDAP group resolution is recursive, meaning a group can refer to another group (see example below). There is no detection for broken LDAP group definitions having a cycle of group references and such a situation leads to a failure in GRANT or REFRESH_LDAP_GROUP with StackOverflowError. An LDAP group entry can look like below: dn: cn=group1,ou=group,dc=example,dc=com objectClass: groupOfNames cn: group1 gidNumber: 1001 member: uid=user1,ou=group,dc=example,dc=com member: uid=user2,ou=group,dc=example,dc=com member: cn=group11,ou=group,dc=example,dc=com Note There is NO multi-group support for users yet, so if a user has been granted access by two LDAP groups only the first one will take effect. If a user belongs to LDAP group as well as granted permissions separately as a user, then the latter is given precedence. So even if LDAP group permission is later revoked (or user is removed from LDAP group), the user will continue to have permissions unless explicitly revoked as a user. LDAPGROUP is now a reserved word, so cannot be used for a user name.","title":"LDAP Groups in SnappyData Authorization"},{"location":"security/configuring_network_encryption_and_authentication_using_ssl/","text":"Configuring Network Encryption and Authentication using SSL \u00b6 The network communication between the server and client can be encrypted using SSL. For more information refer to Enabling SSL Encryption in Different Socket Endpoints of SnappyData .","title":"Configuring Network Encryption and Authentication using SSL"},{"location":"security/configuring_network_encryption_and_authentication_using_ssl/#configuring-network-encryption-and-authentication-using-ssl","text":"The network communication between the server and client can be encrypted using SSL. For more information refer to Enabling SSL Encryption in Different Socket Endpoints of SnappyData .","title":"Configuring Network Encryption and Authentication using SSL"},{"location":"security/launching_the_cluster_in_secure_mode/","text":"Launching the Cluster in Secure Mode \u00b6 SnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system. Authentication Properties \u00b6 To enable LDAP authentication, set the following authentication properties in the configuration files conf/locators , conf/servers , and conf/leads files. auth-provider : The authentication provider. Set the auth-provider property to LDAP , to enable LDAP for authenticating all distributed system members as well as clients to the distributed system. server-auth-provider : Peer-to-peer authentication of cluster members is configured in the SnappyData cluster. You can set server-auth-provider property to NONE if you want to disable the peer-to-peer authentication. user : The user name of the administrator starting the cluster password : The password of the administrator starting the cluster J-Dgemfirexd.auth-ldap-server : Set this property to the URL to the LDAP server. J-Dgemfirexd.auth-ldap-search-base : Use this property to limit the search space used when SnappyData verifies a user login ID. Specify the name of the context or object to search, that is a parameter to javax.naming.directory.DirContext.search() . J-Dgemfirexd.auth-ldap-search-dn : If the LDAP server does not allow anonymous binding (or if this functionality is disabled), specify the user distinguished name (DN) to use for binding to the LDAP server for searching. J-Dgemfirexd.auth-ldap-search-pw : The password for the LDAP search user which is used for looking up the DN indicated by configuration parameter Dgemfirexd.auth-ldap-search-dn . Example - Launching Locator in Secure Mode \u00b6 In the below example, we are launching the locator in secure mode, which communicates with the LDAP server at localhost listening on port 389. localhost -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ \\ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-pw=user123 Note You must specify gemfirexd.auth-ldap-* properties as Java system properties by prefixing '-J-D'. If you use SSL-encrypted LDAP and your LDAP server certificate is not recognized by a valid Certificate Authority (CA), you must create a local trust store for each SnappyData member and import the LDAP server certificate to the trust store. See the document on Creating a Keystore for more information. Specify the javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword system properties when you start individual SnappyData members. For example: localhost -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ \\ -J-Dgemfirexd.auth-ldap-server=ldaps://ldapserver:636/ -user=user_name -password=user_pwd \\ -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-pw=user123 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\ -J-Djavax.net.ssl.trustStore=/Users/user1/snappydata/keystore_name \\ -J-Djavax.net.ssl.trustStorePassword=keystore_password javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword must be specified as Java system properties (using the -J option on the Snappy shell).","title":"Launching the Cluster in Secure Mode"},{"location":"security/launching_the_cluster_in_secure_mode/#launching-the-cluster-in-secure-mode","text":"SnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system.","title":"Launching the Cluster in Secure Mode"},{"location":"security/launching_the_cluster_in_secure_mode/#authentication-properties","text":"To enable LDAP authentication, set the following authentication properties in the configuration files conf/locators , conf/servers , and conf/leads files. auth-provider : The authentication provider. Set the auth-provider property to LDAP , to enable LDAP for authenticating all distributed system members as well as clients to the distributed system. server-auth-provider : Peer-to-peer authentication of cluster members is configured in the SnappyData cluster. You can set server-auth-provider property to NONE if you want to disable the peer-to-peer authentication. user : The user name of the administrator starting the cluster password : The password of the administrator starting the cluster J-Dgemfirexd.auth-ldap-server : Set this property to the URL to the LDAP server. J-Dgemfirexd.auth-ldap-search-base : Use this property to limit the search space used when SnappyData verifies a user login ID. Specify the name of the context or object to search, that is a parameter to javax.naming.directory.DirContext.search() . J-Dgemfirexd.auth-ldap-search-dn : If the LDAP server does not allow anonymous binding (or if this functionality is disabled), specify the user distinguished name (DN) to use for binding to the LDAP server for searching. J-Dgemfirexd.auth-ldap-search-pw : The password for the LDAP search user which is used for looking up the DN indicated by configuration parameter Dgemfirexd.auth-ldap-search-dn .","title":"Authentication Properties"},{"location":"security/launching_the_cluster_in_secure_mode/#example-launching-locator-in-secure-mode","text":"In the below example, we are launching the locator in secure mode, which communicates with the LDAP server at localhost listening on port 389. localhost -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ \\ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-pw=user123 Note You must specify gemfirexd.auth-ldap-* properties as Java system properties by prefixing '-J-D'. If you use SSL-encrypted LDAP and your LDAP server certificate is not recognized by a valid Certificate Authority (CA), you must create a local trust store for each SnappyData member and import the LDAP server certificate to the trust store. See the document on Creating a Keystore for more information. Specify the javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword system properties when you start individual SnappyData members. For example: localhost -auth-provider=LDAP -user=snappy1 -password=snappy1 -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ \\ -J-Dgemfirexd.auth-ldap-server=ldaps://ldapserver:636/ -user=user_name -password=user_pwd \\ -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\ -J-Dgemfirexd.auth-ldap-search-pw=user123 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\ -J-Djavax.net.ssl.trustStore=/Users/user1/snappydata/keystore_name \\ -J-Djavax.net.ssl.trustStorePassword=keystore_password javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword must be specified as Java system properties (using the -J option on the Snappy shell).","title":"Example - Launching Locator in Secure Mode"},{"location":"security/row_level_security/","text":"Implementing Row Level Security \u00b6 The following topics are covered in this section: Overview of Row Level Security Activating Row Level Security Creating a Policy Enabling Row Level Security Viewing Policy Details Combining Multiple Policies Policy Application on Join Queries Dropping a Policy Overview of Row Level Security \u00b6 Policy is a rule that is implemented by using a filter expression. In SnappyData, you can apply security policies to a table at row level that can restrict, on a per-user basis, the rows that must be returned for normal queries by data modification commands. For activating this row level security , a system property must be added to the configuration files of servers, leads, and locators. To restrict the permissions of a user at row level, create a simple policy for a table that can be applied on a per user basis and then enable the row level security for that table. Activating Row Level Security \u00b6 For activating Row Level Security, a system property -J-Dsnappydata.enable-rls=true must be added to the configuration files of servers, leads, and locators when you configure the cluster . By default this is off. If this property is not added, you cannot enable the Row Level Security and an exception is thrown when you attempt to create the policy. Warning When this property is set to true , the Smart Connector access to SnappyData will fail with java.lang.IllegalStateException: Row level security (snappydata.enable-rls) does not allow smart connector mode exception. Creating a Policy \u00b6 A policy, which is a rule, can be created for row tables, column tables, and external GemFire tables. Permissions can be granted to users on DML ops to access the tables for which policies are created. Note Policies are restrictive by default. Here in the following example, a table named clients is created and access permissions are granted to three users: # Create table named clients. CREATE TABLE clients ( ID INT PRIMARY KEY, ACCOUNT_NAME STRING NOT NULL, ACCOUNT_MANAGER STRING NOT NULL ) USING ROW OPTIONS(); # Grant access to three users. GRANT SELECT ON clients TO tom, harris, greg; Initially all the users can view all the records in the table. You can restrict the permissions for viewing the records, only after you enable the row level security and apply a policy. For example, you are connected to database testrls as user tom . Initially user tom can view all the records from the table as shown: $ SELECT * FROM clients; id | account_name | account_manager ----+--------------+----------------- 1 | ABC | tom 2 | PQR | harris 3 | XYZ | greg (3 rows) You can create a policy for a table which can be applied to a user or a LDAP group using the following syntax. CREATE POLICY name ON table_name FOR SELECT [ TO { LDAP GROUP | CURRENT_USER | <USER_NAME> } [, ...] ] [ USING ( using_expression ) ] Note CURRENT_USER implies to any user who is excecuting the query. In the following example, we create a policy named just_own_clients where a user can view only the row where that user is the account manager. For example, if we want this rule to apply only to user Tom , then we can create the policy as shown: CREATE POLICY just_own_clients ON clients FOR SELECT TO TOM USING ACCOUNT_MANAGER = CURRENT_USER(); As per the the above policy, user Tom can see only one row, where as other users can view all the rows. The same can be also applied to a LDAP group as shown: CREATE POLICY just_own_clients ON clients FOR SELECT TO ldapgroup:group1 USING ACCOUNT_MANAGER = CURRENT_USER(); After the row level security policy is enabled, the policy gets applied to the corresponding users. Enabling Row Level Security \u00b6 The policy which is created to restrict row level permissions is effective only after you enable the row level security. To enable row level security, execute the following ALTER DDL command: ALTER TABLE <table_name> ENABLE ROW LEVEL SECURITY; For example, ALTER TABLE clients ENABLE ROW LEVEL SECURITY; Now the users are permitted to view the records of only those rows that are permitted on the basis of the policy. For example, user tom is allowed to view the records of a specific row in table clients . // tom is the user $ SELECT * FROM clients; id | account_name | account_manager ----+--------------+----------------- 2 | PQR | tom (1 row) Viewing Policy Details \u00b6 The policy details can be viewed from a virtual table named SYS.SYSPOLICIES . The details of the policy are shown in the following columns: Column Description NAME Name of the policy. SCHEMANAME Schema Name of the table on which policy is applied. TABLENAME Table on which policy is applied. POLICYFOR The operation for which the policy is intended. For example, SELECT, UPDATE, INSERT etc. For now it will be only \u201cSELECT\u201d APPLYTO The comma separated string of User Names ( or CURRENT_USER) or LDAP group on which the policy applies. FILTER The filter associated with the policy. OWNER Owner of the policy which is the same as the owner of the target table. Combining Multiple Policies \u00b6 Multiple policies can be defined for a table and combined as a statement. As policies are table-specific, each policy, that is applied to a table, must have a unique name. Tables in different schemas can have policies with the same name which can then have different conditions. If multiple policies are applied on a table, the filters will be evaluated as AND . Here in the following example, multiple policies are created for the table named mytable and row level security is enabled. The current user here is tom . CREATE POLICY mypolicy1 on mytable using user_col = current_user(); CREATE POLICY mypolicy2 on mytable using id < 4; CREATE POLICY mypolicy3 on mytable using account_name = \u2018XYZ\u2019; ALTER TABLE mytable ENABLE ROW LEVEL SECURITY; These policies are combined as shown in this example: SELECT * FROM mytable WHERE user_col = current_user() # current_user is <table owner> AND id<4 AND account_name = \u2018XYZ\u2019; $ select * from mytable; id | account_name | account_manager ----+--------------+----------------- 3 | XYZ | tom (1 row) Policy Application on Join Queries \u00b6 Refer the following example for policy application in case of join queries: CREATE TABLE customers ( customer_id int, name string, hidden boolean ) using column options(); INSERT INTO customers (customer_id, name)... CREATE TABLE orders ( order_id int, customer_id int ) using column options(); INSERT INTO orders (order_id, customer_id) ... An untrusted user that will be doing SELECTs only: Let the untrusted user group be named by LDAP (AD group) group as untrusted_group GRANT SELECT ON customers TO ldapgroup:untrusted_group; GRANT SELECT ON orders TO ldapgroup:untrusted_group; A policy that makes hidden customers invisible to the untrusted user: CREATE POLICY no_hidden_customers ON customers FOR SELECT TO ldapgroup:untrusted_group USING (hidden is false); ALTER TABLE customers ENABLE ROW LEVEL SECURITY; SELECT name FROM orders JOIN customers ON customer_id WHERE order_id = 4711; Dropping a Policy \u00b6 Since the policy is dependent on the table, only the owner of the table can drop a policy. To drop a policy, use the following syntax: DROP POLICY policy_name For example, DROP POLICY just_own_clients Caution If you drop a table, all the policies associated with the table will also get dropped.","title":"Implementing Row Level Security"},{"location":"security/row_level_security/#implementing-row-level-security","text":"The following topics are covered in this section: Overview of Row Level Security Activating Row Level Security Creating a Policy Enabling Row Level Security Viewing Policy Details Combining Multiple Policies Policy Application on Join Queries Dropping a Policy","title":"Implementing  Row Level Security"},{"location":"security/row_level_security/#overview-of-row-level-security","text":"Policy is a rule that is implemented by using a filter expression. In SnappyData, you can apply security policies to a table at row level that can restrict, on a per-user basis, the rows that must be returned for normal queries by data modification commands. For activating this row level security , a system property must be added to the configuration files of servers, leads, and locators. To restrict the permissions of a user at row level, create a simple policy for a table that can be applied on a per user basis and then enable the row level security for that table.","title":"Overview of Row Level Security"},{"location":"security/row_level_security/#activating-row-level-security","text":"For activating Row Level Security, a system property -J-Dsnappydata.enable-rls=true must be added to the configuration files of servers, leads, and locators when you configure the cluster . By default this is off. If this property is not added, you cannot enable the Row Level Security and an exception is thrown when you attempt to create the policy. Warning When this property is set to true , the Smart Connector access to SnappyData will fail with java.lang.IllegalStateException: Row level security (snappydata.enable-rls) does not allow smart connector mode exception.","title":"Activating Row Level Security"},{"location":"security/row_level_security/#creating-a-policy","text":"A policy, which is a rule, can be created for row tables, column tables, and external GemFire tables. Permissions can be granted to users on DML ops to access the tables for which policies are created. Note Policies are restrictive by default. Here in the following example, a table named clients is created and access permissions are granted to three users: # Create table named clients. CREATE TABLE clients ( ID INT PRIMARY KEY, ACCOUNT_NAME STRING NOT NULL, ACCOUNT_MANAGER STRING NOT NULL ) USING ROW OPTIONS(); # Grant access to three users. GRANT SELECT ON clients TO tom, harris, greg; Initially all the users can view all the records in the table. You can restrict the permissions for viewing the records, only after you enable the row level security and apply a policy. For example, you are connected to database testrls as user tom . Initially user tom can view all the records from the table as shown: $ SELECT * FROM clients; id | account_name | account_manager ----+--------------+----------------- 1 | ABC | tom 2 | PQR | harris 3 | XYZ | greg (3 rows) You can create a policy for a table which can be applied to a user or a LDAP group using the following syntax. CREATE POLICY name ON table_name FOR SELECT [ TO { LDAP GROUP | CURRENT_USER | <USER_NAME> } [, ...] ] [ USING ( using_expression ) ] Note CURRENT_USER implies to any user who is excecuting the query. In the following example, we create a policy named just_own_clients where a user can view only the row where that user is the account manager. For example, if we want this rule to apply only to user Tom , then we can create the policy as shown: CREATE POLICY just_own_clients ON clients FOR SELECT TO TOM USING ACCOUNT_MANAGER = CURRENT_USER(); As per the the above policy, user Tom can see only one row, where as other users can view all the rows. The same can be also applied to a LDAP group as shown: CREATE POLICY just_own_clients ON clients FOR SELECT TO ldapgroup:group1 USING ACCOUNT_MANAGER = CURRENT_USER(); After the row level security policy is enabled, the policy gets applied to the corresponding users.","title":"Creating a Policy"},{"location":"security/row_level_security/#enabling-row-level-security","text":"The policy which is created to restrict row level permissions is effective only after you enable the row level security. To enable row level security, execute the following ALTER DDL command: ALTER TABLE <table_name> ENABLE ROW LEVEL SECURITY; For example, ALTER TABLE clients ENABLE ROW LEVEL SECURITY; Now the users are permitted to view the records of only those rows that are permitted on the basis of the policy. For example, user tom is allowed to view the records of a specific row in table clients . // tom is the user $ SELECT * FROM clients; id | account_name | account_manager ----+--------------+----------------- 2 | PQR | tom (1 row)","title":"Enabling Row Level Security"},{"location":"security/row_level_security/#viewing-policy-details","text":"The policy details can be viewed from a virtual table named SYS.SYSPOLICIES . The details of the policy are shown in the following columns: Column Description NAME Name of the policy. SCHEMANAME Schema Name of the table on which policy is applied. TABLENAME Table on which policy is applied. POLICYFOR The operation for which the policy is intended. For example, SELECT, UPDATE, INSERT etc. For now it will be only \u201cSELECT\u201d APPLYTO The comma separated string of User Names ( or CURRENT_USER) or LDAP group on which the policy applies. FILTER The filter associated with the policy. OWNER Owner of the policy which is the same as the owner of the target table.","title":"Viewing Policy Details"},{"location":"security/row_level_security/#combining-multiple-policies","text":"Multiple policies can be defined for a table and combined as a statement. As policies are table-specific, each policy, that is applied to a table, must have a unique name. Tables in different schemas can have policies with the same name which can then have different conditions. If multiple policies are applied on a table, the filters will be evaluated as AND . Here in the following example, multiple policies are created for the table named mytable and row level security is enabled. The current user here is tom . CREATE POLICY mypolicy1 on mytable using user_col = current_user(); CREATE POLICY mypolicy2 on mytable using id < 4; CREATE POLICY mypolicy3 on mytable using account_name = \u2018XYZ\u2019; ALTER TABLE mytable ENABLE ROW LEVEL SECURITY; These policies are combined as shown in this example: SELECT * FROM mytable WHERE user_col = current_user() # current_user is <table owner> AND id<4 AND account_name = \u2018XYZ\u2019; $ select * from mytable; id | account_name | account_manager ----+--------------+----------------- 3 | XYZ | tom (1 row)","title":"Combining Multiple Policies"},{"location":"security/row_level_security/#policy-application-on-join-queries","text":"Refer the following example for policy application in case of join queries: CREATE TABLE customers ( customer_id int, name string, hidden boolean ) using column options(); INSERT INTO customers (customer_id, name)... CREATE TABLE orders ( order_id int, customer_id int ) using column options(); INSERT INTO orders (order_id, customer_id) ... An untrusted user that will be doing SELECTs only: Let the untrusted user group be named by LDAP (AD group) group as untrusted_group GRANT SELECT ON customers TO ldapgroup:untrusted_group; GRANT SELECT ON orders TO ldapgroup:untrusted_group; A policy that makes hidden customers invisible to the untrusted user: CREATE POLICY no_hidden_customers ON customers FOR SELECT TO ldapgroup:untrusted_group USING (hidden is false); ALTER TABLE customers ENABLE ROW LEVEL SECURITY; SELECT name FROM orders JOIN customers ON customer_id WHERE order_id = 4711;","title":"Policy Application on Join Queries"},{"location":"security/row_level_security/#dropping-a-policy","text":"Since the policy is dependent on the table, only the owner of the table can drop a policy. To drop a policy, use the following syntax: DROP POLICY policy_name For example, DROP POLICY just_own_clients Caution If you drop a table, all the policies associated with the table will also get dropped.","title":"Dropping a Policy"},{"location":"security/specify_encrypt_passwords_conf_client/","text":"Specifying Encrypted Passwords in Conf Files or in Client Connections \u00b6 SnappyData allows you to specify encrypted passwords, if you do not want to specify plain text passwords, in conf files or in JDBC/ODBC client connections URLs, while launching the SnappyData cluster in a secure mode. SnappyData provides an utility script called snappy-encrypt-password.sh to generate encrypted passwords of system users who launch the cluster. This script is located under sbin directory of SnappyData product installation. You can generate encrypted passwords before starting the SnappyData cluster and use it in conf files of the SnappyData servers, locators and leads. You can also generate encrypted passwords for JDBC/ODBC client connections when SnappyData cluster is already started. For that case, using the SYS.ENCRYPT_PASSWORD system procedure is a preferable approach as mentioned in the following note. This script accepts list of users as input, prompts for each user\u2019s password and outputs the encrypted password on the console. A special user AUTH_LDAP_SEARCH_PW can be used to generate encrypted password for the LDAP search user, used for looking up the DN indicated by configuration parameter -J-Dgemfirexd.auth-ldap-search-pw in SnappyData conf files. Note Make sure that SnappyData system is not running when this script is run, because this script launches a locator using the parameters specified in user\u2019s conf/locators file. The script connects to the existing cluster to generate the password if locators could not be started. However, this may not work if the user is a LDAP search user who does not have permissions to access Snappydata cluster. It is recommended to directly connect to cluster and then run the SQL CALL SYS.ENCRYPT_PASSWORD('<user>', '<password>', 'AES', 0) , if encrypted password is to be generated after you start the cluster. The encrypted secret that is returned is specific to this particular SnappyData distributed system, because the system uses a unique private key to generate the secret. An obfuscated version of the private key is stored in the persistent data dictionary (SnappyData catalog). If the existing data dictionary is ever deleted and recreated, then you must generate and use a new encrypted secret for use with the new distributed system. Also the encrypted password cannot be used in any other SnappyData installation, even if the user and password is the same. You need to generate the encrypted password separately for every other SnappyData installation. Script Usage \u00b6 snappy-encrypt-password.sh <user1> <user2> ... Example Output \u00b6 In the example output shown, snappy-encrypt-password.sh script is invoked for users user1 and AUTH_LDAP_SEARCH_PW (special user used to indicate LDAP search user, used for looking up the DN). The script outputs encrypted password for user1 and AUTH_LDAP_SEARCH_PW . $ ./sbin/snappy-encrypt-password.sh user1 AUTH_LDAP_SEARCH_PW Enter password for user1: user123 (atual password is not shown on the console) Re-enter password for user1: user123 (atual password is not shown on the console) Enter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console) Re-enter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console) Logs generated in /home/xyz/<snappydata_install_dir>/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 2379 status: running Distributed system now has 1 members. Started DRDA server on: localhost/127.0.0.1[1527] SnappyData version 1.3.1 snappy> Using CONNECTION0 snappy> ENCRYPTED_PASSWORD -------------------------------------------------------------------------------------------------------------------------------- user1 = v13b607k2j611b8584423b2ea584c970fefd041f77f 1 row selected snappy> ENCRYPTED_PASSWORD -------------------------------------------------------------------------------------------------------------------------------- AUTH_LDAP_SEARCH_PW = v13b607k2j65384028e5090a8990e2ce17d43da3de9 1 row selected snappy> The SnappyData Locator on 172.16.62.1(localhost-locator-1) has stopped. Startup Configuration Examples \u00b6 You can then use the above encrypted password in the configuration of servers, locators and leads. You can either edit the conf files or use the environment variables for startup options of locators, servers, and leads. Configuring by Editing conf Files \u00b6 Following is an example of the conf/locators file where instead of plain text, encrypted passwords are used. Similar change must be done to conf/servers and conf/leads files. $ cat conf/locators localhost -auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j611b8584423b2ea584c970fefd041f77f -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j65384028e5090a8990e2ce17d43da3de9 You can then start the SnappyData system. Configuring with Environment Variables for Startup Options \u00b6 An alternate way to specify the security options is by using the environment variable for LOCATOR_STARTUP_OPTIONS , SERVER_STARTUP_OPTIONS and LEAD_STARTUP_OPTIONS for locators, servers, and leads respectively. These startup variables can be specified in conf/spark-env.sh file. This file is sourced when starting the SnappyData system. A template file ( conf/spark-env.sh.template ) is provided in the conf directory for reference. You can copy this file and use it to configure security options. For example: # create a spark-env.sh from the template file $cp conf/spark-env.sh.template conf/spark-env.sh # edit the conf/spark-env.sh file to add security configuration as shown below SECURITY_ARGS=\"-auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j637b2ae24e60be46613391117b7f234d0 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j6174404428eee3374411d97d1d497d3b8\" LOCATOR_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d SERVER_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d LEAD_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d Using Encrypted Password in Client Connections \u00b6 You can generate encrypted password for use in JDBC/ODBC client connections by executing a system procedure with a statement such as CALL SYS.ENCRYPT_PASSWORD('<user>', '<password>', 'AES', 0) . For example: snappy> call sys.encrypt_password('user2', 'user2123', 'AES', 0); ENCRYPTED_PASSWORD -------------------------------------------------------------------------------------------------------------------------------- user2 = v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610 1 row selected This procedure accepts user id and plain text password as input arguments and outputs the encrypted password. Note In the current release of SnappyData, the last two parameters should be \u201cAES\u201d and \u201c0\u201d. The encrypted password can be used in JDBC/ODBC client connections. For example using snappy shell: snappy> connect client 'localhost:1527;user=user2;password=v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610'; Using CONNECTION1","title":"Specifying Encrypted Passwords in Conf Files or in Client Connections"},{"location":"security/specify_encrypt_passwords_conf_client/#specifying-encrypted-passwords-in-conf-files-or-in-client-connections","text":"SnappyData allows you to specify encrypted passwords, if you do not want to specify plain text passwords, in conf files or in JDBC/ODBC client connections URLs, while launching the SnappyData cluster in a secure mode. SnappyData provides an utility script called snappy-encrypt-password.sh to generate encrypted passwords of system users who launch the cluster. This script is located under sbin directory of SnappyData product installation. You can generate encrypted passwords before starting the SnappyData cluster and use it in conf files of the SnappyData servers, locators and leads. You can also generate encrypted passwords for JDBC/ODBC client connections when SnappyData cluster is already started. For that case, using the SYS.ENCRYPT_PASSWORD system procedure is a preferable approach as mentioned in the following note. This script accepts list of users as input, prompts for each user\u2019s password and outputs the encrypted password on the console. A special user AUTH_LDAP_SEARCH_PW can be used to generate encrypted password for the LDAP search user, used for looking up the DN indicated by configuration parameter -J-Dgemfirexd.auth-ldap-search-pw in SnappyData conf files. Note Make sure that SnappyData system is not running when this script is run, because this script launches a locator using the parameters specified in user\u2019s conf/locators file. The script connects to the existing cluster to generate the password if locators could not be started. However, this may not work if the user is a LDAP search user who does not have permissions to access Snappydata cluster. It is recommended to directly connect to cluster and then run the SQL CALL SYS.ENCRYPT_PASSWORD('<user>', '<password>', 'AES', 0) , if encrypted password is to be generated after you start the cluster. The encrypted secret that is returned is specific to this particular SnappyData distributed system, because the system uses a unique private key to generate the secret. An obfuscated version of the private key is stored in the persistent data dictionary (SnappyData catalog). If the existing data dictionary is ever deleted and recreated, then you must generate and use a new encrypted secret for use with the new distributed system. Also the encrypted password cannot be used in any other SnappyData installation, even if the user and password is the same. You need to generate the encrypted password separately for every other SnappyData installation.","title":"Specifying Encrypted Passwords in Conf Files or in Client Connections"},{"location":"security/specify_encrypt_passwords_conf_client/#script-usage","text":"snappy-encrypt-password.sh <user1> <user2> ...","title":"Script Usage"},{"location":"security/specify_encrypt_passwords_conf_client/#example-output","text":"In the example output shown, snappy-encrypt-password.sh script is invoked for users user1 and AUTH_LDAP_SEARCH_PW (special user used to indicate LDAP search user, used for looking up the DN). The script outputs encrypted password for user1 and AUTH_LDAP_SEARCH_PW . $ ./sbin/snappy-encrypt-password.sh user1 AUTH_LDAP_SEARCH_PW Enter password for user1: user123 (atual password is not shown on the console) Re-enter password for user1: user123 (atual password is not shown on the console) Enter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console) Re-enter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console) Logs generated in /home/xyz/<snappydata_install_dir>/work/localhost-locator-1/snappylocator.log SnappyData Locator pid: 2379 status: running Distributed system now has 1 members. Started DRDA server on: localhost/127.0.0.1[1527] SnappyData version 1.3.1 snappy> Using CONNECTION0 snappy> ENCRYPTED_PASSWORD -------------------------------------------------------------------------------------------------------------------------------- user1 = v13b607k2j611b8584423b2ea584c970fefd041f77f 1 row selected snappy> ENCRYPTED_PASSWORD -------------------------------------------------------------------------------------------------------------------------------- AUTH_LDAP_SEARCH_PW = v13b607k2j65384028e5090a8990e2ce17d43da3de9 1 row selected snappy> The SnappyData Locator on 172.16.62.1(localhost-locator-1) has stopped.","title":"Example Output"},{"location":"security/specify_encrypt_passwords_conf_client/#startup-configuration-examples","text":"You can then use the above encrypted password in the configuration of servers, locators and leads. You can either edit the conf files or use the environment variables for startup options of locators, servers, and leads.","title":"Startup Configuration Examples"},{"location":"security/specify_encrypt_passwords_conf_client/#configuring-by-editing-conf-files","text":"Following is an example of the conf/locators file where instead of plain text, encrypted passwords are used. Similar change must be done to conf/servers and conf/leads files. $ cat conf/locators localhost -auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j611b8584423b2ea584c970fefd041f77f -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j65384028e5090a8990e2ce17d43da3de9 You can then start the SnappyData system.","title":"Configuring by Editing conf Files"},{"location":"security/specify_encrypt_passwords_conf_client/#configuring-with-environment-variables-for-startup-options","text":"An alternate way to specify the security options is by using the environment variable for LOCATOR_STARTUP_OPTIONS , SERVER_STARTUP_OPTIONS and LEAD_STARTUP_OPTIONS for locators, servers, and leads respectively. These startup variables can be specified in conf/spark-env.sh file. This file is sourced when starting the SnappyData system. A template file ( conf/spark-env.sh.template ) is provided in the conf directory for reference. You can copy this file and use it to configure security options. For example: # create a spark-env.sh from the template file $cp conf/spark-env.sh.template conf/spark-env.sh # edit the conf/spark-env.sh file to add security configuration as shown below SECURITY_ARGS=\"-auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j637b2ae24e60be46613391117b7f234d0 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j6174404428eee3374411d97d1d497d3b8\" LOCATOR_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d SERVER_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d LEAD_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d","title":"Configuring with Environment Variables for Startup Options"},{"location":"security/specify_encrypt_passwords_conf_client/#using-encrypted-password-in-client-connections","text":"You can generate encrypted password for use in JDBC/ODBC client connections by executing a system procedure with a statement such as CALL SYS.ENCRYPT_PASSWORD('<user>', '<password>', 'AES', 0) . For example: snappy> call sys.encrypt_password('user2', 'user2123', 'AES', 0); ENCRYPTED_PASSWORD -------------------------------------------------------------------------------------------------------------------------------- user2 = v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610 1 row selected This procedure accepts user id and plain text password as input arguments and outputs the encrypted password. Note In the current release of SnappyData, the last two parameters should be \u201cAES\u201d and \u201c0\u201d. The encrypted password can be used in JDBC/ODBC client connections. For example using snappy shell: snappy> connect client 'localhost:1527;user=user2;password=v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610'; Using CONNECTION1","title":"Using Encrypted Password in Client Connections"},{"location":"security/user_names_for_authentication_authorization_and_membership/","text":"User Names for Authentication, Authorization, and Membership \u00b6 User Names are Converted to Authorization Identifiers \u00b6 User names in the SnappyData system are known as authorization identifiers. The authorization identifier is a string that represents the name of the user if one was provided in the connection request. After the authorization identifier is passed to the SnappyData system, it becomes an SQL92Identifier. SQL92Identifier is a kind of identifier that represents a database object such as a table or column. A SQL92Identifier is case-insensitive (it is converted to all caps) unless it is delimited with double quotes. A SQL92Identifier is limited to 128 characters and has other limitations. All user names must be valid authorization identifiers even if user authentication is turned off, and even if all users are allowed access to all databases. Handling Case Sensitivity and Special Characters in User Names \u00b6 If an external authentication system is used, SnappyData does not convert a user's name to an authorization identifier until after authentication has occurred (but before the user is authorized). For example, with an example user named Fred: Within the user authentication system, Fred might be known as FRed. If the external user authentication service is case-sensitive, Fred must always be typed as: connect client 'localhost:1527;user=FRed;password=flintstone'; Within the SnappyData user authorization system, Fred becomes a case-insensitive authorization identifier. Here, FRed is known as FRED. Also consider a second example, where Fred has a slightly different name within the user authentication system: Within the user authentication system, Fred is known as Fred. You must now put double quotes around the username, because it is not a valid SQL92Identifier. SnappyData removes the double quotes when passing the name to the external authentication system. connect client 'localhost:1527;user=\"Fred!\";password=flintstone'; Within the SnappyData user authorization system, Fred now becomes a case-sensitive authorization identifier. In this case, Fred is known as Fred. As shown in the first example, the external authentication system may be case-sensitive, whereas the authorization identifier within SnappyData may not be. If your authentication system allows two distinct users whose names differ by case, delimit all user names within the connection request to make all user names case-sensitive within the SnappyData system. In addition, you must also delimit user names that do not conform to SQL92Identifier rules with double quotes.","title":"User Names for Authentication, Authorization, and Membership"},{"location":"security/user_names_for_authentication_authorization_and_membership/#user-names-for-authentication-authorization-and-membership","text":"","title":"User Names for Authentication, Authorization, and Membership "},{"location":"security/user_names_for_authentication_authorization_and_membership/#user-names-are-converted-to-authorization-identifiers","text":"User names in the SnappyData system are known as authorization identifiers. The authorization identifier is a string that represents the name of the user if one was provided in the connection request. After the authorization identifier is passed to the SnappyData system, it becomes an SQL92Identifier. SQL92Identifier is a kind of identifier that represents a database object such as a table or column. A SQL92Identifier is case-insensitive (it is converted to all caps) unless it is delimited with double quotes. A SQL92Identifier is limited to 128 characters and has other limitations. All user names must be valid authorization identifiers even if user authentication is turned off, and even if all users are allowed access to all databases.","title":"User Names are Converted to Authorization Identifiers"},{"location":"security/user_names_for_authentication_authorization_and_membership/#handling-case-sensitivity-and-special-characters-in-user-names","text":"If an external authentication system is used, SnappyData does not convert a user's name to an authorization identifier until after authentication has occurred (but before the user is authorized). For example, with an example user named Fred: Within the user authentication system, Fred might be known as FRed. If the external user authentication service is case-sensitive, Fred must always be typed as: connect client 'localhost:1527;user=FRed;password=flintstone'; Within the SnappyData user authorization system, Fred becomes a case-insensitive authorization identifier. Here, FRed is known as FRED. Also consider a second example, where Fred has a slightly different name within the user authentication system: Within the user authentication system, Fred is known as Fred. You must now put double quotes around the username, because it is not a valid SQL92Identifier. SnappyData removes the double quotes when passing the name to the external authentication system. connect client 'localhost:1527;user=\"Fred!\";password=flintstone'; Within the SnappyData user authorization system, Fred now becomes a case-sensitive authorization identifier. In this case, Fred is known as Fred. As shown in the first example, the external authentication system may be case-sensitive, whereas the authorization identifier within SnappyData may not be. If your authentication system allows two distinct users whose names differ by case, delimit all user names within the connection request to make all user names case-sensitive within the SnappyData system. In addition, you must also delimit user names that do not conform to SQL92Identifier rules with double quotes.","title":"Handling Case Sensitivity and Special Characters in User Names"},{"location":"statistics/","text":"Evaluating Statistics for the System \u00b6 SnappyData provides statistics for analyzing system performance. Any member of a distributed system, including SnappyData servers, locators, and peer clients, can collect and archive this statistical data. SnappyData samples statistics at a configurable interval and writes them to an archive. The archives can be read at any time, including at runtime. You can view and analyze runtime or archived historical data using snappy-shell stats which is a command-line tool provided with the SnappyData product. Note SnappyData statistics use the Java System.nanoTimer for nanosecond timing. This method provides nanosecond precision, but not necessarily nanosecond accuracy. For more information, see the online Java documentation for System.nanoTimer for the JRE you are using with SnappyData. Runtime viewing of statistics archives files is not necessarily real-time, because of file system buffering. More Information Collecting System Statistics Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties. Using Visual Statistics Display (VSD) Tool The third-party VSD tool can be used to analyze statistics in a graphical way.","title":"Evaluating Statistics for the System"},{"location":"statistics/#evaluating-statistics-for-the-system","text":"SnappyData provides statistics for analyzing system performance. Any member of a distributed system, including SnappyData servers, locators, and peer clients, can collect and archive this statistical data. SnappyData samples statistics at a configurable interval and writes them to an archive. The archives can be read at any time, including at runtime. You can view and analyze runtime or archived historical data using snappy-shell stats which is a command-line tool provided with the SnappyData product. Note SnappyData statistics use the Java System.nanoTimer for nanosecond timing. This method provides nanosecond precision, but not necessarily nanosecond accuracy. For more information, see the online Java documentation for System.nanoTimer for the JRE you are using with SnappyData. Runtime viewing of statistics archives files is not necessarily real-time, because of file system buffering. More Information Collecting System Statistics Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties. Using Visual Statistics Display (VSD) Tool The third-party VSD tool can be used to analyze statistics in a graphical way.","title":"Evaluating Statistics for the System"},{"location":"statistics/collecting_system_stats/","text":"Collecting System Statistics \u00b6 Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties. You can enable statistics collection per-member using the boot properties: statistic-sampling-enabled enable-time-statistics statistic-archive-file Note You must include the statistic-archive-file and specify a valid file name in order to enable statistics collection. These boot properties help you configure a member's statistic archive file location and size: archive-disk-space-limit archive-file-size-limit statistic-sample-rate To collect statement-level statistics and time-based, statement-level statistics for a specific connection (rather than globally or per-member), use these connection properties with a peer client connection: enable-stats enable-timestats statistic-archive-file These properties can only be used with a peer client connect. Note Because of the overhead required for taking many timestamps, it is recommended that you enable time-based statistics only during testing and debugging. Use statement-level statistics only when the number of individual statements is small, such as when using prepared statements. SnappyData creates a separate statistics instance for each individual statement.","title":"Collecting System Statistics"},{"location":"statistics/collecting_system_stats/#collecting-system-statistics","text":"Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties. You can enable statistics collection per-member using the boot properties: statistic-sampling-enabled enable-time-statistics statistic-archive-file Note You must include the statistic-archive-file and specify a valid file name in order to enable statistics collection. These boot properties help you configure a member's statistic archive file location and size: archive-disk-space-limit archive-file-size-limit statistic-sample-rate To collect statement-level statistics and time-based, statement-level statistics for a specific connection (rather than globally or per-member), use these connection properties with a peer client connection: enable-stats enable-timestats statistic-archive-file These properties can only be used with a peer client connect. Note Because of the overhead required for taking many timestamps, it is recommended that you enable time-based statistics only during testing and debugging. Use statement-level statistics only when the number of individual statements is small, such as when using prepared statements. SnappyData creates a separate statistics instance for each individual statement.","title":"Collecting System Statistics"},{"location":"statistics/vsd/","text":"Using VSD to Analyze Statistics \u00b6 Note Visual Statistics Display (VSD) is a third-party tool and is not shipped with SnappyData. It is available from GemTalk Systems or Pivotal GemFire under their own respective licenses. The Visual Statistics Display (VSD) reads the sampled statistics from one or more archives and produces graphical displays for analysis. VSD is installed with SnappyData in the tools subdirectory. VSD\u2019s extensive online help offers complete reference information about the tool. Note Ensure the following for running VSD: Install 32-bit libraries on 64-bit Linux: \"yum install glibc.i686 libX11.i686\" on RHEL/CentOS \"apt-get install libc6:i386 libx11-6:i386\" on Ubuntu/Debian like systems Locally running X server. For example, an X server implementation like, XQuartz for Mac OS, Xming for Windows OS, and Xorg which is installed by default for Linux systems. More information Installing and Running VSD Transaction Performance Table Performance SQL Statement Performance Memory Usage Client Connections CPU Usage","title":"Using VSD to Analyze Statistics"},{"location":"statistics/vsd/#using-vsd-to-analyze-statistics","text":"Note Visual Statistics Display (VSD) is a third-party tool and is not shipped with SnappyData. It is available from GemTalk Systems or Pivotal GemFire under their own respective licenses. The Visual Statistics Display (VSD) reads the sampled statistics from one or more archives and produces graphical displays for analysis. VSD is installed with SnappyData in the tools subdirectory. VSD\u2019s extensive online help offers complete reference information about the tool. Note Ensure the following for running VSD: Install 32-bit libraries on 64-bit Linux: \"yum install glibc.i686 libX11.i686\" on RHEL/CentOS \"apt-get install libc6:i386 libx11-6:i386\" on Ubuntu/Debian like systems Locally running X server. For example, an X server implementation like, XQuartz for Mac OS, Xming for Windows OS, and Xorg which is installed by default for Linux systems. More information Installing and Running VSD Transaction Performance Table Performance SQL Statement Performance Memory Usage Client Connections CPU Usage","title":"Using VSD to Analyze Statistics"},{"location":"statistics/vsd/running_vsd/","text":"Installing and Running VSD \u00b6 Start the VSD tool, load statistics files, and maintain the view you want on your statistics. Install VSD \u00b6 Note Visual Statistics Display (VSD) is a third-party tool and is not shipped with SnappyData. It is available from GemTalk Systems or Pivotal GemFire under their own respective licenses. You can open the VSD tool by running bin/vsd in the installation packages from above. The VSD tool installation has the following subdirectories: bin . The scripts/binaries that can be used to run VSD lib . The jars and binary libraries needed to run VSD Start VSD \u00b6 Linux/Unix, MacOS: $ ./bin/vsd Windows OS: Run the vsd executable from command prompt or otherwise. > .\\bin\\vsd Load a Statistics File into VSD \u00b6 You have several options for loading a statistics file into VSD: Include the name of one or more statistics files on the VSD command line. Example: vsd <filename.gfs> ... Browse for an existing statistics file through Main > Load Data File. Type the full path in the Directories field, in the Files field select the file name, and then press OK to load the data file. Switch to a statistics file that you\u2019ve already loaded by clicking the down-arrow next to the Files field. After you load the data file, the VSD main window displays a list of entities for which statistics are available. VSD uses color to distinguish between entities that are still running (shown in green) and those that have stopped (shown in black). Maintain a Current View of the Data File \u00b6 If you select the menu item File > Auto Update , VSD automatically updates your display, and any associated charts, whenever the data file changes. Alternatively, you can choose File > Update periodically to update the display manually. About Statistics \u00b6 The statistics values (Y-axis) are plotted over time (X-axis). This makes it easy to see how statistics are trending, and to correlate different statistics. Some statistics are cumulative from when the SnappyData system was started. Other statistics are instantaneous values that may change in any way between sample collection. Cumulative statistics are best charted per second or per sample, so that the VSD chart is readable. Absolute values are best charted as No Filter.","title":"Installing and Running VSD"},{"location":"statistics/vsd/running_vsd/#installing-and-running-vsd","text":"Start the VSD tool, load statistics files, and maintain the view you want on your statistics.","title":"Installing and Running VSD"},{"location":"statistics/vsd/running_vsd/#install-vsd","text":"Note Visual Statistics Display (VSD) is a third-party tool and is not shipped with SnappyData. It is available from GemTalk Systems or Pivotal GemFire under their own respective licenses. You can open the VSD tool by running bin/vsd in the installation packages from above. The VSD tool installation has the following subdirectories: bin . The scripts/binaries that can be used to run VSD lib . The jars and binary libraries needed to run VSD","title":"Install VSD"},{"location":"statistics/vsd/running_vsd/#start-vsd","text":"Linux/Unix, MacOS: $ ./bin/vsd Windows OS: Run the vsd executable from command prompt or otherwise. > .\\bin\\vsd","title":"Start VSD"},{"location":"statistics/vsd/running_vsd/#load-a-statistics-file-into-vsd","text":"You have several options for loading a statistics file into VSD: Include the name of one or more statistics files on the VSD command line. Example: vsd <filename.gfs> ... Browse for an existing statistics file through Main > Load Data File. Type the full path in the Directories field, in the Files field select the file name, and then press OK to load the data file. Switch to a statistics file that you\u2019ve already loaded by clicking the down-arrow next to the Files field. After you load the data file, the VSD main window displays a list of entities for which statistics are available. VSD uses color to distinguish between entities that are still running (shown in green) and those that have stopped (shown in black).","title":"Load a Statistics File into VSD"},{"location":"statistics/vsd/running_vsd/#maintain-a-current-view-of-the-data-file","text":"If you select the menu item File > Auto Update , VSD automatically updates your display, and any associated charts, whenever the data file changes. Alternatively, you can choose File > Update periodically to update the display manually.","title":"Maintain a Current View of the Data File"},{"location":"statistics/vsd/running_vsd/#about-statistics","text":"The statistics values (Y-axis) are plotted over time (X-axis). This makes it easy to see how statistics are trending, and to correlate different statistics. Some statistics are cumulative from when the SnappyData system was started. Other statistics are instantaneous values that may change in any way between sample collection. Cumulative statistics are best charted per second or per sample, so that the VSD chart is readable. Absolute values are best charted as No Filter.","title":"About Statistics"},{"location":"statistics/vsd/vsd-connection-stats/","text":"Client Connections \u00b6 SnappyData provides several statistics to help you determine the frequency and duration of client connections to the distributed system. The following tables describe some commonly-used connection statistics. Examine the VSD output for information about additional connection statistics. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsIdle Description The number of client connections that were idle at the statistic sampling time. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsOpen Description The total number of client connections that were open (both active and idle connections) at the statistic sampling time. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsOpened, peerConnectionsOpened Description The total number of client connections that were successfully opened up to the statistic sampling time. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsFailed, peerConnectionsFailed Description The total number of unsuccessful connection attempts that were made up to the statistic sampling time. Example: Selecting Connection Statistics \u00b6 This figure illustrates how to select the connection statistics for graphing.","title":"Client Connections"},{"location":"statistics/vsd/vsd-connection-stats/#client-connections","text":"SnappyData provides several statistics to help you determine the frequency and duration of client connections to the distributed system. The following tables describe some commonly-used connection statistics. Examine the VSD output for information about additional connection statistics. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsIdle Description The number of client connections that were idle at the statistic sampling time. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsOpen Description The total number of client connections that were open (both active and idle connections) at the statistic sampling time. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsOpened, peerConnectionsOpened Description The total number of client connections that were successfully opened up to the statistic sampling time. Type ConnectionStats Name ConnectionStats Statistic clientConnectionsFailed, peerConnectionsFailed Description The total number of unsuccessful connection attempts that were made up to the statistic sampling time.","title":"Client Connections"},{"location":"statistics/vsd/vsd-connection-stats/#example-selecting-connection-statistics","text":"This figure illustrates how to select the connection statistics for graphing.","title":"Example: Selecting Connection Statistics"},{"location":"statistics/vsd/vsd_cpu/","text":"CPU Usage \u00b6 SnappyData provides host and JVM statistics for examining system load. An example follows the table. Type LinuxSystemStats Name <hostname> Statistic cpus Description Number of CPUs on this host. Type LinuxSystemStats Name <hostname> Statistic cpuActive, cpuSystem, cpuUser Description Percentage of the total available time that has been used in a non-idle state, in system (kernel) code and in user code. This is aggregated across all CPUs. Type LinuxSystemStats Name <hostname> Statistic loadAverage1, loadAverage5, loadAverage15 Description Average number of threads in the run queue or waiting for disk I/O over the last 1, 5, 15 minutes. Type VMStats Name vmStats Statistic processCpuTime Description CPU time, measured in nanoseconds, used by the process. Example: CPU Usage by System and User \u00b6 This VSD chart shows the CPU usage in a fabric server, and breaks it down into system and user. The majority of the time is spent in user code. The first few minutes correspond to loading various tables. The rest is the main workload, during which many clients are executing transactions.","title":"CPU Usage"},{"location":"statistics/vsd/vsd_cpu/#cpu-usage","text":"SnappyData provides host and JVM statistics for examining system load. An example follows the table. Type LinuxSystemStats Name <hostname> Statistic cpus Description Number of CPUs on this host. Type LinuxSystemStats Name <hostname> Statistic cpuActive, cpuSystem, cpuUser Description Percentage of the total available time that has been used in a non-idle state, in system (kernel) code and in user code. This is aggregated across all CPUs. Type LinuxSystemStats Name <hostname> Statistic loadAverage1, loadAverage5, loadAverage15 Description Average number of threads in the run queue or waiting for disk I/O over the last 1, 5, 15 minutes. Type VMStats Name vmStats Statistic processCpuTime Description CPU time, measured in nanoseconds, used by the process.","title":"CPU Usage"},{"location":"statistics/vsd/vsd_cpu/#example-cpu-usage-by-system-and-user","text":"This VSD chart shows the CPU usage in a fabric server, and breaks it down into system and user. The majority of the time is spent in user code. The first few minutes correspond to loading various tables. The rest is the main workload, during which many clients are executing transactions.","title":"Example: CPU Usage by System and User"},{"location":"statistics/vsd/vsd_memory/","text":"Memory Usage \u00b6 SnappyData provides statistics for system memory, JVM heap, garbage collection, and table sizes. You can use these statistics to analyze your application's memory usage. An example follows the table. Type CachePerfStats Name RegionStats-<table> and RegsionStats-partition-<table> Statistic entries Description Number of rows in the replicated or partitioned table. Type PartitionedRegionStats Name PartitionedRegion/<schema>/<table>Statistics Statistic dataStoreEntryCount and dataStoreBytesInUse Description Number of rows/bytes in a partitioned table, including redundant copies. Type VMMemoryUsageStats Name vmHeapMemoryStats Statistic usedMemory and maxMemory Description Used heap and maximum heap, in bytes. Type VMMemoryUsageStats Name OffHeapMemoryStats Statistic usedMemory, maxMemory, freeMemory Description Used off-heap memory, maximum (amount allocated) off-heap memory, and free off-heap memory in bytes. Type VMMemoryPoolStats Name ParSurvivorSpace, ParEdenSpace, CMSOldGen Statistic currentUsedMemory Description Estimated used memory, in bytes, for each heap memory pool. Type VMGCStats Name ParNew, ConcurrentMarkSweep Statistic collectionTime Description Approximate elapsed time that this garbage collector spent doing collections. Type LinuxSystemStats Name <hostname> Statistic pagesSwappedIn, pagesSwappedOut Description Number of pages that have been brought into memory from disk or flushed out of memory to disk by the operating system. These paging operations seriously degrade performance. Example: Heap Usage \u00b6 This VSD chart shows the heap usage in a fabric server plotted against the entry counts in each of the replicated and partitioned tables used in an application. This suggests that heap is growing over time due to an increase in the number of rows in the ORDER_LINE table. Several associated tables are growing in size as well. This growth can be verified by looking at the bytes used by each of the partitioned tables. The ORDER_LINE table is responsible for most of the heap growth.","title":"Memory Usage"},{"location":"statistics/vsd/vsd_memory/#memory-usage","text":"SnappyData provides statistics for system memory, JVM heap, garbage collection, and table sizes. You can use these statistics to analyze your application's memory usage. An example follows the table. Type CachePerfStats Name RegionStats-<table> and RegsionStats-partition-<table> Statistic entries Description Number of rows in the replicated or partitioned table. Type PartitionedRegionStats Name PartitionedRegion/<schema>/<table>Statistics Statistic dataStoreEntryCount and dataStoreBytesInUse Description Number of rows/bytes in a partitioned table, including redundant copies. Type VMMemoryUsageStats Name vmHeapMemoryStats Statistic usedMemory and maxMemory Description Used heap and maximum heap, in bytes. Type VMMemoryUsageStats Name OffHeapMemoryStats Statistic usedMemory, maxMemory, freeMemory Description Used off-heap memory, maximum (amount allocated) off-heap memory, and free off-heap memory in bytes. Type VMMemoryPoolStats Name ParSurvivorSpace, ParEdenSpace, CMSOldGen Statistic currentUsedMemory Description Estimated used memory, in bytes, for each heap memory pool. Type VMGCStats Name ParNew, ConcurrentMarkSweep Statistic collectionTime Description Approximate elapsed time that this garbage collector spent doing collections. Type LinuxSystemStats Name <hostname> Statistic pagesSwappedIn, pagesSwappedOut Description Number of pages that have been brought into memory from disk or flushed out of memory to disk by the operating system. These paging operations seriously degrade performance.","title":"Memory Usage"},{"location":"statistics/vsd/vsd_memory/#example-heap-usage","text":"This VSD chart shows the heap usage in a fabric server plotted against the entry counts in each of the replicated and partitioned tables used in an application. This suggests that heap is growing over time due to an increase in the number of rows in the ORDER_LINE table. Several associated tables are growing in size as well. This growth can be verified by looking at the bytes used by each of the partitioned tables. The ORDER_LINE table is responsible for most of the heap growth.","title":"Example: Heap Usage"},{"location":"statistics/vsd/vsd_statements/","text":"SQL Statement Performance \u00b6 With statement-level statistics enabled, statistics are available for all DML statements. An example follows the table. Type StatementStats Name INSERT<dml>, SELECT<dml>, UPDATE<dml>, DELETE<dml> Statistic NumExecutesEnded Description The number of times that a statement was executed. Example: Prepared Statements \u00b6 This VSD chart shows statistics for two fabric servers for prepared statements that insert rows in the history table. The first round of inserts occurs during initial data loading. The second round occurs as part of normal business operations. This chart also shows that the application used different prepared statements for data loading and subsequent transactions, with slightly different ordering in the fields. If the statements had been identical, there would be only one instance per server.","title":"SQL Statement Performance"},{"location":"statistics/vsd/vsd_statements/#sql-statement-performance","text":"With statement-level statistics enabled, statistics are available for all DML statements. An example follows the table. Type StatementStats Name INSERT<dml>, SELECT<dml>, UPDATE<dml>, DELETE<dml> Statistic NumExecutesEnded Description The number of times that a statement was executed.","title":"SQL Statement Performance"},{"location":"statistics/vsd/vsd_statements/#example-prepared-statements","text":"This VSD chart shows statistics for two fabric servers for prepared statements that insert rows in the history table. The first round of inserts occurs during initial data loading. The second round occurs as part of normal business operations. This chart also shows that the application used different prepared statements for data loading and subsequent transactions, with slightly different ordering in the fields. If the statements had been identical, there would be only one instance per server.","title":"Example: Prepared Statements"},{"location":"statistics/vsd/vsd_tables/","text":"Table Performance \u00b6 You can get an idea of the relative performance of inserts, updates, and selects by looking at the underlying statistics. An example follows the table. Type CachePerfStats Name RegionStats-<table> Statistic creates, puts, deletes Description Number of times that an entry is added to, replaced in, or read from the replicated table. Type CachePerfStats Name RegionStats-partition-<table> Statistic creates, puts, deletes Description Number of times that an entry is added to, replaced in, or read from the partitioned table. Example: Read and Write Performance \u00b6 These VSD charts show the rates of reads and writes to the various tables in the application. You can use these statistics to see that the NEW_ORDER table is growing over time. After the initial orders are loaded, new orders are being placed (created) faster than customers are paying for (destroying) them.","title":"Table Performance"},{"location":"statistics/vsd/vsd_tables/#table-performance","text":"You can get an idea of the relative performance of inserts, updates, and selects by looking at the underlying statistics. An example follows the table. Type CachePerfStats Name RegionStats-<table> Statistic creates, puts, deletes Description Number of times that an entry is added to, replaced in, or read from the replicated table. Type CachePerfStats Name RegionStats-partition-<table> Statistic creates, puts, deletes Description Number of times that an entry is added to, replaced in, or read from the partitioned table.","title":"Table Performance"},{"location":"statistics/vsd/vsd_tables/#example-read-and-write-performance","text":"These VSD charts show the rates of reads and writes to the various tables in the application. You can use these statistics to see that the NEW_ORDER table is growing over time. After the initial orders are loaded, new orders are being placed (created) faster than customers are paying for (destroying) them.","title":"Example: Read and Write Performance"},{"location":"statistics/vsd/vsd_transactions/","text":"Transaction Performance \u00b6 SnappyData provides statistics for transaction commits, rollbacks, and failures You can monitor SnappyData transactions with VSD. You can use these statistics to see the transaction rate. An example follows the table. Type CachePerfStats Name cachePerfStats Statistic txCommits, txRollbacks Description The number of times a transaction has committed or rolled back. Type CachePerfStats Name cachePerfStats Statistic txSuccessLifeTime, txRollbackLifeTime Description The total time spent in a transaction that committed or rolled back. Example: Transaction Rate and Latency \u00b6 This VSD chart shows the aggregated transaction rate across three FabricServer instances, along with the rollback rate. With time statistics enabled, you can compute the average latency for each sample point as the total time spent in the transaction divided by the number of transactions in each sample. The chart also shows that the average latency across all samples is 6.76 ms.","title":"Transaction Performance"},{"location":"statistics/vsd/vsd_transactions/#transaction-performance","text":"SnappyData provides statistics for transaction commits, rollbacks, and failures You can monitor SnappyData transactions with VSD. You can use these statistics to see the transaction rate. An example follows the table. Type CachePerfStats Name cachePerfStats Statistic txCommits, txRollbacks Description The number of times a transaction has committed or rolled back. Type CachePerfStats Name cachePerfStats Statistic txSuccessLifeTime, txRollbackLifeTime Description The total time spent in a transaction that committed or rolled back.","title":"Transaction Performance"},{"location":"statistics/vsd/vsd_transactions/#example-transaction-rate-and-latency","text":"This VSD chart shows the aggregated transaction rate across three FabricServer instances, along with the rollback rate. With time statistics enabled, you can compute the average latency for each sample point as the total time spent in the transaction divided by the number of transactions in each sample. The chart also shows that the average latency across all samples is 6.76 ms.","title":"Example: Transaction Rate and Latency"},{"location":"troubleshooting/","text":"Troubleshooting Common Problems \u00b6 The goal of this section is to determine why something does not work as expected and explain how to resolve the problem. The following topics are covered in this section: Member Startup Problems Recovering from a ConflictingPersistentDataException Preventing disk full errors Recovering from disk full errors Resolving Catalog Inconsistency Issues Collecting logs, stats and dumps using the collect-debug-artifacts script Troubleshooting Error Messages","title":"Troubleshooting Common Problems"},{"location":"troubleshooting/#troubleshooting-common-problems","text":"The goal of this section is to determine why something does not work as expected and explain how to resolve the problem. The following topics are covered in this section: Member Startup Problems Recovering from a ConflictingPersistentDataException Preventing disk full errors Recovering from disk full errors Resolving Catalog Inconsistency Issues Collecting logs, stats and dumps using the collect-debug-artifacts script Troubleshooting Error Messages","title":"Troubleshooting Common Problems"},{"location":"troubleshooting/catalog_inconsistency/","text":"Resolving Catalog Inconsistency Issues \u00b6 A SnappyData catalog internally maintains table metadata in two catalogs: Data dictionary required by SnappyData store Hive metastore required by Spark In rare conditions, SnappyData catalog may become inconsistent, if an entry for the table exists only in one of the catalogs instead of exisiting in both. One of the symptoms for such an inconsistency is that you get an error that indicates that the table you are creating already exists. However, when you drop the same table, the table is not found. For example: snappy> create table t1(col1 int); ERROR 42000: (SQLState=42000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Syntax error or analysis exception: createTable: Table APP.T1 already exists.; snappy> drop table t1; ERROR 42X05: (SQLState=42X05 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Table/View 'APP.T1' does not exist. In such cases, you can use a system procedure, SYS.REPAIR_CATALOG (Boolean removeInconsistentEntries , Boolean removeTablesWithData ) to remove catalog inconsistencies. The following parameters are accepted by the SYS.REPAIR_CATALOG procedure: removeInconsistentEntries - If true , removes inconsistent entries from the catalog. removeTablesWithData - If true , removes entries for tables even if those tables contain data. By default the entries are not removed. Example: Resolving Catalog Inconsistency Issues \u00b6 When both parameters are set to false , the SYS.REPAIR_CATALOG procedure checks for any inconsistencies in the catalog and prints warning messages in the SnappyData system server log. snappy> call sys.repair_catalog('false', 'false'); In case of inconsistencies, the log will contain messages as shown in the following example: 18/08/06 17:28:36.456 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1] 18/08/06 17:28:36.457 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Use system procedure SYS.REPAIR_CATALOG() to remove inconsistency You can then remove the catalog inconsistency by passing removeInconsistentEntries parameter as true . snappy> call sys.repair_catalog('true', 'false'); Later, you can examine the log to check which entries were removed. Following is a sample log for reference: 18/08/06 17:34:26.548 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1] 18/08/06 17:34:26.548 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Removing table APP.T1 from Hive metastore","title":"Resolving Catalog Inconsistency Issues"},{"location":"troubleshooting/catalog_inconsistency/#resolving-catalog-inconsistency-issues","text":"A SnappyData catalog internally maintains table metadata in two catalogs: Data dictionary required by SnappyData store Hive metastore required by Spark In rare conditions, SnappyData catalog may become inconsistent, if an entry for the table exists only in one of the catalogs instead of exisiting in both. One of the symptoms for such an inconsistency is that you get an error that indicates that the table you are creating already exists. However, when you drop the same table, the table is not found. For example: snappy> create table t1(col1 int); ERROR 42000: (SQLState=42000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Syntax error or analysis exception: createTable: Table APP.T1 already exists.; snappy> drop table t1; ERROR 42X05: (SQLState=42X05 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Table/View 'APP.T1' does not exist. In such cases, you can use a system procedure, SYS.REPAIR_CATALOG (Boolean removeInconsistentEntries , Boolean removeTablesWithData ) to remove catalog inconsistencies. The following parameters are accepted by the SYS.REPAIR_CATALOG procedure: removeInconsistentEntries - If true , removes inconsistent entries from the catalog. removeTablesWithData - If true , removes entries for tables even if those tables contain data. By default the entries are not removed.","title":"Resolving Catalog Inconsistency Issues"},{"location":"troubleshooting/catalog_inconsistency/#example-resolving-catalog-inconsistency-issues","text":"When both parameters are set to false , the SYS.REPAIR_CATALOG procedure checks for any inconsistencies in the catalog and prints warning messages in the SnappyData system server log. snappy> call sys.repair_catalog('false', 'false'); In case of inconsistencies, the log will contain messages as shown in the following example: 18/08/06 17:28:36.456 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1] 18/08/06 17:28:36.457 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Use system procedure SYS.REPAIR_CATALOG() to remove inconsistency You can then remove the catalog inconsistency by passing removeInconsistentEntries parameter as true . snappy> call sys.repair_catalog('true', 'false'); Later, you can examine the log to check which entries were removed. Following is a sample log for reference: 18/08/06 17:34:26.548 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1] 18/08/06 17:34:26.548 IST ThriftProcessor-3<tid=0x82> WARN snappystore: CATALOG: Removing table APP.T1 from Hive metastore","title":"Example: Resolving Catalog Inconsistency Issues"},{"location":"troubleshooting/collect_debug_artifacts/","text":"Collecting Logs, Stats, and Dumps using the collect-debug-artifacts script \u00b6 This section uses the term 'node' frequently. A node denotes a server or a locator member when a purely SnappyData system is there. In a SnappyData distributed system a node can mean server, locator or lead member. The script collect-debug-artifacts enables you to collect the debug information like logs and stats. It also has an option to dump stacks of the running system. Details of all the options and capabilities of the script can be found below. The main purpose of this is to ease the collection of these information. The script collects all the artifacts node wise and outputs a tar file which contains member wise information. Pre-requisites for running the script: The script assumes certain conditions to be fulfilled before it is invoked. Please ensure that these requirements are fulfilled because the script does not validate these. The conditions are: This script is expected to be run by a user who has read and write permissions on the output directories of all the SnappyData nodes. The user should have one way passwordless ssh setup from one machine to the other machines where the SnappyData nodes are running. Below is the usage of the script <linux-shell> ./sbin/collect-debug-artifacts.sh -h Usage: collect-debug-artifacts [ -c conffile|--conf=conffile|--config=conffile ] [ -o resultdir|--out=resultdir|--outdir=resultdir ] [ -h|--help ] [ -a|--all ] [ -d|--dump ] [ -v|--verbose ] [ -s starttimestamp|--start=starttimestamp ] [ -e endtimestamp|--end=endtimestamp ] [ -x debugtarfile|--extract=debugtarfile ] Timestamp format: YYYY-MM-DD HH:MM[:SS] Options: All the options of the script are optional. By default the script tries to get the current logs. All the logs starting from the last restart and the last file before that. It also brings all the stat file in the output directory. However if you want to change this behavior of the script you can use the following options to collect the debug information as per your requirements. Please note that no stack dumps are collected by default. You need to use the '-d, --dump' option to get the stack dumps. -h, --help Prints a usage message summary briefly summarizing the command line options -c, --conf The script uses a configration file which has three configuration elements. 1. MEMBERS_FILE -- This is a text file which has member information. Each line has the host machine name followed by the full path to the run directory of the member. This file is generated automatically when the sbin/start-all-scripts.sh is used. 2. NO_OF_STACK_DUMPS -- This parameter tells the script that how many stack dumps will be attempted per member/node of the running system. 3. INTERVAL_BETWEEN_DUMPS -- The amount of time in seconds the script waits between registering stack dumps. -o, --out, --outdir The directory where the output file in the form of tar, will be created. -a, --all With '-a or --all' option all the logs and stats file are collected from each members output directory. -d, --dump Stack dumps are not collected by default or with -a, --all option. The user need to explicitly provide this argument if the stack dumps need to be collected. -v, --verbose verbose mode is on. -s, --start The script can also be asked to collect log files for specified time interval. The time interval can be specified using the start time and an end time parameter. Both the parameter needs to be specified. The format in which the time stamp can be specified is 'YYYY-MM-DD HH:MM[:SS]' -x, --extract=debugtarfile To extract the contents of the tar file. Timestamp format: YYYY-MM-DD HH:MM[:SS] Along with these options, if you want to filter unwanted lines from the logs, then you must specify EGREP_INVERT_MATCH_PATTERNS with a list of patterns in the conf/debug.conf.template . The lines that match those patterns will not be collected by the script. For example: EGREP_INVERT_MATCH_PATTERNS='tid|gemfire' For offline analysis, as the production logs can be pretty huge and not easily sharable, you can provide a list of patterns which can be excluded for collection. A list of patterns can be specified in the conf > debug.conf file. Before doing this, you must rename the debug.conf.template file to debug.conf . The lines matching those patterns will be ignored by the script and excluded for collection.","title":"Collecting Logs, Stats and Dumps Using the collect-debug-artifacts Script"},{"location":"troubleshooting/collect_debug_artifacts/#collecting-logs-stats-and-dumps-using-the-collect-debug-artifacts-script","text":"This section uses the term 'node' frequently. A node denotes a server or a locator member when a purely SnappyData system is there. In a SnappyData distributed system a node can mean server, locator or lead member. The script collect-debug-artifacts enables you to collect the debug information like logs and stats. It also has an option to dump stacks of the running system. Details of all the options and capabilities of the script can be found below. The main purpose of this is to ease the collection of these information. The script collects all the artifacts node wise and outputs a tar file which contains member wise information. Pre-requisites for running the script: The script assumes certain conditions to be fulfilled before it is invoked. Please ensure that these requirements are fulfilled because the script does not validate these. The conditions are: This script is expected to be run by a user who has read and write permissions on the output directories of all the SnappyData nodes. The user should have one way passwordless ssh setup from one machine to the other machines where the SnappyData nodes are running. Below is the usage of the script <linux-shell> ./sbin/collect-debug-artifacts.sh -h Usage: collect-debug-artifacts [ -c conffile|--conf=conffile|--config=conffile ] [ -o resultdir|--out=resultdir|--outdir=resultdir ] [ -h|--help ] [ -a|--all ] [ -d|--dump ] [ -v|--verbose ] [ -s starttimestamp|--start=starttimestamp ] [ -e endtimestamp|--end=endtimestamp ] [ -x debugtarfile|--extract=debugtarfile ] Timestamp format: YYYY-MM-DD HH:MM[:SS] Options: All the options of the script are optional. By default the script tries to get the current logs. All the logs starting from the last restart and the last file before that. It also brings all the stat file in the output directory. However if you want to change this behavior of the script you can use the following options to collect the debug information as per your requirements. Please note that no stack dumps are collected by default. You need to use the '-d, --dump' option to get the stack dumps. -h, --help Prints a usage message summary briefly summarizing the command line options -c, --conf The script uses a configration file which has three configuration elements. 1. MEMBERS_FILE -- This is a text file which has member information. Each line has the host machine name followed by the full path to the run directory of the member. This file is generated automatically when the sbin/start-all-scripts.sh is used. 2. NO_OF_STACK_DUMPS -- This parameter tells the script that how many stack dumps will be attempted per member/node of the running system. 3. INTERVAL_BETWEEN_DUMPS -- The amount of time in seconds the script waits between registering stack dumps. -o, --out, --outdir The directory where the output file in the form of tar, will be created. -a, --all With '-a or --all' option all the logs and stats file are collected from each members output directory. -d, --dump Stack dumps are not collected by default or with -a, --all option. The user need to explicitly provide this argument if the stack dumps need to be collected. -v, --verbose verbose mode is on. -s, --start The script can also be asked to collect log files for specified time interval. The time interval can be specified using the start time and an end time parameter. Both the parameter needs to be specified. The format in which the time stamp can be specified is 'YYYY-MM-DD HH:MM[:SS]' -x, --extract=debugtarfile To extract the contents of the tar file. Timestamp format: YYYY-MM-DD HH:MM[:SS] Along with these options, if you want to filter unwanted lines from the logs, then you must specify EGREP_INVERT_MATCH_PATTERNS with a list of patterns in the conf/debug.conf.template . The lines that match those patterns will not be collected by the script. For example: EGREP_INVERT_MATCH_PATTERNS='tid|gemfire' For offline analysis, as the production logs can be pretty huge and not easily sharable, you can provide a list of patterns which can be excluded for collection. A list of patterns can be specified in the conf > debug.conf file. Before doing this, you must rename the debug.conf.template file to debug.conf . The lines matching those patterns will be ignored by the script and excluded for collection.","title":"Collecting Logs, Stats, and Dumps using the collect-debug-artifacts script"},{"location":"troubleshooting/member_startup_problems/","text":"Member Startup Problems \u00b6 This section provides information and resolutions for the issues faced during the startup of cluster members. The following issues are included here: Delayed startup due to unavailable disk stores Delayed startup due to missing disk stores To avoid delayed startup and recovery, the following actions are recommended: Use the built-in snappy-start-all.sh and snappy-stop-all.sh scripts to start and stop the cluster. If for some reason those scripts are not used, then when possible, first shut down the data store members after disk stores have been synchronized in the system. Shut down remaining locator members after the data stores have stopped. Ensure that all persistent members are restarted properly. See Recovering from a ConflictingPersistentDataException for more information. Delayed Startup Due to Unavailable Disk Stores \u00b6 When you start SnappyData members, startup delays can occur if specific disk store files on other members are unavailable. This is part of the normal startup behavior and is designed to help ensure data consistency. For example, consider the following startup message for a locator ( locator2 ): SnappyData Locator pid: 23537 status: waiting Waiting for DataDictionary (DiskId: 531fc5bb-1720-4836-a468-3d738a21af63, Location: /snappydata/locator2/./datadictionary) on: [DiskId: aa77785a-0f03-4441-84f7-6eb6547d7833, Location: /snappydata/server1/./datadictionary] [DiskId: f417704b-fff4-4b99-81a2-75576d673547, Location: /snappydata/locator1/./datadictionary] Here, the startup messages indicate that locator2 is waiting for the persistent datadictionary files on locator1 and server1 to become available. SnappyData always persists the data dictionary for indexes and tables that you create, even if you do not configure those tables to persist their stored data. The startup messages above indicate that locator1 or locator2 might potentially store a newer copy of the data dictionary for the distributed system. Continuing the startup by booting the server1 data store yields: Starting SnappyData Server using locators for peer discovery: localhost[10337],localhost[10338] Starting network server for SnappyData Server at address localhost/127.0.0.1[1529] Logs generated in /snappydata/server1/gfxdserver.log The server is still starting. 15 seconds have elapsed since the last log message: Region /_DDL_STMTS_META_REGION has potentially stale data. It is waiting for another member to recover the latest data. My persistent id: DiskStore ID: aa77785a-0f03-4441-84f7-6eb6547d7833 Name: Location: /10.0.1.31:/snappydata/server1/./datadictionary up Members with potentially new data: [ DiskStore ID: f417704b-fff4-4b99-81a2-75576d673547 Name: Location: /10.0.1.31:/snappydata/locator1/./datadictionary ] Use the \"snappy-shell list-missing-disk-stores\" command to see all disk stores that are being waited on by other members. The data store startup messages indicate that locator1 has \"potentially new data\" for the data dictionary. In this case, both locator2 and server1 were shut down before locator1 in the system, so those members are waiting on locator1 to ensure that they have the latest version of the data dictionary. The above messages for data stores and locators may indicate that some members were not started. If the indicated disk store persistence files are available on the missing member, simply start that member and allow the running members to recover. For example, in the above system you would simply start locator1 and allow locator2 and server1 to synchronize their data. Delayed Startup Due to Missing Disk Stores \u00b6 Sometimes a cluster does not get started, if the disk store files are missing from one of servers in the cluster. For example, you start a cluster that consists of server1 and server2 . Suppose the disk store files in server1 are unavailable due to corruption or deletion. server1 , where the files were missing, attempts to start up as a new member, but it cannot due to InternalGemFireError and server2 cannot start because it is waiting for the missing disk stores on server1 . In such a case, you can unblock the waiting server. In case of more than two servers, despite of unblocking the waiting diskstores, one server can be still waiting upon the dependent server to come up. In such a case, change the order of the servers in the conf file and then restart the cluster. Unblocking the Disk Store \u00b6 In the following sample startup log message that is displayed, you are notified that the server1 which has process ID number (PID) 21474 cannot come up because it joined the cluster as a new member and server2 with PID 21582 is waiting for the missing disk stores on server1 . SnappyData Server pid: 21474 status: stopped Error starting server process: InternalGemFireError: None of the previous persistent node is up. - See log file for details. SnappyData Server pid: 21582 status: waiting Member disk state is not the most current. Table __PR._B__APP_ADJUSTMENT_4 at location /home/xyz/snappy/snappydata/server1/snappy-internal-delta is waiting for disk recovery from following members: [/127.0.0.1] [DiskId: 9791b9ff-7df3-44e8-99c8-7d62a3387002, Location: /home/xyz/snappy/snappydata/server1/snappy-internal-delta] Execute the ./bin/snappy list-missing-disk-stores -locators=<host>:<peer-discovery-port> command to view all the disk stores that are missing and awaited upon by other servers in the cluster. Check the messages in the start_snappyserver.log file of the server which is waiting for the missing disk stores. The following sample message is displayed in the start_snappyserver.log file for the servers: [info 2018/07/10 12:00:16.302 IST <Recovery thread for bucket _B__APP_ADJUSTMENT_4> tid=0x4c] Region /APP/SNAPPYSYS_INTERNAL____ADJUSTMENT_COLUMN_STORE_, bucket 4 has potentially stale data. It is waiting for another member to recover the latest data. My persistent id: DiskStore ID: 93e7e9cf-a513-4c67-89c3-da7e94a08efb Location: /127.0.0.1:/home/xyz/snappy/snappydata/server2 Members with potentially new data: [ DiskStore ID: 9791b9ff-7df3-44e8-99c8-7d62a3387002 Location: /127.0.0.1:/home/xyz/snappy/snappydata/server1 ] In this sample log message, the diskID of server2 is waiting upon the diskID of server1 , which is missing. To start server2 , the waiting disk store in that server must be unblocked. Here it is shown that the diskID 93e7e9cf-a513-4c67-89c3-da7e94a08efb of server2 is waiting upon the diskID 9791b9ff-7df3-44e8-99c8-7d62a3387002 of server1 , which is missing. Run the unblock-disk-store utility, in the following format, to unblock the waiting disk store: ./bin/snappy unblock-disk-store <diskID of the waiting server> -locators=<host>:<peer-discovery-port> For example, ./bin/snappy unblock-disk-store 93e7e9cf-a513-4c67-89c3-da7e94a08efb -locators=localhost:10334 Restart the cluster and keep unblocking such disk stores that are displayed in the logs until all the servers reach the running status. Note There is no loss of data when you unblock the disk stores. Rebalancing Data on Servers \u00b6 After unblocking the disk stores, if you notice that one of the server in the cluster has more data as compared to the other servers, you can distribute the data among the servers. This ensures that each server carries almost equal data. To balance the data equally on the servers, do the following: Connect to snappy shell and obtain the jdbc client connection. Run the rebalance command. snappy> call sys.rebalance_all_buckets(); Revoking Disk Stores that Prevent Startup \u00b6 If a member cannot be restarted even after unblocking the disk store and restarting after re-ordering the servers in the conf file, only then use the revoke-missing-disk-store command. Caution This can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the revoke-missing-disk-store command as a last resort. Contact support@tibco.com if you need to use the revoke-missing-disk-store command.","title":"Member Startup Problems"},{"location":"troubleshooting/member_startup_problems/#member-startup-problems","text":"This section provides information and resolutions for the issues faced during the startup of cluster members. The following issues are included here: Delayed startup due to unavailable disk stores Delayed startup due to missing disk stores To avoid delayed startup and recovery, the following actions are recommended: Use the built-in snappy-start-all.sh and snappy-stop-all.sh scripts to start and stop the cluster. If for some reason those scripts are not used, then when possible, first shut down the data store members after disk stores have been synchronized in the system. Shut down remaining locator members after the data stores have stopped. Ensure that all persistent members are restarted properly. See Recovering from a ConflictingPersistentDataException for more information.","title":"Member Startup Problems"},{"location":"troubleshooting/member_startup_problems/#delayed-startup-due-to-unavailable-disk-stores","text":"When you start SnappyData members, startup delays can occur if specific disk store files on other members are unavailable. This is part of the normal startup behavior and is designed to help ensure data consistency. For example, consider the following startup message for a locator ( locator2 ): SnappyData Locator pid: 23537 status: waiting Waiting for DataDictionary (DiskId: 531fc5bb-1720-4836-a468-3d738a21af63, Location: /snappydata/locator2/./datadictionary) on: [DiskId: aa77785a-0f03-4441-84f7-6eb6547d7833, Location: /snappydata/server1/./datadictionary] [DiskId: f417704b-fff4-4b99-81a2-75576d673547, Location: /snappydata/locator1/./datadictionary] Here, the startup messages indicate that locator2 is waiting for the persistent datadictionary files on locator1 and server1 to become available. SnappyData always persists the data dictionary for indexes and tables that you create, even if you do not configure those tables to persist their stored data. The startup messages above indicate that locator1 or locator2 might potentially store a newer copy of the data dictionary for the distributed system. Continuing the startup by booting the server1 data store yields: Starting SnappyData Server using locators for peer discovery: localhost[10337],localhost[10338] Starting network server for SnappyData Server at address localhost/127.0.0.1[1529] Logs generated in /snappydata/server1/gfxdserver.log The server is still starting. 15 seconds have elapsed since the last log message: Region /_DDL_STMTS_META_REGION has potentially stale data. It is waiting for another member to recover the latest data. My persistent id: DiskStore ID: aa77785a-0f03-4441-84f7-6eb6547d7833 Name: Location: /10.0.1.31:/snappydata/server1/./datadictionary up Members with potentially new data: [ DiskStore ID: f417704b-fff4-4b99-81a2-75576d673547 Name: Location: /10.0.1.31:/snappydata/locator1/./datadictionary ] Use the \"snappy-shell list-missing-disk-stores\" command to see all disk stores that are being waited on by other members. The data store startup messages indicate that locator1 has \"potentially new data\" for the data dictionary. In this case, both locator2 and server1 were shut down before locator1 in the system, so those members are waiting on locator1 to ensure that they have the latest version of the data dictionary. The above messages for data stores and locators may indicate that some members were not started. If the indicated disk store persistence files are available on the missing member, simply start that member and allow the running members to recover. For example, in the above system you would simply start locator1 and allow locator2 and server1 to synchronize their data.","title":"Delayed Startup Due to Unavailable Disk Stores"},{"location":"troubleshooting/member_startup_problems/#delayed-startup-due-to-missing-disk-stores","text":"Sometimes a cluster does not get started, if the disk store files are missing from one of servers in the cluster. For example, you start a cluster that consists of server1 and server2 . Suppose the disk store files in server1 are unavailable due to corruption or deletion. server1 , where the files were missing, attempts to start up as a new member, but it cannot due to InternalGemFireError and server2 cannot start because it is waiting for the missing disk stores on server1 . In such a case, you can unblock the waiting server. In case of more than two servers, despite of unblocking the waiting diskstores, one server can be still waiting upon the dependent server to come up. In such a case, change the order of the servers in the conf file and then restart the cluster.","title":"Delayed Startup Due to Missing Disk Stores"},{"location":"troubleshooting/member_startup_problems/#unblocking-the-disk-store","text":"In the following sample startup log message that is displayed, you are notified that the server1 which has process ID number (PID) 21474 cannot come up because it joined the cluster as a new member and server2 with PID 21582 is waiting for the missing disk stores on server1 . SnappyData Server pid: 21474 status: stopped Error starting server process: InternalGemFireError: None of the previous persistent node is up. - See log file for details. SnappyData Server pid: 21582 status: waiting Member disk state is not the most current. Table __PR._B__APP_ADJUSTMENT_4 at location /home/xyz/snappy/snappydata/server1/snappy-internal-delta is waiting for disk recovery from following members: [/127.0.0.1] [DiskId: 9791b9ff-7df3-44e8-99c8-7d62a3387002, Location: /home/xyz/snappy/snappydata/server1/snappy-internal-delta] Execute the ./bin/snappy list-missing-disk-stores -locators=<host>:<peer-discovery-port> command to view all the disk stores that are missing and awaited upon by other servers in the cluster. Check the messages in the start_snappyserver.log file of the server which is waiting for the missing disk stores. The following sample message is displayed in the start_snappyserver.log file for the servers: [info 2018/07/10 12:00:16.302 IST <Recovery thread for bucket _B__APP_ADJUSTMENT_4> tid=0x4c] Region /APP/SNAPPYSYS_INTERNAL____ADJUSTMENT_COLUMN_STORE_, bucket 4 has potentially stale data. It is waiting for another member to recover the latest data. My persistent id: DiskStore ID: 93e7e9cf-a513-4c67-89c3-da7e94a08efb Location: /127.0.0.1:/home/xyz/snappy/snappydata/server2 Members with potentially new data: [ DiskStore ID: 9791b9ff-7df3-44e8-99c8-7d62a3387002 Location: /127.0.0.1:/home/xyz/snappy/snappydata/server1 ] In this sample log message, the diskID of server2 is waiting upon the diskID of server1 , which is missing. To start server2 , the waiting disk store in that server must be unblocked. Here it is shown that the diskID 93e7e9cf-a513-4c67-89c3-da7e94a08efb of server2 is waiting upon the diskID 9791b9ff-7df3-44e8-99c8-7d62a3387002 of server1 , which is missing. Run the unblock-disk-store utility, in the following format, to unblock the waiting disk store: ./bin/snappy unblock-disk-store <diskID of the waiting server> -locators=<host>:<peer-discovery-port> For example, ./bin/snappy unblock-disk-store 93e7e9cf-a513-4c67-89c3-da7e94a08efb -locators=localhost:10334 Restart the cluster and keep unblocking such disk stores that are displayed in the logs until all the servers reach the running status. Note There is no loss of data when you unblock the disk stores.","title":"Unblocking the Disk Store"},{"location":"troubleshooting/member_startup_problems/#rebalancing-data-on-servers","text":"After unblocking the disk stores, if you notice that one of the server in the cluster has more data as compared to the other servers, you can distribute the data among the servers. This ensures that each server carries almost equal data. To balance the data equally on the servers, do the following: Connect to snappy shell and obtain the jdbc client connection. Run the rebalance command. snappy> call sys.rebalance_all_buckets();","title":"Rebalancing Data on Servers"},{"location":"troubleshooting/member_startup_problems/#revoking-disk-stores-that-prevent-startup","text":"If a member cannot be restarted even after unblocking the disk store and restarting after re-ordering the servers in the conf file, only then use the revoke-missing-disk-store command. Caution This can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the revoke-missing-disk-store command as a last resort. Contact support@tibco.com if you need to use the revoke-missing-disk-store command.","title":"Revoking Disk Stores that Prevent Startup"},{"location":"troubleshooting/oom/","text":"Troubleshooting Out-of-Memory(OOM)Errors \u00b6 Whenever SnappyData faces an Out-of-Memory(OOM) situation, the processes are killed. The pid files that are created in the SnappyData work directory are an indication that an OOM error has occured. Additional files can also be found based on the provided cluster configuration options. These files can be used for further analysis. You can use the following pointers to troubleshoot the OOM error: jvmkill_pid.log (pid of process) file: This file can be found in the work directory which indicates that the system has faced an OOM error and has been killed. The pid can be found in the file name. Following files can also be found depending on the additional configuration provided: java_pid(pidof process)-(time when process killed).hprof(pid) file is found if an additional configuration ( -XX:+HeapDumpOnOutOfMemoryError ) is given. For example, java_pid1891-20190504_123015.hprof1891 . The hprof file contains the heapdump that is useful for analysis. In case the additional configuration is not provided, the java_pid(pid of process)-(time when process killed).jmap file can be found. This contains the map of heap for analysis. In case there are any exceptions while writing the heap dump files, you will find a log entry of the respective process. For example, Failed to log heap histogram for pid: (pid of process)","title":"Troubleshooting Out-of-Memory (OOM) Error"},{"location":"troubleshooting/oom/#troubleshooting-out-of-memoryoomerrors","text":"Whenever SnappyData faces an Out-of-Memory(OOM) situation, the processes are killed. The pid files that are created in the SnappyData work directory are an indication that an OOM error has occured. Additional files can also be found based on the provided cluster configuration options. These files can be used for further analysis. You can use the following pointers to troubleshoot the OOM error: jvmkill_pid.log (pid of process) file: This file can be found in the work directory which indicates that the system has faced an OOM error and has been killed. The pid can be found in the file name. Following files can also be found depending on the additional configuration provided: java_pid(pidof process)-(time when process killed).hprof(pid) file is found if an additional configuration ( -XX:+HeapDumpOnOutOfMemoryError ) is given. For example, java_pid1891-20190504_123015.hprof1891 . The hprof file contains the heapdump that is useful for analysis. In case the additional configuration is not provided, the java_pid(pid of process)-(time when process killed).jmap file can be found. This contains the map of heap for analysis. In case there are any exceptions while writing the heap dump files, you will find a log entry of the respective process. For example, Failed to log heap histogram for pid: (pid of process)","title":"Troubleshooting Out-of-Memory(OOM)Errors"},{"location":"troubleshooting/preventing_disk_full_errors/","text":"Preventing Disk Full Errors \u00b6 It is important to monitor the disk usage of SnappyData members. If a member lacks sufficient disk space for a disk store, the member attempts to shut down the disk store and its associated tables, and logs an error message. After you make sufficient disk space available to the member, you can restart the member. A shutdown due to a member running out of disk space can cause loss of data, data file corruption, log file corruption and other error conditions that can negatively impact your applications. You can prevent disk file errors using the following techniques: Use default pre-allocation for disk store files and disk store metadata files. Pre-allocation reserves disk space for these files and leaves the member in a healthy state when the disk store is shut down, allowing you to restart the member once sufficient disk space has been made available. Pre-allocation is configured by default. Pre-allocation is governed by the following system properties: Disk store files \u2014 set the gemfire.preAllocateDisk system property to true (the default). Disk store metadata files \u2014 set the gemfire.preAllocateIF system property to true (the default). Note SnappyData recommends using ext4 filesystems on Linux platforms, because ext4 supports pre-allocation which speeds disk startup performance. If you are using ext3 filesystems in latency-sensitive environments with high write throughput, you can improve disk startup performance by setting the MAXLOGSIZE property of a disk store to a value lower than the default 1 GB. See CREATE DISKSTORE . Monitor SnappyData logs for low disk space warnings. SnappyData logs disk space warnings in the following situations: Log file directory \u2014logs a warning if the available space is less than 100 MB. Disk store directory \u2014logs a warning if the usable space is less than 1.15 times the space required to create a new oplog file. Data dictionary \u2014logs a warning if the remaining space is less than 50 MB.","title":"Preventing Disk Full Errors"},{"location":"troubleshooting/preventing_disk_full_errors/#preventing-disk-full-errors","text":"It is important to monitor the disk usage of SnappyData members. If a member lacks sufficient disk space for a disk store, the member attempts to shut down the disk store and its associated tables, and logs an error message. After you make sufficient disk space available to the member, you can restart the member. A shutdown due to a member running out of disk space can cause loss of data, data file corruption, log file corruption and other error conditions that can negatively impact your applications. You can prevent disk file errors using the following techniques: Use default pre-allocation for disk store files and disk store metadata files. Pre-allocation reserves disk space for these files and leaves the member in a healthy state when the disk store is shut down, allowing you to restart the member once sufficient disk space has been made available. Pre-allocation is configured by default. Pre-allocation is governed by the following system properties: Disk store files \u2014 set the gemfire.preAllocateDisk system property to true (the default). Disk store metadata files \u2014 set the gemfire.preAllocateIF system property to true (the default). Note SnappyData recommends using ext4 filesystems on Linux platforms, because ext4 supports pre-allocation which speeds disk startup performance. If you are using ext3 filesystems in latency-sensitive environments with high write throughput, you can improve disk startup performance by setting the MAXLOGSIZE property of a disk store to a value lower than the default 1 GB. See CREATE DISKSTORE . Monitor SnappyData logs for low disk space warnings. SnappyData logs disk space warnings in the following situations: Log file directory \u2014logs a warning if the available space is less than 100 MB. Disk store directory \u2014logs a warning if the usable space is less than 1.15 times the space required to create a new oplog file. Data dictionary \u2014logs a warning if the remaining space is less than 50 MB.","title":"Preventing Disk Full Errors"},{"location":"troubleshooting/recovering_from_a_conflictingpersistentdataexception/","text":"Recovering from a ConflictingPersistentDataException \u00b6 If you receive a ConflictingPersistentDataException during startup, it indicates that you have multiple copies of some persistent data and SnappyData cannot determine which copy to use. Normally SnappyData uses metadata to automatically determine which copy of persistent data to use. Each member persists, along with the data dictionary or table data, a list of other members that have the data and whether their data is up to date. A ConflictingPersistentDataException happens when two members compare their metadata and find that it is inconsistent\u2014they either don\u2019t know about each other, or they both believe that the other member has stale data. The following are some scenarios that can cause a ConflictingPersistentDataException . Independently-created copies Trying to merge two independently-created distributed systems into a single distributed system causes a ConflictingPersistentDataException . There are a few ways to end up with independently-created systems: Configuration problems may cause SnappyData members connect to different locators that are not aware of each other. To avoid this problem, ensure that all locators and data stores always specify the same, complete list of locator addresses at startup (for example, locators=locator1[10334],locator2[10334],locator3[10334] ). All persistent members in a system may be shut down, after which a brand new set of different persistent members attempts to start up. Trying to merge independent systems by pointing all members to the same set of locators then results in a ConflictingPersistentDataException . SnappyData cannot merge independently-created data for the same table. Instead, you need to export the data from one of the systems and import it into the other system. See Exporting and Importing Data with SnappyData Starting new members first Starting a brand new member with no persistent data before starting older members that have persistent data can cause a ConflictingPersistentDataException . This can happen by accident if you shut down the system, then add a new member to the startup scripts, and finally start all members in parallel. In this case, the new member may start first. If this occurs, the new member creates an empty, independent copy of the data before the older members start up. When the older members start, the situation is similar to that described above in \u201cIndependently-created copies.\u201d In this case, the fix is simply to move aside or delete the (empty) persistence files for the new member, shut down the new member, and finally restart the older members. After the older members have fully recovered, restart the new member. A network split, with enable-network-partition-detection set to false With enable-network-partition-detection set to true, SnappyData detects a network partition and shuts down members to prevent a \"split brain.\" In this case no conflicts should occur when the system is restored. However, if enable-network-partition-detection is false, SnappyData cannot prevent a \"split brain\" after a network partition. Instead, each side of the network partition records that the other side of the partition has stale data. When the partition is healed and persistent members are restarted, they find a conflict because each side believes the other side's members are stale. In some cases it may be possible to choose between sides of the network partition and keep only the data from one side of the partition. Otherwise you may need to salvage data and import it into a fresh system. Resolving a ConflictingPersistentDataException If you receive a ConflictingPersistentDataException , you will not be able to start all of your members and have them join the same distributed system. First, determine if there is one part of the system that you can recover. For example, if you just added some new members to the system, try to start up without including those members. For the remaining members, use the data extractor tool to extract data from the persistence files and import it into a running system.","title":"Recovering from a ConflictingPersistentDataException"},{"location":"troubleshooting/recovering_from_a_conflictingpersistentdataexception/#recovering-from-a-conflictingpersistentdataexception","text":"If you receive a ConflictingPersistentDataException during startup, it indicates that you have multiple copies of some persistent data and SnappyData cannot determine which copy to use. Normally SnappyData uses metadata to automatically determine which copy of persistent data to use. Each member persists, along with the data dictionary or table data, a list of other members that have the data and whether their data is up to date. A ConflictingPersistentDataException happens when two members compare their metadata and find that it is inconsistent\u2014they either don\u2019t know about each other, or they both believe that the other member has stale data. The following are some scenarios that can cause a ConflictingPersistentDataException . Independently-created copies Trying to merge two independently-created distributed systems into a single distributed system causes a ConflictingPersistentDataException . There are a few ways to end up with independently-created systems: Configuration problems may cause SnappyData members connect to different locators that are not aware of each other. To avoid this problem, ensure that all locators and data stores always specify the same, complete list of locator addresses at startup (for example, locators=locator1[10334],locator2[10334],locator3[10334] ). All persistent members in a system may be shut down, after which a brand new set of different persistent members attempts to start up. Trying to merge independent systems by pointing all members to the same set of locators then results in a ConflictingPersistentDataException . SnappyData cannot merge independently-created data for the same table. Instead, you need to export the data from one of the systems and import it into the other system. See Exporting and Importing Data with SnappyData Starting new members first Starting a brand new member with no persistent data before starting older members that have persistent data can cause a ConflictingPersistentDataException . This can happen by accident if you shut down the system, then add a new member to the startup scripts, and finally start all members in parallel. In this case, the new member may start first. If this occurs, the new member creates an empty, independent copy of the data before the older members start up. When the older members start, the situation is similar to that described above in \u201cIndependently-created copies.\u201d In this case, the fix is simply to move aside or delete the (empty) persistence files for the new member, shut down the new member, and finally restart the older members. After the older members have fully recovered, restart the new member. A network split, with enable-network-partition-detection set to false With enable-network-partition-detection set to true, SnappyData detects a network partition and shuts down members to prevent a \"split brain.\" In this case no conflicts should occur when the system is restored. However, if enable-network-partition-detection is false, SnappyData cannot prevent a \"split brain\" after a network partition. Instead, each side of the network partition records that the other side of the partition has stale data. When the partition is healed and persistent members are restarted, they find a conflict because each side believes the other side's members are stale. In some cases it may be possible to choose between sides of the network partition and keep only the data from one side of the partition. Otherwise you may need to salvage data and import it into a fresh system. Resolving a ConflictingPersistentDataException If you receive a ConflictingPersistentDataException , you will not be able to start all of your members and have them join the same distributed system. First, determine if there is one part of the system that you can recover. For example, if you just added some new members to the system, try to start up without including those members. For the remaining members, use the data extractor tool to extract data from the persistence files and import it into a running system.","title":"Recovering from a ConflictingPersistentDataException"},{"location":"troubleshooting/recovering_from_disk_full_errors/","text":"Recovering from Disk Full Errors \u00b6 If a member of your SnappyData distributed system fails due to a disk full error condition, add or make additional disk capacity available and attempt to restart the member normally. If the member does not restart and there is a redundant copy of its tables in a disk store on another member, you can restore the member using the following steps: Delete or move the disk store files from the failed member. Revoke the member using the revoke-disk-store command. Note This can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the revoke-missing-disk-store command as a last resort. Contact support@tibco.com if you need to use the revoke-missing-disk-store command Restart the member.","title":"Recovering from Disk Full Errors"},{"location":"troubleshooting/recovering_from_disk_full_errors/#recovering-from-disk-full-errors","text":"If a member of your SnappyData distributed system fails due to a disk full error condition, add or make additional disk capacity available and attempt to restart the member normally. If the member does not restart and there is a redundant copy of its tables in a disk store on another member, you can restore the member using the following steps: Delete or move the disk store files from the failed member. Revoke the member using the revoke-disk-store command. Note This can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the revoke-missing-disk-store command as a last resort. Contact support@tibco.com if you need to use the revoke-missing-disk-store command Restart the member.","title":"Recovering from Disk Full Errors"},{"location":"troubleshooting/troubleshooting_error_messages/","text":"Troubleshooting Error Messages \u00b6 Error messages provide information about problems that might occur when setting up the SnappyData cluster or when running queries. You can use the following information to resolve such problems. The following topics are covered in this section: Region {0} has potentially stale data. It is waiting for another member to recover the latest data. XCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query {0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}] Region {0} bucket {1} has persistent data that is no longer online stored at these locations: {2} ForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\" Node went down or data no longer available while iterating the results SmartConnector catalog is not up to date. Please reconstruct the Dataset and retry the operation. Cannot parse config: String: 1: Expecting end of input or a comma, got ':' java.lang.IllegalStateException: Detected both log4j-over-slf4j.jar AND bound slf4j-log4j12.jar on the class path, preempting StackOverflowError Bad PutInto performance even when input dataframe size is small. Error Message: Region {0} has potentially stale data. It is waiting for another member to recover the latest data. My persistent id: {1} Members with potentially new data: {2}Use the \"{3} list-missing-disk-stores\" command to see all disk stores that are being waited on by other members. Diagnosis: The above message is typically displayed during start up when a member waits for other members in the cluster to be available, as the table data on disk is not the most current. The status of the member is displayed as waiting in such cases when you check the status of the cluster using the snappy-status-all.sh command. Solution: The status of the waiting members change to online once all the members are online and the status of the waiting members is updated. Users can check whether the status is changed from waiting to online by using the snappy-status-all.sh command or by checking the SnappyData Monitoring Console . Error Message: XCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query Diagnosis: This error message is reported when a system runs on low available memory. In such cases, the queries may get aborted and an error is reported to prevent the server from crashing due to low available memory. Once the heap memory usage falls below critical-heap-percentage the queries run successfully. Solution: To avoid such issues, review your memory configuration and make sure that you have allocated enough heap memory. You can also configure tables for eviction so that table rows are evicted from memory and overflow to disk when the system crosses eviction threshold. For more details refer to best practices for memory management Message: {0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}] Diagnosis: The above warning message is displayed when a member is awaiting for a response from another member on the system and response has not been received for some time. Solution: This generally means that there is a resource issue in (most likely) the member that is in waiting status. Check whether there is a garbage collection activity going on in the member being waited for. Due of large GC pauses, the member may not be responding in the stipulated time. In such cases, review your memory configuration and consider whether you can configure to use off-heap memory . Error Message: Region {0} bucket {1} has persistent data that is no longer online stored at these locations: {2} Diagnosis: In partitioned tables that are persisted to disk, if you have any of the members offline, the partitioned table is still available, but, may have some buckets represented only in offline disk stores. In this case, methods that access the bucket entries report a PartitionOfflineException error. Solution: If possible, bring the missing member online. This restores the buckets to memory and you can work with them again. Error Message: ForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\" Diagnosis: A distributed system member\u2019s Cache and DistributedSystem are forcibly closed by the system membership coordinator if it becomes sick or too slow to respond to heartbeat requests. The log file for the member displays a ForcedDisconnectException with the message. One possible reason for this could be that large GC pauses are causing the member to be unresponsive when the GC is in progress. Solution: To minimize the chances of this happening, you can increase the DistributedSystem property member-timeout . This setting also controls the length of time required to notice a network failure. Also, review your memory configuration and configure to use off-heap memory . Error Message: Node went down or data no longer available while iterating the results. Diagnosis: In cases where a node fails while a JDBC/ODBC client or job is consuming result of a query, then it can result in the query failing with such an exception. Solution: This is expected behaviour where the product does not retry, since partial results are already consumed by the application. Application must retry the entire query after discarding any changes due to partial results that are consumed. Message: SmartConnector catalog is not up to date. Please reconstruct the Dataset and retry the operation. OR Table schema changed due to DROP/CREATE/ALTER operation. Please retry the operation. Diagnosis: In the Smart Connector mode, this error message is seen in the logs if SnappyData catalog is changed due to a DDL operation such as CREATE/DROP/ALTER. For performance reasons, SnappyData Smart Connector caches the catalog in the Smart Connector cluster. If there is a catalog change in SnappyData embedded cluster, this error is logged to prevent unexpected errors due to schema changes. Solution: If a Select query encounters this error, that query is retried internally by the product up to ten times, and user intervention may not be necessary. However, after the ten retries, the query shows an exception, and the application needs to retry the query. Also, if the user application is performing DataFrame/DataSet operations, you must recreate the DataFrame/DataSet and retry the operation. In such cases, the application needs to catch exceptions of type org.apache.spark.sql.execution.CatalogStaleException and j ava.sql.SQLException (with SQLState=X0Z39) and retry the operation. Check the following code snippet to get a better understanding of how you can handle this scenario: int retryCount = 0; int maxRetryAttempts = 5; while (true) { try { // dataset/dataframe operations goes here (e.g. deleteFrom, putInto) return; } catch (Exception ex) { if (retryCount >= maxRetryAttempts || !isRetriableException(ex)) { throw ex; } else { log.warn(\"Encountered a retriable exception. Will retry processing batch.\" + \" maxRetryAttempts:\"+maxRetryAttempts+\", retryCount:\"+retryCount+\"\", ex); retryCount++; } } } private boolean isRetriableException(Exception ex) { Throwable cause = ex; do { if ((cause instanceof SQLException && ((SQLException)cause).getSQLState().equals(\"X0Z39\")) || cause instanceof CatalogStaleException || (cause instanceof TaskCompletionListenerException && cause.getMessage() .startsWith(\"java.sql.SQLException: (SQLState=X0Z39\"))) { return true; } cause = cause.getCause(); } while (cause != null); return false; } Error Message: Snappy job submission result: { \"status\": \"ERROR\", \"result\": \"Cannot parse config: String: 1: Expecting end of input or a comma, got ':' (if you intended ':' to be part of a key or string value, try enclosing the key or value in double quotes, or you may be able to rename the file .properties rather than .conf)\" } Diagnosis: This error message is reported when snappy-job submission configuration parameter contains a colon : . The typesafe configuration parser (used by job-server) cannot handle : as part of key or value unless it is enclosed in double quotes. Sample job-submit command: ./snappy-job.sh submit --app-name app1 --conf kafka-brokers=localhost:9091 --class Test --app-jar \"/path/to/app-jar\" Note Note that the kafka-brokers config value (localhost:9091) is having a : Solution: To avoid this issue enclose the value containing colon : with double quotes so that the typesafe config parser can parse it. Also since the parameter is passed to the bash script (snappy-job) the quotes needs to be escaped properly otherwise it will get ignored by bash. Check the following example for the correct method of passing this configuration: ./snappy-job.sh submit --app-name app1 --conf kafka-brokers=\\\"localhost:9091\\\" --class Test --app-jar \"/path/to/app-jar\" Note Check the value of kafka-brokers property enclosed in escaped double quotes: \\\"localhost:9091\\\" Error Message: java.lang.IllegalStateException: Detected both log4j-over-slf4j.jar AND bound slf4j-log4j12.jar on the class path, preempting StackOverflowError Diagnosis: This error message can be seen if application uses SnappyData JDBC driver shadow jar and the application has a dependency on log4j-over-slf4j package/jar. This is because, the SnappyData JDBC driver has a dependency on slf4j-log4j12 which cannot co-exist with 'log4j-over-slf4j' package. Solution: To avoid getting log4j and slf4j-log4j12 dependencies in the driver, you can link the non-fat JDBC client jar ( snappydata-store-client*.jar ) in your application and exclude log4j and slf4j-log4j12 dependencies from it. Note that the snappydata-store-client jar does not contain some of the SnappyData extensions (Scala imiplicits) that are required when SnappyData Spark-JDBC connector is used. That is when accessing SnappyData from another Spark cluster using JDBC dataframes as mentioned here . If these SnappyData extensions are to be used, then in addition to above mentioned jar, snappydata-jdbc*-only.jar dependency will be required. This is available on maven repo and can be accessed using classifier: 'only' along with snappydata-jdbc cordinates. Following is an example for adding this dependency using gradle: build.gradle example that uses snappydata-store-client jar and snappydata-jdbc*only.jar . The example uses 1.0.2.2 SnappyData version, replace it with the version required by the application. Example dependencies { compile group: 'io.snappydata', name: 'snappydata-jdbc_2.11', version: '1.0.2.2', classifier: 'only' // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client // SnappyData \"no-fat\" JDBC jar compile group: 'io.snappydata', name: 'snappydata-store-client', version: '1.6.2.1' // If users want to add his own 'log4j-over-slf4j' dependency compile group: 'org.slf4j', name: 'log4j-over-slf4j', version: '1.7.26' } // exclude the 'log4j' and 'slf4j-log4j12' dependencies configurations.all { exclude group: 'log4j', module: 'log4j' exclude group: 'org.slf4j', module: 'slf4j-log4j12' } Error Message: Bad PutInto performance even when input dataframe size is small. Diagnosis: PutInto operation internally performs join. If the input dataframe size is small (less than spark.sql.autoBroadcastJoinThreshold which defaults to 10 MB) then the join should ideally result into broadcast join giving better performance than sort merge join (which will be chosen otherwise). However, some sources don't provide size statistics and in that case the size of dataframe results into value of spark.sql.defaultSizeInBytes property which defaults to Long.MaxValue . In this case even if the actual size of the input dataframe is less than that of spark.sql.autoBroadcastJoinThreshold , the PutInto operation will always end up using sort merge join resulting into poor performance. Solution: Solution for this issue is to override default value of spark.sql.defaultSizeInBytes by setting it as part of session configuration with the value matching the approximate size of the actual input dataframe. The property can be set using the following SQL command: set spark.sql.defaultSizeInBytes = <some long value> For example, set spark.sql.defaultSizeInBytes = 10000 Using a SnappySession instance this can be set as follows: snappySession.sql(\u201cset spark.sql.defaultSizeInBytes = 10000\u201d) Note that this is a session level property hence all the operation performed using the same session will end up using same overridden value.","title":"Troubleshooting Error Messages"},{"location":"troubleshooting/troubleshooting_error_messages/#troubleshooting-error-messages","text":"Error messages provide information about problems that might occur when setting up the SnappyData cluster or when running queries. You can use the following information to resolve such problems. The following topics are covered in this section: Region {0} has potentially stale data. It is waiting for another member to recover the latest data. XCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query {0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}] Region {0} bucket {1} has persistent data that is no longer online stored at these locations: {2} ForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\" Node went down or data no longer available while iterating the results SmartConnector catalog is not up to date. Please reconstruct the Dataset and retry the operation. Cannot parse config: String: 1: Expecting end of input or a comma, got ':' java.lang.IllegalStateException: Detected both log4j-over-slf4j.jar AND bound slf4j-log4j12.jar on the class path, preempting StackOverflowError Bad PutInto performance even when input dataframe size is small. Error Message: Region {0} has potentially stale data. It is waiting for another member to recover the latest data. My persistent id: {1} Members with potentially new data: {2}Use the \"{3} list-missing-disk-stores\" command to see all disk stores that are being waited on by other members. Diagnosis: The above message is typically displayed during start up when a member waits for other members in the cluster to be available, as the table data on disk is not the most current. The status of the member is displayed as waiting in such cases when you check the status of the cluster using the snappy-status-all.sh command. Solution: The status of the waiting members change to online once all the members are online and the status of the waiting members is updated. Users can check whether the status is changed from waiting to online by using the snappy-status-all.sh command or by checking the SnappyData Monitoring Console . Error Message: XCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query Diagnosis: This error message is reported when a system runs on low available memory. In such cases, the queries may get aborted and an error is reported to prevent the server from crashing due to low available memory. Once the heap memory usage falls below critical-heap-percentage the queries run successfully. Solution: To avoid such issues, review your memory configuration and make sure that you have allocated enough heap memory. You can also configure tables for eviction so that table rows are evicted from memory and overflow to disk when the system crosses eviction threshold. For more details refer to best practices for memory management Message: {0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}] Diagnosis: The above warning message is displayed when a member is awaiting for a response from another member on the system and response has not been received for some time. Solution: This generally means that there is a resource issue in (most likely) the member that is in waiting status. Check whether there is a garbage collection activity going on in the member being waited for. Due of large GC pauses, the member may not be responding in the stipulated time. In such cases, review your memory configuration and consider whether you can configure to use off-heap memory . Error Message: Region {0} bucket {1} has persistent data that is no longer online stored at these locations: {2} Diagnosis: In partitioned tables that are persisted to disk, if you have any of the members offline, the partitioned table is still available, but, may have some buckets represented only in offline disk stores. In this case, methods that access the bucket entries report a PartitionOfflineException error. Solution: If possible, bring the missing member online. This restores the buckets to memory and you can work with them again. Error Message: ForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\" Diagnosis: A distributed system member\u2019s Cache and DistributedSystem are forcibly closed by the system membership coordinator if it becomes sick or too slow to respond to heartbeat requests. The log file for the member displays a ForcedDisconnectException with the message. One possible reason for this could be that large GC pauses are causing the member to be unresponsive when the GC is in progress. Solution: To minimize the chances of this happening, you can increase the DistributedSystem property member-timeout . This setting also controls the length of time required to notice a network failure. Also, review your memory configuration and configure to use off-heap memory . Error Message: Node went down or data no longer available while iterating the results. Diagnosis: In cases where a node fails while a JDBC/ODBC client or job is consuming result of a query, then it can result in the query failing with such an exception. Solution: This is expected behaviour where the product does not retry, since partial results are already consumed by the application. Application must retry the entire query after discarding any changes due to partial results that are consumed. Message: SmartConnector catalog is not up to date. Please reconstruct the Dataset and retry the operation. OR Table schema changed due to DROP/CREATE/ALTER operation. Please retry the operation. Diagnosis: In the Smart Connector mode, this error message is seen in the logs if SnappyData catalog is changed due to a DDL operation such as CREATE/DROP/ALTER. For performance reasons, SnappyData Smart Connector caches the catalog in the Smart Connector cluster. If there is a catalog change in SnappyData embedded cluster, this error is logged to prevent unexpected errors due to schema changes. Solution: If a Select query encounters this error, that query is retried internally by the product up to ten times, and user intervention may not be necessary. However, after the ten retries, the query shows an exception, and the application needs to retry the query. Also, if the user application is performing DataFrame/DataSet operations, you must recreate the DataFrame/DataSet and retry the operation. In such cases, the application needs to catch exceptions of type org.apache.spark.sql.execution.CatalogStaleException and j ava.sql.SQLException (with SQLState=X0Z39) and retry the operation. Check the following code snippet to get a better understanding of how you can handle this scenario: int retryCount = 0; int maxRetryAttempts = 5; while (true) { try { // dataset/dataframe operations goes here (e.g. deleteFrom, putInto) return; } catch (Exception ex) { if (retryCount >= maxRetryAttempts || !isRetriableException(ex)) { throw ex; } else { log.warn(\"Encountered a retriable exception. Will retry processing batch.\" + \" maxRetryAttempts:\"+maxRetryAttempts+\", retryCount:\"+retryCount+\"\", ex); retryCount++; } } } private boolean isRetriableException(Exception ex) { Throwable cause = ex; do { if ((cause instanceof SQLException && ((SQLException)cause).getSQLState().equals(\"X0Z39\")) || cause instanceof CatalogStaleException || (cause instanceof TaskCompletionListenerException && cause.getMessage() .startsWith(\"java.sql.SQLException: (SQLState=X0Z39\"))) { return true; } cause = cause.getCause(); } while (cause != null); return false; } Error Message: Snappy job submission result: { \"status\": \"ERROR\", \"result\": \"Cannot parse config: String: 1: Expecting end of input or a comma, got ':' (if you intended ':' to be part of a key or string value, try enclosing the key or value in double quotes, or you may be able to rename the file .properties rather than .conf)\" } Diagnosis: This error message is reported when snappy-job submission configuration parameter contains a colon : . The typesafe configuration parser (used by job-server) cannot handle : as part of key or value unless it is enclosed in double quotes. Sample job-submit command: ./snappy-job.sh submit --app-name app1 --conf kafka-brokers=localhost:9091 --class Test --app-jar \"/path/to/app-jar\" Note Note that the kafka-brokers config value (localhost:9091) is having a : Solution: To avoid this issue enclose the value containing colon : with double quotes so that the typesafe config parser can parse it. Also since the parameter is passed to the bash script (snappy-job) the quotes needs to be escaped properly otherwise it will get ignored by bash. Check the following example for the correct method of passing this configuration: ./snappy-job.sh submit --app-name app1 --conf kafka-brokers=\\\"localhost:9091\\\" --class Test --app-jar \"/path/to/app-jar\" Note Check the value of kafka-brokers property enclosed in escaped double quotes: \\\"localhost:9091\\\" Error Message: java.lang.IllegalStateException: Detected both log4j-over-slf4j.jar AND bound slf4j-log4j12.jar on the class path, preempting StackOverflowError Diagnosis: This error message can be seen if application uses SnappyData JDBC driver shadow jar and the application has a dependency on log4j-over-slf4j package/jar. This is because, the SnappyData JDBC driver has a dependency on slf4j-log4j12 which cannot co-exist with 'log4j-over-slf4j' package. Solution: To avoid getting log4j and slf4j-log4j12 dependencies in the driver, you can link the non-fat JDBC client jar ( snappydata-store-client*.jar ) in your application and exclude log4j and slf4j-log4j12 dependencies from it. Note that the snappydata-store-client jar does not contain some of the SnappyData extensions (Scala imiplicits) that are required when SnappyData Spark-JDBC connector is used. That is when accessing SnappyData from another Spark cluster using JDBC dataframes as mentioned here . If these SnappyData extensions are to be used, then in addition to above mentioned jar, snappydata-jdbc*-only.jar dependency will be required. This is available on maven repo and can be accessed using classifier: 'only' along with snappydata-jdbc cordinates. Following is an example for adding this dependency using gradle: build.gradle example that uses snappydata-store-client jar and snappydata-jdbc*only.jar . The example uses 1.0.2.2 SnappyData version, replace it with the version required by the application. Example dependencies { compile group: 'io.snappydata', name: 'snappydata-jdbc_2.11', version: '1.0.2.2', classifier: 'only' // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client // SnappyData \"no-fat\" JDBC jar compile group: 'io.snappydata', name: 'snappydata-store-client', version: '1.6.2.1' // If users want to add his own 'log4j-over-slf4j' dependency compile group: 'org.slf4j', name: 'log4j-over-slf4j', version: '1.7.26' } // exclude the 'log4j' and 'slf4j-log4j12' dependencies configurations.all { exclude group: 'log4j', module: 'log4j' exclude group: 'org.slf4j', module: 'slf4j-log4j12' } Error Message: Bad PutInto performance even when input dataframe size is small. Diagnosis: PutInto operation internally performs join. If the input dataframe size is small (less than spark.sql.autoBroadcastJoinThreshold which defaults to 10 MB) then the join should ideally result into broadcast join giving better performance than sort merge join (which will be chosen otherwise). However, some sources don't provide size statistics and in that case the size of dataframe results into value of spark.sql.defaultSizeInBytes property which defaults to Long.MaxValue . In this case even if the actual size of the input dataframe is less than that of spark.sql.autoBroadcastJoinThreshold , the PutInto operation will always end up using sort merge join resulting into poor performance. Solution: Solution for this issue is to override default value of spark.sql.defaultSizeInBytes by setting it as part of session configuration with the value matching the approximate size of the actual input dataframe. The property can be set using the following SQL command: set spark.sql.defaultSizeInBytes = <some long value> For example, set spark.sql.defaultSizeInBytes = 10000 Using a SnappySession instance this can be set as follows: snappySession.sql(\u201cset spark.sql.defaultSizeInBytes = 10000\u201d) Note that this is a session level property hence all the operation performed using the same session will end up using same overridden value.","title":"Troubleshooting Error Messages"}]}