plugins {
  id "de.undercouch.download" version "3.0.0"
}

apply plugin: 'scala'
apply plugin: 'com.github.johnrengelman.shadow'

compileScala.options.encoding = 'UTF-8'
// fix scala+java mix to all use compileScala which uses correct dependency order
sourceSets.main.scala.srcDir "src/main/java"
sourceSets.test.scala.srcDirs = [ 'src/test/java', 'src/test/scala',
                                  'src/dunit/java', 'src/dunit/scala' ]
sourceSets.main.java.srcDirs = []
sourceSets.test.java.srcDirs = [ ]

def osName = org.gradle.internal.os.OperatingSystem.current()

dependencies {
  provided 'org.scala-lang:scala-library:' + scalaVersion
  provided 'org.scala-lang:scala-reflect:' + scalaVersion
  provided 'org.scala-lang:scala-compiler:' + scalaVersion

  // always use stock spark so that snappy extensions don't get accidently
  // included here in snappy-core code.
  provided("org.apache.spark:spark-core_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-catalyst_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-sql_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-hive_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-streaming_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-streaming-kafka_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-streaming-twitter_${scalaBinaryVersion}:${sparkVersion}")
  provided("org.apache.spark:spark-mllib_${scalaBinaryVersion}:${sparkVersion}")
  provided "org.eclipse.jetty:jetty-servlet:${jettyVersion}"

  if (new File(rootDir, 'store/build.gradle').exists()) {
    compile project(':snappy-store:gemfirexd-client')
    compile project(':snappy-store:gemfirexd-core')
    compile project(':snappy-store:gemfirexd-tools')
    testCompile project(path: ':snappy-store:gemfirexd-tools', configuration: 'testOutput')
  } else {
    compile group: 'io.snappydata', name: 'snappydata-store-client', version: snappyStoreVersion
    compile group: 'io.snappydata', name: 'snappydata-store-core', version: snappyStoreVersion
    compile group: 'io.snappydata', name: 'snappydata-store-tools', version: snappyStoreVersion
    testCompile group: 'io.snappydata', name: 'snappydata-store-tools', version: snappyStoreVersion, classifier: 'tests'
  }

  compile("org.parboiled:parboiled_${scalaBinaryVersion}:2.1.2") {
    exclude(group: 'org.scala-lang', module: 'scala-library')
    exclude(group: 'org.scala-lang', module: 'scala-reflect')
    exclude(group: 'org.scala-lang', module: 'scala-compiler')
  }
  compile 'org.apache.tomcat:tomcat-jdbc:8.0.32'
  compile 'com.zaxxer:HikariCP:2.4.4'
  provided 'com.rabbitmq:amqp-client:3.5.7'

  compile(group: 'com.databricks', name: 'spark-csv_2.10', version: '1.2.0') {
    exclude(group: 'org.scala-lang', module: 'scala-library')
    exclude(group: 'org.scala-lang', module: 'scala-reflect')
    exclude(group: 'org.scala-lang', module: 'scala-compiler')
  }

  testCompile project(':dunit')
  testCompile 'org.scala-lang:scala-actors:' + scalaVersion
  testCompile 'org.scalatest:scalatest_' + scalaBinaryVersion + ':2.2.6'

  testCompile("org.apache.spark:spark-core_${scalaBinaryVersion}:${sparkVersion}:tests")
  testCompile("org.apache.spark:spark-streaming_${scalaBinaryVersion}:${sparkVersion}:tests")

  testRuntime 'org.pegdown:pegdown:1.1.0'
}

task packageScalaDocs(type: Jar, dependsOn: scaladoc) {
  classifier = 'javadoc'
  from scaladoc
}
if (rootProject.hasProperty('enablePublish')) {
  artifacts {
    archives packageScalaDocs, packageSources
  }
}

testClasses.doLast {
  copyTestsCommonResources(buildDir)
}

scalaTest {
  dependsOn ':cleanScalaTest'
  doFirst {
    // cleanup files since scalatest plugin does not honour workingDir yet
    cleanIntermediateFiles(project.path)
  }
  doLast {
    // cleanup files since scalatest plugin does not honour workingDir yet
    cleanIntermediateFiles(project.path)
  }
}

task downloadApacheSparkDist(type: de.undercouch.gradle.tasks.download.Download) {
  outputs.files "${sparkProductDir}.tgz"

  src "https://www.apache.org/dist/spark/spark-${sparkVersion}/${sparkDistName}.tgz"
  dest sparkDistDir

  doFirst {
    mkdir(sparkDistDir)
  }
}

task getApacheSparkDist {
  dependsOn downloadApacheSparkDist

  outputs.files "${sparkProductDir}.tgz", "${sparkProductDir}/README.md"

  doLast {
    if (osName.isWindows()) {
      copy {
        from tarTree(resources.gzip("${sparkDistDir}/${sparkDistName}.tgz"))
        into sparkDistDir
      }
    } else {
      // gradle tarTree does not preserve symlinks (GRADLE-2844)
      exec {
        executable 'tar'
        args 'xfz', "${sparkDistDir}/${sparkDistName}.tgz"
        workingDir = sparkDistDir
      }
    }
  }
}

test.dependsOn ':cleanJUnit'
dunitTest.dependsOn getApacheSparkDist
check.dependsOn test, scalaTest, dunitTest

shadowJar {
  zip64 = true

  mergeServiceFiles()
  exclude 'log4j.properties'

  if (rootProject.hasProperty('enablePublish')) {
    createdBy = "SnappyData Build Team"
  } else {
    createdBy = System.getProperty("user.name")
  }
  manifest {
    attributes(
      "Manifest-Version"  : "1.0",
      "Created-By"        : createdBy,
      "Title"             : "snappydata-core_${scalaBinaryVersion}",
      "Version"           : version,
      "Vendor"            : "SnappyData, Inc."
    )
  }
}

// write the POM for spark-package
String sparkPackageName = "snappydata-${version}"

task sparkPackagePom(dependsOn: shadowJar) << {
  file("${rootProject.buildDir}/distributions").mkdirs()
  pom {
    project {
      groupId 'SnappyDataInc'
      artifactId 'snappydata'
      version "${version}-s_${scalaBinaryVersion}"
      licenses {
        license {
          name 'The Apache Software License, Version 2.0'
          url 'http://www.apache.org/licenses/LICENSE-2.0.txt'
          distribution 'repo'
        }
      }
    }
    whenConfigured { p -> p.dependencies.clear() }
  }.writeTo("${rootProject.buildDir}/distributions/${sparkPackageName}.pom")
  copy {
    from "${buildDir}/libs"
    into "${rootProject.buildDir}/distributions"
    include "${shadowJar.archiveName}"
    rename { filename -> "${sparkPackageName}.jar" }
  }
}
task sparkPackage(type: Zip, dependsOn: sparkPackagePom) {
  archiveName "${sparkPackageName}.zip"
  destinationDir = file("${rootProject.buildDir}/distributions")
  outputs.upToDateWhen { false }

  from ("${rootProject.buildDir}/distributions") {
    include "${sparkPackageName}.jar"
    include "${sparkPackageName}.pom"
  }
}
