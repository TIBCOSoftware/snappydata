hydra.Prms-testRequirement = "Test to perform dmlOps on table with overflow data";
hydra.Prms-testDescription = "Test starts snappy and spark cluster. It creates tables and loads data into it.
Later dml operations are executed on the table. The dml operation should perform even if the data
is overflown to disk.";

INCLUDE $JTESTS/io/snappydata/hydra/testDMLOps/jdbcClient/jdbcClient.inc;

//task for running dmlOps using smart connector mode
TASK         taskClass   = io.snappydata.hydra.testDMLOps.SnappyDMLOpsUtil taskMethod  = HydraTask_performDMLOpsInApp
    io.snappydata.hydra.cluster.SnappyPrms-sparkJobClassNames = io.snappydata.hydra.testDMLOps.ValidateDMLOpApp
    io.snappydata.hydra.cluster.SnappyPrms-userAppJar = snappydata-store-scala-tests*tests.jar
    io.snappydata.hydra.cluster.SnappyPrms-hasDynamicAppProps = true
    threadGroups = snappyStoreThreads
    ;

//task for running dmlOps
TASK        taskClass   = io.snappydata.hydra.testDMLOps.SnappyDMLOpsUtil taskMethod  = HydraTask_performDMLOpsInJob
  io.snappydata.hydra.cluster.SnappyPrms-jobClassNames = io.snappydata.hydra.testDMLOps.ValidateDMLOpJob
  io.snappydata.hydra.cluster.SnappyPrms-userAppJar = snappydata-store-scala-tests*tests.jar
  io.snappydata.hydra.cluster.SnappyPrms-hasDynamicAppProps = true
  threadGroups = snappyStoreThreads;

//task for running dmlOps using JDBC connection
TASK        taskClass   = io.snappydata.hydra.testDMLOps.SnappyDMLOpsUtil taskMethod  = HydraTask_performDMLOp
  threadGroups = snappyStoreThreads;

io.snappydata.hydra.testDMLOps.SnappySchemaPrms-dmlOperations = ONEOF insert update insert insert delete insert FOENO;


