{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nSnappyData fuses Apache Spark with an in-memory database to deliver a data engine capable of processing streams, transactions and interactive analytics in a single cluster.\n\n\nThe Challenge with Spark and Remote Data Sources\n\n\nApache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and can access disparate data sources in a highly parallelized manner for its distributed computations. Typically, data is fetched lazily as a result of SQL query or a Dataset (RDD) getting materialized. This can be quite inefficient and expensive if the data set has to be repeatedly processed. Caching within Spark is immutable and still requires the application to periodically refresh the data set, let alone having to bear the burden of duplicating the dataset. \n\n\nAnalytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Spark to do the aggregation. Caching within Spark is immutable and results in stale insight.\n\n\nThe SnappyData Approach\n\n\nAt SnappyData, a very different approach is taken. SnappyData fuses a low latency, highly available in-memory transactional database (GemFireXD) into Spark with shared memory management and optimizations. Data in the highly available in-memory store is laid out using the same columnar format as Spark. All query engine operators are more optimized through better vectorization and code generation. The net effect is, an order of magnitude performance improvement when compared to native Spark caching, and more than two orders of magnitude better Spark performance when working with external data sources.\n\n\nEssentially, Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Spark) and running analytic SQL queries.\n\n\n\n\nConceptually, you can think of SnappyData as an \nin-memory database that uses Spark's API and SQL as its interface and computational engine\n- to process streams, work with myriad data sources like HDFS, and process data through a rich set of higher level abstractions. While the SnappyData engine is primarily designed for SQL processing, applications can work with Objects through Spark RDDs and the newly introduced Spark Datasets.\n\n\nAny Spark DataFrame can be easily managed as a SnappyData Table or conversely any table can be accessed as a DataFrame.\n\n\nBy default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted, Spark executors are automatically launched within the SnappyData process space (JVMs). There is no need to connect and manage external data store clusters. The SnappyData store can synchronously replicate for high availability (HA) with strong consistency and store/recover from disk for additional reliability.\n\n\nKey Features\n\n\n\n\n\n\n100% compatible with Spark\n- Use SnappyData as a database, but also use any of the Spark APIs - ML, Graph, etc.\n\n\n\n\n\n\nIn-memory row and column stores\n: Run the store collocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster)\n\n\n\n\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\n\n\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, DataFrame APIs or declaratively specify your streams and how you want it processed. You do not need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\n\n\n\n\nNot-Only SQL\n: Use either as a SQL database or work with JSON or even arbitrary Application Objects. Essentially, any Spark RDD/DataSet can also be persisted into SnappyData tables (type system same as Spark DataFrames). \n\n\n\n\n\n\nInteractive analytics using Synopsis Data Engine (SDE)\n: Multiple synopses techniques are introduced through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.\n\n\n\n\n\n\nMutate, transact on data in Spark\n: You can use SQL to insert, update, delete data in tables as one would expect. Extensions to Spark\u2019s context are also provided so you can mutate data in your Spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store.\n\n\n\n\n\n\nOptimizations - Indexing\n: You can index your RowStore and the GemFire SQL optimizer, which automatically uses in-memory indexes when available.\n\n\n\n\n\n\nOptimizations - collocation\n: SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be collocated using declarative custom partitioning strategies (for example, common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent.\n\n\n\n\n\n\nHigh availability not just Fault tolerance\n: Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster. It is deeply integrated with a membership-based distributed system to detect and handle failures, instantaneously providing applications continuous HA.\n\n\n\n\n\n\nDurability and recovery:\n Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.\n\n\n\n\n\n\n\n\nExtensions to the Spark Runtime\n\n\nSnappyData makes the following contributions to deliver a unified and optimized runtime.\n\n\n\n\n\n\nIntegrating an operational in-memory data store with Spark\u2019s computational model\n: A number of extensions are introduced to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy. \n This design is extended by allowing low latency and fine- grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, the runtime is extended with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby the state is shared across many clients and applications. \n\n\n\n\n\n\nUnified API for OLAP, OLTP, and Streaming\n: Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL.\nWhile Spark deserves much of the credit for being the first of its kind to offer a unified API, SnappyData further extends its API to: \n\n\n\n\n\n\nAllow for OLTP operations, for example, transactions and mutations (inserts/updates/deletions) on tables \n\n\n\n\n\n\nConfirm with SQL standards, for example, allowing tables alterations, constraints, indexes, and   \n\n\n\n\n\n\nSupport declarative stream processing in SQL\n\n\n\n\n\n\n\n\n\n\nOptimizing Spark application execution times\n: Our goal is to eliminate the need for yet another external store (for example, a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting collocated schema designs (tables and streams) where related data is collocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios.\n\n\n\n\n\n\nSynopsis Data Engine support built into Spark\n: The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time. \nThe SDE engine enables you to:\n\n\n\n\n\n\nIntelligently sample the data set on frequently accessed dimensions to have a good representation across the entire data set (stratified sampling). Queries can execute on samples and return answers instantly.\n\n\n\n\n\n\nCompute estimates for any ad hoc query from the sample(s). It can also provide error estimates for arbitrarily complex queries on streams.\n\n\n\n\n\n\nProvide simple knobs for the user to trade off speed for accuracy, i.e. simple SQL extensions so the user can specify the error tolerance for all queries. When query error is higher than tolerance level, the system automatically delegates the query to the source.\n\n\n\n\n\n\nExpress their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts.\n\n\n\n\n\n\n\n\n\n\nSpark Challenges for Mixed Workloads (OLTP, OLAP)\n\n\nSpark is designed as a computational engine for processing batch jobs. Each Spark application (for example, a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an external storage tier, such as HDFS. SnappyData, on the other hand, targets a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database in the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and decoupled from individual applications.\n\n\nA second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces:\n\n\n\n\n\n\nA single point of contention for all requests, and \n\n\n\n\n\n\nA barrier for achieving high availability (HA). Executors are shut down if the driver fails, requiring a full refresh of any cached state.\n\n\n\n\n\n\nSpark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, more complex data structures  needs to be managed (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mutability characteristics and conformance to SQL.\n\n\nFinally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "SnappyData fuses Apache Spark with an in-memory database to deliver a data engine capable of processing streams, transactions and interactive analytics in a single cluster.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#the-challenge-with-spark-and-remote-data-sources", 
            "text": "Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and can access disparate data sources in a highly parallelized manner for its distributed computations. Typically, data is fetched lazily as a result of SQL query or a Dataset (RDD) getting materialized. This can be quite inefficient and expensive if the data set has to be repeatedly processed. Caching within Spark is immutable and still requires the application to periodically refresh the data set, let alone having to bear the burden of duplicating the dataset.   Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Spark to do the aggregation. Caching within Spark is immutable and results in stale insight.", 
            "title": "The Challenge with Spark and Remote Data Sources"
        }, 
        {
            "location": "/#the-snappydata-approach", 
            "text": "At SnappyData, a very different approach is taken. SnappyData fuses a low latency, highly available in-memory transactional database (GemFireXD) into Spark with shared memory management and optimizations. Data in the highly available in-memory store is laid out using the same columnar format as Spark. All query engine operators are more optimized through better vectorization and code generation. The net effect is, an order of magnitude performance improvement when compared to native Spark caching, and more than two orders of magnitude better Spark performance when working with external data sources.  Essentially, Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Spark) and running analytic SQL queries.   Conceptually, you can think of SnappyData as an  in-memory database that uses Spark's API and SQL as its interface and computational engine - to process streams, work with myriad data sources like HDFS, and process data through a rich set of higher level abstractions. While the SnappyData engine is primarily designed for SQL processing, applications can work with Objects through Spark RDDs and the newly introduced Spark Datasets.  Any Spark DataFrame can be easily managed as a SnappyData Table or conversely any table can be accessed as a DataFrame.  By default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted, Spark executors are automatically launched within the SnappyData process space (JVMs). There is no need to connect and manage external data store clusters. The SnappyData store can synchronously replicate for high availability (HA) with strong consistency and store/recover from disk for additional reliability.", 
            "title": "The SnappyData Approach"
        }, 
        {
            "location": "/#key-features", 
            "text": "100% compatible with Spark - Use SnappyData as a database, but also use any of the Spark APIs - ML, Graph, etc.    In-memory row and column stores : Run the store collocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster)    SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.    SQL based extensions for streaming processing : Use native Spark streaming, DataFrame APIs or declaratively specify your streams and how you want it processed. You do not need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.    Not-Only SQL : Use either as a SQL database or work with JSON or even arbitrary Application Objects. Essentially, any Spark RDD/DataSet can also be persisted into SnappyData tables (type system same as Spark DataFrames).     Interactive analytics using Synopsis Data Engine (SDE) : Multiple synopses techniques are introduced through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.    Mutate, transact on data in Spark : You can use SQL to insert, update, delete data in tables as one would expect. Extensions to Spark\u2019s context are also provided so you can mutate data in your Spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store.    Optimizations - Indexing : You can index your RowStore and the GemFire SQL optimizer, which automatically uses in-memory indexes when available.    Optimizations - collocation : SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be collocated using declarative custom partitioning strategies (for example, common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent.    High availability not just Fault tolerance : Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster. It is deeply integrated with a membership-based distributed system to detect and handle failures, instantaneously providing applications continuous HA.    Durability and recovery:  Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.", 
            "title": "Key Features"
        }, 
        {
            "location": "/#extensions-to-the-spark-runtime", 
            "text": "SnappyData makes the following contributions to deliver a unified and optimized runtime.    Integrating an operational in-memory data store with Spark\u2019s computational model : A number of extensions are introduced to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy.   This design is extended by allowing low latency and fine- grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, the runtime is extended with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby the state is shared across many clients and applications.     Unified API for OLAP, OLTP, and Streaming : Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL.\nWhile Spark deserves much of the credit for being the first of its kind to offer a unified API, SnappyData further extends its API to:     Allow for OLTP operations, for example, transactions and mutations (inserts/updates/deletions) on tables     Confirm with SQL standards, for example, allowing tables alterations, constraints, indexes, and       Support declarative stream processing in SQL      Optimizing Spark application execution times : Our goal is to eliminate the need for yet another external store (for example, a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting collocated schema designs (tables and streams) where related data is collocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios.    Synopsis Data Engine support built into Spark : The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time.  The SDE engine enables you to:    Intelligently sample the data set on frequently accessed dimensions to have a good representation across the entire data set (stratified sampling). Queries can execute on samples and return answers instantly.    Compute estimates for any ad hoc query from the sample(s). It can also provide error estimates for arbitrarily complex queries on streams.    Provide simple knobs for the user to trade off speed for accuracy, i.e. simple SQL extensions so the user can specify the error tolerance for all queries. When query error is higher than tolerance level, the system automatically delegates the query to the source.    Express their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts.", 
            "title": "Extensions to the Spark Runtime"
        }, 
        {
            "location": "/#spark-challenges-for-mixed-workloads-oltp-olap", 
            "text": "Spark is designed as a computational engine for processing batch jobs. Each Spark application (for example, a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an external storage tier, such as HDFS. SnappyData, on the other hand, targets a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database in the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and decoupled from individual applications.  A second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces:    A single point of contention for all requests, and     A barrier for achieving high availability (HA). Executors are shut down if the driver fails, requiring a full refresh of any cached state.    Spark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, more complex data structures  needs to be managed (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mutability characteristics and conformance to SQL.  Finally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Spark Challenges for Mixed Workloads (OLTP, OLAP)"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Getting Started in 5 Minutes or Less\n\n\nWelcome to the Getting Started section! \n\nMultiple options are provided for getting started with SnappyData. \nDepending on your preference you can try any of the following options:\n\n\n\n\n\n\nOption 1: Getting Started with your Spark Distribution\n\n\n\n\n\n\nOption 2: Getting Started Using Spark Scala APIs\n\n\n\n\n\n\nOption 3: 20X Faster than Spark 2.0.2 Caching\n\n\n\n\n\n\nOption 4: Getting Started using SQL\n\n\n\n\n\n\nOption 5: Getting Started by Installing SnappyData On-Premise\n\n\n\n\n\n\nOption 6: Getting Started on AWS\n\n\n\n\n\n\nOption 7: Getting Started with Docker Image\n\n\n\n\n\n\n\n\nNote\n\n\nSupport for Microsoft Azure will be provided in future releases.\n\n\n\n\n\n\nOption 1: Getting Started with your Spark Distribution\n\n\nIf you are a Spark developer and already using Spark 2.0.0, 2.0.1 or 2.0.2 the fastest way to work with SnappyData is to add SnappyData as a dependency. For instance, using \"package\" option of Spark Shell.\n\n\nThis section contains instructions and examples using which, you can try out SnappyData in 5 minutes or less. You are encouraged to also try out the quick performance benchmark to see the 20X advantage over Spark's native caching performance.\n\n\nOpen a command terminal and go to the location of the Spark installation directory:\n\n\n$ cd \nSpark_Install_dir\n\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \nSnappyDataInc:snappydata:0.9-s_2.11\n\n\n\n\n\nThis opens a Spark Shell and downloads the relevant SnappyData files to your local machine. Depending on your network connection speed, it may take some time to download the files. \nAll SnappyData metadata as well as persistent data is stored in the directory \nquickstartdatadir\n.\n\n\n\nIn this document, it is assumed that you are either familiar with Spark or SQL (not necessarily both). Basic database capabilities like working with Columnar and Row-oriented tables, querying and updating these tables is showcased.\n\n\nTables in SnappyData exhibit many operational capabilities like disk persistence, redundancy for HA, eviction, etc. For more information, you can refer to the \ndetailed documentation\n. \n\n\nWhile SnappyData supports Scala, Java, Python, SQL APIs for this quick start you can choose to work with Scala APIs or SQL depending on your preference.\n\n\n\n\nOption 2: Getting Started Using Spark Scala APIs\n\n\nCreate a SnappySession\n: A SnappySession extends SparkSession so you can mutate data, get much higher performance, etc.\n\n\nscala\n val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\n//Import snappy extensions\nscala\n import snappy.implicits._\n\n\n\n\nCreate a small dataset using Spark's APIs\n:\n\n\nscala\n val ds = Seq((1,\na\n), (2, \nb\n), (3, \nc\n)).toDS()\n\n\n\n\nDefine a schema for the table\n:\n\n\nscala\n  import org.apache.spark.sql.types._\nscala\n  val tableSchema = StructType(Array(StructField(\nCustKey\n, IntegerType, false),\n          StructField(\nCustName\n, StringType, false)))\n\n\n\n\nCreate a column table with a simple schema [String, Int] and default options\n:\nFor detailed option refer to the \nRow and Column Tables\n section.\n\n\n//Column tables manage data is columnar form and offer superier performance for analytic class queries.\nscala\n  snappy.createTable(tableName = \ncolTable\n,\n          provider = \ncolumn\n, // Create a SnappyData Column table\n          schema = tableSchema,\n          options = Map.empty[String, String], // Map for options.\n          allowExisting = false)\n\n\n\n\nSnappyData (SnappySession) extends SparkSession, so you can simply use all the Spark's APIs.\n\n\nInsert the created DataSet to the column table \"colTable\"\n:\n\n\nscala\n  ds.write.insertInto(\ncolTable\n)\n// Check the total row count.\nscala\n  snappy.table(\ncolTable\n).count\n\n\n\n\nCreate a row object using Spark's API and insert the Row to the table\n:\nUnlike Spark DataFrames SnappyData column tables are mutable. You can insert new rows to a column table.\n\n\n// Insert a new record\nscala\n  import org.apache.spark.sql.Row\nscala\n  snappy.insert(\ncolTable\n, Row(10, \nf\n))\n// Check the total row count after inserting the row.\nscala\n  snappy.table(\ncolTable\n).count\n\n\n\n\nCreate a \"row\" table with a simple schema [String, Int] and default options\n: \nFor detailed option refer to the \nRow and Column Tables\n section.\n\n\n//Row formatted tables are better when datasets constantly change or access is selective (like based on a key).\nscala\n  snappy.createTable(tableName = \nrowTable\n,\n          provider = \nrow\n,\n          schema = tableSchema,\n          options = Map.empty[String, String],\n          allowExisting = false)\n\n\n\n\nInsert the created DataSet to the row table \"rowTable\"\n:\n\n\nscala\n  ds.write.insertInto(\nrowTable\n)\n//Check the row count\nscala\n  snappy.table(\nrowTable\n).count\n\n\n\n\nInsert a new record\n:\n\n\nscala\n  snappy.insert(\nrowTable\n, Row(4, \nd\n))\n//Check the row count now\nscala\n  snappy.table(\nrowTable\n).count\n\n\n\n\nChange some data in a row table\n:\n\n\n//Updating a row for customer with custKey = 1\nscala\n  snappy.update(tableName = \nrowTable\n, filterExpr = \nCUSTKEY=1\n,\n                newColumnValues = Row(\nd\n), updateColumns = \nCUSTNAME\n)\n\nscala\n  snappy.table(\nrowTable\n).orderBy(\nCUSTKEY\n).show\n\n//Delete the row for customer with custKey = 1\nscala\n  snappy.delete(tableName = \nrowTable\n, filterExpr = \nCUSTKEY=1\n)\n\n\n\n\n\n//Drop the existing tables\nscala\n  snappy.dropTable(\nrowTable\n, ifExists = true)\nscala\n  snappy.dropTable(\ncolTable\n, ifExists = true)\n\n\n\n\n\n\n\n\nOption 3: 20X Faster than Spark 2.0.2\n\n\nHere you are walked through a simple benchmark to compare SnappyData to Spark 2.0.2 performance.\nMillions of rows are loaded into a cached Spark DataFrame, run some analytic queries measuring its performance and then, repeat the same using SnappyData's column table. \n\n\n\n\nNote\n\n\nIt is recommended that you should have at least 4GB of RAM reserved for this test. \n\n\n\n\nStart the Spark Shell using any of the options mentioned below:\n\n\nIf you are using your own Spark installation(2.0.0, 2.0.1 or 2.0.2):\n\n\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \nSnappyDataInc:snappydata:0.9-s_2.11\n --driver-java-options=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\n\n\n\n\n\nIf you have downloaded SnappyData\n:\n\n\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --driver-java-options=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\n\n\n\n\n\nIf you are using Docker\n:\n\n\n$ docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell --driver-memory=4g --driver-java-options=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\n\n\n\n\n\nTo get the Performance Numbers\n\n\nEnsure that you are in a Spark Shell, and then follow the instruction below to get the performance numbers.\n\n\nDefine a function \"benchmark\"\n, which tells us the average time to run queries after doing the initial warm-ups.\n\n\nscala\n  def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: =\n Unit) {\n          for (i \n- 1 to warmups) {\n            f\n          }\n          val startTime = System.nanoTime\n          for (i \n- 1 to times) {\n            f\n          }\n          val endTime = System.nanoTime\n          println(s\nAverage time taken in $name for $times runs: \n +\n            (endTime - startTime).toDouble / (times * 1000000.0) + \n millis\n)\n        }\n\n\n\n\nCreate a DataFrame and temp table using Spark's range method\n:\nCache it in Spark to get optimal performance. This creates a DataFrame of 100 million records.You can change the number of rows based on  your memory availability.\n\n\nscala\n  var testDF = spark.range(100000000).selectExpr(\nid\n, \nconcat('sym', cast((id % 100) as STRING)) as sym\n)\nscala\n  testDF.cache\nscala\n  testDF.createOrReplaceTempView(\nsparkCacheTable\n)\n\n\n\n\nRun a query and to check the performance\n:\nThe queries use average of a field, without any where clause. This ensures that it touches all records while scanning.\n\n\nscala\n  benchmark(\nSpark perf\n) {spark.sql(\nselect sym, avg(id) from sparkCacheTable group by sym\n).collect()}\n\n\n\n\nClean up the JVM\n:\nThis ensures that all in memory artifacts for Spark is cleaned up.\n\n\nscala\n  testDF.unpersist()\nscala\n  System.gc()\nscala\n  System.runFinalization()\n\n\n\n\nCreate a SnappyContex\n:\n\n\nscala\n  val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\n\n\n\n\nCreate similar 100 million record DataFrame\n:\n\n\nscala\n  testDF = snappy.range(100000000).selectExpr(\nid\n, \nconcat('sym', cast((id % 100) as varchar(10))) as sym\n)\n\n\n\n\nCreate the table\n:\n\n\nscala\n  snappy.sql(\ndrop table if exists snappyTable\n)\nscala\n  snappy.sql(\ncreate table snappyTable (id bigint not null, sym varchar(10) not null) using column\n)\n\n\n\n\nInsert the created DataFrame into the table and measure its performance\n:\n\n\nscala\n  benchmark(\nSnappy insert perf\n, 1, 0) {testDF.write.insertInto(\nsnappyTable\n) }\n\n\n\n\nNow let us run the same benchmark against Spark DataFrame\n:\n\n\nscala\n  benchmark(\nSnappy perf\n) {snappy.sql(\nselect sym, avg(id) from snappyTable group by sym\n).collect()}\n\n\n\n\nscala\n :q // Quit the Spark Shell\n\n\n\n\n\n\nNote\n\n\nThis benchmark code is tested in system with  4 CPUs (Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz) and 16GiB System Memory. In a AWS t2.xlarge (Variable ECUs, 4 vCPUs, 2.4 GHz, Intel Xeon Family, 16 GiB memory, EBS only) instance too SnappyData is approx 16 to 18 times faster than Spark 2.0.2 .\n\n\n\n\n \n\n\nOption 4: Getting Started using SQL\n\n\nSQL using Spark SQL-invoked using the Session API is illustrated. You can also use any SQL client tool (for example, Snappy SQL Shell). For an example, refer to the \nHow-to\n section.\n\n\nCreate a column table with a simple schema [Int, String] and default options.\n\nFor details on the options refer to the \nRow and Column Tables\n section.\n\n\nscala\n  snappy.sql(\ncreate table colTable(CustKey Integer, CustName String) using column options()\n)\n\n\n\n\n//Insert couple of records to the column table\nscala\n  snappy.sql(\ninsert into colTable values(1, 'a')\n)\nscala\n  snappy.sql(\ninsert into colTable values(2, 'b')\n)\nscala\n  snappy.sql(\ninsert into colTable values(3, '3')\n)\n\n\n\n\n// Check the total row count now\nscala\n  snappy.sql(\nselect count(*) from colTable\n).show\n\n\n\n\nCreate a row table with primary key\n:\n\n\n//Row formatted tables are better when datasets constantly change or access is selective (like based on a key).\nscala\n  snappy.sql(\ncreate table rowTable(CustKey Integer NOT NULL PRIMARY KEY, \n +\n            \nCustName String) using row options()\n)\n\n\n\n\nIf you create a table using standard SQL (i.e. no 'row options' clause) it creates a replicated Row table.\n\n\n//Insert couple of records to the row table\nscala\n  snappy.sql(\ninsert into rowTable values(1, 'a')\n)\nscala\n  snappy.sql(\ninsert into rowTable values(2, 'b')\n)\nscala\n  snappy.sql(\ninsert into rowTable values(3, '3')\n)\n\n\n\n\n//Update some rows\nscala\n  snappy.sql(\nupdate rowTable set CustName='d' where custkey = 1\n)\nscala\n  snappy.sql(\nselect * from rowTable order by custkey\n).show\n\n\n\n\n//Drop the existing tables\nscala\n  snappy.sql(\ndrop table if exists rowTable \n)\nscala\n  snappy.sql(\ndrop table if exists colTable \n)\n\n\n\n\nscala\n :q //Quit the Spark Shell\n\n\n\n\nNow that you have seen the basic working of SnappyData tables, let us run the \nbenchmark\n code to see the performance of SnappyData and compare it to Spark's native cache performance.\n\n\n\n\nOption 5: Getting Started by Installing SnappyData On-Premise\n\n\nDownload the latest version of SnappyData from the \nSnappyData Release Page\n page, which lists the latest and previous releases of SnappyData.\n\n\n$ tar -xzf snappydata-0.9-bin.tar.gz\n$ cd snappydata-0.9-bin/\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log\n\n\n\n\nIt opens a Spark Shell. All SnappyData metadata as well as persistent data is stored in the directory \nquickstartdatadir\n. Follow the steps mentioned \nhere\n\n\n\n\nOption 6: Getting Started on AWS\n\n\nYou can quickly create a single host SnappyData cluster (i.e. one lead node, one data node and a locator in a single EC2 instance) through the AWS CloudFormation.\n\n\nPrerequisites\n\n\nBefore you begin:\n\n\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources from CloudFormation\n\n\n\n\n\n\nSign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.\n\n\n\n\n\n\nCreate an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster\n\n\n\n\n\n\nTo launch the cluster from EC2 click \nhere\n and follow the instructions below.\n\n\n\n\n\n\nThe AWS Login Screen is displayed. Enter your AWS login credentials. \n\n\n\n\n\n\nThe \nSelect Template page\n is displayed. The URL for the template (JSON format) is pre-populated. Click \nNext\n to continue.\n\n\n\n\nNote\n\n\nYou are placed in your default region. You can either continue in the selected region or change it in the console. \n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSpecify Details\n page, you can:\n\n\n\n\n\n\nProvide the stack name: Enter a name for the stack. The stack name must contain only letters, numbers, dashes and should start with an alpha character. This is a mandatory field.\n\n\n\n\n\n\nSelect Instance Type: By default, the c4.2xlarge instance (with 8 CPU core and 15 GB RAM) is selected. This is the recommended instance size for running this quickstart.\n\n\n\n\n\n\nSelect KeyPairName: Select a key pair from the list of key pairs available to you. This is a mandatory field.\n\n\n\n\n\n\nSearch VPCID: Select the VPC ID from the drop-down list. Your instance(s) is launched within this VPC. This is a mandatory field.\n \n\n\n\n\n\n\n\n\n\n\n\nClick \nNext\n. \n\n\n\n\n\n\nOn the \nOptions\n page, click \nNext\n to continue using the provided default values.\n\n\n\n\n\n\nOn the \nReview\n page, verify the details and click \nCreate\n to create a stack. \n\n\n\n\n\n\n\n\n\n\nThe next page lists the existing stacks. Click \nRefresh\n to view the updated list. Select the stack to view its status. \nWhen the cluster has started, the status of the stack changes to \nCREATE_COMPLETE\n. This process may take 4-5 minutes to complete.\n\n\n\n\n\n\n\n\nNote\n\n\nIf the status of the stack displays as \nROLLBACK_IN_PROGRESS\n or \nDELETE_COMPLETE\n, the stack creation may have failed. Some common causes of the failure are:\n\n\n\n\n\n\nInsufficient Permissions\n: Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.\n\n\n\n\n\n\nInvalid Keypair\n: Verify that the EC2 key pair exists in the region you selected in the iSight CloudBuilder creation steps.\n\n\n\n\n\n\nLimit Exceeded\n: Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.\n\n\n\n\n\n\n\n\n\n\n\n\nYour cluster is now running. You can explore it using Apache Zeppelin, which provides web-based notebooks for data exploration. The Apache Zeppelin server has already been started on the instance for you. Simply follow its link (URL) from the \nOutputs\n tab.\n\n    \n\n\n\n\n\n\nFor more information, refer to the \nApache Zeppelin\n section or refer to the \nApache Zeppelin documentation\n.\n\n\n\n\nNote\n\n\n\n\n\n\nMulti-node cluster set up on AWS via CloudFormation will be supported in future releases. However, users can set it up using the \nEC2 scripts\n.\n\n\n\n\n\n\nTo stop incurring charges for the instance, you can either terminate the instance or delete the stack after you are done playing with the cluster. However, you cannot connect to or restart an instance after you have terminated it.\n\n\n\n\n\n\n\n\n\n\nOption 7: Getting Started with Docker Image\n\n\nSnappyData comes with a pre-configured container with Docker. The container has binaries for SnappyData. This enables you to easily try the quick start program and more, with SnappyData.\n\n\nThis section assumes you have already installed Docker and it is configured properly. Refer to \nDocker documentation\n for more details.\n\n\nVerify that Docker is installed\n: In the command prompt run the command:\n\n\n$ docker run hello-world\n\n\n\n\n\n\n\nNote\n\n\nEnsure that the Docker containers have access to at least 4GB of RAM on your machine.\n\n\n\n\nGet the Docker Image:\n In the command prompt, type the following command to get the docker image. This starts the container and takes you to the Spark Shell.\n\n\n$  docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell\n\n\n\n\nIt starts downloading the latest image files to your local machine. Depending on your network connection, it may take some time.\nOnce you are inside the Spark Shell with the \"$ scala\n\" prompt, you can follow the steps explained \nhere\n.\n\n\nFor more details about SnappyData docker image see \nSnappy Cloud Tools\n.\n\n\nMore Information\n\n\nFor more examples of the common operations, you can refer to the \nHow-tos\n section. \n\n\nIf you have questions or queries you can contact us through our \ncommunity channels\n.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/quickstart/#getting-started-in-5-minutes-or-less", 
            "text": "Welcome to the Getting Started section!  \nMultiple options are provided for getting started with SnappyData. \nDepending on your preference you can try any of the following options:    Option 1: Getting Started with your Spark Distribution    Option 2: Getting Started Using Spark Scala APIs    Option 3: 20X Faster than Spark 2.0.2 Caching    Option 4: Getting Started using SQL    Option 5: Getting Started by Installing SnappyData On-Premise    Option 6: Getting Started on AWS    Option 7: Getting Started with Docker Image     Note  Support for Microsoft Azure will be provided in future releases.", 
            "title": "Getting Started in 5 Minutes or Less"
        }, 
        {
            "location": "/quickstart/#option-1-getting-started-with-your-spark-distribution", 
            "text": "If you are a Spark developer and already using Spark 2.0.0, 2.0.1 or 2.0.2 the fastest way to work with SnappyData is to add SnappyData as a dependency. For instance, using \"package\" option of Spark Shell.  This section contains instructions and examples using which, you can try out SnappyData in 5 minutes or less. You are encouraged to also try out the quick performance benchmark to see the 20X advantage over Spark's native caching performance.  Open a command terminal and go to the location of the Spark installation directory:  $ cd  Spark_Install_dir \n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages  SnappyDataInc:snappydata:0.9-s_2.11   This opens a Spark Shell and downloads the relevant SnappyData files to your local machine. Depending on your network connection speed, it may take some time to download the files. \nAll SnappyData metadata as well as persistent data is stored in the directory  quickstartdatadir .  \nIn this document, it is assumed that you are either familiar with Spark or SQL (not necessarily both). Basic database capabilities like working with Columnar and Row-oriented tables, querying and updating these tables is showcased.  Tables in SnappyData exhibit many operational capabilities like disk persistence, redundancy for HA, eviction, etc. For more information, you can refer to the  detailed documentation .   While SnappyData supports Scala, Java, Python, SQL APIs for this quick start you can choose to work with Scala APIs or SQL depending on your preference.", 
            "title": "Option 1: Getting Started with your Spark Distribution"
        }, 
        {
            "location": "/quickstart/#option-2-getting-started-using-spark-scala-apis", 
            "text": "Create a SnappySession : A SnappySession extends SparkSession so you can mutate data, get much higher performance, etc.  scala  val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\n//Import snappy extensions\nscala  import snappy.implicits._  Create a small dataset using Spark's APIs :  scala  val ds = Seq((1, a ), (2,  b ), (3,  c )).toDS()  Define a schema for the table :  scala   import org.apache.spark.sql.types._\nscala   val tableSchema = StructType(Array(StructField( CustKey , IntegerType, false),\n          StructField( CustName , StringType, false)))  Create a column table with a simple schema [String, Int] and default options :\nFor detailed option refer to the  Row and Column Tables  section.  //Column tables manage data is columnar form and offer superier performance for analytic class queries.\nscala   snappy.createTable(tableName =  colTable ,\n          provider =  column , // Create a SnappyData Column table\n          schema = tableSchema,\n          options = Map.empty[String, String], // Map for options.\n          allowExisting = false)  SnappyData (SnappySession) extends SparkSession, so you can simply use all the Spark's APIs.  Insert the created DataSet to the column table \"colTable\" :  scala   ds.write.insertInto( colTable )\n// Check the total row count.\nscala   snappy.table( colTable ).count  Create a row object using Spark's API and insert the Row to the table :\nUnlike Spark DataFrames SnappyData column tables are mutable. You can insert new rows to a column table.  // Insert a new record\nscala   import org.apache.spark.sql.Row\nscala   snappy.insert( colTable , Row(10,  f ))\n// Check the total row count after inserting the row.\nscala   snappy.table( colTable ).count  Create a \"row\" table with a simple schema [String, Int] and default options : \nFor detailed option refer to the  Row and Column Tables  section.  //Row formatted tables are better when datasets constantly change or access is selective (like based on a key).\nscala   snappy.createTable(tableName =  rowTable ,\n          provider =  row ,\n          schema = tableSchema,\n          options = Map.empty[String, String],\n          allowExisting = false)  Insert the created DataSet to the row table \"rowTable\" :  scala   ds.write.insertInto( rowTable )\n//Check the row count\nscala   snappy.table( rowTable ).count  Insert a new record :  scala   snappy.insert( rowTable , Row(4,  d ))\n//Check the row count now\nscala   snappy.table( rowTable ).count  Change some data in a row table :  //Updating a row for customer with custKey = 1\nscala   snappy.update(tableName =  rowTable , filterExpr =  CUSTKEY=1 ,\n                newColumnValues = Row( d ), updateColumns =  CUSTNAME )\n\nscala   snappy.table( rowTable ).orderBy( CUSTKEY ).show\n\n//Delete the row for customer with custKey = 1\nscala   snappy.delete(tableName =  rowTable , filterExpr =  CUSTKEY=1 )  //Drop the existing tables\nscala   snappy.dropTable( rowTable , ifExists = true)\nscala   snappy.dropTable( colTable , ifExists = true)", 
            "title": "Option 2: Getting Started Using Spark Scala APIs"
        }, 
        {
            "location": "/quickstart/#option-3-20x-faster-than-spark-202", 
            "text": "Here you are walked through a simple benchmark to compare SnappyData to Spark 2.0.2 performance.\nMillions of rows are loaded into a cached Spark DataFrame, run some analytic queries measuring its performance and then, repeat the same using SnappyData's column table.    Note  It is recommended that you should have at least 4GB of RAM reserved for this test.    Start the Spark Shell using any of the options mentioned below:  If you are using your own Spark installation(2.0.0, 2.0.1 or 2.0.2):  # Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages  SnappyDataInc:snappydata:0.9-s_2.11  --driver-java-options= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g   If you have downloaded SnappyData :  # Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --driver-java-options= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g   If you are using Docker :  $ docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell --driver-memory=4g --driver-java-options= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g", 
            "title": "Option 3: 20X Faster than Spark 2.0.2"
        }, 
        {
            "location": "/quickstart/#to-get-the-performance-numbers", 
            "text": "Ensure that you are in a Spark Shell, and then follow the instruction below to get the performance numbers.  Define a function \"benchmark\" , which tells us the average time to run queries after doing the initial warm-ups.  scala   def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: =  Unit) {\n          for (i  - 1 to warmups) {\n            f\n          }\n          val startTime = System.nanoTime\n          for (i  - 1 to times) {\n            f\n          }\n          val endTime = System.nanoTime\n          println(s Average time taken in $name for $times runs:   +\n            (endTime - startTime).toDouble / (times * 1000000.0) +   millis )\n        }  Create a DataFrame and temp table using Spark's range method :\nCache it in Spark to get optimal performance. This creates a DataFrame of 100 million records.You can change the number of rows based on  your memory availability.  scala   var testDF = spark.range(100000000).selectExpr( id ,  concat('sym', cast((id % 100) as STRING)) as sym )\nscala   testDF.cache\nscala   testDF.createOrReplaceTempView( sparkCacheTable )  Run a query and to check the performance :\nThe queries use average of a field, without any where clause. This ensures that it touches all records while scanning.  scala   benchmark( Spark perf ) {spark.sql( select sym, avg(id) from sparkCacheTable group by sym ).collect()}  Clean up the JVM :\nThis ensures that all in memory artifacts for Spark is cleaned up.  scala   testDF.unpersist()\nscala   System.gc()\nscala   System.runFinalization()  Create a SnappyContex :  scala   val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)  Create similar 100 million record DataFrame :  scala   testDF = snappy.range(100000000).selectExpr( id ,  concat('sym', cast((id % 100) as varchar(10))) as sym )  Create the table :  scala   snappy.sql( drop table if exists snappyTable )\nscala   snappy.sql( create table snappyTable (id bigint not null, sym varchar(10) not null) using column )  Insert the created DataFrame into the table and measure its performance :  scala   benchmark( Snappy insert perf , 1, 0) {testDF.write.insertInto( snappyTable ) }  Now let us run the same benchmark against Spark DataFrame :  scala   benchmark( Snappy perf ) {snappy.sql( select sym, avg(id) from snappyTable group by sym ).collect()}  scala  :q // Quit the Spark Shell   Note  This benchmark code is tested in system with  4 CPUs (Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz) and 16GiB System Memory. In a AWS t2.xlarge (Variable ECUs, 4 vCPUs, 2.4 GHz, Intel Xeon Family, 16 GiB memory, EBS only) instance too SnappyData is approx 16 to 18 times faster than Spark 2.0.2 .", 
            "title": "To get the Performance Numbers"
        }, 
        {
            "location": "/quickstart/#option-4-getting-started-using-sql", 
            "text": "SQL using Spark SQL-invoked using the Session API is illustrated. You can also use any SQL client tool (for example, Snappy SQL Shell). For an example, refer to the  How-to  section.  Create a column table with a simple schema [Int, String] and default options. \nFor details on the options refer to the  Row and Column Tables  section.  scala   snappy.sql( create table colTable(CustKey Integer, CustName String) using column options() )  //Insert couple of records to the column table\nscala   snappy.sql( insert into colTable values(1, 'a') )\nscala   snappy.sql( insert into colTable values(2, 'b') )\nscala   snappy.sql( insert into colTable values(3, '3') )  // Check the total row count now\nscala   snappy.sql( select count(*) from colTable ).show  Create a row table with primary key :  //Row formatted tables are better when datasets constantly change or access is selective (like based on a key).\nscala   snappy.sql( create table rowTable(CustKey Integer NOT NULL PRIMARY KEY,   +\n             CustName String) using row options() )  If you create a table using standard SQL (i.e. no 'row options' clause) it creates a replicated Row table.  //Insert couple of records to the row table\nscala   snappy.sql( insert into rowTable values(1, 'a') )\nscala   snappy.sql( insert into rowTable values(2, 'b') )\nscala   snappy.sql( insert into rowTable values(3, '3') )  //Update some rows\nscala   snappy.sql( update rowTable set CustName='d' where custkey = 1 )\nscala   snappy.sql( select * from rowTable order by custkey ).show  //Drop the existing tables\nscala   snappy.sql( drop table if exists rowTable  )\nscala   snappy.sql( drop table if exists colTable  )  scala  :q //Quit the Spark Shell  Now that you have seen the basic working of SnappyData tables, let us run the  benchmark  code to see the performance of SnappyData and compare it to Spark's native cache performance.", 
            "title": "Option 4: Getting Started using SQL"
        }, 
        {
            "location": "/quickstart/#option-5-getting-started-by-installing-snappydata-on-premise", 
            "text": "Download the latest version of SnappyData from the  SnappyData Release Page  page, which lists the latest and previous releases of SnappyData.  $ tar -xzf snappydata-0.9-bin.tar.gz\n$ cd snappydata-0.9-bin/\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log  It opens a Spark Shell. All SnappyData metadata as well as persistent data is stored in the directory  quickstartdatadir . Follow the steps mentioned  here", 
            "title": "Option 5: Getting Started by Installing SnappyData On-Premise"
        }, 
        {
            "location": "/quickstart/#option-6-getting-started-on-aws", 
            "text": "You can quickly create a single host SnappyData cluster (i.e. one lead node, one data node and a locator in a single EC2 instance) through the AWS CloudFormation.", 
            "title": "Option 6: Getting Started on AWS"
        }, 
        {
            "location": "/quickstart/#prerequisites", 
            "text": "Before you begin:    Ensure that you have an existing AWS account with required permissions to launch EC2 resources from CloudFormation    Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.    Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster    To launch the cluster from EC2 click  here  and follow the instructions below.    The AWS Login Screen is displayed. Enter your AWS login credentials.     The  Select Template page  is displayed. The URL for the template (JSON format) is pre-populated. Click  Next  to continue.   Note  You are placed in your default region. You can either continue in the selected region or change it in the console.        On the  Specify Details  page, you can:    Provide the stack name: Enter a name for the stack. The stack name must contain only letters, numbers, dashes and should start with an alpha character. This is a mandatory field.    Select Instance Type: By default, the c4.2xlarge instance (with 8 CPU core and 15 GB RAM) is selected. This is the recommended instance size for running this quickstart.    Select KeyPairName: Select a key pair from the list of key pairs available to you. This is a mandatory field.    Search VPCID: Select the VPC ID from the drop-down list. Your instance(s) is launched within this VPC. This is a mandatory field.        Click  Next .     On the  Options  page, click  Next  to continue using the provided default values.    On the  Review  page, verify the details and click  Create  to create a stack.       The next page lists the existing stacks. Click  Refresh  to view the updated list. Select the stack to view its status. \nWhen the cluster has started, the status of the stack changes to  CREATE_COMPLETE . This process may take 4-5 minutes to complete.     Note  If the status of the stack displays as  ROLLBACK_IN_PROGRESS  or  DELETE_COMPLETE , the stack creation may have failed. Some common causes of the failure are:    Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.    Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the iSight CloudBuilder creation steps.    Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.       Your cluster is now running. You can explore it using Apache Zeppelin, which provides web-based notebooks for data exploration. The Apache Zeppelin server has already been started on the instance for you. Simply follow its link (URL) from the  Outputs  tab. \n        For more information, refer to the  Apache Zeppelin  section or refer to the  Apache Zeppelin documentation .   Note    Multi-node cluster set up on AWS via CloudFormation will be supported in future releases. However, users can set it up using the  EC2 scripts .    To stop incurring charges for the instance, you can either terminate the instance or delete the stack after you are done playing with the cluster. However, you cannot connect to or restart an instance after you have terminated it.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/quickstart/#option-7-getting-started-with-docker-image", 
            "text": "SnappyData comes with a pre-configured container with Docker. The container has binaries for SnappyData. This enables you to easily try the quick start program and more, with SnappyData.  This section assumes you have already installed Docker and it is configured properly. Refer to  Docker documentation  for more details.  Verify that Docker is installed : In the command prompt run the command:  $ docker run hello-world   Note  Ensure that the Docker containers have access to at least 4GB of RAM on your machine.   Get the Docker Image:  In the command prompt, type the following command to get the docker image. This starts the container and takes you to the Spark Shell.  $  docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell  It starts downloading the latest image files to your local machine. Depending on your network connection, it may take some time.\nOnce you are inside the Spark Shell with the \"$ scala \" prompt, you can follow the steps explained  here .  For more details about SnappyData docker image see  Snappy Cloud Tools .", 
            "title": "Option 7: Getting Started with Docker Image"
        }, 
        {
            "location": "/quickstart/#more-information", 
            "text": "For more examples of the common operations, you can refer to the  How-tos  section.   If you have questions or queries you can contact us through our  community channels .", 
            "title": "More Information"
        }, 
        {
            "location": "/install/", 
            "text": "Overview\n\n\nThe following installation options are available:\n\n\n\n\n\n\nInstall On Premise\n\n\n\n\n\n\nSetting up Cluster on Amazon Web Services (AWS)\n\n\n\n\n\n\nBuilding from Source\n\n\n\n\n\n\n\n\nNote\n\n\nConfiguring the limit for Open Files and Threads/Processes \n On a Linux system you can set the limit of open files and thread processes in the \n/etc/security/limits.conf\n file. \nA minimum of \n8192\n is recommended for open file descriptors limit and \n128K\n is recommended for the number of active threads. \nA typical configuration used for SnappyData servers and leads can look like:\n\n\n\n\nsnappydata          hard    nofile      81920\nsnappydata          soft    nofile      8192\nsnappydata          hard    nproc       unlimited\nsnappydata          soft    nproc       524288\nsnappydata          hard    sigpending  unlimited\nsnappydata          soft    sigpending  524288\n\n\n\n\nHere \nsnappydata\n is the user name under which the SnappyData processes are started. \n\n\n\n\nInstall On-Premise\n\n\nSnappyData runs on UNIX-like systems (for example, Linux, Mac OS). With on-premises installation, SnappyData is installed and operated from your in-house computing infrastructure.\n\n\nPrerequisites\n\n\nBefore you start the installation, make sure that Java SE Development Kit 8 is installed, and the \nJAVA_HOME\n environment variable is set on each computer.\n\n\nDownload SnappyData\n\n\nDownload the latest version of SnappyData from the \nSnappyData Release\n page, which lists the latest and previous releases of SnappyData.\n\n\nThe packages are available in compressed files (.zip and .tar format). On this page, you can also view details of features and enhancements introduced in specific releases.\n\n\n\n\n\n\nSnappyData 0.9 download link\n\n\n(tar.gz)\n \n(zip)\n\n\n\n\n\n\nSnappyData 0.9 (user-provided Hadoop) download link\n \n(tar.gz)\n \n(zip)\n\n\n\n\n\n\n\n\nSingle Host Installation\n\n\nThis is the simplest form of deployment and can be used for testing and POCs.\n\n\nOpen the command prompt and run the following command to extract the downloaded archive file and to go the location of the SnappyData home directory.\n\n\n$ tar -xzf snappydata-0.9-bin.tar.gz\n$ cd snappydata-0.9-bin/\n\n\n\n\nStart a basic cluster with one data node, one lead, and one locator\n\n\n./sbin/snappy-start-all.sh\n\n\n\n\nFor custom configuration and to start more nodes,  see the section \nHow to Configure the SnappyData cluster\n.\n\n\nMulti-Host Installation\n\n\nFor real life use cases, you need multiple machines on which SnappyData can be deployed. You can start one or more SnappyData node on a single machine based on your machine size.\n\n\nMachines with a Shared Path\n\n\nIf all your machines can share a path over an NFS or similar protocol, then follow the steps below:\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that the \n/etc/hosts\n correctly configures the host and IP address of each SnappyData member machine.\n\n\n\n\n\n\nEnsure that SSH is supported and you have configured all machines to be accessed by \npasswordless SSH\n.\n\n\n\n\n\n\nSteps to Set up the Cluster\n\n\n\n\n\n\nCopy the downloaded binaries to the shared folder.\n\n\n\n\n\n\nExtract the downloaded archive file and go to SnappyData home directory.\n\n\n$ tar -xzf snappydata-0.9-bin.tar.gz\n$ cd snappydata-0.9-bin/\n\n\n\n\n\n\n\nConfigure the cluster as described in \nHow to Configure SnappyData cluster\n.\n\n\n\n\n\n\nAfter configuring each of the components, run the \nsnappy-start-all.sh\n script:\n\n\n./sbin/snappy-start-all.sh\n\n\n\n\n\n\n\nThis creates a default folder named \nwork\n and stores all SnappyData member's artifacts separately. Each member folder is identified by the name of the node.\n\n\nIf SSH is not supported then follow the instructions in the Machines without a Shared Path section.\n\n\nMachines without a Shared Path\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that the \n/etc/hosts\n correctly configures the host and IP Address of each SnappyData member machine.\n\n\n\n\n\n\nOn each host machine, create a new member working directory for each SnappyData member, that you want to run the host. \n The member working directory provides a default location for the log, persistence, and status files for each member, and is also used as the default location for locating the member's configuration files.\n\nFor example, if you want to run both a locator and server member on the local machine, create separate directories for each member.\n\n\n\n\n\n\nTo Configure the Cluster\n\n\n\n\n\n\nCopy and extract the downloaded binaries on each machine\n\n\n\n\n\n\nIndividually configure and start each member\n\n\n\n\n\n\n\n\nNote\n\n\nAll configuration parameter are provided as command line arguments rather than reading from a conf file.\n\n\n\n\nThe example below starts a cluster by individually launching locator, server and lead processes.\n\n\n$ bin/snappy locator start  -dir=/node-a/locator1\n$ bin/snappy server start  -dir=/node-b/server1  -locators:localhost:10334\n$ bin/snappy leader start  -dir=/node-c/lead1  -locators:localhost:10334\n\n$ bin/snappy locator stop -dir=/node-a/locator1\n$ bin/snappy server stop -dir=/node-b/server1\n$ bin/snappy leader stop -dir=/node-c/lead1top\n\n\n\n\n\n\nSetting up Cluster on Amazon Web Services (AWS)\n\n\nUsing AWS Management Console\n\n\nYou can launch a SnappyData cluster on Amazon EC2 instance(s) using AMI provided by SnappyData. For more information on launching an EC2 instance, refer to the \nAWS documentation\n.\n\n\nPrerequisites\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources\n\n\nLaunching the Instance\n\n\nTo launch the instance and start SnappyData cluster:\n\n\n\n\n\n\nOpen the \nAmazon EC2 console\n and sign in using your AWS login credentials.\n\n\n\n\n\n\nThe current region is displayed at the top of the screen. Select the region where you want to launch the instance.\n\n\n\n\n\n\nClick \nLaunch Instance\n from the Amazon EC2 console dashboard.\n\n\n\n\n\n\nOn the \nChoose an Amazon Machine Image (AMI)\n page, select \nCommunity AMIs\n from the left pane.\n\n\n\n\n\n\nEnter \nSnappyData\n in the search box, and press \nEnter\n on your keyboard.\n\n\n\n\n\n\nThe search result is displayed. From the search results, click \nSelect\n to choose the AMI with the latest release version.\n\n\n\n\n\n\nOn the \nChoose an Instance Type\n page, select the instance type as per the requirement of your use case and then click \nReview and Launch\n to launch the instance with default configurations. \n\n\n\n\nNote\n\n\n\n\n\n\nYou can also continue customizing your instance before you launch the instance. Refer to the AWS documentation for more information.\n\n\n\n\n\n\nWhen configuring the security groups, ensure that you open at least ports 22 (for SSH access to the EC2 instance) and 5050 (for access to Snappy UI).\n\n\n\n\n\n\n\n\n\n\n\n\nYou are directed to the last step \nReview Instance Launch\n. Check the details of your instance, and click \nLaunch\n.\n\n\n\n\n\n\nIn the \nSelect an existing key pair or create a new key pair\n dialog box, select a key pair.\n\n\n\n\n\n\nClick \nLaunch\n to launch the instances.\n\n\n\n\n\n\nThe dashboard which lists the instances is displayed. Click \nRefresh\n to view the updated list and the status of the instance creation.\n\n\n\n\n\n\nOnce the status of the instance changes to \nrunning\n, you have successfully created and launched the instance with the SnappyData AMI.\n\n\n\n\n\n\nUse SSH to connect to the instance using the \nubuntu\n username. You require:\n\n\n\n\n\n\nThe private key file of the key pair with which the instance was launched, and\n\n\n\n\n\n\nDetails of the public hostname or IP address of the instance.\nRefer to the following documentation, for more information on \naccessing an EC2 instance\n.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe public hostname/IP address information is available on the EC2 dashboard \n \nDescription\n tab. \n\n\n\n\n\n\nThe SnappyData binaries are automatically downloaded and extracted to the location \n/snappydata/downloads/\n and Java 8 is installed. \n\n\n\n\n\n\n\n\n\n\n\n\nFollow the \nsteps described here\n to continue. \n\n\n\n\n\n\n\n\nUsing SnappyData EC2 Scripts\n\n\nThe \nsnappy-ec2\n script enables users to quickly launch and manage SnappyData clusters on Amazon EC2. You can also configure the individual nodes of the cluster by providing properties in specific conf files which the script reads before launching the cluster.\n\n\nThe \nsnappy-ec2\n script has been derived from the \nspark-ec2\n script available in \nApache Spark 1.6\n.\n\n\nThe scripts are available on GitHub in the \nsnappy-cloud-tools repository\n and also as a \n.tar.gz\n file.\n\n\n\n\nNote\n\n\nThe EC2 script is under development. Feel free to try it out and provide your feedback.\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources\n\n\n\n\n\n\nCreate an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster\n\nRefer to the Amazon Web Services EC2 documentation for more information on \ngenerating your own EC2 Key Pair\n.\n\n\n\n\n\n\nUsing the AWS Secret Access Key and the Access Key ID, set the two environment variables, \nAWS_SECRET_ACCESS_KEY\n and \nAWS_ACCESS_KEY_ID\n. You can find this information in the AWS IAM console page.\n\nIf you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file.\n\n\nFor example:\n\nexport AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112\n\n\nexport AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10\n\n\n\n\n\n\nEnsure Python v 2.7 or later is installed on your local computer.\n\n\n\n\n\n\nCluster Management\n\n\nLaunching SnappyData Cluster\n\n\nIn the command prompt, go to the directory where the \nsnappydata-ec2-\nversion\n.tar.gz\n is extracted or to the aws/ec2 directory where the \nSnappyData cloud tools repository\n is cloned locally.\n\n\nEnter the command in the following format.\n\n\n./snappy-ec2 -k \nyour-key-name\n -i \nyour-keyfile-path\n \naction\n \nyour-cluster-name\n\n\nHere, \nyour-key-name\n refers to the EC2 Key Pair, \nyour-keyfile-path\n refers to the path to the key file and \naction\n refers to the action to be performed (for example, launch, start, stop).\n\n\nBy default, the script starts one instance of a locator, lead and server each.\nThe script identifies each cluster by its unique cluster name (you provided) and internally ties members (locators, leads, and stores/servers) of the cluster with EC2 security groups.\n\n\nThe  names and details of the members are automatically derived from the provided cluster name.\n\n\nFor example, if you launch a cluster named \nmy-cluster\n, the locator is available in security group named \nmy-cluster-locator\n and the store/server are available in \nmy-cluster-store\n.\n\n\nWhen running the script you can also specify properties like the number of stores and region.\n\n\nFor example, using the following command, you can start a SnappyData cluster named \nsnappydata-cluster\n with 2 stores (or servers) in the N. California (us-west-1) region on AWS. It also starts an Apache Zeppelin server on the instance where lead is running.\n\n\nThe examples below assume that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin=embedded --region=us-west-1 launch snappydata-cluster\n\n\n\n\nTo start Apache Zeppelin on a separate instance, use \n--with-zeppelin=non-embedded\n.\n\n\nSpecifying Properties\n\n\nIf you want to configure each of the locator, lead or server with specific properties, you can do so by specifying them in files named \nlocators\n, \nleads\n or \nservers\n, respectively and placing these under aws/ec2/deploy/home/ec2-user/snappydata/. Refer to \nthis SnappyData documentation page\n for example on how to write these conf files.\n\n\nThis is similar to how one would provide properties to SnappyData cluster nodes while launching it using the \nsbin/snappy-start-all.sh\n script.\n\n\nThe important difference here is that, instead of the host names of the locator, lead or store, you have to write {{LOCATOR_N}}, {{LEAD_N}} or {{SERVER_N}} in these files, respectively. N stands for Nth locator, lead or server. The script replaces these with the actual host name of the members when they are launched.\n\n\nThe sample conf files for a cluster with 2 locators, 1 lead and 2 stores are given below:\n\n\nlocators\n\n\n{{LOCATOR_0}} -peer-discovery-port=9999 -heap-size=1024m\n{{LOCATOR_1}} -peer-discovery-port=9888 -heap-size=1024m\n\n\n\n\nleads\n\n\n{{LEAD_0}} -heap-size=4096m -spark.ui.port=3333 -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -spark.executor.cores=10\n\n\n\n\nservers\n\n\n{{SERVER_0}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888\n{{SERVER_1}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -client-port=1530\n\n\n\n\nWhen you run \nsnappy-ec2\n, it looks for these files under \naws/ec2/deploy/home/ec2-user/snappydata/\n and, if present, reads them while launching the cluster on Amazon EC2. Ensure that the number of locators, leads or servers specified by options \n--locators\n, \n--leads\n or \n--stores\n must match to the number of entries in their respective conf files.\n\n\nThe script also reads \nsnappy-env.sh\n, if present in this location.\n\n\nStopping a Cluster\n\n\nWhen you stop a cluster, it shuts down the EC2 instances and any data saved on its local instance stores is lost. However, the data saved on EBS volumes, if any, is retained unless the spot-instances were used.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem stop cluster-name\n\n\n\n\nStarting a Cluster\n\n\nWhen you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem start cluster-name\n\n\n\n\n\n\nNote\n\n\nThe start command (or launch command with --resume option) ignores --locators, --leads or --stores options and launches SnappyData cluster on existing instances. But the conf files are read in any case if they are present in the location mentioned above. So you need to ensure that every time you use start command, the number of entries in conf files are equal to the number of instances in their respective security group.\n\n\n\n\nAdding Servers to  a Cluster\n\n\nThis is not yet fully supported via the script. You may have to manually launch an instance with \n(cluster-name)-stores\n group and then use launch command with --resume option.\n\n\nListing Members of a Cluster\n\n\nTo get the first locator's hostname:\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem get-locator cluster-name\n\n\n\n\nTo get the first lead's hostname, use the get-lead command:\n\n\nConnecting to a Cluster\n\n\nYou can connect to any instance of a cluster with SSH using the login command. It logs you into the first lead instance. From there, you can SSH to any other member of the cluster without a password.\nThe SnappyData product directory is located under /home/ec2-user/snappydata/ on all the members.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem login cluster-name\n\n\n\n\nDestroying a Cluster\n\n\nDestroying a cluster destroys all the data on the local instance stores as well as on the attached EBS volumes permanently.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem destroy cluster-name\n\n\n\n\nThis retains the security groups created for this cluster. To delete them as well, use it with --delete-group option.\n\n\nStarting Cluster with Apache Zeppelin\n\n\nOptionally, you can start an instance of Apache Zeppelin server with the cluster. \nApache Zeppelin\n is a web-based notebook that enables interactive notebook. You can start it either on a lead node's instance (\n--with-zeppelin=embedded\n)  or on a separate instance (\n--with-zeppelin=non-embedded\n).\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --with-zeppelin=embedded launch cluster-name\n\n\n\n\nMore options\n\n\nFor a complete list of options this script has, run \n./snappy-ec2\n. The options are also provided below for quick reference.\n\n\nUsage: `snappy-ec2 [options] \naction\n \ncluster_name\n`\n\n\naction\n can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -s STORES, --stores=STORES\n                        Number of stores to launch (default: 1)\n  --locators=LOCATORS   Number of locator nodes to launch (default: 1)\n  --leads=LEADS         Number of lead nodes to launch (default: 1)\n  -w WAIT, --wait=WAIT  DEPRECATED (no longer necessary) - Seconds to wait for\n                        nodes to start\n  -k KEY_PAIR, --key-pair=KEY_PAIR\n                        Key pair to use on instances\n  -i IDENTITY_FILE, --identity-file=IDENTITY_FILE\n                        SSH private key file to use for logging into instances\n  -p PROFILE, --profile=PROFILE\n                        If you have multiple profiles (AWS or boto config),\n                        you can configure additional, named profiles by using\n                        this option (default: none)\n  -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE\n                        Type of instance to launch (default: m3.large).\n                        WARNING: must be 64-bit; small instances won't work\n  --locator-instance-type=LOCATOR_INSTANCE_TYPE\n                        Locator instance type (leave empty for same as\n                        instance-type)\n  -r REGION, --region=REGION\n                        EC2 region used to launch instances in, or to find\n                        them in (default: us-east-1)\n  -z ZONE, --zone=ZONE  Availability zone to launch instances in, or 'all' to\n                        spread stores across multiple (an additional $0.01/Gb\n                        for bandwidthbetween zones applies) (default: a single\n                        zone chosen at random)\n  -a AMI, --ami=AMI     Amazon Machine Image ID to use\n  -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION\n                        Version of SnappyData to use: 'X.Y.Z' (default:\n                        LATEST)\n  --with-zeppelin=WITH_ZEPPELIN\n                        Launch Apache Zeppelin server with the cluster. Use\n                        'embedded' to launch it on lead node and 'non-\n                        embedded' to launch it on a separate instance.\n  --deploy-root-dir=DEPLOY_ROOT_DIR\n                        A directory to copy into/on the first master. Must\n                        be absolute. Note that a trailing slash is handled as\n                        per rsync: If you omit it, the last directory of the\n                        --deploy-root-dir path will be created in / before\n                        copying its contents. If you append the trailing\n                        slash, the directory is not created and its contents\n                        are copied directly into /. (default: none).\n  -D [ADDRESS:]PORT     Use SSH dynamic port forwarding to create a SOCKS\n                        proxy at the given local address (for use with login)\n  --resume              Resume installation on a previously launched cluster\n                        (for debugging)\n  --ebs-vol-size=SIZE   Size (in GB) of each EBS volume.\n  --ebs-vol-type=EBS_VOL_TYPE\n                        EBS volume type (e.g. 'gp2', 'standard').\n  --ebs-vol-num=EBS_VOL_NUM\n                        Number of EBS volumes to attach to each node as\n                        /vol[x]. The volumes will be deleted when the\n                        instances terminate. Only possible on EBS-backed AMIs.\n                        EBS volumes are only attached if --ebs-vol-size \n 0.\n                        Only support up to 8 EBS volumes.\n  --placement-group=PLACEMENT_GROUP\n                        Which placement group to try and launch instances\n                        into. Assumes placement group is already created.\n  --spot-price=PRICE    If specified, launch stores as spot instances with the\n                        given maximum price (in dollars)\n  -u USER, --user=USER  The SSH user you want to connect as (default:\n                        ec2-user)\n  --delete-groups       When destroying a cluster, delete the security groups\n                        that were created\n  --use-existing-locator\n                        Launch fresh stores, but use an existing stopped\n                        locator if possible\n  --user-data=USER_DATA\n                        Path to a user-data file (most AMIs interpret this as\n                        an initialization script)\n  --authorized-address=AUTHORIZED_ADDRESS\n                        Address to authorize on created security groups\n                        (default: 0.0.0.0/0)\n  --additional-security-group=ADDITIONAL_SECURITY_GROUP\n                        Additional security group to place the machines in\n  --additional-tags=ADDITIONAL_TAGS\n                        Additional tags to set on the machines; tags are\n                        comma-separated, while name and value are colon\n                        separated; ex: \nTask:MySnappyProject,Env:production\n\n  --subnet-id=SUBNET_ID\n                        VPC subnet to launch instances in\n  --vpc-id=VPC_ID       VPC to launch instances in\n  --private-ips         Use private IPs for instances rather than public if\n                        VPC/subnet requires that.\n  --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR\n                        Whether instances should terminate when shut down or\n                        just stop\n  --instance-profile-name=INSTANCE_PROFILE_NAME\n                        IAM profile name to launch instances under\n\n\n\n\nLimitations\n\n\nSome of the known limitations of the script are:\n\n\n\n\n\n\nLaunching the cluster on custom AMI (specified via --ami option) will not work if it does not have the user 'ec2-user' with sudo permissions\n\n\n\n\n\n\nSupport for option --user is incomplete\n\n\n\n\n\n\n\n\nBuilding from Source\n\n\nBuilding SnappyData requires JDK 8 installation (\nOracle Java SE\n).\n\n\nQuickstart to build all components of SnappyData:\n\n\nLatest release branch\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.9 --recursive\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nMaster\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git --recursive\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nThe product is in \nbuild-artifacts/scala-2.11/snappy\n\n\nIf you want to build only the top-level SnappyData project but pull in jars for other projects (\nspark\n, \nstore\n, \nspark-jobserver\n):\n\n\nLatest release branch\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.9\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nMaster\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nRepository Layout\n\n\n\n\n\n\ncore\n - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between \nspark\n and \nstore\n (GemFireXD). For example, SnappyContext, row and column store, streaming additions etc.\n\n\n\n\n\n\ncluster\n - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc.\n\n\n\n\n\n\nThis component depends on \ncore\n and \nstore\n.  The code in the \ncluster\n depends on \ncore\n but not the other way round.\n\n\n\n\n\n\nspark\n - \nApache Spark\n code with SnappyData enhancements.\n\n\n\n\n\n\nstore\n - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch.\n\n\n\n\n\n\nspark-jobserver\n - Fork of \nspark-jobserver\n project with some additions to integrate with SnappyData.\n\n\n\n\n\n\nThe \nspark\n, \nstore\n, and \nspark-jobserver\n directories are required to be clones of the respective SnappyData repositories and are integrated into the top-level SnappyData project as git sub-modules. When working with sub-modules, updating the repositories follows the normal \ngit submodules\n. One can add some aliases in gitconfig to aid pull/push like:\n\n\n[alias]\n  spull = !git pull \n git submodule sync --recursive \n git submodule update --init --recursive\n  spush = push --recurse-submodules=on-demand\n\n\n\n\nThe above aliases can serve as useful shortcuts to pull and push all projects from top-level \nsnappydata\n repository.\n\n\nBuilding\n\n\nGradle is the build tool used for all the SnappyData projects. Changes to \nApache Spark\n and \nspark-jobserver\n forks include the addition of Gradle build scripts to allow building them independently as well as a subproject of SnappyData. The only requirement for the build is a JDK 8 installation. The Gradle wrapper script downloads all the other build dependencies as required.\n\n\nIf a user does not want to deal with submodules and only work on SnappyData project, then can clone only the SnappyData repository (without the --recursive option) and the build will pull those SnappyData project jar dependencies from maven central.\n\n\nIf working on all the separate projects integrated inside the top-level SnappyData clone, the Gradle build will recognize the same and build those projects too and include the same in the top-level product distribution jar. The \nspark\n and \nstore\n submodules can also be built and published independently.\n\n\nUseful build and test targets:\n\n\n./gradlew assemble      -  build all the sources\n./gradlew testClasses   -  build all the tests\n./gradlew product       -  build and place the product distribution\n                           (in build-artifacts/scala_2.11/snappy)\n./gradlew distTar       -  create a tar.gz archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew distZip       -  create a zip archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew buildAll      -  build all sources, tests, product, packages (all targets above)\n./gradlew checkAll      -  run testsuites of snappydata components\n./gradlew cleanAll      -  clean all build and test output\n./gradlew runQuickstart -  run the quickstart suite (the \nGetting Started\n section of docs)\n./gradlew precheckin    -  cleanAll, buildAll, scalaStyle, build docs,\n                           and run full snappydata testsuite including quickstart\n./gradlew precheckin -Pstore  -  cleanAll, buildAll, scalaStyle, build docs,\n                           run full snappydata testsuite including quickstart\n                           and also full SnappyData store testsuite\n\n\n\n\nThe default build directory is \nbuild-artifacts/scala-2.11\n for projects. An exception is \nstore\n project, where the default build directory is \nbuild-artifacts/;os;\n where \n;os;\n is \nlinux\n on Linux systems, \nosx\n on Mac, \nwindows\n on Windows.\n\n\nThe usual Gradle test run targets (\ntest\n, \ncheck\n) work as expected for JUnit tests. Separate targets have been provided for running Scala tests (\nscalaTest\n) while the \ncheck\n target runs both the JUnit and ScalaTests. One can run a single Scala test suite class with \nsingleSuite\n option while running a single test within some suite works with the \n--tests\n option:\n\n\n ./gradlew core:scalaTest -PsingleSuite=**.ColumnTableTest  # run all tests in the class\n\n ./gradlew core:scalaTest \\\n\n    --tests \nTest the creation/dropping of table using SQL\n  # run a single test (use full name)\n\n\n\n\nRunning individual tests within some suite works using the \n--tests\n argument.\n\n\nSetting up IntelliJ IDEA with Gradle\n\n\nIntellij IDEA is the IDE commonly used by the SnappyData developers. Those who really prefer Eclipse can try the Scala-IDE and Gradle support but has been seen to not work as well. Steps required for setting up SnappyData with all its components in IDEA are listed below.\n\n\nTo import into IntelliJ IDEA:\n\n\n\n\n\n\nUpdate IntelliJ IDEA to the latest version, including the latest Scala plug-in. Older versions (pre 14.x) have trouble dealing with Scala code, particularly some of the code in Spark. Ensure JDK 8 is installed and IDEA can find it (either in PATH or via \nJAVA_HOME\n).\n\n\n\n\n\n\nIncrease the available JVM heap size for IDEA. Open bin/idea64.vmoptions (assuming 64-bit JVM) and increase -Xmx option to be something like \n-Xmx=2g\n for comfortable use.\n\n\n\n\n\n\nSelect \nImport Project\n, and then point to the SnappyData directory. Use external Gradle import. \nUn-select\n the \"Create separate module per source set\" option while other options can be defaults. Click \nNext\n in the following screens.\n\n\n\n\nNote\n\n\n\n\n\n\nIgnore the \n\"Gradle location is unknown warning\"\n.\n\n\n\n\n\n\nEnsure that the JDK 8 installation has been selected.\n\n\n\n\n\n\nIgnore and dismiss the \n\"Unindexed remote maven repositories found\"\n warning message if seen.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen import is completed, go to \nFile\n Settings\n Editor\n Code Style\n Scala\n. Set the scheme as \nProject\n. Check that the same has been set in Java Code Style too. Click OK to apply and close it. Next, copy \ncodeStyleSettings.xml\n located in the SnappyData top-level directory, to the \n.idea\n directory created by IDEA. Check that the settings are now applied in \nFile\n Settings\n Editor\n Code Style\n Java\n which should display Indent as 2 and continuation indent as 4 (same as Scala).\n\n\n\n\n\n\nIf the Gradle tab is not visible immediately, then select it from window list pop-up at the left-bottom corner of IDE. If you click on that window list icon, then the tabs are displayed permanently.\n\n\n\n\n\n\nGenerate Apache Avro and SnappyData required sources by expanding: \nsnappydata_2.11\n Tasks\n other\n. Right-click on \ngenerateSources\n and run it. The Run option may not be available if indexing is still in progress, wait for indexing to complete, and then try again. \n The first run may some time to complete,  as it downloads jar files and other required files. This step has to be done the first time, or if \n./gradlew clean\n has been run, or if you have made changes to \njavacc/avro/messages.xml\n source files.\n\n\n\n\n\n\nIf you get unexpected \n\"Database not found\"\n or \nNullPointerException\n errors in SnappyData-store/GemFireXD layer, then first thing to try is to run the \ngenerateSources\n target again.\n\n\n\n\n\n\nIncrease the compiler heap sizes or else the build can take quite long especially with integrated \nspark\n and \nstore\n. In \nFile\n Settings\n Build, Execution, Deployment\n Compiler increase\n, \nBuild process heap size\n to say 1536 or 2048. Similarly, increase JVM maximum heap size in \nLanguages \n Frameworks\n Scala Compiler Server\n to 1536 or 2048.\n\n\n\n\n\n\nTest the full build.\n\n\n\n\n\n\nFor JUnit tests configuration also append \n/build-artifacts\n to the working directory. That is, open \nRun\n Edit Configurations\n, expand \nDefaults\n and select \nJUnit\n, the working directory should be \n\\$MODULE_DIR\\$/build-artifacts\n. Likewise append \nbuild-artifacts\n to working directory for ScalaTest. Without this all intermediate log and other files pollute the source tree and then need to be cleaned manually.\n\n\n\n\n\n\nRunning a ScalaTest/JUnit\n\n\nRunning Scala/JUnit tests from IntelliJ IDEA is straightforward.\n\n\n\n\n\n\nWhen selecting a run configuration for JUnit/ScalaTest, avoid selecting the Gradle one (green round icon) otherwise an external Gradle process is launched that can start building the project again and won't be cleanly integrated with IDEA. Use the normal JUnit (red+green arrows icon) or ScalaTest (JUnit like with red overlay).\n\n\n\n\n\n\nFor JUnit tests, ensure that working directory is the top-level \n\\$MODULE_DIR\\$/build-artifacts\n as mentioned before. Otherwise, many SnappyData-store tests will fail to find the resource files required in tests. They also pollute the files etc, so when launched this will allow those to go into \nbuild-artifacts\n that is easier to clean. For that reason, it is preferable to do the same for ScalaTests.\n\n\n\n\n\n\nSome of the tests use data files from the \ntests-common\n directory. For such tests, run the Gradle task \nsnappydata_2.11\n Tasks\n other\n copyResourcesAll\n to copy the resources in build area where IDEA runs can find it.", 
            "title": "Download and Install"
        }, 
        {
            "location": "/install/#overview", 
            "text": "The following installation options are available:    Install On Premise    Setting up Cluster on Amazon Web Services (AWS)    Building from Source     Note  Configuring the limit for Open Files and Threads/Processes   On a Linux system you can set the limit of open files and thread processes in the  /etc/security/limits.conf  file.  A minimum of  8192  is recommended for open file descriptors limit and  128K  is recommended for the number of active threads.  A typical configuration used for SnappyData servers and leads can look like:   snappydata          hard    nofile      81920\nsnappydata          soft    nofile      8192\nsnappydata          hard    nproc       unlimited\nsnappydata          soft    nproc       524288\nsnappydata          hard    sigpending  unlimited\nsnappydata          soft    sigpending  524288  Here  snappydata  is the user name under which the SnappyData processes are started.", 
            "title": "Overview"
        }, 
        {
            "location": "/install/#install-on-premise", 
            "text": "SnappyData runs on UNIX-like systems (for example, Linux, Mac OS). With on-premises installation, SnappyData is installed and operated from your in-house computing infrastructure.", 
            "title": "Install On-Premise"
        }, 
        {
            "location": "/install/#prerequisites", 
            "text": "Before you start the installation, make sure that Java SE Development Kit 8 is installed, and the  JAVA_HOME  environment variable is set on each computer.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#download-snappydata", 
            "text": "Download the latest version of SnappyData from the  SnappyData Release  page, which lists the latest and previous releases of SnappyData.  The packages are available in compressed files (.zip and .tar format). On this page, you can also view details of features and enhancements introduced in specific releases.    SnappyData 0.9 download link  (tar.gz)   (zip)    SnappyData 0.9 (user-provided Hadoop) download link   (tar.gz)   (zip)", 
            "title": "Download SnappyData"
        }, 
        {
            "location": "/install/#single-host-installation", 
            "text": "This is the simplest form of deployment and can be used for testing and POCs.  Open the command prompt and run the following command to extract the downloaded archive file and to go the location of the SnappyData home directory.  $ tar -xzf snappydata-0.9-bin.tar.gz\n$ cd snappydata-0.9-bin/  Start a basic cluster with one data node, one lead, and one locator  ./sbin/snappy-start-all.sh  For custom configuration and to start more nodes,  see the section  How to Configure the SnappyData cluster .", 
            "title": "Single Host Installation"
        }, 
        {
            "location": "/install/#multi-host-installation", 
            "text": "For real life use cases, you need multiple machines on which SnappyData can be deployed. You can start one or more SnappyData node on a single machine based on your machine size.", 
            "title": "Multi-Host Installation"
        }, 
        {
            "location": "/install/#machines-with-a-shared-path", 
            "text": "If all your machines can share a path over an NFS or similar protocol, then follow the steps below:", 
            "title": "Machines with a Shared Path"
        }, 
        {
            "location": "/install/#prerequisites_1", 
            "text": "Ensure that the  /etc/hosts  correctly configures the host and IP address of each SnappyData member machine.    Ensure that SSH is supported and you have configured all machines to be accessed by  passwordless SSH .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#steps-to-set-up-the-cluster", 
            "text": "Copy the downloaded binaries to the shared folder.    Extract the downloaded archive file and go to SnappyData home directory.  $ tar -xzf snappydata-0.9-bin.tar.gz\n$ cd snappydata-0.9-bin/    Configure the cluster as described in  How to Configure SnappyData cluster .    After configuring each of the components, run the  snappy-start-all.sh  script:  ./sbin/snappy-start-all.sh    This creates a default folder named  work  and stores all SnappyData member's artifacts separately. Each member folder is identified by the name of the node.  If SSH is not supported then follow the instructions in the Machines without a Shared Path section.", 
            "title": "Steps to Set up the Cluster"
        }, 
        {
            "location": "/install/#machines-without-a-shared-path", 
            "text": "", 
            "title": "Machines without a Shared Path"
        }, 
        {
            "location": "/install/#prerequisites_2", 
            "text": "Ensure that the  /etc/hosts  correctly configures the host and IP Address of each SnappyData member machine.    On each host machine, create a new member working directory for each SnappyData member, that you want to run the host.   The member working directory provides a default location for the log, persistence, and status files for each member, and is also used as the default location for locating the member's configuration files. For example, if you want to run both a locator and server member on the local machine, create separate directories for each member.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#to-configure-the-cluster", 
            "text": "Copy and extract the downloaded binaries on each machine    Individually configure and start each member     Note  All configuration parameter are provided as command line arguments rather than reading from a conf file.   The example below starts a cluster by individually launching locator, server and lead processes.  $ bin/snappy locator start  -dir=/node-a/locator1\n$ bin/snappy server start  -dir=/node-b/server1  -locators:localhost:10334\n$ bin/snappy leader start  -dir=/node-c/lead1  -locators:localhost:10334\n\n$ bin/snappy locator stop -dir=/node-a/locator1\n$ bin/snappy server stop -dir=/node-b/server1\n$ bin/snappy leader stop -dir=/node-c/lead1top", 
            "title": "To Configure the Cluster"
        }, 
        {
            "location": "/install/#setting-up-cluster-on-amazon-web-services-aws", 
            "text": "", 
            "title": "Setting up Cluster on Amazon Web Services (AWS)"
        }, 
        {
            "location": "/install/#using-aws-management-console", 
            "text": "You can launch a SnappyData cluster on Amazon EC2 instance(s) using AMI provided by SnappyData. For more information on launching an EC2 instance, refer to the  AWS documentation .", 
            "title": "Using AWS Management Console"
        }, 
        {
            "location": "/install/#prerequisites_3", 
            "text": "Ensure that you have an existing AWS account with required permissions to launch EC2 resources", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#launching-the-instance", 
            "text": "To launch the instance and start SnappyData cluster:    Open the  Amazon EC2 console  and sign in using your AWS login credentials.    The current region is displayed at the top of the screen. Select the region where you want to launch the instance.    Click  Launch Instance  from the Amazon EC2 console dashboard.    On the  Choose an Amazon Machine Image (AMI)  page, select  Community AMIs  from the left pane.    Enter  SnappyData  in the search box, and press  Enter  on your keyboard.    The search result is displayed. From the search results, click  Select  to choose the AMI with the latest release version.    On the  Choose an Instance Type  page, select the instance type as per the requirement of your use case and then click  Review and Launch  to launch the instance with default configurations.    Note    You can also continue customizing your instance before you launch the instance. Refer to the AWS documentation for more information.    When configuring the security groups, ensure that you open at least ports 22 (for SSH access to the EC2 instance) and 5050 (for access to Snappy UI).       You are directed to the last step  Review Instance Launch . Check the details of your instance, and click  Launch .    In the  Select an existing key pair or create a new key pair  dialog box, select a key pair.    Click  Launch  to launch the instances.    The dashboard which lists the instances is displayed. Click  Refresh  to view the updated list and the status of the instance creation.    Once the status of the instance changes to  running , you have successfully created and launched the instance with the SnappyData AMI.    Use SSH to connect to the instance using the  ubuntu  username. You require:    The private key file of the key pair with which the instance was launched, and    Details of the public hostname or IP address of the instance.\nRefer to the following documentation, for more information on  accessing an EC2 instance .     Note    The public hostname/IP address information is available on the EC2 dashboard    Description  tab.     The SnappyData binaries are automatically downloaded and extracted to the location  /snappydata/downloads/  and Java 8 is installed.        Follow the  steps described here  to continue.", 
            "title": "Launching the Instance"
        }, 
        {
            "location": "/install/#using-snappydata-ec2-scripts", 
            "text": "The  snappy-ec2  script enables users to quickly launch and manage SnappyData clusters on Amazon EC2. You can also configure the individual nodes of the cluster by providing properties in specific conf files which the script reads before launching the cluster.  The  snappy-ec2  script has been derived from the  spark-ec2  script available in  Apache Spark 1.6 .  The scripts are available on GitHub in the  snappy-cloud-tools repository  and also as a  .tar.gz  file.   Note  The EC2 script is under development. Feel free to try it out and provide your feedback.", 
            "title": "Using SnappyData EC2 Scripts"
        }, 
        {
            "location": "/install/#prerequisites_4", 
            "text": "Ensure that you have an existing AWS account with required permissions to launch EC2 resources    Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster Refer to the Amazon Web Services EC2 documentation for more information on  generating your own EC2 Key Pair .    Using the AWS Secret Access Key and the Access Key ID, set the two environment variables,  AWS_SECRET_ACCESS_KEY  and  AWS_ACCESS_KEY_ID . You can find this information in the AWS IAM console page. \nIf you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file.  For example: export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112  export AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10    Ensure Python v 2.7 or later is installed on your local computer.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#cluster-management", 
            "text": "", 
            "title": "Cluster Management"
        }, 
        {
            "location": "/install/#launching-snappydata-cluster", 
            "text": "In the command prompt, go to the directory where the  snappydata-ec2- version .tar.gz  is extracted or to the aws/ec2 directory where the  SnappyData cloud tools repository  is cloned locally.  Enter the command in the following format.  ./snappy-ec2 -k  your-key-name  -i  your-keyfile-path   action   your-cluster-name  Here,  your-key-name  refers to the EC2 Key Pair,  your-keyfile-path  refers to the path to the key file and  action  refers to the action to be performed (for example, launch, start, stop).  By default, the script starts one instance of a locator, lead and server each.\nThe script identifies each cluster by its unique cluster name (you provided) and internally ties members (locators, leads, and stores/servers) of the cluster with EC2 security groups.  The  names and details of the members are automatically derived from the provided cluster name.  For example, if you launch a cluster named  my-cluster , the locator is available in security group named  my-cluster-locator  and the store/server are available in  my-cluster-store .  When running the script you can also specify properties like the number of stores and region.  For example, using the following command, you can start a SnappyData cluster named  snappydata-cluster  with 2 stores (or servers) in the N. California (us-west-1) region on AWS. It also starts an Apache Zeppelin server on the instance where lead is running.  The examples below assume that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin=embedded --region=us-west-1 launch snappydata-cluster  To start Apache Zeppelin on a separate instance, use  --with-zeppelin=non-embedded .", 
            "title": "Launching SnappyData Cluster"
        }, 
        {
            "location": "/install/#specifying-properties", 
            "text": "If you want to configure each of the locator, lead or server with specific properties, you can do so by specifying them in files named  locators ,  leads  or  servers , respectively and placing these under aws/ec2/deploy/home/ec2-user/snappydata/. Refer to  this SnappyData documentation page  for example on how to write these conf files.  This is similar to how one would provide properties to SnappyData cluster nodes while launching it using the  sbin/snappy-start-all.sh  script.  The important difference here is that, instead of the host names of the locator, lead or store, you have to write {{LOCATOR_N}}, {{LEAD_N}} or {{SERVER_N}} in these files, respectively. N stands for Nth locator, lead or server. The script replaces these with the actual host name of the members when they are launched.  The sample conf files for a cluster with 2 locators, 1 lead and 2 stores are given below:  locators  {{LOCATOR_0}} -peer-discovery-port=9999 -heap-size=1024m\n{{LOCATOR_1}} -peer-discovery-port=9888 -heap-size=1024m  leads  {{LEAD_0}} -heap-size=4096m -spark.ui.port=3333 -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -spark.executor.cores=10  servers  {{SERVER_0}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888\n{{SERVER_1}} -heap-size=4096m -locators={{LOCATOR_0}}:9999,{{LOCATOR_1}}:9888 -client-port=1530  When you run  snappy-ec2 , it looks for these files under  aws/ec2/deploy/home/ec2-user/snappydata/  and, if present, reads them while launching the cluster on Amazon EC2. Ensure that the number of locators, leads or servers specified by options  --locators ,  --leads  or  --stores  must match to the number of entries in their respective conf files.  The script also reads  snappy-env.sh , if present in this location.", 
            "title": "Specifying Properties"
        }, 
        {
            "location": "/install/#stopping-a-cluster", 
            "text": "When you stop a cluster, it shuts down the EC2 instances and any data saved on its local instance stores is lost. However, the data saved on EBS volumes, if any, is retained unless the spot-instances were used.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem stop cluster-name", 
            "title": "Stopping a Cluster"
        }, 
        {
            "location": "/install/#starting-a-cluster", 
            "text": "When you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem start cluster-name   Note  The start command (or launch command with --resume option) ignores --locators, --leads or --stores options and launches SnappyData cluster on existing instances. But the conf files are read in any case if they are present in the location mentioned above. So you need to ensure that every time you use start command, the number of entries in conf files are equal to the number of instances in their respective security group.", 
            "title": "Starting a Cluster"
        }, 
        {
            "location": "/install/#adding-servers-to-a-cluster", 
            "text": "This is not yet fully supported via the script. You may have to manually launch an instance with  (cluster-name)-stores  group and then use launch command with --resume option.", 
            "title": "Adding Servers to  a Cluster"
        }, 
        {
            "location": "/install/#listing-members-of-a-cluster", 
            "text": "To get the first locator's hostname:  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem get-locator cluster-name  To get the first lead's hostname, use the get-lead command:", 
            "title": "Listing Members of a Cluster"
        }, 
        {
            "location": "/install/#connecting-to-a-cluster", 
            "text": "You can connect to any instance of a cluster with SSH using the login command. It logs you into the first lead instance. From there, you can SSH to any other member of the cluster without a password.\nThe SnappyData product directory is located under /home/ec2-user/snappydata/ on all the members.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem login cluster-name", 
            "title": "Connecting to a Cluster"
        }, 
        {
            "location": "/install/#destroying-a-cluster", 
            "text": "Destroying a cluster destroys all the data on the local instance stores as well as on the attached EBS volumes permanently.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem destroy cluster-name  This retains the security groups created for this cluster. To delete them as well, use it with --delete-group option.", 
            "title": "Destroying a Cluster"
        }, 
        {
            "location": "/install/#starting-cluster-with-apache-zeppelin", 
            "text": "Optionally, you can start an instance of Apache Zeppelin server with the cluster.  Apache Zeppelin  is a web-based notebook that enables interactive notebook. You can start it either on a lead node's instance ( --with-zeppelin=embedded )  or on a separate instance ( --with-zeppelin=non-embedded ).  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --with-zeppelin=embedded launch cluster-name", 
            "title": "Starting Cluster with Apache Zeppelin"
        }, 
        {
            "location": "/install/#more-options", 
            "text": "For a complete list of options this script has, run  ./snappy-ec2 . The options are also provided below for quick reference.  Usage: `snappy-ec2 [options]  action   cluster_name ` action  can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -s STORES, --stores=STORES\n                        Number of stores to launch (default: 1)\n  --locators=LOCATORS   Number of locator nodes to launch (default: 1)\n  --leads=LEADS         Number of lead nodes to launch (default: 1)\n  -w WAIT, --wait=WAIT  DEPRECATED (no longer necessary) - Seconds to wait for\n                        nodes to start\n  -k KEY_PAIR, --key-pair=KEY_PAIR\n                        Key pair to use on instances\n  -i IDENTITY_FILE, --identity-file=IDENTITY_FILE\n                        SSH private key file to use for logging into instances\n  -p PROFILE, --profile=PROFILE\n                        If you have multiple profiles (AWS or boto config),\n                        you can configure additional, named profiles by using\n                        this option (default: none)\n  -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE\n                        Type of instance to launch (default: m3.large).\n                        WARNING: must be 64-bit; small instances won't work\n  --locator-instance-type=LOCATOR_INSTANCE_TYPE\n                        Locator instance type (leave empty for same as\n                        instance-type)\n  -r REGION, --region=REGION\n                        EC2 region used to launch instances in, or to find\n                        them in (default: us-east-1)\n  -z ZONE, --zone=ZONE  Availability zone to launch instances in, or 'all' to\n                        spread stores across multiple (an additional $0.01/Gb\n                        for bandwidthbetween zones applies) (default: a single\n                        zone chosen at random)\n  -a AMI, --ami=AMI     Amazon Machine Image ID to use\n  -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION\n                        Version of SnappyData to use: 'X.Y.Z' (default:\n                        LATEST)\n  --with-zeppelin=WITH_ZEPPELIN\n                        Launch Apache Zeppelin server with the cluster. Use\n                        'embedded' to launch it on lead node and 'non-\n                        embedded' to launch it on a separate instance.\n  --deploy-root-dir=DEPLOY_ROOT_DIR\n                        A directory to copy into/on the first master. Must\n                        be absolute. Note that a trailing slash is handled as\n                        per rsync: If you omit it, the last directory of the\n                        --deploy-root-dir path will be created in / before\n                        copying its contents. If you append the trailing\n                        slash, the directory is not created and its contents\n                        are copied directly into /. (default: none).\n  -D [ADDRESS:]PORT     Use SSH dynamic port forwarding to create a SOCKS\n                        proxy at the given local address (for use with login)\n  --resume              Resume installation on a previously launched cluster\n                        (for debugging)\n  --ebs-vol-size=SIZE   Size (in GB) of each EBS volume.\n  --ebs-vol-type=EBS_VOL_TYPE\n                        EBS volume type (e.g. 'gp2', 'standard').\n  --ebs-vol-num=EBS_VOL_NUM\n                        Number of EBS volumes to attach to each node as\n                        /vol[x]. The volumes will be deleted when the\n                        instances terminate. Only possible on EBS-backed AMIs.\n                        EBS volumes are only attached if --ebs-vol-size   0.\n                        Only support up to 8 EBS volumes.\n  --placement-group=PLACEMENT_GROUP\n                        Which placement group to try and launch instances\n                        into. Assumes placement group is already created.\n  --spot-price=PRICE    If specified, launch stores as spot instances with the\n                        given maximum price (in dollars)\n  -u USER, --user=USER  The SSH user you want to connect as (default:\n                        ec2-user)\n  --delete-groups       When destroying a cluster, delete the security groups\n                        that were created\n  --use-existing-locator\n                        Launch fresh stores, but use an existing stopped\n                        locator if possible\n  --user-data=USER_DATA\n                        Path to a user-data file (most AMIs interpret this as\n                        an initialization script)\n  --authorized-address=AUTHORIZED_ADDRESS\n                        Address to authorize on created security groups\n                        (default: 0.0.0.0/0)\n  --additional-security-group=ADDITIONAL_SECURITY_GROUP\n                        Additional security group to place the machines in\n  --additional-tags=ADDITIONAL_TAGS\n                        Additional tags to set on the machines; tags are\n                        comma-separated, while name and value are colon\n                        separated; ex:  Task:MySnappyProject,Env:production \n  --subnet-id=SUBNET_ID\n                        VPC subnet to launch instances in\n  --vpc-id=VPC_ID       VPC to launch instances in\n  --private-ips         Use private IPs for instances rather than public if\n                        VPC/subnet requires that.\n  --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR\n                        Whether instances should terminate when shut down or\n                        just stop\n  --instance-profile-name=INSTANCE_PROFILE_NAME\n                        IAM profile name to launch instances under", 
            "title": "More options"
        }, 
        {
            "location": "/install/#limitations", 
            "text": "Some of the known limitations of the script are:    Launching the cluster on custom AMI (specified via --ami option) will not work if it does not have the user 'ec2-user' with sudo permissions    Support for option --user is incomplete", 
            "title": "Limitations"
        }, 
        {
            "location": "/install/#building-from-source", 
            "text": "Building SnappyData requires JDK 8 installation ( Oracle Java SE ).  Quickstart to build all components of SnappyData:  Latest release branch   git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.9 --recursive  cd snappydata  ./gradlew product  Master   git clone https://github.com/SnappyDataInc/snappydata.git --recursive  cd snappydata  ./gradlew product  The product is in  build-artifacts/scala-2.11/snappy  If you want to build only the top-level SnappyData project but pull in jars for other projects ( spark ,  store ,  spark-jobserver ):  Latest release branch   git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.9  cd snappydata  ./gradlew product  Master   git clone https://github.com/SnappyDataInc/snappydata.git  cd snappydata  ./gradlew product", 
            "title": "Building from Source"
        }, 
        {
            "location": "/install/#repository-layout", 
            "text": "core  - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between  spark  and  store  (GemFireXD). For example, SnappyContext, row and column store, streaming additions etc.    cluster  - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc.    This component depends on  core  and  store .  The code in the  cluster  depends on  core  but not the other way round.    spark  -  Apache Spark  code with SnappyData enhancements.    store  - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch.    spark-jobserver  - Fork of  spark-jobserver  project with some additions to integrate with SnappyData.    The  spark ,  store , and  spark-jobserver  directories are required to be clones of the respective SnappyData repositories and are integrated into the top-level SnappyData project as git sub-modules. When working with sub-modules, updating the repositories follows the normal  git submodules . One can add some aliases in gitconfig to aid pull/push like:  [alias]\n  spull = !git pull   git submodule sync --recursive   git submodule update --init --recursive\n  spush = push --recurse-submodules=on-demand  The above aliases can serve as useful shortcuts to pull and push all projects from top-level  snappydata  repository.", 
            "title": "Repository Layout"
        }, 
        {
            "location": "/install/#building", 
            "text": "Gradle is the build tool used for all the SnappyData projects. Changes to  Apache Spark  and  spark-jobserver  forks include the addition of Gradle build scripts to allow building them independently as well as a subproject of SnappyData. The only requirement for the build is a JDK 8 installation. The Gradle wrapper script downloads all the other build dependencies as required.  If a user does not want to deal with submodules and only work on SnappyData project, then can clone only the SnappyData repository (without the --recursive option) and the build will pull those SnappyData project jar dependencies from maven central.  If working on all the separate projects integrated inside the top-level SnappyData clone, the Gradle build will recognize the same and build those projects too and include the same in the top-level product distribution jar. The  spark  and  store  submodules can also be built and published independently.  Useful build and test targets:  ./gradlew assemble      -  build all the sources\n./gradlew testClasses   -  build all the tests\n./gradlew product       -  build and place the product distribution\n                           (in build-artifacts/scala_2.11/snappy)\n./gradlew distTar       -  create a tar.gz archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew distZip       -  create a zip archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew buildAll      -  build all sources, tests, product, packages (all targets above)\n./gradlew checkAll      -  run testsuites of snappydata components\n./gradlew cleanAll      -  clean all build and test output\n./gradlew runQuickstart -  run the quickstart suite (the  Getting Started  section of docs)\n./gradlew precheckin    -  cleanAll, buildAll, scalaStyle, build docs,\n                           and run full snappydata testsuite including quickstart\n./gradlew precheckin -Pstore  -  cleanAll, buildAll, scalaStyle, build docs,\n                           run full snappydata testsuite including quickstart\n                           and also full SnappyData store testsuite  The default build directory is  build-artifacts/scala-2.11  for projects. An exception is  store  project, where the default build directory is  build-artifacts/;os;  where  ;os;  is  linux  on Linux systems,  osx  on Mac,  windows  on Windows.  The usual Gradle test run targets ( test ,  check ) work as expected for JUnit tests. Separate targets have been provided for running Scala tests ( scalaTest ) while the  check  target runs both the JUnit and ScalaTests. One can run a single Scala test suite class with  singleSuite  option while running a single test within some suite works with the  --tests  option:   ./gradlew core:scalaTest -PsingleSuite=**.ColumnTableTest  # run all tests in the class  ./gradlew core:scalaTest \\     --tests  Test the creation/dropping of table using SQL   # run a single test (use full name)  Running individual tests within some suite works using the  --tests  argument.", 
            "title": "Building"
        }, 
        {
            "location": "/install/#setting-up-intellij-idea-with-gradle", 
            "text": "Intellij IDEA is the IDE commonly used by the SnappyData developers. Those who really prefer Eclipse can try the Scala-IDE and Gradle support but has been seen to not work as well. Steps required for setting up SnappyData with all its components in IDEA are listed below.  To import into IntelliJ IDEA:    Update IntelliJ IDEA to the latest version, including the latest Scala plug-in. Older versions (pre 14.x) have trouble dealing with Scala code, particularly some of the code in Spark. Ensure JDK 8 is installed and IDEA can find it (either in PATH or via  JAVA_HOME ).    Increase the available JVM heap size for IDEA. Open bin/idea64.vmoptions (assuming 64-bit JVM) and increase -Xmx option to be something like  -Xmx=2g  for comfortable use.    Select  Import Project , and then point to the SnappyData directory. Use external Gradle import.  Un-select  the \"Create separate module per source set\" option while other options can be defaults. Click  Next  in the following screens.   Note    Ignore the  \"Gradle location is unknown warning\" .    Ensure that the JDK 8 installation has been selected.    Ignore and dismiss the  \"Unindexed remote maven repositories found\"  warning message if seen.       When import is completed, go to  File  Settings  Editor  Code Style  Scala . Set the scheme as  Project . Check that the same has been set in Java Code Style too. Click OK to apply and close it. Next, copy  codeStyleSettings.xml  located in the SnappyData top-level directory, to the  .idea  directory created by IDEA. Check that the settings are now applied in  File  Settings  Editor  Code Style  Java  which should display Indent as 2 and continuation indent as 4 (same as Scala).    If the Gradle tab is not visible immediately, then select it from window list pop-up at the left-bottom corner of IDE. If you click on that window list icon, then the tabs are displayed permanently.    Generate Apache Avro and SnappyData required sources by expanding:  snappydata_2.11  Tasks  other . Right-click on  generateSources  and run it. The Run option may not be available if indexing is still in progress, wait for indexing to complete, and then try again.   The first run may some time to complete,  as it downloads jar files and other required files. This step has to be done the first time, or if  ./gradlew clean  has been run, or if you have made changes to  javacc/avro/messages.xml  source files.    If you get unexpected  \"Database not found\"  or  NullPointerException  errors in SnappyData-store/GemFireXD layer, then first thing to try is to run the  generateSources  target again.    Increase the compiler heap sizes or else the build can take quite long especially with integrated  spark  and  store . In  File  Settings  Build, Execution, Deployment  Compiler increase ,  Build process heap size  to say 1536 or 2048. Similarly, increase JVM maximum heap size in  Languages   Frameworks  Scala Compiler Server  to 1536 or 2048.    Test the full build.    For JUnit tests configuration also append  /build-artifacts  to the working directory. That is, open  Run  Edit Configurations , expand  Defaults  and select  JUnit , the working directory should be  \\$MODULE_DIR\\$/build-artifacts . Likewise append  build-artifacts  to working directory for ScalaTest. Without this all intermediate log and other files pollute the source tree and then need to be cleaned manually.", 
            "title": "Setting up IntelliJ IDEA with Gradle"
        }, 
        {
            "location": "/install/#running-a-scalatestjunit", 
            "text": "Running Scala/JUnit tests from IntelliJ IDEA is straightforward.    When selecting a run configuration for JUnit/ScalaTest, avoid selecting the Gradle one (green round icon) otherwise an external Gradle process is launched that can start building the project again and won't be cleanly integrated with IDEA. Use the normal JUnit (red+green arrows icon) or ScalaTest (JUnit like with red overlay).    For JUnit tests, ensure that working directory is the top-level  \\$MODULE_DIR\\$/build-artifacts  as mentioned before. Otherwise, many SnappyData-store tests will fail to find the resource files required in tests. They also pollute the files etc, so when launched this will allow those to go into  build-artifacts  that is easier to clean. For that reason, it is preferable to do the same for ScalaTests.    Some of the tests use data files from the  tests-common  directory. For such tests, run the Gradle task  snappydata_2.11  Tasks  other  copyResourcesAll  to copy the resources in build area where IDEA runs can find it.", 
            "title": "Running a ScalaTest/JUnit"
        }, 
        {
            "location": "/migration/migration/", 
            "text": "Overview\n\n\nThis guide provides information related to the migration of systems running SnappyData 0.8 to SnappyData 0.9. We assume that you have SnappyData 0.8 already installed, and you are migrating to the latest version of SnappyData.\n\n\nBefore you begin migrating, ensure that you understand the new features and any specific requirements for that release. For more information see, \nmigrating from SnappyData version 0.8 to version 0.9\n\n\nBefore you begin migration\n:\n\n\n\n\n\n\nBackup the existing environment: Make sure you create a backup of the locator, lead, and server configuration files that exist in the \nconf\n folder located in the SnappyData home directory. \n\n\n\n\n\n\nStop the cluster and verify that all members are stopped: You can shutdown the cluster using the \nsbin/snappy-stop-all.sh\n command. To ensure that all the members have been shut down correctly, use the \nsbin/snappy-status-all.sh\n command.\n\n\n\n\n\n\nRe-install SnappyData: After you have stopped the cluster, \ninstall SnappyData 0.9\n.\n\n\n\n\n\n\nReconfigure your cluster.", 
            "title": "Migration Guide"
        }, 
        {
            "location": "/migration/migration/#overview", 
            "text": "This guide provides information related to the migration of systems running SnappyData 0.8 to SnappyData 0.9. We assume that you have SnappyData 0.8 already installed, and you are migrating to the latest version of SnappyData.  Before you begin migrating, ensure that you understand the new features and any specific requirements for that release. For more information see,  migrating from SnappyData version 0.8 to version 0.9  Before you begin migration :    Backup the existing environment: Make sure you create a backup of the locator, lead, and server configuration files that exist in the  conf  folder located in the SnappyData home directory.     Stop the cluster and verify that all members are stopped: You can shutdown the cluster using the  sbin/snappy-stop-all.sh  command. To ensure that all the members have been shut down correctly, use the  sbin/snappy-status-all.sh  command.    Re-install SnappyData: After you have stopped the cluster,  install SnappyData 0.9 .    Reconfigure your cluster.", 
            "title": "Overview"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/", 
            "text": "Instruction for Migrating from SnappyData 0.8-0.9\n\n\n\n\nNote\n\n\nUpgrade of on-disk data files is not supported for this release. This document only contains instructions for users migrating from SnappyData 0.8 to SnappyData 0.9. After you have re-configured your cluster (point 4 below), you must reload your data into SnappyData tables.\n\n\n\n\nMemory Management: Heap and Off-Heap\n\n\nSnappyData can now be configured to use both off-heap and on-heap storage. The \nmemory-size\n and \nheap-size\n properties control the off-heap and on-heap sizes of the SnappyData server process. \n\n\nRow tables are always stored on on-heap storage. You can now configure column tables to use off-heap storage. Off-heap storage is also recommended for production environments. Several artifacts in the product, however, require on-heap memory, and therefore minimum heap size is also required in such cases. \nFor example:\n\n To use row tables: According to the row table size requirements, configure the heap size. Currently, row tables in SnappyData do not use off-heap memory.\n\n To read-write Parquet and CSV: Parquet and CSV read-write are memory consuming activities, and still, use heap memory. Ensure that you provide sufficient heap memory in such cases.\n* When most of your data reside in column tables, use off-heap memory. They are faster and put less pressure on garbage collection threads.\n\n\nThe following properties have been added for memory management:\n\n\n\n\n\n\n\n\n\nProperties\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmemory-size\n\n\nThe total memory that can be used by the node for column storage and execution in off-heap. The default value is 0 (OFF_HEAP is not used by default).\n\n\n\n\n\n\ncritical-heap-percentage\n\n\nThe portion of memory that is reserved for system use, unaccounted on-the-fly objects, JVM, GC overhead etc. The default value is 90% and can be increased to 95% or similar for large heaps (\n20G). This is only applicable to heap-size accounting. \nOff-heap configuration with memory-size uses the entire available memory and does not have any reserved area.\n\n\n\n\n\n\nspark.memory.storageFraction\n\n\nThe fraction of total storage memory that is immune to eviction. Execution can never grow into this portion of memory. \nIf set to a higher value, less working memory may be available for execution, and tasks may overflow to disk. It is recommended that you do not modify the default setting.\n\n\n\n\n\n\nspark.memory.storageMaxFraction\n\n\nThe fraction of total memory used for storage and immune to eviction. Execution can never grow into this portion of memory. \nIf set to a higher value, less working memory may be available for execution, and tasks may overflow to disk. It is recommended that you do not modify the default setting.\n\n\n\n\n\n\nspark.memory.storageMaxFraction\n\n\nSpecifies how much off-heap memory can be consumed by storage. The default is 0.9 of the total \nmemory-size\n (off-heap size). Beyond this, all data in storage gets evicted to disk. The split between execution and storage is governed by the \nspark.memory.storageFraction\n property, but storage grows into execution space up to this limit if space is available in the execution area. However, if execution requires space, then it evicts storage to the \nspark.memory.storageFraction\n limit. \nNormally you do not need to modify this property even if queries are expected to take lots of execution space. It is better to use the \nspark.memory.storageFraction\n property to control the split between storage and execution.\n\n\n\n\n\n\n\n\nTables Persistent To Disk By Default\n\n\nIn the previous releases (0.8 and earlier), tables were stored in memory by default, and users had to configure the \npersistence\n clause to store data on disk.\nFrom this release onwards, all tables persist to disk by default and can be explicitly turned OFF for pure memory-only tables by specifying the \npersistence\n option as \nnone\n.\n\n\nChanges to Properties\n\n\nIn this release, changes have been made to the properties (\nmemory management\n and \nnew\n/\ndeleted\n properties). Make sure that you familiarise yourself with changes before you re-configure your cluster.\n\n\n\nNew Property\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsnappydata.connection\n\n\nThis property points to thrift network server running on a locator or a data server of a running SnappyData cluster. The value of this property is a combination of locator or server and JDBC client port on which the thrift network server listens for connections (The port that is specified by the \nclient-port\n property and defaults to 1527 or the next available port on the locator/server).\n\n\n\n\n\n\n\n\n\n\nDeleted Property\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsnappydata.store.locators\n\n\nInstructs the connector to acquire cluster connectivity, catalog metadata and registers it locally in the Spark cluster.\nThis property has been deleted and replaced with \n`snappydata.connection\n.", 
            "title": "Migrating from Version 0.8 to Version 0.9"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/#instruction-for-migrating-from-snappydata-08-09", 
            "text": "Note  Upgrade of on-disk data files is not supported for this release. This document only contains instructions for users migrating from SnappyData 0.8 to SnappyData 0.9. After you have re-configured your cluster (point 4 below), you must reload your data into SnappyData tables.", 
            "title": "Instruction for Migrating from SnappyData 0.8-0.9"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/#memory-management-heap-and-off-heap", 
            "text": "SnappyData can now be configured to use both off-heap and on-heap storage. The  memory-size  and  heap-size  properties control the off-heap and on-heap sizes of the SnappyData server process.   Row tables are always stored on on-heap storage. You can now configure column tables to use off-heap storage. Off-heap storage is also recommended for production environments. Several artifacts in the product, however, require on-heap memory, and therefore minimum heap size is also required in such cases. \nFor example:  To use row tables: According to the row table size requirements, configure the heap size. Currently, row tables in SnappyData do not use off-heap memory.  To read-write Parquet and CSV: Parquet and CSV read-write are memory consuming activities, and still, use heap memory. Ensure that you provide sufficient heap memory in such cases.\n* When most of your data reside in column tables, use off-heap memory. They are faster and put less pressure on garbage collection threads.  The following properties have been added for memory management:     Properties  Description      memory-size  The total memory that can be used by the node for column storage and execution in off-heap. The default value is 0 (OFF_HEAP is not used by default).    critical-heap-percentage  The portion of memory that is reserved for system use, unaccounted on-the-fly objects, JVM, GC overhead etc. The default value is 90% and can be increased to 95% or similar for large heaps ( 20G). This is only applicable to heap-size accounting.  Off-heap configuration with memory-size uses the entire available memory and does not have any reserved area.    spark.memory.storageFraction  The fraction of total storage memory that is immune to eviction. Execution can never grow into this portion of memory.  If set to a higher value, less working memory may be available for execution, and tasks may overflow to disk. It is recommended that you do not modify the default setting.    spark.memory.storageMaxFraction  The fraction of total memory used for storage and immune to eviction. Execution can never grow into this portion of memory.  If set to a higher value, less working memory may be available for execution, and tasks may overflow to disk. It is recommended that you do not modify the default setting.    spark.memory.storageMaxFraction  Specifies how much off-heap memory can be consumed by storage. The default is 0.9 of the total  memory-size  (off-heap size). Beyond this, all data in storage gets evicted to disk. The split between execution and storage is governed by the  spark.memory.storageFraction  property, but storage grows into execution space up to this limit if space is available in the execution area. However, if execution requires space, then it evicts storage to the  spark.memory.storageFraction  limit.  Normally you do not need to modify this property even if queries are expected to take lots of execution space. It is better to use the  spark.memory.storageFraction  property to control the split between storage and execution.", 
            "title": "Memory Management: Heap and Off-Heap"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/#tables-persistent-to-disk-by-default", 
            "text": "In the previous releases (0.8 and earlier), tables were stored in memory by default, and users had to configure the  persistence  clause to store data on disk.\nFrom this release onwards, all tables persist to disk by default and can be explicitly turned OFF for pure memory-only tables by specifying the  persistence  option as  none .", 
            "title": "Tables Persistent To Disk By Default"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/#changes-to-properties", 
            "text": "In this release, changes have been made to the properties ( memory management  and  new / deleted  properties). Make sure that you familiarise yourself with changes before you re-configure your cluster.", 
            "title": "Changes to Properties"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/#new-property", 
            "text": "Property  Description      snappydata.connection  This property points to thrift network server running on a locator or a data server of a running SnappyData cluster. The value of this property is a combination of locator or server and JDBC client port on which the thrift network server listens for connections (The port that is specified by the  client-port  property and defaults to 1527 or the next available port on the locator/server).", 
            "title": "New Property"
        }, 
        {
            "location": "/migration/migration-0.8-0.9/#deleted-property", 
            "text": "Property  Description      snappydata.store.locators  Instructs the connector to acquire cluster connectivity, catalog metadata and registers it locally in the Spark cluster. This property has been deleted and replaced with  `snappydata.connection .", 
            "title": "Deleted Property"
        }, 
        {
            "location": "/howto/", 
            "text": "Overview\n\n\nThis section introduces you to several common operations such as starting a cluster, working with tables (load, query, update), working with streams and running approximate queries.\n\n\nRunning the Examples:\n\nTopics in this section refer to source code examples that are shipped with the product. Instructions to run these examples can be found in the source code.\n\n\nSource code for these examples is located in the \nquickstart/src/main/scala/org/apache/spark/examples/snappydata\n and in \nquickstart/python\n directories of the SnappyData product distribution.\n\n\nRefer to the \nGetting Started\n to either run SnappyData on premise, using AWS or Docker. \n\n\nYou can run the examples in any of the following ways:\n\n\n\n\n\n\nIn the Local Mode\n: By using \nbin/run-example\n script (to run Scala examples) or by using \nbin/spark-submit\n script (to run Python examples). The examples run collocated with Spark+SnappyData Store in the same JVM. \n\n\n\n\n\n\nAs a Job\n: Many of the Scala examples are also implemented as a SnappyData job. In this case, examples can be submitted as a job to a running SnappyData cluster. Refer to \njobs\n section for details on how to run a job.\n\n\n\n\n\n\n\n\nNote\n\n\nSnappyData also supports Java API. Refer to the \ndocumentation\n for more details on Java API.\n\n\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nHow to Start a SnappyData Cluster\n\n\n\n\n\n\nHow to Run Spark Job inside the Cluster\n\n\n\n\n\n\nHow to Access SnappyData Store from existing Spark Installation using Smart Connector\n\n\n\n\n\n\nHow to Create Row Tables and Run Queries\n\n\n\n\n\n\nHow to Create Column Tables and Run Queries\n\n\n\n\n\n\nHow to Load Data in SnappyData Tables\n\n\n\n\n\n\nHow to Perform a Collocated Join\n\n\n\n\n\n\nHow to Connect using JDBC Driver\n\n\n\n\n\n\nHow to Store and Query JSON Objects\n\n\n\n\n\n\nHow to Store and Query Objects\n\n\n\n\n\n\nHow to Use Stream Processing with SnappyData\n\n\n\n\n\n\nHow to Use Synopsis Data Engine to Run Approximate Queries\n\n\n\n\n\n\nHow to Use Python to Create Tables and Run Queries\n\n\n\n\n\n\nHow to Connect using ODBC Driver\n\n\n\n\n\n\nHow to Connect to the Cluster from External Clients\n\n\n\n\n\n\nHow to Use Apache Zeppelin with SnappyData\n\n\n\n\n\n\n\n\nHow to Start a SnappyData Cluster\n\n\nStart SnappyData Cluster on a Single Machine\n\n\nIf you have \ndownloaded and extracted\n the SnappyData product distribution, navigate to the SnappyData product root directory.\n\n\nStart the Cluster\n: Run the \nsbin/snappy-start-all.sh\n script to start SnappyData cluster on your single machine using default settings. This starts one lead node, one locator, and one data server.\n\n\n$ sbin/snappy-start-all.sh\n\n\n\n\nIt may take 30 seconds or more to bootstrap the entire cluster on your local machine.\n\n\nSample Output\n: The sample output for \nsnappy-start-all.sh\n is displayed as:\n\n\nStarting SnappyData Locator using peer discovery on: localhost[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user/snappyData/work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 9368 status: running\nStarting SnappyData Server using locators for peer discovery: user1-laptop[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user1/snappyData/work/localhost-server-1/snappyserver.log\nSnappyData Server pid: 9519 status: running\n  Distributed system now has 2 members.\n  Other members: localhost(9368:locator)\nv0\n:16944\nStarting SnappyData Leader using locators for peer discovery: user1-laptop[10334]\nLogs generated in /home/user1/snappyData/work/localhost-lead-1/snappyleader.log\nSnappyData Leader pid: 9699 status: running\n  Distributed system now has 3 members.\n  Other members: localhost(9368:locator)\nv0\n:16944, 192.168.63.1(9519:datastore)\nv1\n:46966\n\n\n\n\nCheck Status\n: You can check the status of a running cluster using the following command:\n\n\n$ sbin/snappy-status-all.sh\nSnappyData Locator pid: 9368 status: running\nSnappyData Server pid: 9519 status: running\n  Distributed system now has 2 members.\n  Other members: localhost(9368:locator)\nv0\n:16944\nSnappyData Leader pid: 9699 status: running\n  Distributed system now has 3 members.\n  Other members: localhost(9368:locator)\nv0\n:16944, 192.168.63.1(9519:datastore)\nv1\n:46966\n\n\n\n\nYou can check SnappyData UI by opening \nhttp://\nleadHostname\n:5050\n in browser, where \nleadHostname\n is the host name of your lead node. Use \nSnappy SQL shell\n to connect to the cluster and perform various SQL operations.\n\n\nShutdown Cluster\n: You can shutdown the cluster using the \nsbin/snappy-stop-all.sh\n command:\n\n\n$ sbin/snappy-stop-all.sh\nThe SnappyData Leader has stopped.\nThe SnappyData Server has stopped.\nThe SnappyData Locator has stopped.\n\n\n\n\nStart SnappyData Cluster on Multiple Hosts\n\n\nTo start the cluster on multiple hosts:\n\n\n\n\n\n\nThe easiest way to run SnappyData on multiple nodes is to use a shared file system such as NFS on all the nodes.\n You can also extract the product distribution on each node of the cluster. If all nodes have NFS access, install SnappyData on any one of the nodes.\n\n\n\n\n\n\nCreate the configuration files using the templates provided in the \nconf\n folder. Copy the existing template files \nservers.template\n, \nlocators.template\n, \nleads.template\n, and rename them to \nservers\n, \nlocators\n, \nleads\n.\n\n Edit the files to include the hostnames on which to start the server, locator, and lead. Refer to the \nconfiguration\n section for more information on properties.\n\n\n\n\n\n\nStart the cluster using \nsbin/snappy-start-all.sh\n. SnappyData starts the cluster using SSH.\n\n\n\n\n\n\n\n\nNote\n\n\nIt is recommended that you set up passwordless SSH on all hosts in the cluster. Refer to the documentation for more details on \ninstallation\n and \ncluster configuration\n.\n\n\n\n\n\n\nHow to Run Spark Code inside the Cluster\n\n\nA Spark program that runs inside a SnappyData cluster is implemented as a SnappyData job.\n\n\nImplementing a Job\n: \nA SnappyData job is a class or object that implements SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. In the \nrunSnappyJob\n method of the job, you implement the logic for your Spark program using SnappySession object instance passed to it. You can perform all operations such as create table, load data, execute queries using the SnappySession. \n\nAny of the Spark APIs can be invoked by a SnappyJob.\n\n\nclass CreatePartitionedRowTable extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappySession, jobConfig: Config): Any\n\n  /**\n  SnappyData calls this function to validate the job input and reject invalid job requests.\n  You can implement custom validations here, for example, validating the configuration parameters\n  **/\n  def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation\n}\n\n\n\n\nDependencies\n:\nTo compile your job, use the Maven/SBT dependencies for the latest released version of SnappyData.\n\n\nExample: Maven dependency\n:\n\n\n!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --\n\n\ndependency\n\n    \ngroupId\nio.snappydata\n/groupId\n\n    \nartifactId\nsnappydata-cluster_2.11\n/artifactId\n\n    \nversion\n0.9\n/version\n\n\n/dependency\n\n\n\n\n\nExample: SBT dependency\n:\n\n\n// https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies += \nio.snappydata\n % \nsnappydata-cluster_2.11\n % \n0.9\n\n\n\n\n\nRunning the Job\n: \nOnce you create a jar file for SnappyData job, use \nbin/snappy-job.sh\n to submit the job to SnappyData cluster and run the job. This is similar to Spark-submit for any Spark application. \n\n\nFor example, to run the job implemented in \nCreatePartitionedRowTable.scala\n you can use the following command.\nHere \nquickstart.jar\n contains the program and is bundled in the product distribution.\n\nFor example, the command submits the job and runs it as:\n\n\n # first cd to your SnappyData product dir\n $ cd $SNAPPY_HOME\n $ bin/snappy-job.sh submit\n    --app-name CreatePartitionedRowTable\n    --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable\n    --app-jar examples/jars/quickstart.jar\n    --lead hostNameOfLead:8090\n\n\n\n\nIn the above comand, \n--lead\n option specifies the host name of the lead node along with the port on which it accepts jobs (default 8090).\n\n\nOutput\n: It returns output similar to:\n\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  }\n}\n\n\n\n\nCheck Status\n: You can check the status of the job using the Job ID listed above:\n\n\nbin/snappy-job.sh status --lead hostNameOfLead:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n\n\n\nRefer to the \nBuilding SnappyData applications using Spark API\n section of the documentation for more details.\n\n\n\n\nHow to Access SnappyData store from an Existing Spark Installation using Smart Connector\n\n\nSnappyData comes with a Smart Connector that enables Spark applications to work with the SnappyData cluster, from any compatible Spark cluster (you can use any distribution that is compatible with Apache Spark 2.0.x). The Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. This is no different than how Spark applications today work with stores like Cassandra, Redis, etc.\n\n\nFor more information on the various modes, refer to the \nSnappyData Smart Connector\n section of the documentation.\n\n\nCode Example:\n\nThe code example for this mode is in \nSmartConnectorExample.scala\n\n\nConfigure a SnappySession\n: \n\n\nThe code below shows how to initialize a SparkSession. Here the property \nsnappydata.connection\n instructs the connector to acquire cluster connectivity and catalog metadata and registers it locally in the Spark cluster. Its value is consists of  locator host and JDBC client port on which the locator listens for connections (default 1527).\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nSmartConnectorExample\n)\n        // It can be any master URL\n        .master(\nlocal[4]\n)\n         // snappydata.connection property enables the application to interact with SnappyData store\n        .config(\nsnappydata.connection\n, \nlocalhost:1527\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\n\nCreate Table and Run Queries\n: \nYou can now create tables and run queries in SnappyData store using your Apache Spark program\n\n\n    // reading an already created SnappyStore table SNAPPY_COL_TABLE\n    val colTable = snSession.table(\nSNAPPY_COL_TABLE\n)\n    colTable.show(10)\n\n    snSession.dropTable(\nTestColumnTable\n, ifExists = true)\n\n    // Creating a table from a DataFrame\n    val dataFrame = snSession.range(1000).selectExpr(\nid\n, \nfloor(rand() * 10000) as k\n)\n\n    snSession.sql(\ncreate table TestColumnTable (id bigint not null, k bigint not null) using column\n)\n\n    // insert data in TestColumnTable\n    dataFrame.write.insertInto(\nTestColumnTable\n)\n\n\n\n\n\n\nHow to Use Snappy SQL shell (snappy-sql)\n\n\nsnappy-sql\n can be used to execute SQL on SnappyData cluster. In the background, \nsnappy-sql\n uses JDBC connections to execute SQL.\n\n\nConnect to a SnappyData Cluster\n: \nUse the \nsnappy-sql\n and \nconnect client\n command on the Snappy SQL Shell\n\n\n$ bin/snappy-sql\nsnappy\n connect client '\nlocatorHostName\n:1527';\n\n\n\n\nWhere \nlocatorHostName\n is the host name of the node on which the locator is started and \n1527\n is the default port on which the locator listens for connections. \n\n\nExecute SQL queries\n: Once connected you can execute SQL queries using \nsnappy-sql\n\n\nsnappy\n CREATE TABLE APP.PARTSUPP (PS_PARTKEY INTEGER NOT NULL PRIMARY KEY, PS_SUPPKEY INTEGER NOT NULL, PS_AVAILQTY INTEGER NOT NULL, PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL) USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') ;\n\nsnappy\n INSERT INTO APP.PARTSUPP VALUES(100, 1, 5000, 100);\n\nsnappy\n INSERT INTO APP.PARTSUPP VALUES(200, 2, 50, 10);\n\nsnappy\n SELECT * FROM APP.PARTSUPP;\nPS_PARTKEY |PS_SUPPKEY |PS_AVAILQTY|PS_SUPPLYCOST    \n-----------------------------------------------------\n100        |1          |5000       |100.00           \n200        |2          |50         |10.00            \n\n2 rows selected\n\n\n\n\n\nView the members of cluster\n: \nUse the \nshow members\n command\n\n\nsnappy\n show members;\nID                            |HOST                          |KIND                          |STATUS              |NETSERVERS                    |SERVERGROUPS                  \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n192.168.63.1(21412)\nv1\n:61964 |192.168.63.1                  |datastore(normal)             |RUNNING             |localhost/127.0.0.1[1528]     |                              \n192.168.63.1(21594)\nv2\n:29474 |192.168.63.1                  |accessor(normal)              |RUNNING             |                              |IMPLICIT_LEADER_SERVERGROUP   \nlocalhost(21262)\nv0\n:22770    |localhost                     |locator(normal)               |RUNNING             |localhost/127.0.0.1[1527]     |                              \n\n3 rows selected\n\n\n\n\n\nView the list tables in a schema\n: \nUse \nshow tables in \nschema\n command\n\n\nsnappy\n show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE|REMARKS             \n-----------------------------------------------------------------------------------\nAPP                 |PARTSUPP                      |TABLE     |                    \n\n1 row selected\n\n\n\n\n\n\nHow to Create Row Tables and Run Queries\n\n\nEach record in a Row table is managed in contiguous memory, and therefore, optimized for selective queries (For example. key based point lookup ) or updates. \nA row table can either be replicated to all nodes or partitioned across nodes. It can be created by using DataFrame API or using SQL.\n\n\nRefer to the \nRow and column tables\n documentation for complete list of attributes for row tables.\n\n\nFull source code, for example, to create and perform operations on replicated and partitioned row table can be found in \nCreateReplicatedRowTable.scala\n and \nCreatePartitionedRowTable.scala\n\n\nCreate a Row Table using DataFrame API:\n\n\nThe code snippet below shows how to create a replicated row table using API.\n\n\nGet a SnappySession\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nCreateReplicatedRowTable\n)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate the Table using API\n:\nFirst, define the table schema and then create the table using createTable API\n\n\n    val schema = StructType(Array(StructField(\nS_SUPPKEY\n, IntegerType, false),\n      StructField(\nS_NAME\n, StringType, false),\n      StructField(\nS_ADDRESS\n, StringType, false),\n      StructField(\nS_NATIONKEY\n, IntegerType, false),\n      StructField(\nS_PHONE\n, StringType, false),\n      StructField(\nS_ACCTBAL\n, DecimalType(15, 2), false),\n      StructField(\nS_COMMENT\n, StringType, false)\n    ))\n\n    // props1 map specifies the properties for the table to be created\n    // \nPERSISTENCE\n flag indicates that the table data should be persisted to\n    // disk asynchronously\n    val props1 = Map(\nPERSISTENCE\n -\n \nasynchronous\n)\n    // create a row table using createTable API\n    snSession.createTable(\nSUPPLIER\n, \nrow\n, schema, props1)\n\n\n\n\nCreating a Row table using SQL\n:\nThe same table can be created using SQL as shown below:\n\n\n    // First drop the table if it exists\n    snSession.sql(\nDROP TABLE IF EXISTS SUPPLIER\n)\n    // Create a row table using SQL\n    // \nPERSISTENCE\n that the table data should be persisted to disk asynchronously\n    // For complete list of attributes refer the documentation\n    snSession.sql(\n      \nCREATE TABLE SUPPLIER ( \n +\n          \nS_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n +\n          \nS_NAME STRING NOT NULL, \n +\n          \nS_ADDRESS STRING NOT NULL, \n +\n          \nS_NATIONKEY INTEGER NOT NULL, \n +\n          \nS_PHONE STRING NOT NULL, \n +\n          \nS_ACCTBAL DECIMAL(15, 2) NOT NULL, \n +\n          \nS_COMMENT STRING NOT NULL \n +\n          \n) USING ROW OPTIONS (PERSISTENCE 'asynchronous')\n)\n\n\n\n\nYou can perform various operations such as inset data, mutate it (update/delete), select data from the table. All these operations can be done either through APIs or by using SQL queries.\nFor example:\n\n\nTo insert data in the SUPPLIER table:\n \n\n\n    snSession.sql(\nINSERT INTO SUPPLIER VALUES(1, 'SUPPLIER1', 'CHICAGO, IL', 0, '555-543-789', 10000, ' ')\n)\n    snSession.sql(\nINSERT INTO SUPPLIER VALUES(2, 'SUPPLIER2', 'BOSTON, MA', 0, '555-234-489', 20000, ' ')\n)\n    snSession.sql(\nINSERT INTO SUPPLIER VALUES(3, 'SUPPLIER3', 'NEWYORK, NY', 0, '555-743-785', 34000, ' ')\n)\n    snSession.sql(\nINSERT INTO SUPPLIER VALUES(4, 'SUPPLIER4', 'SANHOSE, CA', 0, '555-321-098', 1000, ' ')\n)\n\n\n\n\nTo print the contents of the SUPPLIER table:\n \n\n\n    var tableData = snSession.sql(\nSELECT * FROM SUPPLIER\n).collect()\n    tableData.foreach(println)\n\n\n\n\nTo update the table account balance for SUPPLIER4:\n \n\n\n    snSession.sql(\nUPDATE SUPPLIER SET S_ACCTBAL = 50000 WHERE S_NAME = 'SUPPLIER4'\n)\n\n\n\n\nTo print contents of the SUPPLIER table after update\n \n\n\n    tableData = snSession.sql(\nSELECT * FROM SUPPLIER\n).collect()\n    tableData.foreach(println)\n\n\n\n\nTo delete the records for SUPPLIER2 and SUPPLIER3\n \n\n\n    snSession.sql(\nDELETE FROM SUPPLIER WHERE S_NAME = 'SUPPLIER2' OR S_NAME = 'SUPPLIER3'\n)\n\n\n\n\nTo print the contents of the SUPPLIER table after delete\n\n\n    tableData = snSession.sql(\nSELECT * FROM SUPPLIER\n).collect()\n    tableData.foreach(println)\n\n\n\n\n\n\nHow to Create Column Tables and Run Queries\n\n\nColumn tables organize and manage data in columnar form such that modern day CPUs can traverse and run computations like a sum or an average fast (as the values are available in contiguous memory).\n\n\nRefer to the \nRow and column tables\n documentation for the complete list of attributes for column tables.\n\n\nFull source code, for example, to create and perform operations on column table can be found in \nCreateColumnTable.scala\n\n\nCreate a Column Table using DataFrame API\n\n\nThe code snippet below shows how to create a column table using DataFrame API.\n\n\nGet a SnappySession\n:\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nCreateColumnTable\n)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nDefine the table schema\n\n\nval tableSchema = StructType(Array(StructField(\nC_CUSTKEY\n, IntegerType, false),\n      StructField(\nC_NAME\n, StringType, false),\n      StructField(\nC_ADDRESS\n, StringType, false),\n      StructField(\nC_NATIONKEY\n, IntegerType, false),\n      StructField(\nC_PHONE\n, StringType, false),\n      StructField(\nC_ACCTBAL\n, DecimalType(15, 2), false),\n      StructField(\nC_MKTSEGMENT\n, StringType, false),\n      StructField(\nC_COMMENT\n, StringType, false)\n    ))\n\n\n\n\nCreate the table and load data from CSV\n\n\n    // props1 map specifies the properties for the table to be created\n    // \nPARTITION_BY\n attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\n    val props1 = Map(\nPARTITION_BY\n -\n \nC_CUSTKEY\n)\n    snSession.createTable(\nCUSTOMER\n, \ncolumn\n, tableSchema, props1)\n\n     val tableSchema = snSession.table(\nCUSTOMER\n).schema\n    // insert some data in it\n    // loading data in CUSTOMER table from a text file with delimited columns\n    val customerDF = snSession.read.schema(schema = tableSchema).csv(\nquickstart/src/main/resources/customer.csv\n)\n    customerDF.write.insertInto(\nCUSTOMER\n)\n\n\n\n\nCreate a Column Table using SQL\n\n\nThe same table can be created using SQL as shown below:\n\n\n    snSession.sql(\nCREATE TABLE CUSTOMER ( \n +\n        \nC_CUSTKEY     INTEGER NOT NULL,\n +\n        \nC_NAME        VARCHAR(25) NOT NULL,\n +\n        \nC_ADDRESS     VARCHAR(40) NOT NULL,\n +\n        \nC_NATIONKEY   INTEGER NOT NULL,\n +\n        \nC_PHONE       VARCHAR(15) NOT NULL,\n +\n        \nC_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n +\n        \nC_MKTSEGMENT  VARCHAR(10) NOT NULL,\n +\n        \nC_COMMENT     VARCHAR(117) NOT NULL)\n +\n        \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n\n\n\nYou can execute selected queries on a column table, join the column table with other tables, and append data to it.\n\n\n\n\nHow to Load Data in SnappyData Tables\n\n\nYou can use Spark's DataFrameReader API in order to load data into SnappyData tables using different formats (Parquet, CSV, JSON etc.).\n\n\nCode Example\n\n\nGet a SnappySession\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nCreateColumnTable\n)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nA column table is created as follows\n\n\n    snSession.sql(\nCREATE TABLE CUSTOMER ( \n +\n        \nC_CUSTKEY     INTEGER NOT NULL,\n +\n        \nC_NAME        VARCHAR(25) NOT NULL,\n +\n        \nC_ADDRESS     VARCHAR(40) NOT NULL,\n +\n        \nC_NATIONKEY   INTEGER NOT NULL,\n +\n        \nC_PHONE       VARCHAR(15) NOT NULL,\n +\n        \nC_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n +\n        \nC_MKTSEGMENT  VARCHAR(10) NOT NULL,\n +\n        \nC_COMMENT     VARCHAR(117) NOT NULL)\n +\n        \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n\n\n\nLoad data in the CUSTOMER table from a CSV file by using DataFrameReader API\n\n\n    val tableSchema = snSession.table(\nCUSTOMER\n).schema\n    val customerDF = snSession.read.schema(schema = tableSchema).csv(s\n$dataFolder/customer.csv\n)\n    customerDF.write.insertInto(\nCUSTOMER\n)\n\n\n\n\nFor Parquet file, use code as given below\n\n\nval customerDF = snSession.read.parquet(s\n$dataDir/customer_parquet\n)\ncustomerDF.write.insertInto(\nCUSTOMER\n)\n\n\n\n\nInferring schema from data file\n\n\nA schema for the table can be inferred from the data file. In this case, you do not need to create a table before loading the data. In the code snippet below, first DataFrame for a Parquet file is created and then saveAsTable API is used to create a table with data loaded from it.\n\n\n    val customerDF = snSession.read.parquet(s\nquickstart/src/main/resources/customerparquet\n)\n    // props1 map specifies the properties for the table to be created\n    // \nPARTITION_BY\n attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\n    val props1 = Map(\nPARTITION_BY\n -\n \nC_CUSTKEY\n)\n    customerDF.write.format(\ncolumn\n).mode(\nappend\n).options(props1).saveAsTable(\nCUSTOMER\n)\n\n\n\n\nIn the code snippet below a schema is inferred from a CSV file. Column names are derived from the header in the file.\n\n\n    val customer_csv_DF = snSession.read.option(\nheader\n, \ntrue\n)\n        .option(\ninferSchema\n, \ntrue\n).csv(\nquickstart/src/main/resources/customer_with_headers.csv\n)\n\n    // props1 map specifies the properties for the table to be created\n    // \nPARTITION_BY\n attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY),\n    // For complete list of attributes refer the documentation\n    val props1 = Map(\nPARTITION_BY\n -\n \nC_CUSTKEY\n)\n    customer_csv_DF.write.format(\ncolumn\n).mode(\nappend\n).options(props1).saveAsTable(\nCUSTOMER\n)\n\n\n\n\nThe source code to load the data from a CSV/Parquet files is in \nCreateColumnTable.scala\n. Source for the code to load data from a JSON file can be found in \nWorkingWithJson.scala\n\n\n\n\nHow to Perform a Collocated Join\n\n\nWhen two tables are partitioned on columns and collocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData server. Collocating the data of two tables based on a partitioning column's value is a best practice if you frequently perform queries on those tables that join on that column.\nWhen collocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data.\n\n\nCode Example: ORDERS table is collocated with CUSTOMER table\n\n\nA partitioned table can be collocated with another partitioned table by using the \"COLOCATE_WITH\" attribute in the table options. \n\nFor example, in the code snippet below, the ORDERS table is collocated with the CUSTOMER table. The complete source for this example can be found in the file \nCollocatedJoinExample.scala\n\n\nGet a SnappySession\n:\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nCollocatedJoinExample\n)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate Table Customer:\n\n\n    snSession.sql(\nCREATE TABLE CUSTOMER ( \n +\n        \nC_CUSTKEY     INTEGER NOT NULL,\n +\n        \nC_NAME        VARCHAR(25) NOT NULL,\n +\n        \nC_ADDRESS     VARCHAR(40) NOT NULL,\n +\n        \nC_NATIONKEY   INTEGER NOT NULL,\n +\n        \nC_PHONE       VARCHAR(15) NOT NULL,\n +\n        \nC_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n +\n        \nC_MKTSEGMENT  VARCHAR(10) NOT NULL,\n +\n        \nC_COMMENT     VARCHAR(117) NOT NULL)\n +\n        \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n\n\n\nCreate Table Orders:\n\n\n    snSession.sql(\nCREATE TABLE ORDERS  ( \n +\n        \nO_ORDERKEY       INTEGER NOT NULL,\n +\n        \nO_CUSTKEY        INTEGER NOT NULL,\n +\n        \nO_ORDERSTATUS    CHAR(1) NOT NULL,\n +\n        \nO_TOTALPRICE     DECIMAL(15,2) NOT NULL,\n +\n        \nO_ORDERDATE      DATE NOT NULL,\n +\n        \nO_ORDERPRIORITY  CHAR(15) NOT NULL,\n +\n        \nO_CLERK          CHAR(15) NOT NULL,\n +\n        \nO_SHIPPRIORITY   INTEGER NOT NULL,\n +\n        \nO_COMMENT        VARCHAR(79) NOT NULL) \n +\n        \nUSING COLUMN OPTIONS (PARTITION_BY 'O_CUSTKEY', \n +\n        \nCOLOCATE_WITH 'CUSTOMER' )\n)\n\n\n\n\nPerform a Collocate join:\n \n\n\n    // Selecting orders for all customers\n    val result = snSession.sql(\nSELECT C_CUSTKEY, C_NAME, O_ORDERKEY, O_ORDERSTATUS, O_ORDERDATE, \n +\n        \nO_TOTALPRICE FROM CUSTOMER, ORDERS WHERE C_CUSTKEY = O_CUSTKEY\n).collect()\n\n\n\n\n\n\nHow to Connect using JDBC Driver\n\n\nYou can connect to and execute queries against SnappyData cluster using JDBC driver. The connection URL typically points to one of the locators. The locator passes the information of all available servers based on which, the driver automatically connects to one of the servers.\n\n\nTo connect to the SnappyData cluster\n: Using JDBC, use URL of the form \njdbc:snappydata://\nlocatorHostName\n:\nlocatorClientPort\n/\n\n\nWhere the \nlocatorHostName\n is the hostname of the node on which the locator is started and \nlocatorClientPort\n is the port on which the locator accepts client connections (default 1527).\n\n\nCode Example:\n\n\nConnect to a SnappyData cluster using JDBC on default client port\n\n\nThe code snippet shows how to connect to a SnappyData cluster using JDBC on default client port 1527. The complete source code of the example is located at \nJDBCExample.scala\n\n\nval url: String = s\njdbc:snappydata://localhost:1527/\n\nval conn1 = DriverManager.getConnection(url)\n\nval stmt1 = conn1.createStatement()\n// Creating a table (PARTSUPP) using JDBC connection\nstmt1.execute(\nDROP TABLE IF EXISTS APP.PARTSUPP\n)\nstmt1.execute(\nCREATE TABLE APP.PARTSUPP ( \n +\n     \nPS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,\n +\n     \nPS_SUPPKEY     INTEGER NOT NULL,\n +\n     \nPS_AVAILQTY    INTEGER NOT NULL,\n +\n     \nPS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)\n +\n    \nUSING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\n)\n\n// Inserting records in PARTSUPP table via batch inserts\nval preparedStmt1 = conn1.prepareStatement(\nINSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\n)\n\nvar x = 0\nfor (x \n- 1 to 10) {\n  preparedStmt1.setInt(1, x*100)\n  preparedStmt1.setInt(2, x)\n  preparedStmt1.setInt(3, x*1000)\n  preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2))\n  preparedStmt1.addBatch()\n}\npreparedStmt1.executeBatch()\npreparedStmt1.close()\n\n\n\n\n\n\nNote\n\n\nIf the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the \nio.snappydata.jdbc.ClientDriver\n class.\n\n\n\n\n\n\nHow to Store and Query JSON Objects\n\n\nYou can insert JSON data in SnappyData tables and execute queries on the tables.\n\n\nCode Example: Loads JSON data from a JSON file into a column table and executes query\n\n\nThe code snippet loads JSON data from a JSON file into a column table and executes the query against it.\nThe source code for JSON example is located at \nWorkingWithJson.scala\n. After creating SnappySession, the JSON file is read using Spark API and loaded into a SnappyData table.\n\n\nGet a SnappySession\n:\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nWorkingWithJson\n)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate a DataFrame from the JSON file\n:\n\n\n    val some_people_path = s\nquickstart/src/main/resources/some_people.json\n\n    // Read a JSON file using Spark API\n    val people = snSession.read.json(some_people_path)\n    people.printSchema()\n\n\n\n\nCreate a SnappyData table and insert the JSON data in it using the DataFrame\n:\n\n\n    //Drop the table if it exists\n    snSession.dropTable(\npeople\n, ifExists = true)\n\n   //Create a columnar table with the Json DataFrame schema\n    snSession.createTable(tableName = \npeople\n,\n      provider = \ncolumn\n,\n      schema = people.schema,\n      options = Map.empty[String,String],\n      allowExisting = false)\n\n    // Write the created DataFrame to the columnar table\n    people.write.insertInto(\npeople\n)\n\n\n\n\nAppend more data from a second JSON file\n:\n\n\n    // Append more people to the column table\n    val more_people_path = s\nquickstart/src/main/resources/more_people.json\n\n\n    //Explicitly passing schema to handle record level field mismatch\n    // e.g. some records have \ndistrict\n field while some do not.\n    val morePeople = snSession.read.schema(people.schema).json(more_people_path)\n    morePeople.write.insertInto(\npeople\n)\n\n    //print schema of the table\n    println(\nPrint Schema of the table\\n################\n)\n    println(snSession.table(\npeople\n).schema)\n\n\n\n\nExecute queries and return the results\n\n\n    // Query it like any other table\n    val nameAndAddress = snSession.sql(\nSELECT \n +\n        \nname, \n +\n        \naddress.city, \n +\n        \naddress.state, \n +\n        \naddress.district, \n +\n        \naddress.lane \n +\n        \nFROM people\n)\n\nnameAndAddress.toJSON.show()\n\n\n\n\n\n\n\nHow to Store and Query Objects\n\n\nYou can use domain object to load data into SnappyData tables and select the data by executing queries against the table.\n\n\nCode Example: Insert Person object into the column table\n\n\nThe code snippet below inserts Person objects into a column table. The source code for this example is located at \nWorkingWithObjects.scala\n. After creating SnappySession, the Person objects is inserted using Spark API and loads into a SnappyData table.\n\n\nGet a SnappySession\n:\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nCreateReplicatedRowTable\n)\n        .master(\nlocal[4]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate DataFrame objects\n:\n\n\n    //Import the implicits for automatic conversion between Objects to DataSets.\n    import snSession.implicits._\n\n    // Create a Dataset using Spark APIs\n    val people = Seq(Person(\nTom\n, Address(\nColumbus\n, \nOhio\n), Map(\nfrnd1\n-\n \n8998797979\n, \nfrnd2\n -\n \n09878786886\n))\n      , Person(\nNed\n, Address(\nSan Diego\n, \nCalifornia\n), Map.empty[String,String])).toDS()\n\n\n\n\nCreate a SnappyData table and insert data into it\n:\n\n\n    //Drop the table if it exists.\n    snSession.dropTable(\nPersons\n, ifExists = true)\n\n    //Create a columnar table with a Struct to store Address\n    snSession.sql(\nCREATE table Persons(name String, address Struct\ncity: String, state:String\n, \n +\n         \nemergencyContacts Map\nString,String\n) using column options()\n)\n\n    // Write the created DataFrame to the columnar table.\n    people.write.insertInto(\nPersons\n)\n\n    //print schema of the table\n    println(\nPrint Schema of the table\\n################\n)\n    println(snSession.table(\nPersons\n).schema)\n\n\n    // Append more people to the column table\n    val morePeople = Seq(Person(\nJon Snow\n, Address(\nColumbus\n, \nOhio\n), Map.empty[String,String]),\n      Person(\nRob Stark\n, Address(\nSan Diego\n, \nCalifornia\n), Map.empty[String,String]),\n      Person(\nMichael\n, Address(\nNull\n, \nCalifornia\n), Map.empty[String,String])).toDS()\n\n    morePeople.write.insertInto(\nPersons\n)\n\n\n\n\nExecute query on the table and return results\n:\n\n\n    // Query it like any other table\n    val nameAndAddress = snSession.sql(\nSELECT name, address, emergencyContacts FROM Persons\n)\n\n    //Reconstruct the objects from obtained Row\n    val allPersons = nameAndAddress.as[Person]\n    //allPersons is a Spark Dataset of Person objects. \n    // Use of the Dataset APIs to transform, query this data set. \n\n\n\n\n\n\nHow to Use Stream Processing with SnappyData\n\n\nSnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and to integrate with the built-in store. In SnappyData, you can define streams declaratively from any SQL client, register continuous queries on streams, mutate SnappyData tables based on the streaming data. For more information on streaming, refer to the \ndocumentation\n.\n\n\nCode Example\n: \nCode example for streaming is in \nStreamingExample.scala\n. The code snippets below shows how to declare a stream table, register continuous queries(CQ) and update SnappyData table using the stream data.\n\n\nFirst get a SnappySession and a SnappyStreamingContext\n: \nHere SnappyStreamingContext is initialized in a batch duration of one second.\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(getClass.getSimpleName)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snsc = new SnappyStreamingContext(spark.sparkContext, Seconds(1))\n\n\n\n\nThe example starts an embedded Kafka instance on which a few messages are published. SnappyData processes these message and updates a table based on the stream data.\n\n\nThe SQL below shows how to declare a stream table using SQL. The rowConverter attribute specifies a class used to return Row objects from the received stream messages.\n\n\n    snsc.sql(\n      \ncreate stream table adImpressionStream (\n +\n          \n time_stamp timestamp,\n +\n          \n publisher string,\n +\n          \n advertiser string,\n +\n          \n website string,\n +\n          \n geo string,\n +\n          \n bid double,\n +\n          \n cookie string) \n + \n using directkafka_stream options(\n +\n          \n rowConverter 'org.apache.spark.examples.snappydata.RowsConverter',\n +\n          s\n kafkaParams 'metadata.broker.list-\n$address;auto.offset.reset-\nsmallest',\n +\n          s\n topics 'kafka_topic')\n\n    )\n\n\n\n\nRowsConverter decodes a stream message consisting of comma separated fields and forms a Row object from it.\n\n\nclass RowsConverter extends StreamToRowsConverter with Serializable {\n  override def toRows(message: Any): Seq[Row] = {\n    val log = message.asInstanceOf[String]\n    val fields = log.split(\n,\n)\n    val rows = Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong),\n      fields(1),\n      fields(2),\n      fields(3),\n      fields(4),\n      fields(5).toDouble,\n      fields(6)\n    )))\n    rows\n  }\n}\n\n\n\n\nTo create a row table that is updated based on the streaming data\n:\n\n\nsnsc.sql(\ncreate table publisher_bid_counts(publisher string, bidCount int) using row\n)\n\n\n\n\nTo declare a continuous query that is executed on the streaming data\n: This query returns a number of bids per publisher in one batch.\n\n\n    val resultStream: SchemaDStream = snsc.registerCQ(\nselect publisher, count(bid) as bidCount from \n +\n        \nadImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher\n)\n\n\n\n\nTo process that the result of above continuous query to update the row table publisher_bid_counts\n:\n\n\n    // this conf is used to get a JDBC connection\n    val conf = new ConnectionConfBuilder(snsc.snappySession).build()\n\n    resultStream.foreachDataFrame(df =\n {\n        println(\nData received in streaming window\n)\n        df.show()\n\n        println(\nUpdating table publisher_bid_counts\n)\n        val conn = ConnectionUtil.getConnection(conf)\n        val result = df.collect()\n        val stmt = conn.prepareStatement(\nupdate publisher_bid_counts set \n +\n            s\nbidCount = bidCount + ? where publisher = ?\n)\n\n        result.foreach(row =\n {\n          val publisher = row.getString(0)\n          val bidCount = row.getLong(1)\n          stmt.setLong(1, bidCount)\n          stmt.setString(2, publisher)\n          stmt.addBatch()\n        }\n        )\n        stmt.executeBatch()\n        conn.close()\n      }\n    })\n\n\n\n\n\nTo display the total bids by each publisher by querying publisher_bid_counts table\n:\n\n\nsnsc.snappySession.sql(\nselect publisher, bidCount from publisher_bid_counts\n).show()\n\n\n\n\n\n\n\nHow to Use Synopsis Data Engine to Run Approximate Queries\n\n\nSynopsis Data Engine (SDE) uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire Dataset. The approach trades off query accuracy for fast response time.\nFor more information on  SDE, refer to \nSDE documentation\n.\n\n\nCode Example\n:\nThe complete code example for SDE is in \nSynopsisDataExample.scala\n. The code below creates a sample table and executes queries that run on the sample table.\n\n\nGet a SnappySession\n:\n\n\n    val spark: SparkSession = SparkSession\n        .builder\n        .appName(\nSynopsisDataExample\n)\n        .master(\nlocal[*]\n)\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nThe base column table(AIRLINE) is created from temporary parquet table as follows\n:\n\n\n    // Create temporary staging table to load parquet data\n    snSession.sql(\nCREATE EXTERNAL TABLE STAGING_AIRLINE \n +\n        \nUSING parquet OPTIONS(path \n + s\n'${dataFolder}/airlineParquetData')\n)\n\n    // Create a column table AIRLINE\n    snSession.sql(\nCREATE TABLE AIRLINE USING column AS (SELECT Year AS Year_, \n +\n        \nMonth AS Month_ , DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, \n +\n        \nCRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, \n +\n        \nCRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance, \n +\n        \nTaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay, \n +\n        \nWeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, \n +\n        \nArrDelaySlot FROM STAGING_AIRLINE)\n)\n\n\n\n\nCreate a sample table for the above base table\n:\nAttribute 'qcs' in the statement below specifies the columns used for stratification and attribute 'fraction' specifies how big the sample needs to be (3% of the base table AIRLINE in this case). For more information on Synopsis Data Engine, refer to the \nSDE documentation\n.\n\n\n\n    snSession.sql(\nCREATE SAMPLE TABLE AIRLINE_SAMPLE ON AIRLINE OPTIONS\n +\n        \n(qcs 'UniqueCarrier, Year_, Month_', fraction '0.03')  \n +\n        \nAS (SELECT Year_, Month_ , DayOfMonth, \n +\n        \nDayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier, \n +\n        \nFlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, \n +\n        \nArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, \n +\n        \nCancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, \n +\n        \nNASDelay, SecurityDelay, LateAircraftDelay, ArrDelaySlot FROM AIRLINE)\n)\n\n\n\n\nExecute queries that return approximate results using sample tables\n:\nThe query below returns airlines by number of flights in descending order. The 'with error 0.20' clause in the query below signals query engine to execute the query on the sample table instead of the base table and maximum 20% error is allowed.\n\n\n    var result = snSession.sql(\nselect  count(*) flightRecCount, description AirlineName, \n +\n        \nUniqueCarrier carrierCode ,Year_ from airline , airlineref where \n +\n        \nairline.UniqueCarrier = airlineref.code group by \n +\n        \nUniqueCarrier,description, Year_ order by flightRecCount desc limit \n +\n        \n10 with error 0.20\n).collect()\n    result.foreach(r =\n println(r(0) + \n, \n + r(1) + \n, \n + r(2) + \n, \n + r(3)))\n\n\n\n\nJoin the sample table with a reference table\n:\nYou can join the sample table with a reference table to execute queries. For example a reference table (AIRLINEREF) is created as follows from a parquet data file.\n\n\n    // create temporary staging table to load parquet data\n    snSession.sql(\nCREATE EXTERNAL TABLE STAGING_AIRLINEREF USING \n +\n        \nparquet OPTIONS(path \n + s\n'${dataFolder}/airportcodeParquetData')\n)\n    snSession.sql(\nCREATE TABLE AIRLINEREF USING row AS (SELECT CODE, \n +\n        \nDESCRIPTION FROM STAGING_AIRLINEREF)\n)\n\n\n\n\nJoin the sample table and reference table to find out which airlines arrive on schedule\n:\n\n\n    result = snSession.sql(\nselect AVG(ArrDelay) arrivalDelay, \n +\n        \nrelative_error(arrivalDelay) rel_err, description AirlineName, \n +\n        \nUniqueCarrier carrier from airline, airlineref \n +\n        \nwhere airline.UniqueCarrier = airlineref.Code \n +\n        \ngroup by UniqueCarrier, description order by arrivalDelay \n +\n        \nwith error\n).collect()\n\n   result.foreach(r =\n println(r(0) + \n, \n + r(1) + \n, \n + r(2) + \n, \n + r(3)))\n\n\n\n\n\n\nHow to Use Python to Create Tables and Run Queries\n\n\nDevelopers can write programs in Python to use SnappyData features.\n\n\nFirst create a SnappySession\n:\n\n\n from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)\n\n\n\n\nCreate table using SnappySession\n:\n\n\n    # Creating partitioned table PARTSUPP using SQL\n    snappy.sql(\nDROP TABLE IF EXISTS PARTSUPP\n)\n    # \nPARTITION_BY\n attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY),\n    # For complete list of table attributes refer the documentation\n    # http://snappydatainc.github.io/snappydata/programming_guide\n    snappy.sql(\nCREATE TABLE PARTSUPP ( \n +\n                  \nPS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,\n +\n                  \nPS_SUPPKEY     INTEGER NOT NULL,\n +\n                  \nPS_AVAILQTY    INTEGER NOT NULL,\n +\n                  \nPS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)\n +\n                  \nUSING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY' )\n)\n\n\n\n\nInserting data in table using INSERT query\n:\n\n\n    snappy.sql(\nINSERT INTO PARTSUPP VALUES(100, 1, 5000, 100)\n)\n    snappy.sql(\nINSERT INTO PARTSUPP VALUES(200, 2, 50, 10)\n)\n    snappy.sql(\nINSERT INTO PARTSUPP VALUES(300, 3, 1000, 20)\n)\n    snappy.sql(\nINSERT INTO PARTSUPP VALUES(400, 4, 200, 30)\n)\n    # Printing the contents of the PARTSUPP table\n    snappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nUpdate the data using SQL\n:\n\n\n    # Update the available quantity for PARTKEY 100\n    snappy.sql(\nUPDATE PARTSUPP SET PS_AVAILQTY = 50000 WHERE PS_PARTKEY = 100\n)\n    # Printing the contents of the PARTSUPP table after update\n    snappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nDelete records from the table\n:\n\n\n    # Delete the records for PARTKEY 400\n    snappy.sql(\nDELETE FROM PARTSUPP WHERE PS_PARTKEY = 400\n)\n    # Printing the contents of the PARTSUPP table after delete\n    snappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nCreate table using API\n:\nThis same table can be created by using createTable API. First create a schema and then create the table, and then mutate the table data using API:\n\n\n    # drop the table if it exists\n    snappy.dropTable('PARTSUPP', True)\n\n    schema = StructType([StructField('PS_PARTKEY', IntegerType(), False),\n                     StructField('PS_SUPPKEY', IntegerType(), False),\n                     StructField('PS_AVAILQTY', IntegerType(),False),\n                     StructField('PS_SUPPLYCOST', DecimalType(15, 2), False)\n                     ])\n\n    # \nPARTITION_BY\n attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY)\n    # For complete list of table attributes refer the documentation at\n    # http://snappydatainc.github.io/snappydata/programming_guide\n    snappy.createTable('PARTSUPP', 'row', schema, False, PARTITION_BY = 'PS_PARTKEY')\n\n    # Inserting data in PARTSUPP table using DataFrame\n    tuples = [(100, 1, 5000, Decimal(100)), (200, 2, 50, Decimal(10)),\n              (300, 3, 1000, Decimal(20)), (400, 4, 200, Decimal(30))]\n    rdd = sc.parallelize(tuples)\n    tuplesDF = snappy.createDataFrame(rdd, schema)\n    tuplesDF.write.insertInto(\nPARTSUPP\n)\n    #Printing the contents of the PARTSUPP table\n    snappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n    # Update the available quantity for PARTKEY 100\n    snappy.update(\nPARTSUPP\n, \nPS_PARTKEY =100\n, [50000], [\nPS_AVAILQTY\n])\n    # Printing the contents of the PARTSUPP table after update\n    snappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n    # Delete the records for PARTKEY 400\n    snappy.delete(\nPARTSUPP\n, \nPS_PARTKEY =400\n)\n    # Printing the contents of the PARTSUPP table after delete\n    snappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nThe complete source code for the above example is in \nCreateTable.py\n\n\n\n\nHow to Connect using ODBC Driver\n\n\nYou can connect to SnappyData Cluster using SnappyData ODBC Driver and can execute SQL queries by connecting to any of the servers in the cluster.\n\n\n\nStep 1: Install Visual C++ Redistributable for Visual Studio 2015\n\n\nTo download and install the Visual C++ Redistributable for Visual Studio 2015:\n\n\n\n\n\n\nDownload Visual C++ Redistributable for Visual Studio 2015\n\n\n\n\n\n\nDepending on your Windows installation, download the required version of the SnappyData ODBC Driver.\n\n\n\n\n\n\nSelect \nRun\n to start the installation, and follow the steps to complete the installation.\n\n\n\n\n\n\n\n\nStep 2: Install SnappyData ODBC Driver\n\n\nTo download and install the ODBC driver:\n\n\n\n\n\n\nDownload the SnappyData ODBC Driver from the \nSnappyData Release page\n.\n\n\n\n\n\n\nDepending on your Windows installation, download the 32-bit or 64-bit version of the SnappyData ODBC Driver.\n\n\n\n\n\n\n32-bit for 32-bit platform\n\n\n\n\n\n\n32-bit for 64-bit platform\n \n\n\n\n\n\n\n64-bit for 64-bit platform\n \n\n\n\n\n\n\n\n\n\n\nExtract the contents of the downloaded file.\n\n\n\n\n\n\nDouble-click on the \nSnappyDataODBCDriverInstaller.msi\n file, and follow the steps to complete the installation.\n\n\n\n\nNote\n\n\nEnsure that \nSnappyData version 0.8 or later is installed\n and the \nSnappyData cluster is running\n.\n\n\n\n\n\n\n\n\nConnect to the SnappyData cluster\n\n\nOnce you have installed SnappyData ODBC Driver, you can connect to SnappyData cluster in any of the following ways:\n\n\n\n\n\n\nUse the SnappyData Driver Connection URL:\n\n\nDriver=SnappyData ODBC Driver;server=\nServerHost\n;port=\nServerPort\n;user=\nuserName\n;password=\npassword\n\n\n\n\n\n\n\n\nCreate a SnappyData DSN (Data Source Name) using the installed SnappyData ODBC Driver.\n \n Please refer to the Windows documentation relevant to your operating system for more information on creating a DSN. \nWhen prompted, select the SnappyData ODBC Driver from the driver's list and enter a Data Source name, SnappyData Server Host, Port, User Name and Password. \n\n\n\n\n\n\nRefer to the documentation for detailed information on \nSetting Up SnappyData ODBC Driver and Tableau Desktop\n.  \n\n\n\n\nHow to Connect to the Cluster from External Clients\n\n\nYou can also connect to the SnappyData cluster from a different network as client (DbVisualizer, SQuirreL SQL etc.). \nFor example, you can connect to the cluster on AWS when connecting as a client from your local machine.\n\n\nWhen \nstarting the locator and server\n set the following properties in the \nconf/locators\n and \nconf/servers\n files:\n\n\n\n\n\n\n-hostname-for-clients\n: The public IP address of the locator or server. \n\n\n\n\n\n\n-client-bind-address\n: IP address of the locator or server. \nFor example, add \n-hostname-for-clients=192.168.20.208\n \n \n\n\n\n\nNote\n\n\nBy default, the locator or server binds to localhost. If the IP address is not set, the connection may fail.\n\n\n\n\n\n\n\n\nPort Settings\n: The client, by default, connects to the locator or server at the default port 1527. Ensure that this port is open in your firewall settings. \n You can also change the default port by setting the \n-client-port\n property.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nIf the above properties are not set, when a client tries to connect to the cluster from a different network, the connection may fail and an error may be reported. \n\n\n\n\n\n\nFor ODBC clients, you must use the host and port details of the server and not the locator.\n\n\n\n\n\n\n\n\n\n\nHow to Use Apache Zeppelin with SnappyData\n\n\nStep 1: Download, Install and Configure SnappyData\n\n\n\n\n\n\nDownload and Install SnappyData\n \n\n The table below lists the version of the SnappyData Zeppelin Interpreter and Apache Zeppelin Installer for the supported SnappyData Release.\n\n\n\n\n\n\n\n\nSnappyData Zeppelin Interpreter\n\n\nApache Zeppelin Binary Package\n\n\nSnappyData Release\n\n\n\n\n\n\n\n\n\n\nVersion 0.6.1\n\n\nVersion 0.6\n\n\nRelease 0.7\n \n \nRelease 0.8\n and \nfuture realeases\n\n\n\n\n\n\nVersion 0.7.1\n\n\nVersion 0.7\n\n\nRelease 0.8\n \nand future releases\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure the SnappyData Cluster\n.\n\n\n\n\n\n\nIn \nlead node configuration\n set the following properties:\n\n\n\n\n\n\nEnable the SnappyData Zeppelin interpreter by adding \n-zeppelin.interpreter.enable=true\n \n\n\n\n\n\n\nIn the classpath option, define the location where the SnappyData Interpreter is downloaded by adding \n-classpath=/\ndownload_location\n/snappydata-zeppelin-\nversion_number\n.jar\n.\n\n\n\n\n\n\n\n\n\n\nStart the SnappyData cluster\n\n\n\n\n\n\nExtract the contents of the Zeppelin binary package. \n \n\n\n\n\n\n\nInstall the SnappyData Zeppelin interpreter in Apache Zeppelin by executing the following command from Zeppelin's bin directory: \n\n    \n./install-interpreter.sh --name snappydata --artifact io.snappydata:snappydata-zeppelin:\nsnappydata_interpreter_version_number\n. \n\n    Zeppelin interpreter allows the SnappyData interpreter to be plugged into Zeppelin using which, you can run queries.\n\n\n\n\n\n\nRename the \nzeppelin-site.xml.template\n file (located in zeppelin-\nversion_number\n-bin-all/conf directory) to \nzeppelin-site.xml\n.\n\n\n\n\n\n\nEdit the \nzeppelin-site.xml\n file, and in the \nzeppelin.interpreters\n property, add the following interpreter class names: \norg.apache.zeppelin.interpreter.SnappyDataZeppelinInterpreter,org.apache.zeppelin.interpreter.SnappyDataSqlZeppelinInterpreter\n.\n\n\n\n\n\n\nStart the Zeppelin daemon using the command: \n \nbin/zeppelin-daemon.sh start\n.\n\n\n\n\n\n\nTo ensure that the installation is successful, log into the Zeppelin UI (\nhttp://localhost:8080\n) from your web browser.\n\n\n\n\n\n\nStep 2: Configure SnappyData for Apache Zeppelin\n\n\n\n\n\n\nLog on to Zeppelin from your web browser and select \nInterpreter\n from the \nSettings\n option.\n\n\n\n\n\n\nClick \nCreate\n \n to add an interpreter.  \n\n\n\n\n\n\nFrom the \nInterpreter group\n drop-down select \nsnappydata\n.\n     \n\n\n\n\nNote\n\n\nIf \nsnappydata\n is not displayed in the \nInterpreter group\n drop-down list, try the following options, and then restart Zeppelin daemon: \n\n\n\n\n\n\nDelete the \ninterpreter.json\n file located in the \nconf\n directory (in the Zeppelin home directory).\n\n\n\n\n\n\nDelete the \nzeppelin-spark_\nversion_number\n.jar\n file located in the \ninterpreter/snappydata\n directory (in the Zeppelin home directory).\n\n\n\n\n\n\n\n\n\n\n\n\nClick the \nConnect to existing process\n option. The fields \nHost\n and \nPort\n are displayed.\n\n\n\n\n\n\nSpecify the host on which the SnappyData lead node is executing, and the SnappyData Zeppelin Port (Default is 3768).\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault Values\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHost\n\n\nlocalhost\n\n\nSpecify host on which the SnappyData lead node is executing\n\n\n\n\n\n\nPort\n\n\n3768\n\n\nSpecify the Zeppelin server port\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure the interpreter properties. \nThe table lists the properties required for SnappyData.\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndefault.ur\n\n\njdbc:snappydata://localhost:1527/\n\n\nSpecify the JDBC URL for SnappyData cluster in the format \njdbc:snappydata://\nlocator_hostname\n:1527\n\n\n\n\n\n\ndefault.driver\n\n\ncom.pivotal.gemfirexd.jdbc.ClientDriver\n\n\nSpecify the JDBC driver for SnappyData\n\n\n\n\n\n\nsnappydata.connection\n\n\nlocalhost:1527\n\n\nSpecify the \nhost:clientPort\n combination of the locator for the JDBC connection\n\n\n\n\n\n\nmaster\n\n\nlocal[*]\n\n\nSpecify the URI of the spark master (only local/split mode)\n\n\n\n\n\n\nzeppelin.jdbc.concurrent.use\n\n\ntrue\n\n\nSpecify the Zeppelin scheduler to be used. \nSelect \nTrue\n for Fair and \nFalse\n for FIFO\n\n\n\n\n\n\n\n\n\n\n\n\nIf required, edit other properties, and then click \nSave\n to apply your changes.\n\n\n\n\n\n\nBind the interpreter and set SnappyData as the default interpreter.\n SnappyData Zeppelin Interpreter group consist of two interpreters. Click and drag \nInterpreter_Name\n to the top of the list to set it as the default interpreter. \n\n\n\n\n\n\n\n\nInterpreter Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%snappydata.snappydata or \n %snappydata.spark\n\n\nThis interpreter is used to write Scala code in the paragraph. SnappyContext is injected in this interpreter and can be accessed using variable \nsnc\n\n\n\n\n\n\n%snappydata.sql\n\n\nThis interpreter is used to execute SQL queries on the SnappyData cluster. It also has features of executing approximate queries on the SnappyData cluster.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n to apply your changes.\n\n\n\n\n\n\n\n\nNote\n\n\nYou can modify the default port number of the Zeppelin interpreter by setting the property:\n\n\n-zeppelin.interpreter.port=\nport_number\n in \nlead node configuration\n. \n\n\n\n\nKnown Issue\n\n\nIf you are using SnappyData Zeppelin Interpreter 0.7.1 and Zeppelin Installer 0.7 with SnappyData 0.8 or future releases, approximate result does not work on the sample table, when you execute a paragraph with the \n%sql show-instant-results-first\n directive.\n\n\nMore Information\n\n\nRefer to these sections for information:\n\n\n\n\n\n\nAbout the Interpreter\n \n\n\n\n\n\n\nExample Notebooks", 
            "title": "How Tos"
        }, 
        {
            "location": "/howto/#overview", 
            "text": "This section introduces you to several common operations such as starting a cluster, working with tables (load, query, update), working with streams and running approximate queries.  Running the Examples: \nTopics in this section refer to source code examples that are shipped with the product. Instructions to run these examples can be found in the source code.  Source code for these examples is located in the  quickstart/src/main/scala/org/apache/spark/examples/snappydata  and in  quickstart/python  directories of the SnappyData product distribution.  Refer to the  Getting Started  to either run SnappyData on premise, using AWS or Docker.   You can run the examples in any of the following ways:    In the Local Mode : By using  bin/run-example  script (to run Scala examples) or by using  bin/spark-submit  script (to run Python examples). The examples run collocated with Spark+SnappyData Store in the same JVM.     As a Job : Many of the Scala examples are also implemented as a SnappyData job. In this case, examples can be submitted as a job to a running SnappyData cluster. Refer to  jobs  section for details on how to run a job.     Note  SnappyData also supports Java API. Refer to the  documentation  for more details on Java API.   The following topics are covered in this section:    How to Start a SnappyData Cluster    How to Run Spark Job inside the Cluster    How to Access SnappyData Store from existing Spark Installation using Smart Connector    How to Create Row Tables and Run Queries    How to Create Column Tables and Run Queries    How to Load Data in SnappyData Tables    How to Perform a Collocated Join    How to Connect using JDBC Driver    How to Store and Query JSON Objects    How to Store and Query Objects    How to Use Stream Processing with SnappyData    How to Use Synopsis Data Engine to Run Approximate Queries    How to Use Python to Create Tables and Run Queries    How to Connect using ODBC Driver    How to Connect to the Cluster from External Clients    How to Use Apache Zeppelin with SnappyData", 
            "title": "Overview"
        }, 
        {
            "location": "/howto/#how-to-start-a-snappydata-cluster", 
            "text": "", 
            "title": "How to Start a SnappyData Cluster"
        }, 
        {
            "location": "/howto/#start-snappydata-cluster-on-a-single-machine", 
            "text": "If you have  downloaded and extracted  the SnappyData product distribution, navigate to the SnappyData product root directory.  Start the Cluster : Run the  sbin/snappy-start-all.sh  script to start SnappyData cluster on your single machine using default settings. This starts one lead node, one locator, and one data server.  $ sbin/snappy-start-all.sh  It may take 30 seconds or more to bootstrap the entire cluster on your local machine.  Sample Output : The sample output for  snappy-start-all.sh  is displayed as:  Starting SnappyData Locator using peer discovery on: localhost[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user/snappyData/work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 9368 status: running\nStarting SnappyData Server using locators for peer discovery: user1-laptop[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user1/snappyData/work/localhost-server-1/snappyserver.log\nSnappyData Server pid: 9519 status: running\n  Distributed system now has 2 members.\n  Other members: localhost(9368:locator) v0 :16944\nStarting SnappyData Leader using locators for peer discovery: user1-laptop[10334]\nLogs generated in /home/user1/snappyData/work/localhost-lead-1/snappyleader.log\nSnappyData Leader pid: 9699 status: running\n  Distributed system now has 3 members.\n  Other members: localhost(9368:locator) v0 :16944, 192.168.63.1(9519:datastore) v1 :46966  Check Status : You can check the status of a running cluster using the following command:  $ sbin/snappy-status-all.sh\nSnappyData Locator pid: 9368 status: running\nSnappyData Server pid: 9519 status: running\n  Distributed system now has 2 members.\n  Other members: localhost(9368:locator) v0 :16944\nSnappyData Leader pid: 9699 status: running\n  Distributed system now has 3 members.\n  Other members: localhost(9368:locator) v0 :16944, 192.168.63.1(9519:datastore) v1 :46966  You can check SnappyData UI by opening  http:// leadHostname :5050  in browser, where  leadHostname  is the host name of your lead node. Use  Snappy SQL shell  to connect to the cluster and perform various SQL operations.  Shutdown Cluster : You can shutdown the cluster using the  sbin/snappy-stop-all.sh  command:  $ sbin/snappy-stop-all.sh\nThe SnappyData Leader has stopped.\nThe SnappyData Server has stopped.\nThe SnappyData Locator has stopped.", 
            "title": "Start SnappyData Cluster on a Single Machine"
        }, 
        {
            "location": "/howto/#start-snappydata-cluster-on-multiple-hosts", 
            "text": "To start the cluster on multiple hosts:    The easiest way to run SnappyData on multiple nodes is to use a shared file system such as NFS on all the nodes.  You can also extract the product distribution on each node of the cluster. If all nodes have NFS access, install SnappyData on any one of the nodes.    Create the configuration files using the templates provided in the  conf  folder. Copy the existing template files  servers.template ,  locators.template ,  leads.template , and rename them to  servers ,  locators ,  leads .  Edit the files to include the hostnames on which to start the server, locator, and lead. Refer to the  configuration  section for more information on properties.    Start the cluster using  sbin/snappy-start-all.sh . SnappyData starts the cluster using SSH.     Note  It is recommended that you set up passwordless SSH on all hosts in the cluster. Refer to the documentation for more details on  installation  and  cluster configuration .", 
            "title": "Start SnappyData Cluster on Multiple Hosts"
        }, 
        {
            "location": "/howto/#how-to-run-spark-code-inside-the-cluster", 
            "text": "A Spark program that runs inside a SnappyData cluster is implemented as a SnappyData job.  Implementing a Job : \nA SnappyData job is a class or object that implements SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. In the  runSnappyJob  method of the job, you implement the logic for your Spark program using SnappySession object instance passed to it. You can perform all operations such as create table, load data, execute queries using the SnappySession.  \nAny of the Spark APIs can be invoked by a SnappyJob.  class CreatePartitionedRowTable extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappySession, jobConfig: Config): Any\n\n  /**\n  SnappyData calls this function to validate the job input and reject invalid job requests.\n  You can implement custom validations here, for example, validating the configuration parameters\n  **/\n  def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation\n}  Dependencies :\nTo compile your job, use the Maven/SBT dependencies for the latest released version of SnappyData.  Example: Maven dependency :  !-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --  dependency \n     groupId io.snappydata /groupId \n     artifactId snappydata-cluster_2.11 /artifactId \n     version 0.9 /version  /dependency   Example: SBT dependency :  // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies +=  io.snappydata  %  snappydata-cluster_2.11  %  0.9   Running the Job : \nOnce you create a jar file for SnappyData job, use  bin/snappy-job.sh  to submit the job to SnappyData cluster and run the job. This is similar to Spark-submit for any Spark application.   For example, to run the job implemented in  CreatePartitionedRowTable.scala  you can use the following command.\nHere  quickstart.jar  contains the program and is bundled in the product distribution. \nFor example, the command submits the job and runs it as:   # first cd to your SnappyData product dir\n $ cd $SNAPPY_HOME\n $ bin/snappy-job.sh submit\n    --app-name CreatePartitionedRowTable\n    --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable\n    --app-jar examples/jars/quickstart.jar\n    --lead hostNameOfLead:8090  In the above comand,  --lead  option specifies the host name of the lead node along with the port on which it accepts jobs (default 8090).  Output : It returns output similar to:  {\n   status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  }\n}  Check Status : You can check the status of the job using the Job ID listed above:  bin/snappy-job.sh status --lead hostNameOfLead:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48  Refer to the  Building SnappyData applications using Spark API  section of the documentation for more details.", 
            "title": "How to Run Spark Code inside the Cluster"
        }, 
        {
            "location": "/howto/#how-to-access-snappydata-store-from-an-existing-spark-installation-using-smart-connector", 
            "text": "SnappyData comes with a Smart Connector that enables Spark applications to work with the SnappyData cluster, from any compatible Spark cluster (you can use any distribution that is compatible with Apache Spark 2.0.x). The Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. This is no different than how Spark applications today work with stores like Cassandra, Redis, etc.  For more information on the various modes, refer to the  SnappyData Smart Connector  section of the documentation.  Code Example: \nThe code example for this mode is in  SmartConnectorExample.scala  Configure a SnappySession :   The code below shows how to initialize a SparkSession. Here the property  snappydata.connection  instructs the connector to acquire cluster connectivity and catalog metadata and registers it locally in the Spark cluster. Its value is consists of  locator host and JDBC client port on which the locator listens for connections (default 1527).      val spark: SparkSession = SparkSession\n        .builder\n        .appName( SmartConnectorExample )\n        // It can be any master URL\n        .master( local[4] )\n         // snappydata.connection property enables the application to interact with SnappyData store\n        .config( snappydata.connection ,  localhost:1527 )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  Create Table and Run Queries : \nYou can now create tables and run queries in SnappyData store using your Apache Spark program      // reading an already created SnappyStore table SNAPPY_COL_TABLE\n    val colTable = snSession.table( SNAPPY_COL_TABLE )\n    colTable.show(10)\n\n    snSession.dropTable( TestColumnTable , ifExists = true)\n\n    // Creating a table from a DataFrame\n    val dataFrame = snSession.range(1000).selectExpr( id ,  floor(rand() * 10000) as k )\n\n    snSession.sql( create table TestColumnTable (id bigint not null, k bigint not null) using column )\n\n    // insert data in TestColumnTable\n    dataFrame.write.insertInto( TestColumnTable )", 
            "title": "How to Access SnappyData store from an Existing Spark Installation using Smart Connector"
        }, 
        {
            "location": "/howto/#how-to-use-snappy-sql-shell-snappy-sql", 
            "text": "snappy-sql  can be used to execute SQL on SnappyData cluster. In the background,  snappy-sql  uses JDBC connections to execute SQL.  Connect to a SnappyData Cluster : \nUse the  snappy-sql  and  connect client  command on the Snappy SQL Shell  $ bin/snappy-sql\nsnappy  connect client ' locatorHostName :1527';  Where  locatorHostName  is the host name of the node on which the locator is started and  1527  is the default port on which the locator listens for connections.   Execute SQL queries : Once connected you can execute SQL queries using  snappy-sql  snappy  CREATE TABLE APP.PARTSUPP (PS_PARTKEY INTEGER NOT NULL PRIMARY KEY, PS_SUPPKEY INTEGER NOT NULL, PS_AVAILQTY INTEGER NOT NULL, PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL) USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') ;\n\nsnappy  INSERT INTO APP.PARTSUPP VALUES(100, 1, 5000, 100);\n\nsnappy  INSERT INTO APP.PARTSUPP VALUES(200, 2, 50, 10);\n\nsnappy  SELECT * FROM APP.PARTSUPP;\nPS_PARTKEY |PS_SUPPKEY |PS_AVAILQTY|PS_SUPPLYCOST    \n-----------------------------------------------------\n100        |1          |5000       |100.00           \n200        |2          |50         |10.00            \n\n2 rows selected  View the members of cluster : \nUse the  show members  command  snappy  show members;\nID                            |HOST                          |KIND                          |STATUS              |NETSERVERS                    |SERVERGROUPS                  \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n192.168.63.1(21412) v1 :61964 |192.168.63.1                  |datastore(normal)             |RUNNING             |localhost/127.0.0.1[1528]     |                              \n192.168.63.1(21594) v2 :29474 |192.168.63.1                  |accessor(normal)              |RUNNING             |                              |IMPLICIT_LEADER_SERVERGROUP   \nlocalhost(21262) v0 :22770    |localhost                     |locator(normal)               |RUNNING             |localhost/127.0.0.1[1527]     |                              \n\n3 rows selected  View the list tables in a schema : \nUse  show tables in  schema  command  snappy  show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE|REMARKS             \n-----------------------------------------------------------------------------------\nAPP                 |PARTSUPP                      |TABLE     |                    \n\n1 row selected", 
            "title": "How to Use Snappy SQL shell (snappy-sql)"
        }, 
        {
            "location": "/howto/#how-to-create-row-tables-and-run-queries", 
            "text": "Each record in a Row table is managed in contiguous memory, and therefore, optimized for selective queries (For example. key based point lookup ) or updates. \nA row table can either be replicated to all nodes or partitioned across nodes. It can be created by using DataFrame API or using SQL.  Refer to the  Row and column tables  documentation for complete list of attributes for row tables.  Full source code, for example, to create and perform operations on replicated and partitioned row table can be found in  CreateReplicatedRowTable.scala  and  CreatePartitionedRowTable.scala", 
            "title": "How to Create Row Tables and Run Queries"
        }, 
        {
            "location": "/howto/#create-a-row-table-using-dataframe-api", 
            "text": "The code snippet below shows how to create a replicated row table using API.  Get a SnappySession      val spark: SparkSession = SparkSession\n        .builder\n        .appName( CreateReplicatedRowTable )\n        .master( local[*] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  Create the Table using API :\nFirst, define the table schema and then create the table using createTable API      val schema = StructType(Array(StructField( S_SUPPKEY , IntegerType, false),\n      StructField( S_NAME , StringType, false),\n      StructField( S_ADDRESS , StringType, false),\n      StructField( S_NATIONKEY , IntegerType, false),\n      StructField( S_PHONE , StringType, false),\n      StructField( S_ACCTBAL , DecimalType(15, 2), false),\n      StructField( S_COMMENT , StringType, false)\n    ))\n\n    // props1 map specifies the properties for the table to be created\n    //  PERSISTENCE  flag indicates that the table data should be persisted to\n    // disk asynchronously\n    val props1 = Map( PERSISTENCE  -   asynchronous )\n    // create a row table using createTable API\n    snSession.createTable( SUPPLIER ,  row , schema, props1)  Creating a Row table using SQL :\nThe same table can be created using SQL as shown below:      // First drop the table if it exists\n    snSession.sql( DROP TABLE IF EXISTS SUPPLIER )\n    // Create a row table using SQL\n    //  PERSISTENCE  that the table data should be persisted to disk asynchronously\n    // For complete list of attributes refer the documentation\n    snSession.sql(\n       CREATE TABLE SUPPLIER (   +\n           S_SUPPKEY INTEGER NOT NULL PRIMARY KEY,   +\n           S_NAME STRING NOT NULL,   +\n           S_ADDRESS STRING NOT NULL,   +\n           S_NATIONKEY INTEGER NOT NULL,   +\n           S_PHONE STRING NOT NULL,   +\n           S_ACCTBAL DECIMAL(15, 2) NOT NULL,   +\n           S_COMMENT STRING NOT NULL   +\n           ) USING ROW OPTIONS (PERSISTENCE 'asynchronous') )  You can perform various operations such as inset data, mutate it (update/delete), select data from the table. All these operations can be done either through APIs or by using SQL queries.\nFor example:  To insert data in the SUPPLIER table:        snSession.sql( INSERT INTO SUPPLIER VALUES(1, 'SUPPLIER1', 'CHICAGO, IL', 0, '555-543-789', 10000, ' ') )\n    snSession.sql( INSERT INTO SUPPLIER VALUES(2, 'SUPPLIER2', 'BOSTON, MA', 0, '555-234-489', 20000, ' ') )\n    snSession.sql( INSERT INTO SUPPLIER VALUES(3, 'SUPPLIER3', 'NEWYORK, NY', 0, '555-743-785', 34000, ' ') )\n    snSession.sql( INSERT INTO SUPPLIER VALUES(4, 'SUPPLIER4', 'SANHOSE, CA', 0, '555-321-098', 1000, ' ') )  To print the contents of the SUPPLIER table:        var tableData = snSession.sql( SELECT * FROM SUPPLIER ).collect()\n    tableData.foreach(println)  To update the table account balance for SUPPLIER4:        snSession.sql( UPDATE SUPPLIER SET S_ACCTBAL = 50000 WHERE S_NAME = 'SUPPLIER4' )  To print contents of the SUPPLIER table after update        tableData = snSession.sql( SELECT * FROM SUPPLIER ).collect()\n    tableData.foreach(println)  To delete the records for SUPPLIER2 and SUPPLIER3        snSession.sql( DELETE FROM SUPPLIER WHERE S_NAME = 'SUPPLIER2' OR S_NAME = 'SUPPLIER3' )  To print the contents of the SUPPLIER table after delete      tableData = snSession.sql( SELECT * FROM SUPPLIER ).collect()\n    tableData.foreach(println)", 
            "title": "Create a Row Table using DataFrame API:"
        }, 
        {
            "location": "/howto/#how-to-create-column-tables-and-run-queries", 
            "text": "Column tables organize and manage data in columnar form such that modern day CPUs can traverse and run computations like a sum or an average fast (as the values are available in contiguous memory).  Refer to the  Row and column tables  documentation for the complete list of attributes for column tables.  Full source code, for example, to create and perform operations on column table can be found in  CreateColumnTable.scala", 
            "title": "How to Create Column Tables and Run Queries"
        }, 
        {
            "location": "/howto/#create-a-column-table-using-dataframe-api", 
            "text": "The code snippet below shows how to create a column table using DataFrame API.  Get a SnappySession :      val spark: SparkSession = SparkSession\n        .builder\n        .appName( CreateColumnTable )\n        .master( local[*] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  Define the table schema  val tableSchema = StructType(Array(StructField( C_CUSTKEY , IntegerType, false),\n      StructField( C_NAME , StringType, false),\n      StructField( C_ADDRESS , StringType, false),\n      StructField( C_NATIONKEY , IntegerType, false),\n      StructField( C_PHONE , StringType, false),\n      StructField( C_ACCTBAL , DecimalType(15, 2), false),\n      StructField( C_MKTSEGMENT , StringType, false),\n      StructField( C_COMMENT , StringType, false)\n    ))  Create the table and load data from CSV      // props1 map specifies the properties for the table to be created\n    //  PARTITION_BY  attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\n    val props1 = Map( PARTITION_BY  -   C_CUSTKEY )\n    snSession.createTable( CUSTOMER ,  column , tableSchema, props1)\n\n     val tableSchema = snSession.table( CUSTOMER ).schema\n    // insert some data in it\n    // loading data in CUSTOMER table from a text file with delimited columns\n    val customerDF = snSession.read.schema(schema = tableSchema).csv( quickstart/src/main/resources/customer.csv )\n    customerDF.write.insertInto( CUSTOMER )", 
            "title": "Create a Column Table using DataFrame API"
        }, 
        {
            "location": "/howto/#create-a-column-table-using-sql", 
            "text": "The same table can be created using SQL as shown below:      snSession.sql( CREATE TABLE CUSTOMER (   +\n         C_CUSTKEY     INTEGER NOT NULL,  +\n         C_NAME        VARCHAR(25) NOT NULL,  +\n         C_ADDRESS     VARCHAR(40) NOT NULL,  +\n         C_NATIONKEY   INTEGER NOT NULL,  +\n         C_PHONE       VARCHAR(15) NOT NULL,  +\n         C_ACCTBAL     DECIMAL(15,2)   NOT NULL,  +\n         C_MKTSEGMENT  VARCHAR(10) NOT NULL,  +\n         C_COMMENT     VARCHAR(117) NOT NULL)  +\n         USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )  You can execute selected queries on a column table, join the column table with other tables, and append data to it.", 
            "title": "Create a Column Table using SQL"
        }, 
        {
            "location": "/howto/#how-to-load-data-in-snappydata-tables", 
            "text": "You can use Spark's DataFrameReader API in order to load data into SnappyData tables using different formats (Parquet, CSV, JSON etc.).  Code Example  Get a SnappySession      val spark: SparkSession = SparkSession\n        .builder\n        .appName( CreateColumnTable )\n        .master( local[*] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  A column table is created as follows      snSession.sql( CREATE TABLE CUSTOMER (   +\n         C_CUSTKEY     INTEGER NOT NULL,  +\n         C_NAME        VARCHAR(25) NOT NULL,  +\n         C_ADDRESS     VARCHAR(40) NOT NULL,  +\n         C_NATIONKEY   INTEGER NOT NULL,  +\n         C_PHONE       VARCHAR(15) NOT NULL,  +\n         C_ACCTBAL     DECIMAL(15,2)   NOT NULL,  +\n         C_MKTSEGMENT  VARCHAR(10) NOT NULL,  +\n         C_COMMENT     VARCHAR(117) NOT NULL)  +\n         USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )  Load data in the CUSTOMER table from a CSV file by using DataFrameReader API      val tableSchema = snSession.table( CUSTOMER ).schema\n    val customerDF = snSession.read.schema(schema = tableSchema).csv(s $dataFolder/customer.csv )\n    customerDF.write.insertInto( CUSTOMER )  For Parquet file, use code as given below  val customerDF = snSession.read.parquet(s $dataDir/customer_parquet )\ncustomerDF.write.insertInto( CUSTOMER )  Inferring schema from data file  A schema for the table can be inferred from the data file. In this case, you do not need to create a table before loading the data. In the code snippet below, first DataFrame for a Parquet file is created and then saveAsTable API is used to create a table with data loaded from it.      val customerDF = snSession.read.parquet(s quickstart/src/main/resources/customerparquet )\n    // props1 map specifies the properties for the table to be created\n    //  PARTITION_BY  attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\n    val props1 = Map( PARTITION_BY  -   C_CUSTKEY )\n    customerDF.write.format( column ).mode( append ).options(props1).saveAsTable( CUSTOMER )  In the code snippet below a schema is inferred from a CSV file. Column names are derived from the header in the file.      val customer_csv_DF = snSession.read.option( header ,  true )\n        .option( inferSchema ,  true ).csv( quickstart/src/main/resources/customer_with_headers.csv )\n\n    // props1 map specifies the properties for the table to be created\n    //  PARTITION_BY  attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY),\n    // For complete list of attributes refer the documentation\n    val props1 = Map( PARTITION_BY  -   C_CUSTKEY )\n    customer_csv_DF.write.format( column ).mode( append ).options(props1).saveAsTable( CUSTOMER )  The source code to load the data from a CSV/Parquet files is in  CreateColumnTable.scala . Source for the code to load data from a JSON file can be found in  WorkingWithJson.scala", 
            "title": "How to Load Data in SnappyData Tables"
        }, 
        {
            "location": "/howto/#how-to-perform-a-collocated-join", 
            "text": "When two tables are partitioned on columns and collocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData server. Collocating the data of two tables based on a partitioning column's value is a best practice if you frequently perform queries on those tables that join on that column.\nWhen collocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data.  Code Example: ORDERS table is collocated with CUSTOMER table  A partitioned table can be collocated with another partitioned table by using the \"COLOCATE_WITH\" attribute in the table options.  \nFor example, in the code snippet below, the ORDERS table is collocated with the CUSTOMER table. The complete source for this example can be found in the file  CollocatedJoinExample.scala  Get a SnappySession :      val spark: SparkSession = SparkSession\n        .builder\n        .appName( CollocatedJoinExample )\n        .master( local[*] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  Create Table Customer:      snSession.sql( CREATE TABLE CUSTOMER (   +\n         C_CUSTKEY     INTEGER NOT NULL,  +\n         C_NAME        VARCHAR(25) NOT NULL,  +\n         C_ADDRESS     VARCHAR(40) NOT NULL,  +\n         C_NATIONKEY   INTEGER NOT NULL,  +\n         C_PHONE       VARCHAR(15) NOT NULL,  +\n         C_ACCTBAL     DECIMAL(15,2)   NOT NULL,  +\n         C_MKTSEGMENT  VARCHAR(10) NOT NULL,  +\n         C_COMMENT     VARCHAR(117) NOT NULL)  +\n         USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )  Create Table Orders:      snSession.sql( CREATE TABLE ORDERS  (   +\n         O_ORDERKEY       INTEGER NOT NULL,  +\n         O_CUSTKEY        INTEGER NOT NULL,  +\n         O_ORDERSTATUS    CHAR(1) NOT NULL,  +\n         O_TOTALPRICE     DECIMAL(15,2) NOT NULL,  +\n         O_ORDERDATE      DATE NOT NULL,  +\n         O_ORDERPRIORITY  CHAR(15) NOT NULL,  +\n         O_CLERK          CHAR(15) NOT NULL,  +\n         O_SHIPPRIORITY   INTEGER NOT NULL,  +\n         O_COMMENT        VARCHAR(79) NOT NULL)   +\n         USING COLUMN OPTIONS (PARTITION_BY 'O_CUSTKEY',   +\n         COLOCATE_WITH 'CUSTOMER' ) )  Perform a Collocate join:        // Selecting orders for all customers\n    val result = snSession.sql( SELECT C_CUSTKEY, C_NAME, O_ORDERKEY, O_ORDERSTATUS, O_ORDERDATE,   +\n         O_TOTALPRICE FROM CUSTOMER, ORDERS WHERE C_CUSTKEY = O_CUSTKEY ).collect()", 
            "title": "How to Perform a Collocated Join"
        }, 
        {
            "location": "/howto/#how-to-connect-using-jdbc-driver", 
            "text": "You can connect to and execute queries against SnappyData cluster using JDBC driver. The connection URL typically points to one of the locators. The locator passes the information of all available servers based on which, the driver automatically connects to one of the servers.  To connect to the SnappyData cluster : Using JDBC, use URL of the form  jdbc:snappydata:// locatorHostName : locatorClientPort /  Where the  locatorHostName  is the hostname of the node on which the locator is started and  locatorClientPort  is the port on which the locator accepts client connections (default 1527).  Code Example:  Connect to a SnappyData cluster using JDBC on default client port  The code snippet shows how to connect to a SnappyData cluster using JDBC on default client port 1527. The complete source code of the example is located at  JDBCExample.scala  val url: String = s jdbc:snappydata://localhost:1527/ \nval conn1 = DriverManager.getConnection(url)\n\nval stmt1 = conn1.createStatement()\n// Creating a table (PARTSUPP) using JDBC connection\nstmt1.execute( DROP TABLE IF EXISTS APP.PARTSUPP )\nstmt1.execute( CREATE TABLE APP.PARTSUPP (   +\n      PS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,  +\n      PS_SUPPKEY     INTEGER NOT NULL,  +\n      PS_AVAILQTY    INTEGER NOT NULL,  +\n      PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)  +\n     USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') )\n\n// Inserting records in PARTSUPP table via batch inserts\nval preparedStmt1 = conn1.prepareStatement( INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?) )\n\nvar x = 0\nfor (x  - 1 to 10) {\n  preparedStmt1.setInt(1, x*100)\n  preparedStmt1.setInt(2, x)\n  preparedStmt1.setInt(3, x*1000)\n  preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2))\n  preparedStmt1.addBatch()\n}\npreparedStmt1.executeBatch()\npreparedStmt1.close()   Note  If the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the  io.snappydata.jdbc.ClientDriver  class.", 
            "title": "How to Connect using JDBC Driver"
        }, 
        {
            "location": "/howto/#how-to-store-and-query-json-objects", 
            "text": "You can insert JSON data in SnappyData tables and execute queries on the tables.  Code Example: Loads JSON data from a JSON file into a column table and executes query  The code snippet loads JSON data from a JSON file into a column table and executes the query against it.\nThe source code for JSON example is located at  WorkingWithJson.scala . After creating SnappySession, the JSON file is read using Spark API and loaded into a SnappyData table.  Get a SnappySession :      val spark: SparkSession = SparkSession\n        .builder\n        .appName( WorkingWithJson )\n        .master( local[*] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  Create a DataFrame from the JSON file :      val some_people_path = s quickstart/src/main/resources/some_people.json \n    // Read a JSON file using Spark API\n    val people = snSession.read.json(some_people_path)\n    people.printSchema()  Create a SnappyData table and insert the JSON data in it using the DataFrame :      //Drop the table if it exists\n    snSession.dropTable( people , ifExists = true)\n\n   //Create a columnar table with the Json DataFrame schema\n    snSession.createTable(tableName =  people ,\n      provider =  column ,\n      schema = people.schema,\n      options = Map.empty[String,String],\n      allowExisting = false)\n\n    // Write the created DataFrame to the columnar table\n    people.write.insertInto( people )  Append more data from a second JSON file :      // Append more people to the column table\n    val more_people_path = s quickstart/src/main/resources/more_people.json \n\n    //Explicitly passing schema to handle record level field mismatch\n    // e.g. some records have  district  field while some do not.\n    val morePeople = snSession.read.schema(people.schema).json(more_people_path)\n    morePeople.write.insertInto( people )\n\n    //print schema of the table\n    println( Print Schema of the table\\n################ )\n    println(snSession.table( people ).schema)  Execute queries and return the results      // Query it like any other table\n    val nameAndAddress = snSession.sql( SELECT   +\n         name,   +\n         address.city,   +\n         address.state,   +\n         address.district,   +\n         address.lane   +\n         FROM people )\n\nnameAndAddress.toJSON.show()", 
            "title": "How to Store and Query JSON Objects"
        }, 
        {
            "location": "/howto/#how-to-store-and-query-objects", 
            "text": "You can use domain object to load data into SnappyData tables and select the data by executing queries against the table.  Code Example: Insert Person object into the column table  The code snippet below inserts Person objects into a column table. The source code for this example is located at  WorkingWithObjects.scala . After creating SnappySession, the Person objects is inserted using Spark API and loads into a SnappyData table.  Get a SnappySession :      val spark: SparkSession = SparkSession\n        .builder\n        .appName( CreateReplicatedRowTable )\n        .master( local[4] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  Create DataFrame objects :      //Import the implicits for automatic conversion between Objects to DataSets.\n    import snSession.implicits._\n\n    // Create a Dataset using Spark APIs\n    val people = Seq(Person( Tom , Address( Columbus ,  Ohio ), Map( frnd1 -   8998797979 ,  frnd2  -   09878786886 ))\n      , Person( Ned , Address( San Diego ,  California ), Map.empty[String,String])).toDS()  Create a SnappyData table and insert data into it :      //Drop the table if it exists.\n    snSession.dropTable( Persons , ifExists = true)\n\n    //Create a columnar table with a Struct to store Address\n    snSession.sql( CREATE table Persons(name String, address Struct city: String, state:String ,   +\n          emergencyContacts Map String,String ) using column options() )\n\n    // Write the created DataFrame to the columnar table.\n    people.write.insertInto( Persons )\n\n    //print schema of the table\n    println( Print Schema of the table\\n################ )\n    println(snSession.table( Persons ).schema)\n\n\n    // Append more people to the column table\n    val morePeople = Seq(Person( Jon Snow , Address( Columbus ,  Ohio ), Map.empty[String,String]),\n      Person( Rob Stark , Address( San Diego ,  California ), Map.empty[String,String]),\n      Person( Michael , Address( Null ,  California ), Map.empty[String,String])).toDS()\n\n    morePeople.write.insertInto( Persons )  Execute query on the table and return results :      // Query it like any other table\n    val nameAndAddress = snSession.sql( SELECT name, address, emergencyContacts FROM Persons )\n\n    //Reconstruct the objects from obtained Row\n    val allPersons = nameAndAddress.as[Person]\n    //allPersons is a Spark Dataset of Person objects. \n    // Use of the Dataset APIs to transform, query this data set.", 
            "title": "How to Store and Query Objects"
        }, 
        {
            "location": "/howto/#how-to-use-stream-processing-with-snappydata", 
            "text": "SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and to integrate with the built-in store. In SnappyData, you can define streams declaratively from any SQL client, register continuous queries on streams, mutate SnappyData tables based on the streaming data. For more information on streaming, refer to the  documentation .  Code Example : \nCode example for streaming is in  StreamingExample.scala . The code snippets below shows how to declare a stream table, register continuous queries(CQ) and update SnappyData table using the stream data.  First get a SnappySession and a SnappyStreamingContext : \nHere SnappyStreamingContext is initialized in a batch duration of one second.      val spark: SparkSession = SparkSession\n        .builder\n        .appName(getClass.getSimpleName)\n        .master( local[*] )\n        .getOrCreate\n\n    val snsc = new SnappyStreamingContext(spark.sparkContext, Seconds(1))  The example starts an embedded Kafka instance on which a few messages are published. SnappyData processes these message and updates a table based on the stream data.  The SQL below shows how to declare a stream table using SQL. The rowConverter attribute specifies a class used to return Row objects from the received stream messages.      snsc.sql(\n       create stream table adImpressionStream (  +\n            time_stamp timestamp,  +\n            publisher string,  +\n            advertiser string,  +\n            website string,  +\n            geo string,  +\n            bid double,  +\n            cookie string)   +   using directkafka_stream options(  +\n            rowConverter 'org.apache.spark.examples.snappydata.RowsConverter',  +\n          s  kafkaParams 'metadata.broker.list- $address;auto.offset.reset- smallest',  +\n          s  topics 'kafka_topic') \n    )  RowsConverter decodes a stream message consisting of comma separated fields and forms a Row object from it.  class RowsConverter extends StreamToRowsConverter with Serializable {\n  override def toRows(message: Any): Seq[Row] = {\n    val log = message.asInstanceOf[String]\n    val fields = log.split( , )\n    val rows = Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong),\n      fields(1),\n      fields(2),\n      fields(3),\n      fields(4),\n      fields(5).toDouble,\n      fields(6)\n    )))\n    rows\n  }\n}  To create a row table that is updated based on the streaming data :  snsc.sql( create table publisher_bid_counts(publisher string, bidCount int) using row )  To declare a continuous query that is executed on the streaming data : This query returns a number of bids per publisher in one batch.      val resultStream: SchemaDStream = snsc.registerCQ( select publisher, count(bid) as bidCount from   +\n         adImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher )  To process that the result of above continuous query to update the row table publisher_bid_counts :      // this conf is used to get a JDBC connection\n    val conf = new ConnectionConfBuilder(snsc.snappySession).build()\n\n    resultStream.foreachDataFrame(df =  {\n        println( Data received in streaming window )\n        df.show()\n\n        println( Updating table publisher_bid_counts )\n        val conn = ConnectionUtil.getConnection(conf)\n        val result = df.collect()\n        val stmt = conn.prepareStatement( update publisher_bid_counts set   +\n            s bidCount = bidCount + ? where publisher = ? )\n\n        result.foreach(row =  {\n          val publisher = row.getString(0)\n          val bidCount = row.getLong(1)\n          stmt.setLong(1, bidCount)\n          stmt.setString(2, publisher)\n          stmt.addBatch()\n        }\n        )\n        stmt.executeBatch()\n        conn.close()\n      }\n    })  To display the total bids by each publisher by querying publisher_bid_counts table :  snsc.snappySession.sql( select publisher, bidCount from publisher_bid_counts ).show()", 
            "title": "How to Use Stream Processing with SnappyData"
        }, 
        {
            "location": "/howto/#how-to-use-synopsis-data-engine-to-run-approximate-queries", 
            "text": "Synopsis Data Engine (SDE) uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire Dataset. The approach trades off query accuracy for fast response time.\nFor more information on  SDE, refer to  SDE documentation .  Code Example :\nThe complete code example for SDE is in  SynopsisDataExample.scala . The code below creates a sample table and executes queries that run on the sample table.  Get a SnappySession :      val spark: SparkSession = SparkSession\n        .builder\n        .appName( SynopsisDataExample )\n        .master( local[*] )\n        .getOrCreate\n\n    val snSession = new SnappySession(spark.sparkContext)  The base column table(AIRLINE) is created from temporary parquet table as follows :      // Create temporary staging table to load parquet data\n    snSession.sql( CREATE EXTERNAL TABLE STAGING_AIRLINE   +\n         USING parquet OPTIONS(path   + s '${dataFolder}/airlineParquetData') )\n\n    // Create a column table AIRLINE\n    snSession.sql( CREATE TABLE AIRLINE USING column AS (SELECT Year AS Year_,   +\n         Month AS Month_ , DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime,   +\n         CRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime,   +\n         CRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance,   +\n         TaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay,   +\n         WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay,   +\n         ArrDelaySlot FROM STAGING_AIRLINE) )  Create a sample table for the above base table :\nAttribute 'qcs' in the statement below specifies the columns used for stratification and attribute 'fraction' specifies how big the sample needs to be (3% of the base table AIRLINE in this case). For more information on Synopsis Data Engine, refer to the  SDE documentation .  \n    snSession.sql( CREATE SAMPLE TABLE AIRLINE_SAMPLE ON AIRLINE OPTIONS  +\n         (qcs 'UniqueCarrier, Year_, Month_', fraction '0.03')    +\n         AS (SELECT Year_, Month_ , DayOfMonth,   +\n         DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier,   +\n         FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime,   +\n         ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut,   +\n         Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay,   +\n         NASDelay, SecurityDelay, LateAircraftDelay, ArrDelaySlot FROM AIRLINE) )  Execute queries that return approximate results using sample tables :\nThe query below returns airlines by number of flights in descending order. The 'with error 0.20' clause in the query below signals query engine to execute the query on the sample table instead of the base table and maximum 20% error is allowed.      var result = snSession.sql( select  count(*) flightRecCount, description AirlineName,   +\n         UniqueCarrier carrierCode ,Year_ from airline , airlineref where   +\n         airline.UniqueCarrier = airlineref.code group by   +\n         UniqueCarrier,description, Year_ order by flightRecCount desc limit   +\n         10 with error 0.20 ).collect()\n    result.foreach(r =  println(r(0) +  ,   + r(1) +  ,   + r(2) +  ,   + r(3)))  Join the sample table with a reference table :\nYou can join the sample table with a reference table to execute queries. For example a reference table (AIRLINEREF) is created as follows from a parquet data file.      // create temporary staging table to load parquet data\n    snSession.sql( CREATE EXTERNAL TABLE STAGING_AIRLINEREF USING   +\n         parquet OPTIONS(path   + s '${dataFolder}/airportcodeParquetData') )\n    snSession.sql( CREATE TABLE AIRLINEREF USING row AS (SELECT CODE,   +\n         DESCRIPTION FROM STAGING_AIRLINEREF) )  Join the sample table and reference table to find out which airlines arrive on schedule :      result = snSession.sql( select AVG(ArrDelay) arrivalDelay,   +\n         relative_error(arrivalDelay) rel_err, description AirlineName,   +\n         UniqueCarrier carrier from airline, airlineref   +\n         where airline.UniqueCarrier = airlineref.Code   +\n         group by UniqueCarrier, description order by arrivalDelay   +\n         with error ).collect()\n\n   result.foreach(r =  println(r(0) +  ,   + r(1) +  ,   + r(2) +  ,   + r(3)))", 
            "title": "How to Use Synopsis Data Engine to Run Approximate Queries"
        }, 
        {
            "location": "/howto/#how-to-use-python-to-create-tables-and-run-queries", 
            "text": "Developers can write programs in Python to use SnappyData features.  First create a SnappySession :   from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)  Create table using SnappySession :      # Creating partitioned table PARTSUPP using SQL\n    snappy.sql( DROP TABLE IF EXISTS PARTSUPP )\n    #  PARTITION_BY  attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY),\n    # For complete list of table attributes refer the documentation\n    # http://snappydatainc.github.io/snappydata/programming_guide\n    snappy.sql( CREATE TABLE PARTSUPP (   +\n                   PS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,  +\n                   PS_SUPPKEY     INTEGER NOT NULL,  +\n                   PS_AVAILQTY    INTEGER NOT NULL,  +\n                   PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)  +\n                   USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY' ) )  Inserting data in table using INSERT query :      snappy.sql( INSERT INTO PARTSUPP VALUES(100, 1, 5000, 100) )\n    snappy.sql( INSERT INTO PARTSUPP VALUES(200, 2, 50, 10) )\n    snappy.sql( INSERT INTO PARTSUPP VALUES(300, 3, 1000, 20) )\n    snappy.sql( INSERT INTO PARTSUPP VALUES(400, 4, 200, 30) )\n    # Printing the contents of the PARTSUPP table\n    snappy.sql( SELECT * FROM PARTSUPP ).show()  Update the data using SQL :      # Update the available quantity for PARTKEY 100\n    snappy.sql( UPDATE PARTSUPP SET PS_AVAILQTY = 50000 WHERE PS_PARTKEY = 100 )\n    # Printing the contents of the PARTSUPP table after update\n    snappy.sql( SELECT * FROM PARTSUPP ).show()  Delete records from the table :      # Delete the records for PARTKEY 400\n    snappy.sql( DELETE FROM PARTSUPP WHERE PS_PARTKEY = 400 )\n    # Printing the contents of the PARTSUPP table after delete\n    snappy.sql( SELECT * FROM PARTSUPP ).show()  Create table using API :\nThis same table can be created by using createTable API. First create a schema and then create the table, and then mutate the table data using API:      # drop the table if it exists\n    snappy.dropTable('PARTSUPP', True)\n\n    schema = StructType([StructField('PS_PARTKEY', IntegerType(), False),\n                     StructField('PS_SUPPKEY', IntegerType(), False),\n                     StructField('PS_AVAILQTY', IntegerType(),False),\n                     StructField('PS_SUPPLYCOST', DecimalType(15, 2), False)\n                     ])\n\n    #  PARTITION_BY  attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY)\n    # For complete list of table attributes refer the documentation at\n    # http://snappydatainc.github.io/snappydata/programming_guide\n    snappy.createTable('PARTSUPP', 'row', schema, False, PARTITION_BY = 'PS_PARTKEY')\n\n    # Inserting data in PARTSUPP table using DataFrame\n    tuples = [(100, 1, 5000, Decimal(100)), (200, 2, 50, Decimal(10)),\n              (300, 3, 1000, Decimal(20)), (400, 4, 200, Decimal(30))]\n    rdd = sc.parallelize(tuples)\n    tuplesDF = snappy.createDataFrame(rdd, schema)\n    tuplesDF.write.insertInto( PARTSUPP )\n    #Printing the contents of the PARTSUPP table\n    snappy.sql( SELECT * FROM PARTSUPP ).show()\n\n    # Update the available quantity for PARTKEY 100\n    snappy.update( PARTSUPP ,  PS_PARTKEY =100 , [50000], [ PS_AVAILQTY ])\n    # Printing the contents of the PARTSUPP table after update\n    snappy.sql( SELECT * FROM PARTSUPP ).show()\n\n    # Delete the records for PARTKEY 400\n    snappy.delete( PARTSUPP ,  PS_PARTKEY =400 )\n    # Printing the contents of the PARTSUPP table after delete\n    snappy.sql( SELECT * FROM PARTSUPP ).show()  The complete source code for the above example is in  CreateTable.py", 
            "title": "How to Use Python to Create Tables and Run Queries"
        }, 
        {
            "location": "/howto/#how-to-connect-using-odbc-driver", 
            "text": "You can connect to SnappyData Cluster using SnappyData ODBC Driver and can execute SQL queries by connecting to any of the servers in the cluster.", 
            "title": "How to Connect using ODBC Driver"
        }, 
        {
            "location": "/howto/#step-1-install-visual-c-redistributable-for-visual-studio-2015", 
            "text": "To download and install the Visual C++ Redistributable for Visual Studio 2015:    Download Visual C++ Redistributable for Visual Studio 2015    Depending on your Windows installation, download the required version of the SnappyData ODBC Driver.    Select  Run  to start the installation, and follow the steps to complete the installation.", 
            "title": "Step 1: Install Visual C++ Redistributable for Visual Studio 2015"
        }, 
        {
            "location": "/howto/#step-2-install-snappydata-odbc-driver", 
            "text": "To download and install the ODBC driver:    Download the SnappyData ODBC Driver from the  SnappyData Release page .    Depending on your Windows installation, download the 32-bit or 64-bit version of the SnappyData ODBC Driver.    32-bit for 32-bit platform    32-bit for 64-bit platform      64-bit for 64-bit platform        Extract the contents of the downloaded file.    Double-click on the  SnappyDataODBCDriverInstaller.msi  file, and follow the steps to complete the installation.   Note  Ensure that  SnappyData version 0.8 or later is installed  and the  SnappyData cluster is running .", 
            "title": "Step 2: Install SnappyData ODBC Driver"
        }, 
        {
            "location": "/howto/#connect-to-the-snappydata-cluster", 
            "text": "Once you have installed SnappyData ODBC Driver, you can connect to SnappyData cluster in any of the following ways:    Use the SnappyData Driver Connection URL:  Driver=SnappyData ODBC Driver;server= ServerHost ;port= ServerPort ;user= userName ;password= password     Create a SnappyData DSN (Data Source Name) using the installed SnappyData ODBC Driver.  \n Please refer to the Windows documentation relevant to your operating system for more information on creating a DSN.  When prompted, select the SnappyData ODBC Driver from the driver's list and enter a Data Source name, SnappyData Server Host, Port, User Name and Password.     Refer to the documentation for detailed information on  Setting Up SnappyData ODBC Driver and Tableau Desktop .", 
            "title": "Connect to the SnappyData cluster"
        }, 
        {
            "location": "/howto/#how-to-connect-to-the-cluster-from-external-clients", 
            "text": "You can also connect to the SnappyData cluster from a different network as client (DbVisualizer, SQuirreL SQL etc.).  For example, you can connect to the cluster on AWS when connecting as a client from your local machine.  When  starting the locator and server  set the following properties in the  conf/locators  and  conf/servers  files:    -hostname-for-clients : The public IP address of the locator or server.     -client-bind-address : IP address of the locator or server.  For example, add  -hostname-for-clients=192.168.20.208       Note  By default, the locator or server binds to localhost. If the IP address is not set, the connection may fail.     Port Settings : The client, by default, connects to the locator or server at the default port 1527. Ensure that this port is open in your firewall settings.   You can also change the default port by setting the  -client-port  property.     Note    If the above properties are not set, when a client tries to connect to the cluster from a different network, the connection may fail and an error may be reported.     For ODBC clients, you must use the host and port details of the server and not the locator.", 
            "title": "How to Connect to the Cluster from External Clients"
        }, 
        {
            "location": "/howto/#how-to-use-apache-zeppelin-with-snappydata", 
            "text": "", 
            "title": "How to Use Apache Zeppelin with SnappyData"
        }, 
        {
            "location": "/howto/#step-1-download-install-and-configure-snappydata", 
            "text": "Download and Install SnappyData   \n The table below lists the version of the SnappyData Zeppelin Interpreter and Apache Zeppelin Installer for the supported SnappyData Release.     SnappyData Zeppelin Interpreter  Apache Zeppelin Binary Package  SnappyData Release      Version 0.6.1  Version 0.6  Release 0.7     Release 0.8  and  future realeases    Version 0.7.1  Version 0.7  Release 0.8   and future releases       Configure the SnappyData Cluster .    In  lead node configuration  set the following properties:    Enable the SnappyData Zeppelin interpreter by adding  -zeppelin.interpreter.enable=true      In the classpath option, define the location where the SnappyData Interpreter is downloaded by adding  -classpath=/ download_location /snappydata-zeppelin- version_number .jar .      Start the SnappyData cluster    Extract the contents of the Zeppelin binary package.       Install the SnappyData Zeppelin interpreter in Apache Zeppelin by executing the following command from Zeppelin's bin directory:  \n     ./install-interpreter.sh --name snappydata --artifact io.snappydata:snappydata-zeppelin: snappydata_interpreter_version_number .  \n    Zeppelin interpreter allows the SnappyData interpreter to be plugged into Zeppelin using which, you can run queries.    Rename the  zeppelin-site.xml.template  file (located in zeppelin- version_number -bin-all/conf directory) to  zeppelin-site.xml .    Edit the  zeppelin-site.xml  file, and in the  zeppelin.interpreters  property, add the following interpreter class names:  org.apache.zeppelin.interpreter.SnappyDataZeppelinInterpreter,org.apache.zeppelin.interpreter.SnappyDataSqlZeppelinInterpreter .    Start the Zeppelin daemon using the command:    bin/zeppelin-daemon.sh start .    To ensure that the installation is successful, log into the Zeppelin UI ( http://localhost:8080 ) from your web browser.", 
            "title": "Step 1: Download, Install and Configure SnappyData"
        }, 
        {
            "location": "/howto/#step-2-configure-snappydata-for-apache-zeppelin", 
            "text": "Log on to Zeppelin from your web browser and select  Interpreter  from the  Settings  option.    Click  Create    to add an interpreter.      From the  Interpreter group  drop-down select  snappydata .\n        Note  If  snappydata  is not displayed in the  Interpreter group  drop-down list, try the following options, and then restart Zeppelin daemon:     Delete the  interpreter.json  file located in the  conf  directory (in the Zeppelin home directory).    Delete the  zeppelin-spark_ version_number .jar  file located in the  interpreter/snappydata  directory (in the Zeppelin home directory).       Click the  Connect to existing process  option. The fields  Host  and  Port  are displayed.    Specify the host on which the SnappyData lead node is executing, and the SnappyData Zeppelin Port (Default is 3768).     Property  Default Values  Description      Host  localhost  Specify host on which the SnappyData lead node is executing    Port  3768  Specify the Zeppelin server port       Configure the interpreter properties.  The table lists the properties required for SnappyData.     Property  Value  Description      default.ur  jdbc:snappydata://localhost:1527/  Specify the JDBC URL for SnappyData cluster in the format  jdbc:snappydata:// locator_hostname :1527    default.driver  com.pivotal.gemfirexd.jdbc.ClientDriver  Specify the JDBC driver for SnappyData    snappydata.connection  localhost:1527  Specify the  host:clientPort  combination of the locator for the JDBC connection    master  local[*]  Specify the URI of the spark master (only local/split mode)    zeppelin.jdbc.concurrent.use  true  Specify the Zeppelin scheduler to be used.  Select  True  for Fair and  False  for FIFO       If required, edit other properties, and then click  Save  to apply your changes.    Bind the interpreter and set SnappyData as the default interpreter.  SnappyData Zeppelin Interpreter group consist of two interpreters. Click and drag  Interpreter_Name  to the top of the list to set it as the default interpreter.      Interpreter Name  Description      %snappydata.snappydata or   %snappydata.spark  This interpreter is used to write Scala code in the paragraph. SnappyContext is injected in this interpreter and can be accessed using variable  snc    %snappydata.sql  This interpreter is used to execute SQL queries on the SnappyData cluster. It also has features of executing approximate queries on the SnappyData cluster.       Click  Save  to apply your changes.     Note  You can modify the default port number of the Zeppelin interpreter by setting the property:  -zeppelin.interpreter.port= port_number  in  lead node configuration .", 
            "title": "Step 2: Configure SnappyData for Apache Zeppelin"
        }, 
        {
            "location": "/howto/#known-issue", 
            "text": "If you are using SnappyData Zeppelin Interpreter 0.7.1 and Zeppelin Installer 0.7 with SnappyData 0.8 or future releases, approximate result does not work on the sample table, when you execute a paragraph with the  %sql show-instant-results-first  directive.", 
            "title": "Known Issue"
        }, 
        {
            "location": "/howto/#more-information", 
            "text": "Refer to these sections for information:    About the Interpreter      Example Notebooks", 
            "title": "More Information"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture Overview\n\n\nThis section presents a high-level overview of SnappyData\u2019s core components. It also explains how our data pipeline (as streams) are ingested into our in-memory store and subsequently interacted with, and analyzed.\n\n\nCore Components\n\n\nThe following depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, standard components, such as security and monitoring have been omitted.\n\n\n\n\nThe storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row-oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. Refer to the \nRow/Column table\n section for details on the syntax and available features. \n\n\nTwo primary programming models are supported by SnappyData \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions, to make the language compatible with the SQL standard. One could perceive SnappyData as an SQL database that uses Spark API as its language for stored procedures. Our \nstream processing\n is primarily through Spark Streaming, but it is integrated and runs within our store.\n\n\nThe OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). All OLTP operations are routed immediately to appropriate data partitions without incurring any scheduling overhead.\n\n\nTo support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, SnappyData uses a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.\n\n\nIn addition to the \u201cexact\u201d Dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Synopsis Data Engine (SDE) and exploits appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.\n\n\nTo understand the data flow architecture, you are first walked through a real-time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.\n\n\nData Ingestion Pipeline\n\n\nThe data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive architecture for real-time applications. The steps to support these tasks are depicted in the following figure and explained below.\n\n\n\n\n\n\n\n\nOnce the SnappyData cluster is started and before any live streams can be processed, ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (for example, HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables.\n\n\n\n\n\n\nSpark Streaming\u2019s parallel receivers are relied on to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.\n\n\n\n\n\n\nNext, SQL is used to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), these data structures are seamlessly combined in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high-velocity streams, SnappyData can still provide answers in real time by resorting to approximation.\n\n\n\n\n\n\nThe stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (for example, maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.\n\n\n\n\n\n\nTo prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.\n\n\n\n\n\n\nOnce ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance with users\u2019 requested accuracy.\n\n\n\n\n\n\nHybrid Cluster Manager\n\n\nSpark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (for example, YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors. This is represented in the following figure.\n\n\n\n\nWhile Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet the following additional requirements as an operational database.\n\n\n\n\n\n\nHigh Concurrency\n: SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.\n\n\n\n\n\n\nState Sharing\n: Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real-time data sharing.\n\n\n\n\n\n\nHigh Availability (HA)\n: As a highly concurrent distributed system that offers low latency access to data, applications must be protected from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations, therefore, become an important requirement for SnappyData.\n\n\n\n\n\n\nConsistency\n: As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture, how SnappyData meets each of these requirements is explained in the subsequent sections.\n\n\n\n\n\n\nSnappyData Cluster Architecture\n\n\nA SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members as represented in the below figure.\n\n\n\n\n\n\nLocator: Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n\n\n\n\n\n\nLead Node: The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n\n\n\n\n\n\nData Servers: A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or pass it to the lead node for execution by Spark SQL.\n\n\n\n\n\n\n\n\nSnappyData also has multiple deployment options. For more information refer to, \nDeployment Options\n.\n\n\nInteracting with SnappyData\n\n\n\n\nNote\n\n\nFor the section on the Spark API, it is assumed that users have some familiarity with \ncore Spark, Spark SQL and Spark Streaming concepts\n.\nAnd, you can try out the Spark \nQuick Start\n. All the commands and programs listed in the Spark guides work in SnappyData as well.\nFor the section on SQL, no Spark knowledge is necessary.\n\n\n\n\nTo interact with SnappyData, interfaces are provided for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self-contained Spark application or can share the state with other jobs using the SnappyData store.\n\n\nUnlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.\n\n\n\n\n\n\nLong running executors\n: Executors are running within the SnappyData store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.\n\n\n\n\n\n\nDriver runs in HA configuration\n: Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shut down, taking down all cached state with it. Instead, SnappyData leverages the \nSpark JobServer\n to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA).\n\n\n\n\n\n\nIn this document, mostly the same set of features via the Spark API or using SQL is showcased. If you are familiar with Scala and understand Spark concepts you can choose to skip the SQL part go directly to the \nSpark API section\n.\n\n\nHigh Concurrency in SnappyData\n\n\nThousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into low latency requests and high latency ones.\n\n\nFor low latency operations, Spark\u2019s scheduling mechanism is completely bypassed and directly operate on the data. High latency operations (for example, compute intensive queries) are routed through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.\n\n\nState Sharing in SnappyData\n\n\nA SnappyData cluster is designed to be a long-running clustered database. The state is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture-overview", 
            "text": "This section presents a high-level overview of SnappyData\u2019s core components. It also explains how our data pipeline (as streams) are ingested into our in-memory store and subsequently interacted with, and analyzed.", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/architecture/#core-components", 
            "text": "The following depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, standard components, such as security and monitoring have been omitted.   The storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row-oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. Refer to the  Row/Column table  section for details on the syntax and available features.   Two primary programming models are supported by SnappyData \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions, to make the language compatible with the SQL standard. One could perceive SnappyData as an SQL database that uses Spark API as its language for stored procedures. Our  stream processing  is primarily through Spark Streaming, but it is integrated and runs within our store.  The OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). All OLTP operations are routed immediately to appropriate data partitions without incurring any scheduling overhead.  To support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, SnappyData uses a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.  In addition to the \u201cexact\u201d Dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Synopsis Data Engine (SDE) and exploits appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.  To understand the data flow architecture, you are first walked through a real-time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.", 
            "title": "Core Components"
        }, 
        {
            "location": "/architecture/#data-ingestion-pipeline", 
            "text": "The data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive architecture for real-time applications. The steps to support these tasks are depicted in the following figure and explained below.     Once the SnappyData cluster is started and before any live streams can be processed, ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (for example, HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables.    Spark Streaming\u2019s parallel receivers are relied on to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.    Next, SQL is used to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), these data structures are seamlessly combined in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high-velocity streams, SnappyData can still provide answers in real time by resorting to approximation.    The stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (for example, maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.    To prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.    Once ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance with users\u2019 requested accuracy.", 
            "title": "Data Ingestion Pipeline"
        }, 
        {
            "location": "/architecture/#hybrid-cluster-manager", 
            "text": "Spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (for example, YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors. This is represented in the following figure.   While Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet the following additional requirements as an operational database.    High Concurrency : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.    State Sharing : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real-time data sharing.    High Availability (HA) : As a highly concurrent distributed system that offers low latency access to data, applications must be protected from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations, therefore, become an important requirement for SnappyData.    Consistency : As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture, how SnappyData meets each of these requirements is explained in the subsequent sections.", 
            "title": "Hybrid Cluster Manager"
        }, 
        {
            "location": "/architecture/#snappydata-cluster-architecture", 
            "text": "A SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members as represented in the below figure.    Locator: Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.    Lead Node: The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.    Data Servers: A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or pass it to the lead node for execution by Spark SQL.     SnappyData also has multiple deployment options. For more information refer to,  Deployment Options .", 
            "title": "SnappyData Cluster Architecture"
        }, 
        {
            "location": "/architecture/#interacting-with-snappydata", 
            "text": "Note  For the section on the Spark API, it is assumed that users have some familiarity with  core Spark, Spark SQL and Spark Streaming concepts .\nAnd, you can try out the Spark  Quick Start . All the commands and programs listed in the Spark guides work in SnappyData as well.\nFor the section on SQL, no Spark knowledge is necessary.   To interact with SnappyData, interfaces are provided for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self-contained Spark application or can share the state with other jobs using the SnappyData store.  Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.    Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.    Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shut down, taking down all cached state with it. Instead, SnappyData leverages the  Spark JobServer  to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA).    In this document, mostly the same set of features via the Spark API or using SQL is showcased. If you are familiar with Scala and understand Spark concepts you can choose to skip the SQL part go directly to the  Spark API section .", 
            "title": "Interacting with SnappyData"
        }, 
        {
            "location": "/architecture/#high-concurrency-in-snappydata", 
            "text": "Thousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into low latency requests and high latency ones.  For low latency operations, Spark\u2019s scheduling mechanism is completely bypassed and directly operate on the data. High latency operations (for example, compute intensive queries) are routed through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.", 
            "title": "High Concurrency in SnappyData"
        }, 
        {
            "location": "/architecture/#state-sharing-in-snappydata", 
            "text": "A SnappyData cluster is designed to be a long-running clustered database. The state is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "State Sharing in SnappyData"
        }, 
        {
            "location": "/configuration/", 
            "text": "Overview\n\n\nSnappyData has three main components - Locator, Server, and Lead.\n\n\nThe Lead node embeds a Spark driver and the Server node embeds a Spark Executor. The server node also embeds a SnappyData store.\n\n\nSnappyData cluster can be started with the default configurations using script \nsbin/snappy-start-all.sh\n. This script starts up a locator, one data server, and one lead node. However, SnappyData can be configured to start multiple components on different nodes. \n\nAlso, each component can be configured individually using configuration files. In this section, you can learn how the components can be individually configured and also learn about various other configurations of SnappyData.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nConfiguration Files\n\n\n\n\n\n\nConfiguring Locators\n\n\n\n\n\n\nConfiguring Leads\n\n\n\n\n\n\nConfiguring Data Servers\n\n\n\n\n\n\nSnappyData Specific Properties\n \n\n\n\n\n\n\n\n\n\n\nHDFS with SnappyData Store\n\n\n\n\n\n\nExample for Multiple-Host Configuration\n\n\n\n\n\n\nEnvironment Settings\n\n\n\n\n\n\nHadoop Provided Settings\n\n\n\n\n\n\nPer Component Configuration\n\n\n\n\n\n\nSnappy Command Line Utility\n\n\n\n\n\n\nLogging\n\n\n\n\n\n\nConfiguring SSH Login without Password\n\n\n\n\n\n\nSSL Setup for Client-Server\n\n\n\n\n\n\nSnappyData Properties", 
            "title": "Configuring the Cluster"
        }, 
        {
            "location": "/configuration/#overview", 
            "text": "SnappyData has three main components - Locator, Server, and Lead.  The Lead node embeds a Spark driver and the Server node embeds a Spark Executor. The server node also embeds a SnappyData store.  SnappyData cluster can be started with the default configurations using script  sbin/snappy-start-all.sh . This script starts up a locator, one data server, and one lead node. However, SnappyData can be configured to start multiple components on different nodes.  \nAlso, each component can be configured individually using configuration files. In this section, you can learn how the components can be individually configured and also learn about various other configurations of SnappyData.  The following topics are covered in this section:    Configuration Files    Configuring Locators    Configuring Leads    Configuring Data Servers    SnappyData Specific Properties        HDFS with SnappyData Store    Example for Multiple-Host Configuration    Environment Settings    Hadoop Provided Settings    Per Component Configuration    Snappy Command Line Utility    Logging    Configuring SSH Login without Password    SSL Setup for Client-Server    SnappyData Properties", 
            "title": "Overview"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/", 
            "text": "Configuration Files\n\n\nConfiguration files for locator, lead, and server should be created in the \nconf\n folder located in the SnappyData home directory with names \nlocators\n, \nleads\n, and \nservers\n.\n\n\nTo do so, you can copy the existing template files \nservers.template\n, \nlocators.template\n, \nleads.template\n, and rename them to \nservers\n, \nlocators\n, \nleads\n.\n\n\nThese files contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members.\n\n\n\n\nConfiguring Locators\n\n\nLocators provide discovery service for the cluster. It informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n\n\nIn this file, you can specify:\n\n\n\n\n\n\nThe host name on which a SnappyData locator is started.\n\n\n\n\n\n\nThe startup directory where the logs and configuration files for that locator instance are located.\n\n\n\n\n\n\nSnappyData specific properties that can be passed.\n\n\n\n\n\n\nCreate the configuration file (\nlocators\n) for locators in the \nSnappyData_home/conf\n directory.\n\n\n\n\nConfiguring Leads\n\n\nLead Nodes act as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance, but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n\n\nCreate the configuration file (\nleads\n) for leads in the \nSnappyData_home/conf\n directory.\n\n\n\n\nConfiguring Data Servers\n\n\nData Servers hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or to pass it to the lead node for execution by Spark SQL.\n\n\nCreate the configuration file (\nservers\n) for data servers in the \nSnappyData_home/conf\n directory.\n\n\n\n\nSnappyData Specific Properties\n\n\nThe following are the few important SnappyData properties that you can configure:\n\n\n\n\n\n\n-peer-discovery-port\n: This is a locator specific property. This is the port on which locator listens for member discovery. It defaults to 10334.\n\n\n\n\n\n\n-client-port\n: Port that a member listens on for client connections. \n\n\n\n\n\n\n-locators\n: List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system.\n\nThe list must include all locators in use, and must be configured consistently for every member of the distributed system.\n\n\n\n\n\n\n-dir\n: SnappyData members need to have the working directory. The member working directory provides a default location for the log file, persistence, and status files for each member.\n If not specified, SnappyData creates the member directory in \nSnappyData_HomeDirectory/work\n. \n\n\n\n\n\n\n-classpath\n: Location of user classes required by the SnappyData Server. This path is appended to the current classpath.\n\n\n\n\n\n\n-heap-size\n: Set a fixed heap size and for the Java VM. \n\n\n\n\n\n\n-J\n: Use this to configure any JVM specific property. For example. -J-XX:MaxPermSize=512m. \n\n\n\n\n\n\n-memory-size\n: Specifies the total memory that can be used by the cluster in off-heap mode. Default value is 0 (OFF_HEAP is not used by default).\n\n\n\n\n\n\nRefer to the \nSnappyData properties\n for the list of SnappyData properties for Locators, Leads and Servers.\n\n\n\n\nHDFS with SnappyData Store\n\n\nIf using SnappyData store persistence to Hadoop as documented \nhere\n, then add the \nhbase jar\n explicitly to CLASSPATH. The jar is now packed in the product tree, so that can be used or download from Apache Maven. Then add to \nconf/spark-env.sh\n:\n\n\nexport SPARK_DIST_CLASSPATH=\n/path/to/\nhbase-0.94.27.jar\n\n\nSubstitute the actual path for \n/path/to/\n above\n\n\n\n\nExample for Multiple-Host Configuration\n\n\nLet's say you want to:\n\n\n\n\n\n\nStart two Locators (on node-a:9999 and node-b:8888), two servers (node-c and node-c) and a lead (node-l).\n\n\n\n\n\n\nChange the Spark UI port from 5050 to 9090. \n\n\n\n\n\n\nSet spark.executor.cores as 10 on all servers. \n\n\n\n\n\n\nThe following can be your conf files. \n\n\n$ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10\n\n\n\n\n\n\nNote\n\n\nConfiguration files are consulted when servers are started and also when they are stopped. So, it is recommended to not change the configuration files when the cluster is running. \n\n\n\n\n\n\nEnvironment Settings\n\n\nAny Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in \nSNAPPY_HOME/conf\n. \n\n\n\n\nHadoop Provided Settings\n\n\nIf you want to run SnappyData with an already existing custom Hadoop cluster like MapR or Cloudera you should download Snappy without Hadoop from the download link.\nThis allows you to provide Hadoop at runtime.\n\n\nTo do this you need to put an entry in $SNAPPY-HOME/conf/spark-env.sh as below:\n\n\nexport SPARK_DIST_CLASSPATH=$($OTHER_HADOOP_HOME/bin/hadoop classpath)\n\n\n\n\n\n\nPer Component Configuration\n\n\nMost of the time, components would be sharing the same properties. For example, you would want all servers to start with 4096m while leads to start with 2048m. You can configure these by specifying LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, LEAD_STARTUP_OPTIONS environment variables in \nconf/snappy-env.sh\n. \n\n\n$ cat conf/snappy-env.sh\nSERVER_STARTUP_OPTIONS=\n-heap-size=4096m\n\nLEAD_STARTUP_OPTIONS=\n-heap-size=2048m\n\n\n\n\n\n\n\nSnappyData Command Line Utility\n\n\nInstead of starting SnappyData members using SSH scripts, they can be individually configured and started using the command line. \n\n\n$ bin/snappy locator start  -dir=/node-a/locator1 \n$ bin/snappy server start  -dir=/node-b/server1  -locators=localhost[10334]\n$ bin/snappy leader start  -dir=/node-c/lead1  -locators=localhost[10334]\n\n$ bin/snappy locator stop -dir=/node-a/locator1\n$ bin/snappy server stop -dir=/node-b/server1\n$ bin/snappy leader stop -dir=/node-c/lead1\n\n\n\n\nRefer to the \nSnappyData properties\n for the list of SnappyData properties for Locators, Leads and Servers.\n\n\n\n\nLogging\n\n\nCurrently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property \n-log-file\n as the path of the directory. \n\nThe logging levels can be modified by adding a \nconf/log4j.properties\n file in the product directory. \n\n\n$ cat conf/log4j.properties \nlog4j.logger.org.apache.spark.scheduler.DAGScheduler=DEBUG\nlog4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG\n\n\n\n\n\n\nNote\n\n\nFor a set of applicable class names and default values see the file \nconf/log4j.properties.template\n, which can be used as a starting point. Consult the \nlog4j 1.2.x documentation\n for more details on the configuration file.\n\n\n\n\n\n\nConfiguring SSH Login without Password\n\n\nBy default, Secure Socket Shell (SSH) requires a password for authentication on a remote server.\nThis setting needs to be modified to allow you to login to the remote host through the SSH protocol, without having to enter your SSH password multiple times when working with SnappyData.\n\n\nTo install and configure SSH, do the following:\n\n\n\n\n\n\nInstall SSH\n \n\n    To install SSH,  type:\n    \nsudo apt-get install ssh\n\n    Mac OS X has a built-in SSH client.\n\n\n\n\n\n\nGenerate an RSA key pair\n\n    To generate an RSA key pair run the following command on the client computer,\n    \nssh-keygen -t rsa\n\n    Press \nEnter\n when prompted to enter the file in which to save the key, and for the pass phrase.\n\n\n\n\n\n\nCopy the Public Key\n\n    Once the key pair is generated, copy the contents of the public key file, to the authorized key on the remote site, by typing \ncat ~/.ssh/id_rsa.pub \n ~/.ssh/authorized_keys\n\n\n\n\n\n\n\n\nSSL Setup for Client-Server\n\n\nSnappyData store now has support for Thrift protocol that provides functionality equivalent to JDBC/ODBC protocols and can be used to access the store from other languages that are not yet supported directly by SnappyData. In the command-line, SnappyData locators and servers accept the \n-thrift-server-address\n and -\nthrift-server-port\n arguments to start a Thrift server.\n\n\nThe thrift servers use the Thrift Compact Protocol by default which is not SSL enabled. When using the snappy-start-all.sh script, these properties can be specified in the \nconf/locators\n and \nconf/servers\n files in the product directory like any other locator/server properties.\n\n\nIn the \nconf/locators\n and \nconf/servers\n files, you need to add \n-thrift-ssl\n and required SSL setup in \n-thrift-ssl-properties\n. Refer to the \nSnappyData thrift properties\n section for more information.\n\n\nFrom the Snappy SQL shell:\n\n\nsnappy\n connect client 'host:port;ssl=true;ssl-properties=...';\n\n\n\n\nFor JDBC use the same properties (without the \"thrift-\" prefix) like:\n\n\njdbc:snappydata://host:port/;ssl=true;ssl-properties=...\n\n\n\n\nFor example:\n\n\nFor a self-signed RSA certificate in keystore.jks, you can have the following configuration in the \nconf/locators\n and \nconf/servers\n files:\n\n\nlocalhost -thrift-ssl=true -thrift-ssl-properties=keystore=keystore.jks,keystore-password=password,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256\n\n\n\n\nUse the protocol/ciphers as per requirement. The corresponding setup on client-side can look like:\n\n\nsnappy\n connect client 'localhost:1527;ssl=true;ssl-properties=truststore=keystore.jks,truststore-password=password,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256';", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuration-files", 
            "text": "Configuration files for locator, lead, and server should be created in the  conf  folder located in the SnappyData home directory with names  locators ,  leads , and  servers .  To do so, you can copy the existing template files  servers.template ,  locators.template ,  leads.template , and rename them to  servers ,  locators ,  leads .  These files contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-locators", 
            "text": "Locators provide discovery service for the cluster. It informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.  In this file, you can specify:    The host name on which a SnappyData locator is started.    The startup directory where the logs and configuration files for that locator instance are located.    SnappyData specific properties that can be passed.    Create the configuration file ( locators ) for locators in the  SnappyData_home/conf  directory.", 
            "title": "Configuring Locators"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-leads", 
            "text": "Lead Nodes act as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance, but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.  Create the configuration file ( leads ) for leads in the  SnappyData_home/conf  directory.", 
            "title": "Configuring Leads"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-data-servers", 
            "text": "Data Servers hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or to pass it to the lead node for execution by Spark SQL.  Create the configuration file ( servers ) for data servers in the  SnappyData_home/conf  directory.", 
            "title": "Configuring Data Servers"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#snappydata-specific-properties", 
            "text": "The following are the few important SnappyData properties that you can configure:    -peer-discovery-port : This is a locator specific property. This is the port on which locator listens for member discovery. It defaults to 10334.    -client-port : Port that a member listens on for client connections.     -locators : List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. \nThe list must include all locators in use, and must be configured consistently for every member of the distributed system.    -dir : SnappyData members need to have the working directory. The member working directory provides a default location for the log file, persistence, and status files for each member.  If not specified, SnappyData creates the member directory in  SnappyData_HomeDirectory/work .     -classpath : Location of user classes required by the SnappyData Server. This path is appended to the current classpath.    -heap-size : Set a fixed heap size and for the Java VM.     -J : Use this to configure any JVM specific property. For example. -J-XX:MaxPermSize=512m.     -memory-size : Specifies the total memory that can be used by the cluster in off-heap mode. Default value is 0 (OFF_HEAP is not used by default).    Refer to the  SnappyData properties  for the list of SnappyData properties for Locators, Leads and Servers.", 
            "title": "SnappyData Specific Properties"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#hdfs-with-snappydata-store", 
            "text": "If using SnappyData store persistence to Hadoop as documented  here , then add the  hbase jar  explicitly to CLASSPATH. The jar is now packed in the product tree, so that can be used or download from Apache Maven. Then add to  conf/spark-env.sh :  export SPARK_DIST_CLASSPATH= /path/to/ hbase-0.94.27.jar  Substitute the actual path for  /path/to/  above", 
            "title": "HDFS with SnappyData Store"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#example-for-multiple-host-configuration", 
            "text": "Let's say you want to:    Start two Locators (on node-a:9999 and node-b:8888), two servers (node-c and node-c) and a lead (node-l).    Change the Spark UI port from 5050 to 9090.     Set spark.executor.cores as 10 on all servers.     The following can be your conf files.   $ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10   Note  Configuration files are consulted when servers are started and also when they are stopped. So, it is recommended to not change the configuration files when the cluster is running.", 
            "title": "Example for Multiple-Host Configuration"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#environment-settings", 
            "text": "Any Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in  SNAPPY_HOME/conf .", 
            "title": "Environment Settings"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#hadoop-provided-settings", 
            "text": "If you want to run SnappyData with an already existing custom Hadoop cluster like MapR or Cloudera you should download Snappy without Hadoop from the download link.\nThis allows you to provide Hadoop at runtime.  To do this you need to put an entry in $SNAPPY-HOME/conf/spark-env.sh as below:  export SPARK_DIST_CLASSPATH=$($OTHER_HADOOP_HOME/bin/hadoop classpath)", 
            "title": "Hadoop Provided Settings"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#per-component-configuration", 
            "text": "Most of the time, components would be sharing the same properties. For example, you would want all servers to start with 4096m while leads to start with 2048m. You can configure these by specifying LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, LEAD_STARTUP_OPTIONS environment variables in  conf/snappy-env.sh .   $ cat conf/snappy-env.sh\nSERVER_STARTUP_OPTIONS= -heap-size=4096m \nLEAD_STARTUP_OPTIONS= -heap-size=2048m", 
            "title": "Per Component Configuration"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#snappydata-command-line-utility", 
            "text": "Instead of starting SnappyData members using SSH scripts, they can be individually configured and started using the command line.   $ bin/snappy locator start  -dir=/node-a/locator1 \n$ bin/snappy server start  -dir=/node-b/server1  -locators=localhost[10334]\n$ bin/snappy leader start  -dir=/node-c/lead1  -locators=localhost[10334]\n\n$ bin/snappy locator stop -dir=/node-a/locator1\n$ bin/snappy server stop -dir=/node-b/server1\n$ bin/snappy leader stop -dir=/node-c/lead1  Refer to the  SnappyData properties  for the list of SnappyData properties for Locators, Leads and Servers.", 
            "title": "SnappyData Command Line Utility"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#logging", 
            "text": "Currently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property  -log-file  as the path of the directory.  \nThe logging levels can be modified by adding a  conf/log4j.properties  file in the product directory.   $ cat conf/log4j.properties \nlog4j.logger.org.apache.spark.scheduler.DAGScheduler=DEBUG\nlog4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG   Note  For a set of applicable class names and default values see the file  conf/log4j.properties.template , which can be used as a starting point. Consult the  log4j 1.2.x documentation  for more details on the configuration file.", 
            "title": "Logging"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-ssh-login-without-password", 
            "text": "By default, Secure Socket Shell (SSH) requires a password for authentication on a remote server.\nThis setting needs to be modified to allow you to login to the remote host through the SSH protocol, without having to enter your SSH password multiple times when working with SnappyData.  To install and configure SSH, do the following:    Install SSH   \n    To install SSH,  type:\n     sudo apt-get install ssh \n    Mac OS X has a built-in SSH client.    Generate an RSA key pair \n    To generate an RSA key pair run the following command on the client computer,\n     ssh-keygen -t rsa \n    Press  Enter  when prompted to enter the file in which to save the key, and for the pass phrase.    Copy the Public Key \n    Once the key pair is generated, copy the contents of the public key file, to the authorized key on the remote site, by typing  cat ~/.ssh/id_rsa.pub   ~/.ssh/authorized_keys", 
            "title": "Configuring SSH Login without Password"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#ssl-setup-for-client-server", 
            "text": "SnappyData store now has support for Thrift protocol that provides functionality equivalent to JDBC/ODBC protocols and can be used to access the store from other languages that are not yet supported directly by SnappyData. In the command-line, SnappyData locators and servers accept the  -thrift-server-address  and - thrift-server-port  arguments to start a Thrift server.  The thrift servers use the Thrift Compact Protocol by default which is not SSL enabled. When using the snappy-start-all.sh script, these properties can be specified in the  conf/locators  and  conf/servers  files in the product directory like any other locator/server properties.  In the  conf/locators  and  conf/servers  files, you need to add  -thrift-ssl  and required SSL setup in  -thrift-ssl-properties . Refer to the  SnappyData thrift properties  section for more information.  From the Snappy SQL shell:  snappy  connect client 'host:port;ssl=true;ssl-properties=...';  For JDBC use the same properties (without the \"thrift-\" prefix) like:  jdbc:snappydata://host:port/;ssl=true;ssl-properties=...  For example:  For a self-signed RSA certificate in keystore.jks, you can have the following configuration in the  conf/locators  and  conf/servers  files:  localhost -thrift-ssl=true -thrift-ssl-properties=keystore=keystore.jks,keystore-password=password,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256  Use the protocol/ciphers as per requirement. The corresponding setup on client-side can look like:  snappy  connect client 'localhost:1527;ssl=true;ssl-properties=truststore=keystore.jks,truststore-password=password,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256';", 
            "title": "SSL Setup for Client-Server"
        }, 
        {
            "location": "/configuring_cluster/property_description/", 
            "text": "List of Properties\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nComponents\n\n\n\n\n\n\n\n\n\n\n-J\n\n\nJVM option passed to the spawned SnappyData server JVM. \nFor example, use -J-Xmx1024m to set the JVM heap to 1GB.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-dir\n\n\nWorking directory of the server that contains the SnappyData Server status file and the default location for log file, persistent files, data dictionary, and so forth (defaults to the current directory).\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-classpath\n\n\nLocation of user classes required by the SnappyData Server.\nThis path is appended to the current classpath.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-heap-size\n\n\n Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. \nFor example, -heap-size=1024m. \nIf you use the \n-heap-size\n option, by default SnappyData sets the critical-heap-percentage to 90% of the heap size, and the \neviction-heap-percentage\n to 81% of the \ncritical-heap-percentage\n. \nSnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-memory-size\n\n\nSpecifies the total memory that can be used by the node for column storage and execution in off-heap. Default value is 0 (OFF_HEAP is not used by default)\n\n\nServer\nLead\n\n\n\n\n\n\n-locators\n\n\nList of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. \nThe list must include all locators in use, and must be configured consistently for every member of the distributed system.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-rebalance\n\n\nCauses the new member to trigger a rebalancing operation for all partitioned tables in the system. \nThe system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option.\n\n\nServer\n\n\n\n\n\n\n-bind-address\n\n\nIP address on which the locator is bound. The default behavour is to bind to all local addresses.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-critical-heap-percentage\n\n\nSets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. \nIf you set \n-heap-size\n, the default value for \ncritical-heap-percentage\n is set to 90% of the heap size. \nUse this switch to override the default.\nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.\n\n\nServer\nLead\n\n\n\n\n\n\n-eviction-heap-percentage\n\n\nSets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 81% of whatever is set for \n-critical-heap-percentage\n.\nUse this switch to override the default.\n\n\nServer\nLead\n\n\n\n\n\n\n-critical-off-heap-percentage\n\n\nSets the critical threshold for off-heap memory usage in percentage, 0-100. \nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory.\n\n\nServer\n\n\n\n\n\n\n-eviction-off-heap-percentage\n\n\nSets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. \nBy default, the eviction threshold is 81% of whatever is set for \n-critical-off-heap-percentage\n. \nUse this switch to override the default.\n\n\nServer\n\n\n\n\n\n\n-log-file\n\n\nPath of the file to which this member writes log messages (default is snappyserver.log in the working directory)\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-J-Dgemfirexd.hostname-for-clients\n\n\nHostname or IP address that is sent to clients so they can connect to the locator. The default is the \nbind-address\n of the locator.\n\n\nServer\n\n\n\n\n\n\n-peer-discovery-address\n\n\nUse this as value for port in the \"host:port\" value of \"-locators\" property\n\n\nLocator\n\n\n\n\n\n\n-peer-discovery-port\n\n\nPort on which the locator listens for peer discovery (includes servers as well as other locators).  \nValid values are in the range 1-65535, with a default of 10334.\n\n\nLocator\n\n\n\n\n\n\n-member-timeout\n\n\nUses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:\n 1) First it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.\n 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. \nValid values are in the range 1000..600000.\n\n\nServer\nLocator\nLead\n\n\n\n\n\n\nsnappydata.column.batchSize\n\n\nThe default size of blocks to use for storage in the SnappyData column store. The default value is 25165824 (24M).\n\n\nLead\n\n\n\n\n\n\nthrift-ssl\n\n\nSpecifies if you want to enable or disable SSL. Values: true or false\n\n\nServer\n\n\n\n\n\n\nthrift-ssl-properties\n\n\nComma-separated SSL properties including:\nprotocol\n: default \"TLS\",\nenabled-protocols\n: enabled protocols separated by \":\"\ncipher-suites\n: enabled cipher suites separated by \":\"\nclient-auth\n=(true or false): if client also needs to be authenticated \nkeystore\n: path to key store file \nkeystore-type\n: the type of key-store (default \"JKS\") \nkeystore-password\n: password for the key store file\nkeymanager-type\n: the type of key manager factory \ntruststore\n: path to trust store file\ntruststore-type\n: the type of trust-store (default \"JKS\")\ntruststore-password\n: password for the trust store file \ntrustmanager-type\n: the type of trust manager factory \n\n\nServer\n\n\n\n\n\n\nspark.driver.maxResultSize\n\n\nLimit of total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in lead.\n\n\nLead\n\n\n\n\n\n\nspark.executor.cores\n\n\nThe number of cores to use on each server.\n\n\nLead\n\n\n\n\n\n\nspark.network.timeout\n\n\nDefault timeout for all network interactions while running queries.\n\n\nLead\n\n\n\n\n\n\nspark.local.dir\n\n\nDirectory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.\n\n\nLead\n\n\n\n\n\n\n\n\n\n\nSQL Properties\n\n\nThese properites can be set in the snappy SQL shell or using the configuration properties in the \nconf/leads\n file.\n\n\nFor example: Set in the snappy SQL shell\n\n\nsnappy\n connect client 'localhost:1527';\nsnappy\n set snappydata.column.batchSize=108080;\n\n\n\n\nThis will set the property for the snappy SQL shell's session.\n\n\nSet in the \nconf/leads\n file\n\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 s-snappydata.column.batchSize=108080\n\n\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsnappydata.column.batchSize\n\n\nThe default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes) that will be used to split the data into chunks for efficient storage and retrieval. \n This property can also be set for each table in the \ncreate table\n DDL.\n\n\n\n\n\n\nsnappydata.column.maxDeltaRows\n\n\nThe maximum number of rows that can be in the delta buffer of a column table. The size of delta buffer is already limited by \nColumnBatchSize\n property, but this allows a lower limit on number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of \nColumnBatchSize\n and this property is hit first. It can also be set for each table in the \ncreate table\n DDL, else this setting is used for the \ncreate table\n\n\n\n\n\n\nsnappydata.hashJoinSize\n\n\nThe join would be converted into a hash join if the table is of size less than the \nhashJoinSize\n. Default value is 100 MB.\n\n\n\n\n\n\nsnappydata.hashAggregateSize\n\n\nAggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (with b/k/m/g/t/p suffixes for units) and not the output size. Set this only if there are queries that can return very large number of rows in aggregation results. Default value is set to 0b which means, no limit is set on the size, so the optimized hash aggregation is always used.\n\n\n\n\n\n\n\n\n\n\nSDE Properties\n\n\nThese \nSDE\n properites can be set in the snappy SQL shell or using the configuration properties in the \nconf/leads\n file.\n\n\nFor example: Set in the snappy SQL shell\n\n\nsnappy\n connect client 'localhost:1527';\nsnappy\n set snappydata.flushReservoirThreshold=20000;\n\n\n\n\nSet in the \nconf/leads\n file\n\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 s-snappydata.column.batchSize=108080 -spark.sql.aqp.error=0.5\n\n\n\n\nThis sets the property for the snappy SQL shell's session.\n\n\n\n\n\n\n\n\nProperties\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsnappydata.flushReservoirThreshold\n\n\nReservoirs of sample table will be flushed and stored in columnar format if sampling is done on baset table of size more than flushReservoirThreshold. Default value is 10,000.\n This property must be set in the \nconf/servers\n and \nconf/leads\n file.\n\n\n\n\n\n\nspark.sql.aqp.numBootStrapTrials\n\n\nNumber of bootstrap trials to do for calculating error bounds. Default value is 100. \nThis property must be set in the \nconf/leads\n file.\n\n\n\n\n\n\nspark.sql.aqp.error\n\n\nMaximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. Default value is 0.2. \nThis property can be set as connection property in the Snappy SQL shell.\n\n\n\n\n\n\nspark.sql.aqp.confidence\n\n\nConfidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. \n Default value is 0.95. \nThis property can be set as connection property in the Snappy SQL shell\n\n\n\n\n\n\nsparksql.aqp.behavior\n\n\nThe action to be taken if the error computed goes oustide the error tolerance limit. Default value is \nDO_NOTHING\n. \nThis property can be set as connection property in the Snappy SQL shell", 
            "title": "SnappyData Properties"
        }, 
        {
            "location": "/configuring_cluster/property_description/#list-of-properties", 
            "text": "Property  Description  Components      -J  JVM option passed to the spawned SnappyData server JVM.  For example, use -J-Xmx1024m to set the JVM heap to 1GB.  Server Lead Locator    -dir  Working directory of the server that contains the SnappyData Server status file and the default location for log file, persistent files, data dictionary, and so forth (defaults to the current directory).  Server Lead Locator    -classpath  Location of user classes required by the SnappyData Server. This path is appended to the current classpath.  Server Lead Locator    -heap-size   Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings.  For example, -heap-size=1024m.  If you use the  -heap-size  option, by default SnappyData sets the critical-heap-percentage to 90% of the heap size, and the  eviction-heap-percentage  to 81% of the  critical-heap-percentage .  SnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM.  Server Lead Locator    -memory-size  Specifies the total memory that can be used by the node for column storage and execution in off-heap. Default value is 0 (OFF_HEAP is not used by default)  Server Lead    -locators  List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system.  The list must include all locators in use, and must be configured consistently for every member of the distributed system.  Server Lead Locator    -rebalance  Causes the new member to trigger a rebalancing operation for all partitioned tables in the system.  The system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option.  Server    -bind-address  IP address on which the locator is bound. The default behavour is to bind to all local addresses.  Server Lead Locator    -critical-heap-percentage  Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100.  If you set  -heap-size , the default value for  critical-heap-percentage  is set to 90% of the heap size.  Use this switch to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.  Server Lead    -eviction-heap-percentage  Sets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 81% of whatever is set for  -critical-heap-percentage . Use this switch to override the default.  Server Lead    -critical-off-heap-percentage  Sets the critical threshold for off-heap memory usage in percentage, 0-100.  When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory.  Server    -eviction-off-heap-percentage  Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory.  By default, the eviction threshold is 81% of whatever is set for  -critical-off-heap-percentage .  Use this switch to override the default.  Server    -log-file  Path of the file to which this member writes log messages (default is snappyserver.log in the working directory)  Server Lead Locator    -J-Dgemfirexd.hostname-for-clients  Hostname or IP address that is sent to clients so they can connect to the locator. The default is the  bind-address  of the locator.  Server    -peer-discovery-address  Use this as value for port in the \"host:port\" value of \"-locators\" property  Locator    -peer-discovery-port  Port on which the locator listens for peer discovery (includes servers as well as other locators).   Valid values are in the range 1-65535, with a default of 10334.  Locator    -member-timeout  Uses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:  1) First it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.  2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure.  Valid values are in the range 1000..600000.  Server Locator Lead    snappydata.column.batchSize  The default size of blocks to use for storage in the SnappyData column store. The default value is 25165824 (24M).  Lead    thrift-ssl  Specifies if you want to enable or disable SSL. Values: true or false  Server    thrift-ssl-properties  Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated  keystore : path to key store file  keystore-type : the type of key-store (default \"JKS\")  keystore-password : password for the key store file keymanager-type : the type of key manager factory  truststore : path to trust store file truststore-type : the type of trust-store (default \"JKS\") truststore-password : password for the trust store file  trustmanager-type : the type of trust manager factory   Server    spark.driver.maxResultSize  Limit of total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in lead.  Lead    spark.executor.cores  The number of cores to use on each server.  Lead    spark.network.timeout  Default timeout for all network interactions while running queries.  Lead    spark.local.dir  Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.  Lead", 
            "title": "List of Properties"
        }, 
        {
            "location": "/configuring_cluster/property_description/#sql-properties", 
            "text": "These properites can be set in the snappy SQL shell or using the configuration properties in the  conf/leads  file.  For example: Set in the snappy SQL shell  snappy  connect client 'localhost:1527';\nsnappy  set snappydata.column.batchSize=108080;  This will set the property for the snappy SQL shell's session.  Set in the  conf/leads  file  $ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 s-snappydata.column.batchSize=108080     Property  Description      snappydata.column.batchSize  The default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes) that will be used to split the data into chunks for efficient storage and retrieval.   This property can also be set for each table in the  create table  DDL.    snappydata.column.maxDeltaRows  The maximum number of rows that can be in the delta buffer of a column table. The size of delta buffer is already limited by  ColumnBatchSize  property, but this allows a lower limit on number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of  ColumnBatchSize  and this property is hit first. It can also be set for each table in the  create table  DDL, else this setting is used for the  create table    snappydata.hashJoinSize  The join would be converted into a hash join if the table is of size less than the  hashJoinSize . Default value is 100 MB.    snappydata.hashAggregateSize  Aggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (with b/k/m/g/t/p suffixes for units) and not the output size. Set this only if there are queries that can return very large number of rows in aggregation results. Default value is set to 0b which means, no limit is set on the size, so the optimized hash aggregation is always used.", 
            "title": "SQL Properties"
        }, 
        {
            "location": "/configuring_cluster/property_description/#sde-properties", 
            "text": "These  SDE  properites can be set in the snappy SQL shell or using the configuration properties in the  conf/leads  file.  For example: Set in the snappy SQL shell  snappy  connect client 'localhost:1527';\nsnappy  set snappydata.flushReservoirThreshold=20000;  Set in the  conf/leads  file  $ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 s-snappydata.column.batchSize=108080 -spark.sql.aqp.error=0.5  This sets the property for the snappy SQL shell's session.     Properties  Description      snappydata.flushReservoirThreshold  Reservoirs of sample table will be flushed and stored in columnar format if sampling is done on baset table of size more than flushReservoirThreshold. Default value is 10,000.  This property must be set in the  conf/servers  and  conf/leads  file.    spark.sql.aqp.numBootStrapTrials  Number of bootstrap trials to do for calculating error bounds. Default value is 100.  This property must be set in the  conf/leads  file.    spark.sql.aqp.error  Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. Default value is 0.2.  This property can be set as connection property in the Snappy SQL shell.    spark.sql.aqp.confidence  Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1.   Default value is 0.95.  This property can be set as connection property in the Snappy SQL shell    sparksql.aqp.behavior  The action to be taken if the error computed goes oustide the error tolerance limit. Default value is  DO_NOTHING .  This property can be set as connection property in the Snappy SQL shell", 
            "title": "SDE Properties"
        }, 
        {
            "location": "/programming_guide/", 
            "text": "Overview\n\n\nSnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). \nAll SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames.\n\nIt is therefore recommend that you understand the \nconcepts in SparkSQL\n \nand the \nDataFrame API\n. You can also store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced \nhere\n.\n\n\nIn Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. \nData in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk.\n\n\n\n\nSnappySession and SnappyStreamingContext\n\n\nSnappySession\n is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's \nSparkSession\n to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame.\nSimilarly, \nSnappyStreamingContext\n is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's\n\nStreaming Context\n.\n\n\nAlso SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see \nSnappyData Spark Affinity modes\n.\n\n\nIf you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession.\n\n\nTo Create a SnappySession\n\n\nScala\n\n\n val spark: SparkSession = SparkSession\n         .builder\n         .appName(\nSparkApp\n)\n         .master(\nmaster_url\n)\n         .getOrCreate\n\n val snappy = new SnappySession(spark.sparkContext)\n\n\n\n\nJava\n\n\n SparkSession spark = SparkSession\n       .builder()\n       .appName(\nSparkApp\n)\n       .master(\nmaster_url\n)\n       .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n SnappySession snappy = new SnappySession(spark.sparkContext());\n\n\n\n\nPython\n\n\n from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)\n\n\n\n\nTo Create a SnappyStreamingContext\n\n\nScala\n\n\n val spark: SparkSession = SparkSession\n         .builder\n         .appName(\nSparkApp\n)\n         .master(\nmaster_url\n)\n         .getOrCreate\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n\n\n\n\nJava\n\n\n SparkSession spark = SparkSession\n     .builder()\n     .appName(\nSparkApp\n)\n     .master(\nmaster_url\n)\n     .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n Duration batchDuration = Milliseconds.apply(500);\n JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext(jsc, batchDuration);\n\n\n\n\nPython\n\n\n from pyspark.streaming.snappy.context import SnappyStreamingContext\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n duration = .5\n snsc = SnappyStreamingContext(sc, duration)\n\n\n\n\nIf you are in the Embedded Mode, applications typically submit Jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext. \nThese jobs are the primary mechanism to interact with SnappyData using the Spark API. \nA job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.\n\n\nThe implementation of the \nrunSnappyJob\n function from SnappySQLJob uses a SnappySession to interact with the SnappyData store to process and store tables.\nThe implementation of \nrunSnappyJob\n from SnappyStreamingJob uses a SnappyStreamingContext to create streams and manage the streaming context.\nThe jobs are submitted to the lead node of SnappyData over REST API using a \nspark-submit\n like utility.\n\n\n\n\nSnappyData Jobs\n\n\nTo create a job that can be submitted through the job server, the job must implement the \nSnappySQLJob\n or \nSnappyStreamingJob\n trait. Your job is displayed as:\n\n\nScala\n\n\nclass SnappySampleJob implements SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  def runSnappyJob(snappy: SnappySession, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(snappy: SnappySession, config: Config): SnappyJobValidation\n}\n\n\n\n\nJava\n\n\nclass SnappySampleJob extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation}\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate}\n}\n\n\n\n\n\nScala\n\n\nclass SnappyStreamingSampleJob implements SnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation\n}\n\n\n\n\nJava\n\n\nclass SnappyStreamingSampleJob extends JavaSnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)\n  {//validate}\n}\n\n\n\n\n\n\nNote\n\n\nThe \nJob\n traits are simply extensions of the \nSparkJob\n implemented by \nSpark JobServer\n. \n\n\n\n\n\n\n\n\nrunSnappyJob\n contains the implementation of the Job.\nThe \nSnappySession\n/\nSnappyStreamingContext\n is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.\n\n\n\n\n\n\nisValidJob\n allows for an initial validation of the context and any provided configuration.\n    If the context and configuration can run the job, returning \nspark.jobserver.SnappyJobValid\n allows the job to execute, otherwise returning \nspark.jobserver.SnappyJobInvalid\nreason\n prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an \"HTTP/1.1 400 Bad Request\" status code. Validate helps you prevent running jobs that eventually fail due to a  missing or wrong configuration, and saves both time and resources.\n\n\n\n\n\n\nSee \nexamples\n for Spark and Spark streaming jobs. \n\n\nSnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs.\n\n\nSubmitting Jobs\n\n\nThe following command submits \nCreateAndLoadAirlineDataJob\n. This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its \nrunJob\n method. \nThe program is compiled into a jar file (\nquickstart.jar\n) and submitted to jobs server as shown below.\n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\nThe utility \nsnappy-job.sh\n submits the job and returns a JSON that has a Job Id of this job.\n\n\n\n\n\n\n--lead\n: Specifies the host name of the lead node along with the port on which it accepts jobs (8090)\n\n\n\n\n\n\n--app-name\n: Specifies the name given to the submitted application\n\n\n\n\n\n\n--class\n: Specifies the name of the class that contains implementation of the Spark job to be run\n\n\n\n\n\n\n--app-jar\n: Specifies the jar file that packages the code for Spark job\n\n\n\n\n\n\nThe status returned by the utility is displayed below:\n\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  }\n}\n\n\n\n\nThis Job ID can be used to query the status of the running job. \n\n\n$ bin/snappy-job.sh status  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{\n  \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n\n\n\n\nOnce the tables are created, they can be queried by running another job. Please refer to \nAirlineDataJob\n for implementing the job. \n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\nThe status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results.\n\n\nRunning Python Applications\n\n\nPython users can submit a Python application using \nspark-submit\n in the SnappyData Connector mode. For example, run the command given below to submit a Python application:\n\n\nbin/spark-submit \\\n    --master local[*]  \\\n    --conf snappydata.connection=localhost:1527 \\\n    --conf spark.ui.port=4042  quickstart/python/AirlineDataPythonApp.py\n\n\n\n\nsnappydata.connection\n property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster.\n\n\nStreaming Jobs\n\n\nAn implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying \n--stream\n as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. \nAlternatively, you can specify the name of an existing/pre-created streaming context as \n--context \ncontext-name\n with the \nsubmit\n command.\n\n\nFor example, \nTwitterPopularTagsJob\n can be submitted as follows. \nThis job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets.\n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\\n    --stream\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n982ac142-3550-41e1-aace-6987cb39fec8\n,\n    \ncontext\n: \nsnappyStreamingContext1463987084945028747\n\n  }\n}\n\n\n\n\nTo start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context.\n\n\n$ bin/snappy-job.sh stop  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 982ac142-3550-41e1-aace-6987cb39fec8\n\n$ bin/snappy-job.sh listcontexts  \\\n    --lead hostNameOfLead:8090\n[\nsnappyContext1452598154529305363\n, \nsnappyStreamingContext1463987084945028747\n, \nsnappyStreamingContext\n]\n\n$ bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \\\n    --lead hostNameOfLead:8090\n\n\n\n\nManaging JAR Files\n\n\nSnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster.\n\n\nInstalling a JAR\n \n\n\nUse SQLJ.INSTALL_JAR procedure to install a JAR file\n\n\nSyntax:\n\n\nSQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER)\n\n\n\n\n\n\n\n\nJAR_FILE_PATH  is the full path for the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers\n\n\n\n\n\n\nQUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.\n\n\n\n\n\n\nDEPLOY: This argument is currently ignored.\n\n\n\n\n\n\nExample: Installing a JAR\n\n\nsnappy\n call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0);\n\n\n\n\nReplacing a JAR\n \n\n\nUse  SQLJ.REPLACE_JAR procedure to replace an installed JAR file\n\n\nSyntax:\n\n\nSQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672))\n\n\n\n\n\n\n\n\nJAR_FILE_PATH  is full path for the JAR file. This path must be accessible to server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers.\n\n\n\n\n\n\nQUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.\n\n\n\n\n\n\nExample: Replacing a JAR\n\n\nCALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs')\n\n\n\n\nRemoving a JAR\n \n\n\nUse SQLJ.REMOVE_JAR  procedure to remove a JAR file\n\n\nSyntax:\n\n\nSQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER)\n\n\n\n\n\n\n\n\nQUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.\n\n\n\n\n\n\nUNDEPLOY: This argument is currently ignored.\n\n\n\n\n\n\nExample: Removing a JAR\n\n\nCALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)\n\n\n\n\nUsing SnappyData Shell\n\n\nThe SnappyData SQL Shell (\nsnappy-sql\n) provides a simple command line interface to the SnappyData cluster.\nIt allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. \nInternally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.\n\n\n// from the SnappyData base directory  \n$ cd quickstart/scripts  \n$ ../../bin/snappy-sql\nVersion 2.0-BETA\nsnappy\n \n\n//Connect to the cluster as a client  \nsnappy\n connect client 'localhost:1527'; //It connects to the locator.\n\n//Show active connections  \nsnappy\n show connections;\n\n//Display cluster members by querying a system table  \nsnappy\n select id, kind, status, host, port from sys.members;\n\n//or\nsnappy\n show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy\n run 'create_and_load_column_table.sql';\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy\n run 'create_and_load_row_table.sql';\n\n\n\n\nThe complete list of commands available through \nsnappy_shell\n can be found \nhere\n\n\nUsing the Spark Shell and spark-submit\n\n\nSnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. \nYou however may need to isolate the computational cluster for other reasons. For instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly.\n\n\nTo support such cases it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the \nspark.snappydata.connection\n property should be provided while starting the Spark-shell. \n\n\nTo run all SnappyData functionalities you need to create a \nSnappySession\n.\n\n\n// from the SnappyData base directory  \n# Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter.\n$ bin/spark-shell  --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041\nscala\n\n#Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql\nscala\n val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\nscala\n val airlineDF = snappy.table(\nairline\n).show\nscala\n val resultset = snappy.sql(\nselect * from airline\n)\nNext changed command:\n# Start the Spark standalone cluster from SnappyData base directory \n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\nAny Spark application can also use the SnappyData as store and Spark as a computational engine by providing an extra \nspark.snappydata.connection\n property in the conf.\n\n\n# Start the Spark standalone cluster from SnappyData base directory \n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar\n\n# The results can be seen on the command line.\n\n\n\n\nUsing JDBC with SnappyData\n\n\nSnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information, and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail. \n\n\n\n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection (\njdbc:snappydata://locatorHostName:1527/\n);\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint\n\n\n\n\n\n\nNote\n\n\nIf the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the \nio.snappydata.jdbc.ClientDriver\n class.\n\n\n\n\nMultiple Language Binding using Thrift Protocol\n\n\nSnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData.\nThrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the \nApache Thrift documentation\n.\n\n\nThe JDBC driver for SnappyData that uses the \njdbc:snappydata://\n URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore \njdbc:gemfirexd://\n continues to use the deprecated DRDA protocol.\n\n\nLikewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode (\nsnappy-start-all.sh rowstore\n) the DRDA servers are started as before.\n\n\nTo explicitly start a DRDA server in SnappyData, you can use the \n-drda-server-address\n and \n-drda-server-port\n options for the \nbind address\n and \nport\n respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the \n-thrift-server-address\n and \n-thrift-server-port\n options.\n\n\nRefer to the following documents for information on support provided by SnappyData:\n\n\n\n\n\n\nAbout SnappyData Thrift\n: Contains detailed information about the feature and it's capabilities.\n\n\n\n\n\n\nThe Thrift Interface Definition Language (IDL)\n: This is a Thrift interface definition file for the SnappyData service.\n\n\n\n\n\n\nExample\n:\n Example of the Thrift definitions using the SnappyData Thrift IDL.\n\n\n\n\n\n\nBuilding SnappyData Applications using Spark API\n\n\nSnappySession Usage\n\n\nCreate Columnar Tables using API\n\n\nOther than \ncreate\n and \ndrop\n table, rest are all based on the Spark SQL Data Source APIs.\n\n\nScala\n\n\n val props = Map(\nBUCKETS\n -\n \n2\n)// Number of partitions to use in the SnappyStore\n\n case class Data(COL1: Int, COL2: Int, COL3: Int)\n\n val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n val rdd = spark.sparkContext.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\n\n val df = snappy.createDataFrame(rdd)\n\n // create a column table\n snappy.dropTable(\nCOLUMN_TABLE\n, ifExists = true)\n\n // \ncolumn\n is the table format (that is row or column)\n // dataDF.schema provides the schema for table\n snappy.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, df.schema, props)\n // append dataDF into the table\n df.write.insertInto(\nCOLUMN_TABLE\n)\n\n val results = snappy.sql(\nSELECT * FROM COLUMN_TABLE\n)\n println(\ncontents of column table are:\n)\n results.foreach(r =\n println(r))\n\n\n\n\nJava\n\n\n Map\nString, String\n props1 = new HashMap\n();\n props1.put(\nbuckets\n, \n11\n);\n\n JavaRDD\nRow\n jrdd = jsc.parallelize(Arrays.asList(\n  RowFactory.create(1, 2, 3),\n  RowFactory.create(7, 8, 9),\n  RowFactory.create(9, 2, 3),\n  RowFactory.create(4, 2, 3),\n  RowFactory.create(5, 6, 7)\n ));\n\n StructType schema = new StructType(new StructField[]{\n  new StructField(\ncol1\n, DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField(\ncol2\n, DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField(\ncol3\n, DataTypes.IntegerType, false, Metadata.empty()),\n });\n\n Dataset\nRow\n df = snappy.createDataFrame(jrdd, schema);\n\n// create a column table\n snappy.dropTable(\nCOLUMN_TABLE\n, true);\n\n// \ncolumn\n is the table format (that is row or column)\n// dataDF.schema provides the schema for table\n snappy.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, df.schema(), props1, false);\n// append dataDF into the table\n df.write().insertInto(\nCOLUMN_TABLE\n);\n\n Dataset\nRow\n  results = snappy.sql(\nSELECT * FROM COLUMN_TABLE\n);\n System.out.println(\ncontents of column table are:\n);\n for (Row r : results.select(\ncol1\n, \ncol2\n, \ncol3\n). collectAsList()) {\n   System.out.println(r);\n }\n\n\n\n\nPython\n\n\nfrom pyspark.sql.types import *\n\ndata = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]\nrdd = sc.parallelize(data)\nschema=StructType([StructField(\ncol1\n, IntegerType()),\n                   StructField(\ncol2\n, IntegerType()),\n                   StructField(\ncol3\n, IntegerType())])\n\ndataDF = snappy.createDataFrame(rdd, schema)\n\n# create a column table\nsnappy.dropTable(\nCOLUMN_TABLE\n, True)\n#\ncolumn\n is the table format (that is row or column)\n#dataDF.schema provides the schema for table\nsnappy.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, dataDF.schema, True, buckets=\n11\n)\n\n#append dataDF into the table\ndataDF.write.insertInto(\nCOLUMN_TABLE\n)\nresults1 = snappy.sql(\nSELECT * FROM COLUMN_TABLE\n)\n\nprint(\ncontents of column table are:\n)\nresults1.select(\ncol1\n, \ncol2\n, \ncol3\n). show()\n\n\n\n\nThe optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. \nFor more details about the properties ('props1' map in above example) and \ncreateTable\n API refer to documentation for \nrow and column tables\n.\n\n\nCreate Row Tables using API, Update the Contents of Row Table\n\n\n// create a row format table called ROW_TABLE\nsnappy.dropTable(\nROW_TABLE\n, ifExists = true)\n// \nrow\n is the table format\n// dataDF.schema provides the schema for table\nval props2 = Map.empty[String, String]\nsnappy.createTable(\nROW_TABLE\n, \nrow\n, dataDF.schema, props2)\n\n// append dataDF into the data\ndataDF.write.insertInto(\nROW_TABLE\n)\n\nval results2 = snappy.sql(\nselect * from ROW_TABLE\n)\nprintln(\ncontents of row table are:\n)\nresults2.foreach(println)\n\n// row tables can be mutated\n// for example update \nROW_TABLE\n and set col3 to 99 where\n// criteria \ncol3 = 3\n is true using update API\nsnappy.update(\nROW_TABLE\n, \nCOL3 = 3\n, org.apache.spark.sql.Row(99), \nCOL3\n )\n\nval results3 = snappy.sql(\nSELECT * FROM ROW_TABLE\n)\nprintln(\ncontents of row table are after setting col3 = 99 are:\n)\nresults3.foreach(println)\n\n// update rows using sql update statement\nsnappy.sql(\nUPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\n)\nval results4 = snappy.sql(\nSELECT * FROM ROW_TABLE\n)\nprintln(\ncontents of row table are after setting col1 = 100 are:\n)\nresults4.foreach(println)\n\n\n\n\nSnappyStreamingContext Usage\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.\n\n\nBelow example shows how to use the \nSnappyStreamingContext\n to apply a schema to existing DStream and then query the \nSchemaDStream\n with simple SQL. It also shows the ability of the SnappyStreamingContext to deal with SQL queries.\n\n\nScala\n\n\n import org.apache.spark.sql._\n import org.apache.spark.streaming._\n import scala.collection.mutable\n import org.apache.spark.rdd._\n import org.apache.spark.sql.types._\n import scala.collection.immutable.Map\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n val schema = StructType(List(StructField(\nid\n, IntegerType) ,StructField(\ntext\n, StringType)))\n\n case class ShowCaseSchemaStream (loc:Int, text:String)\n\n snsc.snappyContext.dropTable(\nstreamingExample\n, ifExists = true)\n snsc.snappyContext.createTable(\nstreamingExample\n, \ncolumn\n,  schema, Map.empty[String, String] , false)\n\n def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =\n ShowCaseSchemaStream( i, s\nText$i\n))\n\n val dstream = snsc.queueStream[ShowCaseSchemaStream](\n                 mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))\n\n val schemaDStream = snsc.createSchemaDStream(dstream )\n\n schemaDStream.foreachDataFrame(df =\n {\n     df.write.format(\ncolumn\n).\n     mode(SaveMode.Append).\n     options(Map.empty[String, String]).\n     saveAsTable(\nstreamingExample\n)    })\n\n snsc.start()\n snsc.sql(\nselect count(*) from streamingExample\n).show\n\n\n\n\nJava\n\n\n StructType schema = new StructType(new StructField[]{\n     new StructField(\nid\n, DataTypes.IntegerType, false, Metadata.empty()),\n     new StructField(\ntext\n, DataTypes.StringType, false, Metadata.empty())\n });\n\n Map\nString, String\n props = Collections.emptyMap();\n jsnsc.snappySession().dropTable(\nstreamingExample\n, true);\n jsnsc.snappySession().createTable(\nstreamingExample\n, \ncolumn\n, schema, props, false);\n\n Queue\nJavaRDD\nShowCaseSchemaStream\n rddQueue = new LinkedList\n();// Define a JavaBean named ShowCaseSchemaStream\n rddQueue.add(rddList(jsc, 1, 10));\n rddQueue.add(rddList(jsc, 10, 20));\n rddQueue.add(rddList(jsc, 20, 30));\n\n //rddList methods is defined as\n/* private static JavaRDD\nShowCaseSchemaStream\n rddList(JavaSparkContext jsc, int start, int end){\n    List\nShowCaseSchemaStream\n objs = new ArrayList\n();\n      for(int i= start; i\n=end; i++){\n        objs.add(new ShowCaseSchemaStream(i, String.format(\nText %d\n,i)));\n      }\n    return jsc.parallelize(objs);\n }*/\n\n JavaDStream\nShowCaseSchemaStream\n dStream = jsnsc.queueStream(rddQueue);\n SchemaDStream schemaDStream = jsnsc.createSchemaDStream(dStream, ShowCaseSchemaStream.class);\n\n schemaDStream.foreachDataFrame(new VoidFunction\nDataset\nRow\n() {\n   @Override\n   public void call(Dataset\nRow\n df) {\n     df.write().insertInto(\nstreamingExample\n);\n   }\n });\n\n jsnsc.start();\n\n jsnsc.sql(\nselect count(*) from streamingExample\n).show();\n\n\n\n\nPython\n\n\nfrom pyspark.streaming.snappy.context import SnappyStreamingContext\nfrom pyspark.sql.types import *\n\ndef  rddList(start, end):\n  return sc.parallelize(range(start,  end)).map(lambda i : ( i, \nText\n + str(i)))\n\ndef saveFunction(df):\n   df.write.format(\ncolumn\n).mode(\nappend\n).saveAsTable(\nstreamingExample\n)\n\nschema=StructType([StructField(\nloc\n, IntegerType()),\n                   StructField(\ntext\n, StringType())])\n\nsnsc = SnappyStreamingContext(sc, 1)\n\ndstream = snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])\n\nsnsc._snappycontext.dropTable(\nstreamingExample\n , True)\nsnsc._snappycontext.createTable(\nstreamingExample\n, \ncolumn\n, schema)\n\nschemadstream = snsc.createSchemaDStream(dstream, schema)\nschemadstream.foreachDataFrame(lambda df: saveFunction(df))\nsnsc.start()\ntime.sleep(1)\nsnsc.sql(\nselect count(*) from streamingExample\n).show()\n\n\n\n\n\n\n\n\n\n\nTables in SnappyData\n\n\nRow and Column Tables\n\n\nColumn tables organize and manage data in memory in compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\nRow tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates.\n\n\nCreate table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.\n\n\nDDL and DML Syntax for Tables\n\n\nCREATE TABLE [IF NOT EXISTS] table_name\n   (\n  COLUMN_DEFININTION\n   )\nUSING row | column\nOPTIONS (\nCOLOCATE_WITH 'table_name',  // Default none\nPARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.\nBUCKETS  'NumPartitions', // Default 113\nREDUNDANCY        '1' ,\nEVICTION_BY \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,\nOVERFLOW 'true',\nPERSISTENCE  \u2018ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\nDISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore\nEXPIRE \u2018TIMETOLIVE in seconds',\nCOLUMN_BATCH_SIZE '32000000',\nCOLUMN_MAX_DELTA_ROWS '10000',\n)\n[AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name\n\n\n\n\nRefer to the \nHow-Tos\n section for more information on partitioning and collocating data.\n\n\nFor row format tables column definition can take underlying GemFireXD syntax to create a table. For example, note the PRIMARY KEY clause below.\n\n\nsnappy.sql(\nCREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT)\n         USING row options(BUCKETS '5')\n )\n\n\n\n\nFor column table it is restricted to Spark syntax for column definition\n\n\nsnappy.sql(\nCREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')\n )\n\n\n\n\nYou can also define complex types (Map, Array and StructType) as columns for column tables.\n\n\nsnappy.sql(\nCREATE TABLE tableName (\ncol1 INT , \ncol2 Array\nDecimal\n, \ncol3 Map\nTimestamp, Struct\nx: Int, y: String, z: Decimal(10,5)\n, \ncol6 Struct\na: Int, b: String, c: Decimal(10,5)\n\n) USING column options(BUCKETS '5')\n )\n\n\n\n\nTo access the complex data from JDBC you can see \nJDBCWithComplexTypes\n for examples.\n\n\n\n\nNote\n\n\nClauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.\n\n\n\n\nSpark API for Managing Tables\n\n\nGet a reference to \nSnappySession\n:\n\n\nval snappy: SnappySession = new SnappySession(spark.sparkContext)\n\n\n\nCreate a SnappyStore table using Spark APIs\n\n\nval props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section\ncase class Data(col1: Int, col2: Int, col3: Int)\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sparkContext.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\nval dataDF = snappy.createDataFrame(rdd)\nsnappy.createTable(\"column_table\", \"column\", dataDF.schema, props)\n//or create a row format table\nsnappy.createTable(\"row_table\", \"row\", dataDF.schema, props)\n\n\n\nDrop a SnappyStore table using Spark APIs\n:\n\n\nsnappy.dropTable(tableName, ifExists = true)\n\n\n\n\n\nDDL extensions to SnappyStore Tables\n\n\nThe below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions. \n\n\n\n\n\n\nCOLOCATE_WITH: The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist.\n\n\n\n\n\n\nPARTITION_BY: Use the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning. If not specified, it is a replicated table.\n Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally on a generated scheme.\n The default number of storage partitions (BUCKETS) is 113 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.\n\n\n\n\n\n\nBUCKETS: The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.\n\n\n\n\n\n\nREDUNDANCY: Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.\n\n\n\n\n\n\nEVICTION_BY: Use the EVICTION_BY clause to evict rows automatically from in-memory table, based on different criteria. \nFor column tables, the default eviction setting is LRUHEAPPERCENT and the default action is to overflow to disk. You can also specify the OVERFLOW parameter along with the EVICTION_BY clause. \n\n\n\n\nNote\n\n\nFor column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables no such defaults are set. Row tables allow all the eviction settings.\n\n\n\n\n\n\n\n\nOVERFLOW: If it is set to \nfalse\n the evicted rows are destroyed. If set to \ntrue\n it overflows to a local SnappyStore disk store.\n    When you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table).\n\n\n\n\n\n\nPERSISTENCE: When you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nBy default, both row and column tables are persistent.\n\n\n\n\n\n\nThe option \nPERSISTENT\n has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use \nPERSISTENCE\n instead.\n\n\n\n\n\n\n\n\n\n\n\n\nDISKSTORE: The disk directory where you want to persist the table data. For more information, \nrefer to this document\n.\n\n\n\n\n\n\nEXPIRE: You can use the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured TTL.\n\n\n\n\n\n\nCOLUMN_BATCH_SIZE: The default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M)\n\n\n\n\n\n\nCOLUMN_MAX_DELTA_ROWS: The maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by \nCOLUMN_BATCH_SIZE\n (and thus limits size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The default value is 10000. \n \n\n\n\n\nNote\n\n\nThe following corresponding SQLConf properties for \nCOLUMN_BATCH_SIZE\n and \nCOLUMN_MAX_DELTA_ROWS\n are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL): \n\n\n\n\n\n\nsnappydata.column.batchSize\n - explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit \nCOLUMN_BATCH_SIZE\n specification, then this is inherited for that table property. \n\n\n\n\n\n\nsnappydata.column.maxDeltaRows\n - maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit \nCOLUMN_MAX_DELTA_ROWS\n specification, then this is inherited for that table property. \n\n\n\n\n\n\n\n\n\n\n\n\nRefer to the \nSQL Reference Guide\n for information on the extensions.\n\n\nRestrictions on Column Tables\n\n\n\n\n\n\nColumn tables cannot specify any primary key, unique key constraints\n\n\n\n\n\n\nIndex on column table is not supported\n\n\n\n\n\n\nOption EXPIRE is not applicable for column tables\n\n\n\n\n\n\nOption EVICTION_BY with value LRUCOUNT is not applicable for column tables\n\n\n\n\n\n\nDML Operations on Tables\n\n\n    INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\n    INSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\n    INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\n    UPDATE tablename SET column = value [, column = value ...] [WHERE expression]\n    PUT INTO tableName (column, ...) VALUES (value, ...)\n    DELETE FROM tablename1 [WHERE expression]\n    TRUNCATE TABLE tablename1;\n\n\n\n\nAPI Extensions Provided in SnappyContext\n\n\nSeveral APIs have been added in \nSnappySession\n to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.\n\n\n    //  Applicable for both row and column tables\n    def insert(tableName: String, rows: Row*): Int .\n\n    // Only for row tables\n    def put(tableName: String, rows: Row*): Int\n    def update(tableName: String, filterExpr: String, newColumnValues: Row, \n               updateColumns: String*): Int\n    def delete(tableName: String, filterExpr: String): Int\n\n\n\n\nUsage SnappySession.insert()\n: Insert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\n    val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n                   Seq(5, 6, 7), Seq(1,100,200))\n    data.map { r =\n\n      snappy.insert(\ntableName\n, Row.fromSeq(r))\n    }\n\n\n\n\nUsage SnappySession.put()\n: Upsert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\n    val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n                   Seq(5, 6, 7), Seq(1,100,200))\n    data.map { r =\n\n      snappy.put(tableName, Row.fromSeq(r))\n    }\n\n\n\n\nUsage SnappySession.update()\n: Update all rows in table that match passed filter expression\n\n\n    snappy.update(tableName, \nITEMREF = 3\n , Row(99) , \nITEMREF\n )\n\n\n\n\nUsage SnappySession.delete()\n: Delete all rows in table that match passed filter expression\n\n\n    snappy.delete(tableName, \nITEMREF = 3\n)\n\n\n\n\n\n\n\nRow Buffers for Column Tables\n\n\nGenerally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored.\n\n\nIn SnappyData, the column table consists of two components, delta row buffer and column store. SnappyData tries to support individual insert of single row, as it is stored in a delta row buffer which is write optimized and highly available.\n\n\nOnce the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store.\nAny query on column table also takes into account the row cached buffer. By doing this, it ensures that the query does not miss any data.\n\n\nCatalog in SnappyStore\n\n\nPersistent Hive catalog for all metadata storage is used. All table, schema definition are stored here in a reliable manner. The product intends to quickly recover from driver failover, using GemFireXD itself to store meta information. This gives the ability to query underlying GemFireXD to reconstruct the meta store in case of a driver failover.\n\n\n\n\n\nSQL Reference to the Syntax\n\n\nRefer to the \nSQL Reference Guide\n for information on the syntax.\n\n\nStream processing using SQL\n\n\nSnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. \nHere is a brief overview of \nSpark streaming\n from the Spark Streaming guide. \n\n\nSpark Streaming Overview\n\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like \nmap\n, \nreduce\n, \njoin\n and \nwindow\n.\n\n\nFinally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's \nmachine learning\n and \ngraph processing\n algorithms on data streams.\n\n\n\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n\n\n\nSpark Streaming provides a high-level abstraction called \ndiscretized stream\n or \nDStream\n, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of \nRDDs\n.\u2028\n\n\nAdditional details on the Spark Streaming concepts and programming is covered \nhere\n.\n\n\nSnappyData Streaming Extensions over Spark\n\n\nThe following enhancements over Spark Streaming is provided: \n\n\n\n\n\n\nManage Streams declaratively\n: Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. \n\n\n\n\n\n\nSQL based stream processing\n: With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. \n\n\n\n\n\n\nContinuous queries and time windows\n: Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, the standard SQL is extended to be able to specify the time window for the query. \n\n\n\n\n\n\nOLAP optimizations\n: By integrating and collocating stream processing with the hybrid in-memory storage engine, the product leverages the optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with RowStore.\n\n\n\n\n\n\nApproximate stream analytics\n: When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.\n\n\n\n\n\n\nWorking with Stream Tables\n\n\nSnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.\n\n\n// DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFINITION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'\nOPTIONS (\n// multiple stream source specific options\n  storagelevel '',\n  rowConverter '',\n  topics '',\n  kafkaParams '',\n  consumerKey '',\n  consumerSecret '',\n  accessToken '',\n  accessTokenSecret '',\n  hostname '',\n  port '',\n  directory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT \nbatchInterval\n [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP\n\n\n\n\nFor example to create a stream table using kafka source : \n\n\n val spark: SparkSession = SparkSession\n     .builder\n     .appName(\nSparkApp\n)\n     .master(\nlocal[4]\n)\n     .getOrCreate\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n\n snsc.sql(\ncreate stream table streamTable (userId string, clickStreamLog string) \n +\n     \nusing kafka_stream options (\n +\n     \nstoragelevel 'MEMORY_AND_DISK_SER_2', \n +\n     \nrowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \n +\n     \nkafkaParams 'zookeeper.connect-\nlocalhost:2181;auto.offset.reset-\nsmallest;group.id-\nmyGroupId', \n +\n     \ntopics 'streamTopic:01')\n)\n\n // You can get a handle of underlying DStream of the table\n val dStream = snsc.getSchemaDStream(\nstreamTable\n)\n\n // You can also save the DataFrames to an external table\n dStream.foreachDataFrame(_.write.insertInto(tableName))\n\n\n\n\nThe streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.\n\n\nStream SQL through snappy-sql\n\n\nStart a SnappyData cluster and connect through snappy-sql :\n\n\n//create a connection\nsnappy\n connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy\n streaming init 2secs;\n\n// Create a stream table\nsnappy\n create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy\n streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy\n select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy\n drop table streamTable;\n\n// Stop the streaming\nsnappy\n streaming stop;\n\n\n\n\nSchemaDStream\n\n\nSchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table.\nSome of these ideas (especially naming our abstractions) were borrowed from \nIntel's Streaming SQL project\n.\n\n\nRegistering Continuous Queries\n\n\n//You can join two stream tables and produce a result stream.\nval resultStream = snsc.registerCQ(\nSELECT s1.id, s1.text FROM stream1 window (duration\n    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\n)\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto(\nyourTableName\n))\n\n\n\n\nDynamic (ad-hoc) Continuous Queries\n\n\nUnlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the \nSnappyStreamingContext\n has started.\n\n\nUser Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)\n\n\nUsers can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. \nThe definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.\n\n\n\n\nNote\n\n\nSupport for UDFs is available in SnappyData version 0.8 and future releases.\n\n\n\n\nCreate User Defined Function\n\n\nYou can simply extend any one of the interfaces in the package \norg.apache.spark.sql.api.java\n. \nThese interfaces can be included in your client application by adding \nsnappy-spark-sql_2.11-2.0.3-2.jar\n to your classpath.\n\n\nDefine a UDF class\n\n\nThe number in the interfaces (UDF1 to UDF22) signifies the number of parameters an UDF can take.\n\n\n\n\nNote\n\n\nCurrently, any UDF which can take more than 22 parameters is not supported.\n\n\n\n\npackage some.package\nimport org.apache.spark.sql.api.java.UDF1\n\nclass StringLengthUDF extends UDF1[String, Int] {\n override def call(t1: String): Int = t1.length\n}\n\n\n\n\n \n\n\nCreate the UDF Function\n\n\nAfter defining an UDF you can bundle the UDF class in a JAR file and create the function by using \n./bin/snappy-sql\n of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF.\n\n\nCREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'\n\n\n\n\nFor example:\n\n\nCREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'\n\n\n\n\nYou can write a JAVA or SCALA class to write an UDF implementation. \n\n\n\n\nNote\n\n\nFor input/output types: \n\nThe framework always returns the Java types to the UDFs. So, if you are writing \nscala.math.BigDecimal\n as an input type or output type, an exception is reported. You can use \njava.math.BigDecimal\n in the SCALA code. \n\n\n\n\nReturn Types to UDF program type mapping\n\n\n\n\n\n\n\n\nSnappyData Type\n\n\nUDF Type\n\n\n\n\n\n\n\n\n\n\nSTRING\n\n\njava.lang.String\n\n\n\n\n\n\nINTEGER\n\n\njava.lang.Integer\n\n\n\n\n\n\nLONG\n\n\njava.lang.Long\n\n\n\n\n\n\nDOUBLE\n\n\njava.lang.Double\n\n\n\n\n\n\nDECIMAL\n\n\njava.math.BigDecimal\n\n\n\n\n\n\nDATE\n\n\njava.sql.Date\n\n\n\n\n\n\nTIMESTAMP\n\n\njava.sql.Timestamp\n\n\n\n\n\n\nFLOAT\n\n\njava.lang.Float\n\n\n\n\n\n\nBOOLEAN\n\n\njava.lang.Boolean\n\n\n\n\n\n\nSHORT\n\n\njava.lang.Short\n\n\n\n\n\n\nBYTE\n\n\njava.lang.Byte\n\n\n\n\n\n\n\n\nUse the UDF\n\n\nselect strnglen(string_column) from \ntable\n\n\n\n\n\nIf you try to use an UDF on a different type of column, for example, an \nInt\n column an exception is reported.\n\n\nDrop the Function\n\n\nDROP FUNCTION IF EXISTS udf_name\n\n\n\n\nFor example:\n\n\nDROP FUNCTION IF EXISTS app.strnglen\n\n\n\n\nModify an Existing UDF\n\n\n1) Drop the existing UDF\n\n\n2) Modify the UDF code and \ncreate a new UDF\n. You can create the UDF with the same name as that of the dropped UDF.\n\n\nCreate User Defined Aggregate Functions\n\n\nSnappyData uses same interface as that of Spark to define a User Defined Aggregate Function  \norg.apache.spark.sql.expressions.UserDefinedAggregateFunction\n. For more information refer to this \ndocument\n.", 
            "title": "Programming Guide"
        }, 
        {
            "location": "/programming_guide/#overview", 
            "text": "SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). \nAll SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames. \nIt is therefore recommend that you understand the  concepts in SparkSQL  \nand the  DataFrame API . You can also store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced  here .  In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. \nData in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk.", 
            "title": "Overview"
        }, 
        {
            "location": "/programming_guide/#snappysession-and-snappystreamingcontext", 
            "text": "SnappySession  is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's  SparkSession  to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame.\nSimilarly,  SnappyStreamingContext  is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's Streaming Context .  Also SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see  SnappyData Spark Affinity modes .  If you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession.", 
            "title": "SnappySession and SnappyStreamingContext"
        }, 
        {
            "location": "/programming_guide/#to-create-a-snappysession", 
            "text": "Scala   val spark: SparkSession = SparkSession\n         .builder\n         .appName( SparkApp )\n         .master( master_url )\n         .getOrCreate\n\n val snappy = new SnappySession(spark.sparkContext)  Java   SparkSession spark = SparkSession\n       .builder()\n       .appName( SparkApp )\n       .master( master_url )\n       .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n SnappySession snappy = new SnappySession(spark.sparkContext());  Python   from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)", 
            "title": "To Create a SnappySession"
        }, 
        {
            "location": "/programming_guide/#to-create-a-snappystreamingcontext", 
            "text": "Scala   val spark: SparkSession = SparkSession\n         .builder\n         .appName( SparkApp )\n         .master( master_url )\n         .getOrCreate\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))  Java   SparkSession spark = SparkSession\n     .builder()\n     .appName( SparkApp )\n     .master( master_url )\n     .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n Duration batchDuration = Milliseconds.apply(500);\n JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext(jsc, batchDuration);  Python   from pyspark.streaming.snappy.context import SnappyStreamingContext\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n duration = .5\n snsc = SnappyStreamingContext(sc, duration)  If you are in the Embedded Mode, applications typically submit Jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext. \nThese jobs are the primary mechanism to interact with SnappyData using the Spark API. \nA job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.  The implementation of the  runSnappyJob  function from SnappySQLJob uses a SnappySession to interact with the SnappyData store to process and store tables.\nThe implementation of  runSnappyJob  from SnappyStreamingJob uses a SnappyStreamingContext to create streams and manage the streaming context.\nThe jobs are submitted to the lead node of SnappyData over REST API using a  spark-submit  like utility.", 
            "title": "To Create a SnappyStreamingContext"
        }, 
        {
            "location": "/programming_guide/#snappydata-jobs", 
            "text": "To create a job that can be submitted through the job server, the job must implement the  SnappySQLJob  or  SnappyStreamingJob  trait. Your job is displayed as:  Scala  class SnappySampleJob implements SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  def runSnappyJob(snappy: SnappySession, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(snappy: SnappySession, config: Config): SnappyJobValidation\n}  Java  class SnappySampleJob extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation}\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate}\n}  Scala  class SnappyStreamingSampleJob implements SnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation\n}  Java  class SnappyStreamingSampleJob extends JavaSnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)\n  {//validate}\n}   Note  The  Job  traits are simply extensions of the  SparkJob  implemented by  Spark JobServer .      runSnappyJob  contains the implementation of the Job.\nThe  SnappySession / SnappyStreamingContext  is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.    isValidJob  allows for an initial validation of the context and any provided configuration.\n    If the context and configuration can run the job, returning  spark.jobserver.SnappyJobValid  allows the job to execute, otherwise returning  spark.jobserver.SnappyJobInvalid reason  prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an \"HTTP/1.1 400 Bad Request\" status code. Validate helps you prevent running jobs that eventually fail due to a  missing or wrong configuration, and saves both time and resources.    See  examples  for Spark and Spark streaming jobs.   SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs.", 
            "title": "SnappyData Jobs"
        }, 
        {
            "location": "/programming_guide/#submitting-jobs", 
            "text": "The following command submits  CreateAndLoadAirlineDataJob . This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its  runJob  method. \nThe program is compiled into a jar file ( quickstart.jar ) and submitted to jobs server as shown below.  $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar  The utility  snappy-job.sh  submits the job and returns a JSON that has a Job Id of this job.    --lead : Specifies the host name of the lead node along with the port on which it accepts jobs (8090)    --app-name : Specifies the name given to the submitted application    --class : Specifies the name of the class that contains implementation of the Spark job to be run    --app-jar : Specifies the jar file that packages the code for Spark job    The status returned by the utility is displayed below:  {\n   status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  }\n}  This Job ID can be used to query the status of the running job.   $ bin/snappy-job.sh status  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{\n   duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}  Once the tables are created, they can be queried by running another job. Please refer to  AirlineDataJob  for implementing the job.   $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar  The status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results.", 
            "title": "Submitting Jobs"
        }, 
        {
            "location": "/programming_guide/#running-python-applications", 
            "text": "Python users can submit a Python application using  spark-submit  in the SnappyData Connector mode. For example, run the command given below to submit a Python application:  bin/spark-submit \\\n    --master local[*]  \\\n    --conf snappydata.connection=localhost:1527 \\\n    --conf spark.ui.port=4042  quickstart/python/AirlineDataPythonApp.py  snappydata.connection  property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster.", 
            "title": "Running Python Applications"
        }, 
        {
            "location": "/programming_guide/#streaming-jobs", 
            "text": "An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying  --stream  as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. \nAlternatively, you can specify the name of an existing/pre-created streaming context as  --context  context-name  with the  submit  command.  For example,  TwitterPopularTagsJob  can be submitted as follows. \nThis job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets.  $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\\n    --stream\n\n{\n   status :  STARTED ,\n   result : {\n     jobId :  982ac142-3550-41e1-aace-6987cb39fec8 ,\n     context :  snappyStreamingContext1463987084945028747 \n  }\n}  To start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context.  $ bin/snappy-job.sh stop  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 982ac142-3550-41e1-aace-6987cb39fec8\n\n$ bin/snappy-job.sh listcontexts  \\\n    --lead hostNameOfLead:8090\n[ snappyContext1452598154529305363 ,  snappyStreamingContext1463987084945028747 ,  snappyStreamingContext ]\n\n$ bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \\\n    --lead hostNameOfLead:8090", 
            "title": "Streaming Jobs"
        }, 
        {
            "location": "/programming_guide/#managing-jar-files", 
            "text": "SnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster.  Installing a JAR    Use SQLJ.INSTALL_JAR procedure to install a JAR file  Syntax:  SQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER)    JAR_FILE_PATH  is the full path for the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers    QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.    DEPLOY: This argument is currently ignored.    Example: Installing a JAR  snappy  call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0);  Replacing a JAR    Use  SQLJ.REPLACE_JAR procedure to replace an installed JAR file  Syntax:  SQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672))    JAR_FILE_PATH  is full path for the JAR file. This path must be accessible to server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers.    QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.    Example: Replacing a JAR  CALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs')  Removing a JAR    Use SQLJ.REMOVE_JAR  procedure to remove a JAR file  Syntax:  SQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER)    QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.    UNDEPLOY: This argument is currently ignored.    Example: Removing a JAR  CALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)", 
            "title": "Managing JAR Files"
        }, 
        {
            "location": "/programming_guide/#using-snappydata-shell", 
            "text": "The SnappyData SQL Shell ( snappy-sql ) provides a simple command line interface to the SnappyData cluster.\nIt allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. \nInternally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.  // from the SnappyData base directory  \n$ cd quickstart/scripts  \n$ ../../bin/snappy-sql\nVersion 2.0-BETA\nsnappy  \n\n//Connect to the cluster as a client  \nsnappy  connect client 'localhost:1527'; //It connects to the locator.\n\n//Show active connections  \nsnappy  show connections;\n\n//Display cluster members by querying a system table  \nsnappy  select id, kind, status, host, port from sys.members;\n\n//or\nsnappy  show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy  run 'create_and_load_column_table.sql';\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy  run 'create_and_load_row_table.sql';  The complete list of commands available through  snappy_shell  can be found  here", 
            "title": "Using SnappyData Shell"
        }, 
        {
            "location": "/programming_guide/#using-the-spark-shell-and-spark-submit", 
            "text": "SnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. \nYou however may need to isolate the computational cluster for other reasons. For instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly.  To support such cases it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the  spark.snappydata.connection  property should be provided while starting the Spark-shell.   To run all SnappyData functionalities you need to create a  SnappySession .  // from the SnappyData base directory  \n# Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter.\n$ bin/spark-shell  --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041\nscala \n#Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql\nscala  val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\nscala  val airlineDF = snappy.table( airline ).show\nscala  val resultset = snappy.sql( select * from airline )\nNext changed command:\n# Start the Spark standalone cluster from SnappyData base directory \n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar  Any Spark application can also use the SnappyData as store and Spark as a computational engine by providing an extra  spark.snappydata.connection  property in the conf.  # Start the Spark standalone cluster from SnappyData base directory \n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar\n\n# The results can be seen on the command line.", 
            "title": "Using the Spark Shell and spark-submit"
        }, 
        {
            "location": "/programming_guide/#using-jdbc-with-snappydata", 
            "text": "SnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information, and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail.   \n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection ( jdbc:snappydata://locatorHostName:1527/ );\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint   Note  If the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the  io.snappydata.jdbc.ClientDriver  class.", 
            "title": "Using JDBC with SnappyData"
        }, 
        {
            "location": "/programming_guide/#multiple-language-binding-using-thrift-protocol", 
            "text": "SnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData.\nThrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the  Apache Thrift documentation .  The JDBC driver for SnappyData that uses the  jdbc:snappydata://  URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore  jdbc:gemfirexd://  continues to use the deprecated DRDA protocol.  Likewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode ( snappy-start-all.sh rowstore ) the DRDA servers are started as before.  To explicitly start a DRDA server in SnappyData, you can use the  -drda-server-address  and  -drda-server-port  options for the  bind address  and  port  respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the  -thrift-server-address  and  -thrift-server-port  options.  Refer to the following documents for information on support provided by SnappyData:    About SnappyData Thrift : Contains detailed information about the feature and it's capabilities.    The Thrift Interface Definition Language (IDL) : This is a Thrift interface definition file for the SnappyData service.    Example :\n Example of the Thrift definitions using the SnappyData Thrift IDL.", 
            "title": "Multiple Language Binding using Thrift Protocol"
        }, 
        {
            "location": "/programming_guide/#building-snappydata-applications-using-spark-api", 
            "text": "", 
            "title": "Building SnappyData Applications using Spark API"
        }, 
        {
            "location": "/programming_guide/#snappysession-usage", 
            "text": "", 
            "title": "SnappySession Usage"
        }, 
        {
            "location": "/programming_guide/#create-columnar-tables-using-api", 
            "text": "Other than  create  and  drop  table, rest are all based on the Spark SQL Data Source APIs.", 
            "title": "Create Columnar Tables using API"
        }, 
        {
            "location": "/programming_guide/#scala", 
            "text": "val props = Map( BUCKETS  -   2 )// Number of partitions to use in the SnappyStore\n\n case class Data(COL1: Int, COL2: Int, COL3: Int)\n\n val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n val rdd = spark.sparkContext.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\n\n val df = snappy.createDataFrame(rdd)\n\n // create a column table\n snappy.dropTable( COLUMN_TABLE , ifExists = true)\n\n //  column  is the table format (that is row or column)\n // dataDF.schema provides the schema for table\n snappy.createTable( COLUMN_TABLE ,  column , df.schema, props)\n // append dataDF into the table\n df.write.insertInto( COLUMN_TABLE )\n\n val results = snappy.sql( SELECT * FROM COLUMN_TABLE )\n println( contents of column table are: )\n results.foreach(r =  println(r))", 
            "title": "Scala"
        }, 
        {
            "location": "/programming_guide/#java", 
            "text": "Map String, String  props1 = new HashMap ();\n props1.put( buckets ,  11 );\n\n JavaRDD Row  jrdd = jsc.parallelize(Arrays.asList(\n  RowFactory.create(1, 2, 3),\n  RowFactory.create(7, 8, 9),\n  RowFactory.create(9, 2, 3),\n  RowFactory.create(4, 2, 3),\n  RowFactory.create(5, 6, 7)\n ));\n\n StructType schema = new StructType(new StructField[]{\n  new StructField( col1 , DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField( col2 , DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField( col3 , DataTypes.IntegerType, false, Metadata.empty()),\n });\n\n Dataset Row  df = snappy.createDataFrame(jrdd, schema);\n\n// create a column table\n snappy.dropTable( COLUMN_TABLE , true);\n\n//  column  is the table format (that is row or column)\n// dataDF.schema provides the schema for table\n snappy.createTable( COLUMN_TABLE ,  column , df.schema(), props1, false);\n// append dataDF into the table\n df.write().insertInto( COLUMN_TABLE );\n\n Dataset Row   results = snappy.sql( SELECT * FROM COLUMN_TABLE );\n System.out.println( contents of column table are: );\n for (Row r : results.select( col1 ,  col2 ,  col3 ). collectAsList()) {\n   System.out.println(r);\n }", 
            "title": "Java"
        }, 
        {
            "location": "/programming_guide/#python", 
            "text": "from pyspark.sql.types import *\n\ndata = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]\nrdd = sc.parallelize(data)\nschema=StructType([StructField( col1 , IntegerType()),\n                   StructField( col2 , IntegerType()),\n                   StructField( col3 , IntegerType())])\n\ndataDF = snappy.createDataFrame(rdd, schema)\n\n# create a column table\nsnappy.dropTable( COLUMN_TABLE , True)\n# column  is the table format (that is row or column)\n#dataDF.schema provides the schema for table\nsnappy.createTable( COLUMN_TABLE ,  column , dataDF.schema, True, buckets= 11 )\n\n#append dataDF into the table\ndataDF.write.insertInto( COLUMN_TABLE )\nresults1 = snappy.sql( SELECT * FROM COLUMN_TABLE )\n\nprint( contents of column table are: )\nresults1.select( col1 ,  col2 ,  col3 ). show()  The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. \nFor more details about the properties ('props1' map in above example) and  createTable  API refer to documentation for  row and column tables .", 
            "title": "Python"
        }, 
        {
            "location": "/programming_guide/#create-row-tables-using-api-update-the-contents-of-row-table", 
            "text": "// create a row format table called ROW_TABLE\nsnappy.dropTable( ROW_TABLE , ifExists = true)\n//  row  is the table format\n// dataDF.schema provides the schema for table\nval props2 = Map.empty[String, String]\nsnappy.createTable( ROW_TABLE ,  row , dataDF.schema, props2)\n\n// append dataDF into the data\ndataDF.write.insertInto( ROW_TABLE )\n\nval results2 = snappy.sql( select * from ROW_TABLE )\nprintln( contents of row table are: )\nresults2.foreach(println)\n\n// row tables can be mutated\n// for example update  ROW_TABLE  and set col3 to 99 where\n// criteria  col3 = 3  is true using update API\nsnappy.update( ROW_TABLE ,  COL3 = 3 , org.apache.spark.sql.Row(99),  COL3  )\n\nval results3 = snappy.sql( SELECT * FROM ROW_TABLE )\nprintln( contents of row table are after setting col3 = 99 are: )\nresults3.foreach(println)\n\n// update rows using sql update statement\nsnappy.sql( UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99 )\nval results4 = snappy.sql( SELECT * FROM ROW_TABLE )\nprintln( contents of row table are after setting col1 = 100 are: )\nresults4.foreach(println)", 
            "title": "Create Row Tables using API, Update the Contents of Row Table"
        }, 
        {
            "location": "/programming_guide/#snappystreamingcontext-usage", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.  Below example shows how to use the  SnappyStreamingContext  to apply a schema to existing DStream and then query the  SchemaDStream  with simple SQL. It also shows the ability of the SnappyStreamingContext to deal with SQL queries.", 
            "title": "SnappyStreamingContext Usage"
        }, 
        {
            "location": "/programming_guide/#scala_1", 
            "text": "import org.apache.spark.sql._\n import org.apache.spark.streaming._\n import scala.collection.mutable\n import org.apache.spark.rdd._\n import org.apache.spark.sql.types._\n import scala.collection.immutable.Map\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n val schema = StructType(List(StructField( id , IntegerType) ,StructField( text , StringType)))\n\n case class ShowCaseSchemaStream (loc:Int, text:String)\n\n snsc.snappyContext.dropTable( streamingExample , ifExists = true)\n snsc.snappyContext.createTable( streamingExample ,  column ,  schema, Map.empty[String, String] , false)\n\n def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =  ShowCaseSchemaStream( i, s Text$i ))\n\n val dstream = snsc.queueStream[ShowCaseSchemaStream](\n                 mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))\n\n val schemaDStream = snsc.createSchemaDStream(dstream )\n\n schemaDStream.foreachDataFrame(df =  {\n     df.write.format( column ).\n     mode(SaveMode.Append).\n     options(Map.empty[String, String]).\n     saveAsTable( streamingExample )    })\n\n snsc.start()\n snsc.sql( select count(*) from streamingExample ).show", 
            "title": "Scala"
        }, 
        {
            "location": "/programming_guide/#java_1", 
            "text": "StructType schema = new StructType(new StructField[]{\n     new StructField( id , DataTypes.IntegerType, false, Metadata.empty()),\n     new StructField( text , DataTypes.StringType, false, Metadata.empty())\n });\n\n Map String, String  props = Collections.emptyMap();\n jsnsc.snappySession().dropTable( streamingExample , true);\n jsnsc.snappySession().createTable( streamingExample ,  column , schema, props, false);\n\n Queue JavaRDD ShowCaseSchemaStream  rddQueue = new LinkedList ();// Define a JavaBean named ShowCaseSchemaStream\n rddQueue.add(rddList(jsc, 1, 10));\n rddQueue.add(rddList(jsc, 10, 20));\n rddQueue.add(rddList(jsc, 20, 30));\n\n //rddList methods is defined as\n/* private static JavaRDD ShowCaseSchemaStream  rddList(JavaSparkContext jsc, int start, int end){\n    List ShowCaseSchemaStream  objs = new ArrayList ();\n      for(int i= start; i =end; i++){\n        objs.add(new ShowCaseSchemaStream(i, String.format( Text %d ,i)));\n      }\n    return jsc.parallelize(objs);\n }*/\n\n JavaDStream ShowCaseSchemaStream  dStream = jsnsc.queueStream(rddQueue);\n SchemaDStream schemaDStream = jsnsc.createSchemaDStream(dStream, ShowCaseSchemaStream.class);\n\n schemaDStream.foreachDataFrame(new VoidFunction Dataset Row () {\n   @Override\n   public void call(Dataset Row  df) {\n     df.write().insertInto( streamingExample );\n   }\n });\n\n jsnsc.start();\n\n jsnsc.sql( select count(*) from streamingExample ).show();", 
            "title": "Java"
        }, 
        {
            "location": "/programming_guide/#python_1", 
            "text": "from pyspark.streaming.snappy.context import SnappyStreamingContext\nfrom pyspark.sql.types import *\n\ndef  rddList(start, end):\n  return sc.parallelize(range(start,  end)).map(lambda i : ( i,  Text  + str(i)))\n\ndef saveFunction(df):\n   df.write.format( column ).mode( append ).saveAsTable( streamingExample )\n\nschema=StructType([StructField( loc , IntegerType()),\n                   StructField( text , StringType())])\n\nsnsc = SnappyStreamingContext(sc, 1)\n\ndstream = snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])\n\nsnsc._snappycontext.dropTable( streamingExample  , True)\nsnsc._snappycontext.createTable( streamingExample ,  column , schema)\n\nschemadstream = snsc.createSchemaDStream(dstream, schema)\nschemadstream.foreachDataFrame(lambda df: saveFunction(df))\nsnsc.start()\ntime.sleep(1)\nsnsc.sql( select count(*) from streamingExample ).show()", 
            "title": "Python"
        }, 
        {
            "location": "/programming_guide/#tables-in-snappydata", 
            "text": "", 
            "title": "Tables in SnappyData"
        }, 
        {
            "location": "/programming_guide/#row-and-column-tables", 
            "text": "Column tables organize and manage data in memory in compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates.  Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.", 
            "title": "Row and Column Tables"
        }, 
        {
            "location": "/programming_guide/#ddl-and-dml-syntax-for-tables", 
            "text": "CREATE TABLE [IF NOT EXISTS] table_name\n   (\n  COLUMN_DEFININTION\n   )\nUSING row | column\nOPTIONS (\nCOLOCATE_WITH 'table_name',  // Default none\nPARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.\nBUCKETS  'NumPartitions', // Default 113\nREDUNDANCY        '1' ,\nEVICTION_BY \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,\nOVERFLOW 'true',\nPERSISTENCE  \u2018ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\nDISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore\nEXPIRE \u2018TIMETOLIVE in seconds',\nCOLUMN_BATCH_SIZE '32000000',\nCOLUMN_MAX_DELTA_ROWS '10000',\n)\n[AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name  Refer to the  How-Tos  section for more information on partitioning and collocating data.  For row format tables column definition can take underlying GemFireXD syntax to create a table. For example, note the PRIMARY KEY clause below.  snappy.sql( CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT)\n         USING row options(BUCKETS '5')  )  For column table it is restricted to Spark syntax for column definition  snappy.sql( CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')  )  You can also define complex types (Map, Array and StructType) as columns for column tables.  snappy.sql( CREATE TABLE tableName (\ncol1 INT , \ncol2 Array Decimal , \ncol3 Map Timestamp, Struct x: Int, y: String, z: Decimal(10,5) , \ncol6 Struct a: Int, b: String, c: Decimal(10,5) \n) USING column options(BUCKETS '5')  )  To access the complex data from JDBC you can see  JDBCWithComplexTypes  for examples.   Note  Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.", 
            "title": "DDL and DML Syntax for Tables"
        }, 
        {
            "location": "/programming_guide/#spark-api-for-managing-tables", 
            "text": "Get a reference to  SnappySession :  val snappy: SnappySession = new SnappySession(spark.sparkContext)  Create a SnappyStore table using Spark APIs  val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section\ncase class Data(col1: Int, col2: Int, col3: Int)\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sparkContext.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\nval dataDF = snappy.createDataFrame(rdd)\nsnappy.createTable(\"column_table\", \"column\", dataDF.schema, props)\n//or create a row format table\nsnappy.createTable(\"row_table\", \"row\", dataDF.schema, props)  Drop a SnappyStore table using Spark APIs :  snappy.dropTable(tableName, ifExists = true)", 
            "title": "Spark API for Managing Tables"
        }, 
        {
            "location": "/programming_guide/#ddl-extensions-to-snappystore-tables", 
            "text": "The below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions.     COLOCATE_WITH: The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist.    PARTITION_BY: Use the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning. If not specified, it is a replicated table.  Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally on a generated scheme.  The default number of storage partitions (BUCKETS) is 113 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.    BUCKETS: The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.    REDUNDANCY: Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.    EVICTION_BY: Use the EVICTION_BY clause to evict rows automatically from in-memory table, based on different criteria.  For column tables, the default eviction setting is LRUHEAPPERCENT and the default action is to overflow to disk. You can also specify the OVERFLOW parameter along with the EVICTION_BY clause.    Note  For column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables no such defaults are set. Row tables allow all the eviction settings.     OVERFLOW: If it is set to  false  the evicted rows are destroyed. If set to  true  it overflows to a local SnappyStore disk store.\n    When you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table).    PERSISTENCE: When you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.     Note    By default, both row and column tables are persistent.    The option  PERSISTENT  has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use  PERSISTENCE  instead.       DISKSTORE: The disk directory where you want to persist the table data. For more information,  refer to this document .    EXPIRE: You can use the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured TTL.    COLUMN_BATCH_SIZE: The default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M)    COLUMN_MAX_DELTA_ROWS: The maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by  COLUMN_BATCH_SIZE  (and thus limits size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The default value is 10000.      Note  The following corresponding SQLConf properties for  COLUMN_BATCH_SIZE  and  COLUMN_MAX_DELTA_ROWS  are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL):     snappydata.column.batchSize  - explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit  COLUMN_BATCH_SIZE  specification, then this is inherited for that table property.     snappydata.column.maxDeltaRows  - maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit  COLUMN_MAX_DELTA_ROWS  specification, then this is inherited for that table property.        Refer to the  SQL Reference Guide  for information on the extensions.", 
            "title": "DDL extensions to SnappyStore Tables"
        }, 
        {
            "location": "/programming_guide/#restrictions-on-column-tables", 
            "text": "Column tables cannot specify any primary key, unique key constraints    Index on column table is not supported    Option EXPIRE is not applicable for column tables    Option EVICTION_BY with value LRUCOUNT is not applicable for column tables", 
            "title": "Restrictions on Column Tables"
        }, 
        {
            "location": "/programming_guide/#dml-operations-on-tables", 
            "text": "INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\n    INSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\n    INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\n    UPDATE tablename SET column = value [, column = value ...] [WHERE expression]\n    PUT INTO tableName (column, ...) VALUES (value, ...)\n    DELETE FROM tablename1 [WHERE expression]\n    TRUNCATE TABLE tablename1;", 
            "title": "DML Operations on Tables"
        }, 
        {
            "location": "/programming_guide/#api-extensions-provided-in-snappycontext", 
            "text": "Several APIs have been added in  SnappySession  to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.      //  Applicable for both row and column tables\n    def insert(tableName: String, rows: Row*): Int .\n\n    // Only for row tables\n    def put(tableName: String, rows: Row*): Int\n    def update(tableName: String, filterExpr: String, newColumnValues: Row, \n               updateColumns: String*): Int\n    def delete(tableName: String, filterExpr: String): Int  Usage SnappySession.insert() : Insert one or more [[org.apache.spark.sql.Row]] into an existing table      val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n                   Seq(5, 6, 7), Seq(1,100,200))\n    data.map { r = \n      snappy.insert( tableName , Row.fromSeq(r))\n    }  Usage SnappySession.put() : Upsert one or more [[org.apache.spark.sql.Row]] into an existing table      val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n                   Seq(5, 6, 7), Seq(1,100,200))\n    data.map { r = \n      snappy.put(tableName, Row.fromSeq(r))\n    }  Usage SnappySession.update() : Update all rows in table that match passed filter expression      snappy.update(tableName,  ITEMREF = 3  , Row(99) ,  ITEMREF  )  Usage SnappySession.delete() : Delete all rows in table that match passed filter expression      snappy.delete(tableName,  ITEMREF = 3 )", 
            "title": "API Extensions Provided in SnappyContext"
        }, 
        {
            "location": "/programming_guide/#row-buffers-for-column-tables", 
            "text": "Generally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored.  In SnappyData, the column table consists of two components, delta row buffer and column store. SnappyData tries to support individual insert of single row, as it is stored in a delta row buffer which is write optimized and highly available.  Once the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store.\nAny query on column table also takes into account the row cached buffer. By doing this, it ensures that the query does not miss any data.", 
            "title": "Row Buffers for Column Tables"
        }, 
        {
            "location": "/programming_guide/#catalog-in-snappystore", 
            "text": "Persistent Hive catalog for all metadata storage is used. All table, schema definition are stored here in a reliable manner. The product intends to quickly recover from driver failover, using GemFireXD itself to store meta information. This gives the ability to query underlying GemFireXD to reconstruct the meta store in case of a driver failover.", 
            "title": "Catalog in SnappyStore"
        }, 
        {
            "location": "/programming_guide/#sql-reference-to-the-syntax", 
            "text": "Refer to the  SQL Reference Guide  for information on the syntax.", 
            "title": "SQL Reference to the Syntax"
        }, 
        {
            "location": "/programming_guide/#stream-processing-using-sql", 
            "text": "SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. \nHere is a brief overview of  Spark streaming  from the Spark Streaming guide.", 
            "title": "Stream processing using SQL"
        }, 
        {
            "location": "/programming_guide/#spark-streaming-overview", 
            "text": "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like  map ,  reduce ,  join  and  window .  Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's  machine learning  and  graph processing  algorithms on data streams.   Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.   Spark Streaming provides a high-level abstraction called  discretized stream  or  DStream , which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of  RDDs .\u2028  Additional details on the Spark Streaming concepts and programming is covered  here .", 
            "title": "Spark Streaming Overview"
        }, 
        {
            "location": "/programming_guide/#snappydata-streaming-extensions-over-spark", 
            "text": "The following enhancements over Spark Streaming is provided:     Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions.     SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams.     Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, the standard SQL is extended to be able to specify the time window for the query.     OLAP optimizations : By integrating and collocating stream processing with the hybrid in-memory storage engine, the product leverages the optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with RowStore.    Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.", 
            "title": "SnappyData Streaming Extensions over Spark"
        }, 
        {
            "location": "/programming_guide/#working-with-stream-tables", 
            "text": "SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.  // DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFINITION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'\nOPTIONS (\n// multiple stream source specific options\n  storagelevel '',\n  rowConverter '',\n  topics '',\n  kafkaParams '',\n  consumerKey '',\n  consumerSecret '',\n  accessToken '',\n  accessTokenSecret '',\n  hostname '',\n  port '',\n  directory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT  batchInterval  [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP  For example to create a stream table using kafka source :    val spark: SparkSession = SparkSession\n     .builder\n     .appName( SparkApp )\n     .master( local[4] )\n     .getOrCreate\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n\n snsc.sql( create stream table streamTable (userId string, clickStreamLog string)   +\n      using kafka_stream options (  +\n      storagelevel 'MEMORY_AND_DISK_SER_2',   +\n      rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter',   +\n      kafkaParams 'zookeeper.connect- localhost:2181;auto.offset.reset- smallest;group.id- myGroupId',   +\n      topics 'streamTopic:01') )\n\n // You can get a handle of underlying DStream of the table\n val dStream = snsc.getSchemaDStream( streamTable )\n\n // You can also save the DataFrames to an external table\n dStream.foreachDataFrame(_.write.insertInto(tableName))  The streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.", 
            "title": "Working with Stream Tables"
        }, 
        {
            "location": "/programming_guide/#stream-sql-through-snappy-sql", 
            "text": "Start a SnappyData cluster and connect through snappy-sql :  //create a connection\nsnappy  connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy  streaming init 2secs;\n\n// Create a stream table\nsnappy  create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy  streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy  select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy  drop table streamTable;\n\n// Stop the streaming\nsnappy  streaming stop;", 
            "title": "Stream SQL through snappy-sql"
        }, 
        {
            "location": "/programming_guide/#schemadstream", 
            "text": "SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table.\nSome of these ideas (especially naming our abstractions) were borrowed from  Intel's Streaming SQL project .", 
            "title": "SchemaDStream"
        }, 
        {
            "location": "/programming_guide/#registering-continuous-queries", 
            "text": "//You can join two stream tables and produce a result stream.\nval resultStream = snsc.registerCQ( SELECT s1.id, s1.text FROM stream1 window (duration\n    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id )\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto( yourTableName ))", 
            "title": "Registering Continuous Queries"
        }, 
        {
            "location": "/programming_guide/#dynamic-ad-hoc-continuous-queries", 
            "text": "Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the  SnappyStreamingContext  has started.", 
            "title": "Dynamic (ad-hoc) Continuous Queries"
        }, 
        {
            "location": "/programming_guide/#user-defined-functions-udf-and-user-defined-aggregate-functions-udaf", 
            "text": "Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. \nThe definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.   Note  Support for UDFs is available in SnappyData version 0.8 and future releases.", 
            "title": "User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)"
        }, 
        {
            "location": "/programming_guide/#create-user-defined-function", 
            "text": "You can simply extend any one of the interfaces in the package  org.apache.spark.sql.api.java . \nThese interfaces can be included in your client application by adding  snappy-spark-sql_2.11-2.0.3-2.jar  to your classpath.", 
            "title": "Create User Defined Function"
        }, 
        {
            "location": "/programming_guide/#define-a-udf-class", 
            "text": "The number in the interfaces (UDF1 to UDF22) signifies the number of parameters an UDF can take.   Note  Currently, any UDF which can take more than 22 parameters is not supported.   package some.package\nimport org.apache.spark.sql.api.java.UDF1\n\nclass StringLengthUDF extends UDF1[String, Int] {\n override def call(t1: String): Int = t1.length\n}", 
            "title": "Define a UDF class"
        }, 
        {
            "location": "/programming_guide/#create-the-udf-function", 
            "text": "After defining an UDF you can bundle the UDF class in a JAR file and create the function by using  ./bin/snappy-sql  of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF.  CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'  For example:  CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'  You can write a JAVA or SCALA class to write an UDF implementation.    Note  For input/output types:  \nThe framework always returns the Java types to the UDFs. So, if you are writing  scala.math.BigDecimal  as an input type or output type, an exception is reported. You can use  java.math.BigDecimal  in the SCALA code.    Return Types to UDF program type mapping     SnappyData Type  UDF Type      STRING  java.lang.String    INTEGER  java.lang.Integer    LONG  java.lang.Long    DOUBLE  java.lang.Double    DECIMAL  java.math.BigDecimal    DATE  java.sql.Date    TIMESTAMP  java.sql.Timestamp    FLOAT  java.lang.Float    BOOLEAN  java.lang.Boolean    SHORT  java.lang.Short    BYTE  java.lang.Byte", 
            "title": "Create the UDF Function"
        }, 
        {
            "location": "/programming_guide/#use-the-udf", 
            "text": "select strnglen(string_column) from  table   If you try to use an UDF on a different type of column, for example, an  Int  column an exception is reported.", 
            "title": "Use the UDF"
        }, 
        {
            "location": "/programming_guide/#drop-the-function", 
            "text": "DROP FUNCTION IF EXISTS udf_name  For example:  DROP FUNCTION IF EXISTS app.strnglen", 
            "title": "Drop the Function"
        }, 
        {
            "location": "/programming_guide/#modify-an-existing-udf", 
            "text": "1) Drop the existing UDF  2) Modify the UDF code and  create a new UDF . You can create the UDF with the same name as that of the dropped UDF.", 
            "title": "Modify an Existing UDF"
        }, 
        {
            "location": "/programming_guide/#create-user-defined-aggregate-functions", 
            "text": "SnappyData uses same interface as that of Spark to define a User Defined Aggregate Function   org.apache.spark.sql.expressions.UserDefinedAggregateFunction . For more information refer to this  document .", 
            "title": "Create User Defined Aggregate Functions"
        }, 
        {
            "location": "/deployment/", 
            "text": "Overview\n\n\nIn this section, the various modes available for collocation of related data and computation is discussed.\n\n\nYou can run the SnappyData store in the following modes:\n\n\n\n\n\n\nLocal Mode\n: Used mainly for development, where the client application, the executors, and data store are all running in the same JVM\n\n\n\n\n\n\nEmbedded SnappyData Store Mode\n: The Spark computations and in-memory data store run collocated in the same JVM\n\n\n\n\n\n\nSnappyData Smart Connector Mode\n: Allows you to work with the SnappyData store cluster from any compatible Spark distribution\n\n\n\n\n\n\n\n\nLocal Mode\n\n\nIn this mode, you can execute all the components (client application, executors, and data store) locally in the application's JVM. It is the simplest way to start testing and using SnappyData, as you do not require a cluster, and the  executor threads are launched locally for processing.\n\n\nKey Points\n\n\n\n\n\n\nNo cluster required\n\n\n\n\n\n\nLaunch Single JVM (Single-node Cluster)\n\n\n\n\n\n\nLaunches executor threads locally for processing\n\n\n\n\n\n\nEmbeds the SnappyData in-memory store in-process\n\n\n\n\n\n\nFor development purposes only\n\n\n\n\n\n\n\n\nExample: Using the Local mode for developing SnappyData programs\n\n\nYou can use an IDE of your choice, and provide the below dependency to get SnappyData binaries:\n\n\nExample: Maven dependency\n\n\n!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --\n\n\ndependency\n\n    \ngroupId\nio.snappydata\n/groupId\n\n    \nartifactId\nsnappydata-cluster_2.11\n/artifactId\n\n    \nversion\n0.9\n/version\n\n\n/dependency\n\n\n\n\n\nExample: SBT dependency\n\n\n// https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies += \nio.snappydata\n % \nsnappydata-cluster_2.11\n % \n0.9\n\n\n\n\n\n\nCreate SnappySession\n: To start SnappyData store you need to create a SnappySession in your program\n\n\n val spark: SparkSession = SparkSession\n         .builder\n         .appName(\nSparkApp\n)\n         .master(\nlocal[*]\n)\n         .getOrCreate\n val snappy = new SnappySession(spark.sparkContext)\n\n\n\n\nExample\n: \nLaunch Apache Spark shell and provide SnappyData dependency as a Spark package\n:\nIf you already have Spark2.0 installed in your local machine you can directly use \n--packages\n option to download the SnappyData binaries.\n\n\n./bin/spark-shell --packages \nSnappyDataInc:snappydata:0.9-s_2.11\n\n\n\n\n\n\n\nEmbedded SnappyData Store Mode\n\n\nIn this mode, the Spark computations and in-memory data store run collocated in the same JVM. This is our out of the box configuration and suitable for most SnappyData real-time production environments. You launch SnappyData servers to bootstrap any data from disk, replicas or from external data sources.\nSpark executors are dynamically launched when the first Spark Job arrives.\n\n\nSome of the advantages of this mode are:\n\n\n\n\n\n\nHigh performance\n: All your Spark applications access the table data locally, in-process. The query engine accesses all the data locally by reference and avoids copying (which can be very expensive when working with large volumes).\n\n\n\n\n\n\nDriver High Availability\n: When Spark jobs are submitted, they can now run in an HA configuration. The submitted job becomes visible to a redundant \u201clead\u201d node that prevents the executors to go down when the Spark driver fails. Any submitted Spark job continues to run as long as there is at least one \u201clead\u201d node running.\n\n\n\n\n\n\nLess complex\n: There is only a single cluster to start, monitor, debug and tune.\n\n\n\n\n\n\n\n\nIn this mode, one can write Spark programs using jobs. For more details, refer to the \nSnappyData Jobs\n section.\n\n\nExample: Submit a Spark Job to the SnappyData Cluster\n\n\nbin/snappy-job.sh submit --app-name JsonApp --class org.apache.spark.examples.snappydata.WorkingWithJson --app-jar examples/jars/quickstart.jar --lead [leadHost:port] --conf json_resource_folder=../../quickstart/src/main/resources\n\n\n\n\nAlso, you can use \nSnappySQL\n to create and query tables.\n\n\nYou can either \nstart SnappyData members\n using the \nsnappy-start-all.sh\n script or you can start them individually.\n\n\nHaving the Spark computation embedded in the same JVM allows us to do a number of optimization at query planning level. For example:\n\n\n\n\n\n\nIf the join expression matches the partitioning scheme of tables, a partition to partition join instead of a shuffle based join is done. \n Moreover, if two tables are collocated (while defining the tables) costly data movement can be avoided.\n\n\n\n\n\n\nFor replicated tables, that are present in all the data nodes, a simple local join (local look up)  is done instead of a broadcast join.\n\n\n\n\n\n\nSimilarly inserts to tables groups rows according to table partitioning keys, and route to the JVM hosting the partition. This results in higher ingestion rate.\n\n\n\n\n\n\n\n\nSnappyData Smart Connector Mode\n\n\nIn certain cases, Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).\n\n\nSpecifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.\n\n\n\n\nKey Points:\n\n\n\n\n\n\nCan work with SnappyData store from any compatible Spark distribution\n\n\n\n\n\n\nSpark Cluster executes in its own independent JVM processes\n\n\n\n\n\n\nThe Spark cluster connects to SnappyData as a Spark Data source\n\n\n\n\n\n\nSupports any of the Spark supported resource managers (for example, Spark Standalone Manager, YARN or Mesos)\n\n\n\n\n\n\nSome of the advantages of this mode are:\n\n\nPerformance\n: When Spark partitions store data in \ncolumn tables\n, the connector automatically attempts to localize the partitions into SnappyData store buckets on the local node. The connector uses the same column store format as well as compression techniques in Spark avoiding all data formatting related inefficiencies or unnecessary serialization costs. This is the fastest way to ingest data when Spark and the cluster are operating as independent clusters.\n\n\nWhen storing to \nRow tables\n or when the partitioning in Spark is different than the partitioning configured on the table, data batches could be shuffled across nodes. Whenever Spark applications are writing to SnappyData tables, the data is always batched for the highest possible throughput.\n\n\nWhen queries are executed, while the entire query planning and execution is coordinated by the Spark engine (Catalyst), the smart connector still carries out a number of optimizations, which are listed below:\n\n\n\n\n\n\nRoute jobs to same machines as SnappyData data nodes if the executor nodes are co-hosted on the same machines as the data nodes. Job for each partition tries to fetch only from same machine data store where possible.\n\n\n\n\n\n\nCollocated joins: If the underlying tables are collocated partition-wise, and executor nodes are co-hosting SnappyData data nodes, then the column batches are fetched from local machines and the join itself is partition-wise and does not require any exchange.\n\n\n\n\n\n\nOptimized column batch inserts like in the Embedded mode with job routing to same machines as data stores if possible.\n\n\n\n\n\n\nExample: Launch a Spark local mode cluster and use Smart Connector to access SnappyData cluster\n\n\nStep 1: Start the SnappyData cluster\n:\nYou can either start SnappyData members using the \n_snappy_start_all_\n script or you can start them individually.\n\n\n# start members using the ssh scripts\n$ sbin/snappy-start-all.sh\n\n\n\n\nOR\n\n\n# start members individually\n$ bin/snappy locator start  -dir=/node-a/locator1\n$ bin/snappy server start  -dir=/node-b/server1  -locators:localhost:10334\nbin/snappy leader start  -dir=/node-c/lead1  -locators:localhost:10334\n\n\n\n\nStep 2: Launch the Apache Spark program\n\n\nIn the Local mode\n\n\n\n./bin/spark-shell  --master local[*] --conf spark.snappydata.connection=localhost:1527 --packages \nSnappyDataInc:snappydata:0.9-s_2.11\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe \nspark.snappydata.connection\n property points to the locator of a running SnappyData cluster. The value of this property is a combination of locator host and JDBC client port on which the locator listens for connections (default is 1527).\n\n\n\n\n\n\nIn the Smart Connector mode, all \nsnappydata.*\n SQL configuration properties should be prefixed with \nspark\n. For example, \nspark.snappydata.column.batchSize\n.\n\n\n\n\n\n\n\n\nThis opens a Scala Shell. Create a SnappySession to interact with the SnappyData store.\n\n\n// Create a SnappySession to work with SnappyData store\n$scala \n val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nUsing external cluster manager\n\n\n./bin/spark-submit --class somePackage.someClass  --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 --packages \nSnappyDataInc:snappydata:0.9-s_2.11\n\n\n\n\n\nThe code example for writing a Smart Connector application program is located in \nSmartConnectorExample", 
            "title": "Affinity Modes"
        }, 
        {
            "location": "/deployment/#overview", 
            "text": "In this section, the various modes available for collocation of related data and computation is discussed.  You can run the SnappyData store in the following modes:    Local Mode : Used mainly for development, where the client application, the executors, and data store are all running in the same JVM    Embedded SnappyData Store Mode : The Spark computations and in-memory data store run collocated in the same JVM    SnappyData Smart Connector Mode : Allows you to work with the SnappyData store cluster from any compatible Spark distribution", 
            "title": "Overview"
        }, 
        {
            "location": "/deployment/#local-mode", 
            "text": "In this mode, you can execute all the components (client application, executors, and data store) locally in the application's JVM. It is the simplest way to start testing and using SnappyData, as you do not require a cluster, and the  executor threads are launched locally for processing.  Key Points    No cluster required    Launch Single JVM (Single-node Cluster)    Launches executor threads locally for processing    Embeds the SnappyData in-memory store in-process    For development purposes only     Example: Using the Local mode for developing SnappyData programs  You can use an IDE of your choice, and provide the below dependency to get SnappyData binaries:  Example: Maven dependency  !-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --  dependency \n     groupId io.snappydata /groupId \n     artifactId snappydata-cluster_2.11 /artifactId \n     version 0.9 /version  /dependency   Example: SBT dependency  // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies +=  io.snappydata  %  snappydata-cluster_2.11  %  0.9   Create SnappySession : To start SnappyData store you need to create a SnappySession in your program   val spark: SparkSession = SparkSession\n         .builder\n         .appName( SparkApp )\n         .master( local[*] )\n         .getOrCreate\n val snappy = new SnappySession(spark.sparkContext)  Example :  Launch Apache Spark shell and provide SnappyData dependency as a Spark package :\nIf you already have Spark2.0 installed in your local machine you can directly use  --packages  option to download the SnappyData binaries.  ./bin/spark-shell --packages  SnappyDataInc:snappydata:0.9-s_2.11", 
            "title": "Local Mode"
        }, 
        {
            "location": "/deployment/#embedded-snappydata-store-mode", 
            "text": "In this mode, the Spark computations and in-memory data store run collocated in the same JVM. This is our out of the box configuration and suitable for most SnappyData real-time production environments. You launch SnappyData servers to bootstrap any data from disk, replicas or from external data sources.\nSpark executors are dynamically launched when the first Spark Job arrives.  Some of the advantages of this mode are:    High performance : All your Spark applications access the table data locally, in-process. The query engine accesses all the data locally by reference and avoids copying (which can be very expensive when working with large volumes).    Driver High Availability : When Spark jobs are submitted, they can now run in an HA configuration. The submitted job becomes visible to a redundant \u201clead\u201d node that prevents the executors to go down when the Spark driver fails. Any submitted Spark job continues to run as long as there is at least one \u201clead\u201d node running.    Less complex : There is only a single cluster to start, monitor, debug and tune.     In this mode, one can write Spark programs using jobs. For more details, refer to the  SnappyData Jobs  section.  Example: Submit a Spark Job to the SnappyData Cluster  bin/snappy-job.sh submit --app-name JsonApp --class org.apache.spark.examples.snappydata.WorkingWithJson --app-jar examples/jars/quickstart.jar --lead [leadHost:port] --conf json_resource_folder=../../quickstart/src/main/resources  Also, you can use  SnappySQL  to create and query tables.  You can either  start SnappyData members  using the  snappy-start-all.sh  script or you can start them individually.  Having the Spark computation embedded in the same JVM allows us to do a number of optimization at query planning level. For example:    If the join expression matches the partitioning scheme of tables, a partition to partition join instead of a shuffle based join is done.   Moreover, if two tables are collocated (while defining the tables) costly data movement can be avoided.    For replicated tables, that are present in all the data nodes, a simple local join (local look up)  is done instead of a broadcast join.    Similarly inserts to tables groups rows according to table partitioning keys, and route to the JVM hosting the partition. This results in higher ingestion rate.", 
            "title": "Embedded SnappyData Store Mode"
        }, 
        {
            "location": "/deployment/#snappydata-smart-connector-mode", 
            "text": "In certain cases, Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).  Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.   Key Points:    Can work with SnappyData store from any compatible Spark distribution    Spark Cluster executes in its own independent JVM processes    The Spark cluster connects to SnappyData as a Spark Data source    Supports any of the Spark supported resource managers (for example, Spark Standalone Manager, YARN or Mesos)    Some of the advantages of this mode are:  Performance : When Spark partitions store data in  column tables , the connector automatically attempts to localize the partitions into SnappyData store buckets on the local node. The connector uses the same column store format as well as compression techniques in Spark avoiding all data formatting related inefficiencies or unnecessary serialization costs. This is the fastest way to ingest data when Spark and the cluster are operating as independent clusters.  When storing to  Row tables  or when the partitioning in Spark is different than the partitioning configured on the table, data batches could be shuffled across nodes. Whenever Spark applications are writing to SnappyData tables, the data is always batched for the highest possible throughput.  When queries are executed, while the entire query planning and execution is coordinated by the Spark engine (Catalyst), the smart connector still carries out a number of optimizations, which are listed below:    Route jobs to same machines as SnappyData data nodes if the executor nodes are co-hosted on the same machines as the data nodes. Job for each partition tries to fetch only from same machine data store where possible.    Collocated joins: If the underlying tables are collocated partition-wise, and executor nodes are co-hosting SnappyData data nodes, then the column batches are fetched from local machines and the join itself is partition-wise and does not require any exchange.    Optimized column batch inserts like in the Embedded mode with job routing to same machines as data stores if possible.    Example: Launch a Spark local mode cluster and use Smart Connector to access SnappyData cluster  Step 1: Start the SnappyData cluster :\nYou can either start SnappyData members using the  _snappy_start_all_  script or you can start them individually.  # start members using the ssh scripts\n$ sbin/snappy-start-all.sh  OR  # start members individually\n$ bin/snappy locator start  -dir=/node-a/locator1\n$ bin/snappy server start  -dir=/node-b/server1  -locators:localhost:10334\nbin/snappy leader start  -dir=/node-c/lead1  -locators:localhost:10334  Step 2: Launch the Apache Spark program  In the Local mode  \n./bin/spark-shell  --master local[*] --conf spark.snappydata.connection=localhost:1527 --packages  SnappyDataInc:snappydata:0.9-s_2.11    Note    The  spark.snappydata.connection  property points to the locator of a running SnappyData cluster. The value of this property is a combination of locator host and JDBC client port on which the locator listens for connections (default is 1527).    In the Smart Connector mode, all  snappydata.*  SQL configuration properties should be prefixed with  spark . For example,  spark.snappydata.column.batchSize .     This opens a Scala Shell. Create a SnappySession to interact with the SnappyData store.  // Create a SnappySession to work with SnappyData store\n$scala   val snSession = new SnappySession(spark.sparkContext)  Using external cluster manager  ./bin/spark-submit --class somePackage.someClass  --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 --packages  SnappyDataInc:snappydata:0.9-s_2.11   The code example for writing a Smart Connector application program is located in  SmartConnectorExample", 
            "title": "SnappyData Smart Connector Mode"
        }, 
        {
            "location": "/best_practices/", 
            "text": "Best\u00a0Practices\u00a0for\u00a0Deploying and Managing\u00a0SnappyData\n\n\nThe best practices section provides you guidelines for setting up your cluster and designing your database and the schema to use.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nSetting up the Cluster\n\n\n\n\n\n\nDesign Database and Schema", 
            "title": "Best practices"
        }, 
        {
            "location": "/best_practices/#best-practices-for-deploying-and-managing-snappydata", 
            "text": "The best practices section provides you guidelines for setting up your cluster and designing your database and the schema to use.  The following topics are covered in this section:    Setting up the Cluster    Design Database and Schema", 
            "title": "Best\u00a0Practices\u00a0for\u00a0Deploying and Managing\u00a0SnappyData"
        }, 
        {
            "location": "/best_practices/capacity_planning/", 
            "text": "Overview\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nConcurrency and Management of Cores\n\n\n\n\n\n\nMemory Management: Heap and Off-Heap\n\n\n\n\n\n\nHA Considerations\n\n\n\n\n\n\nImportant Settings\n\n\n\n\n\n\nOperating System Settings\n\n\n\n\n\n\nTable Memory Requirements\n\n\n\n\n\n\nJVM Settings for SnappyData Smart Connector mode and Local mode\n\n\n\n\n\n\n\n\nConcurrency and Management of Cores\n\n\nExecuting queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job has multiple tasks. The number of tasks is determined by the number of partitions of the underlying data.\nConcurrency in SnappyData is tightly bound with the capacity of the cluster, which means, the number of cores available in the cluster determines the number of concurrent tasks that can be run. \n\n\nThe default setting is \nCORES = 2 X number of cores on a machine\n.\n\n\nIt is recommended to use 2 X number of cores on a machine. If more than one server is running on a machine, the cores should be divided accordingly and specified using the \nspark.executor.cores\n property.\n\nspark.executor.cores\n is used to override the number of cores per server.\n\n\nFor example, for a cluster with 2 servers running on two different machines with  4 CPU cores each, a maximum number of tasks that can run concurrently is 16. \n \nIf a table has 17 partitions (buckets, for row or column tables), a scan query on this table creates 17 tasks. This means, 16 tasks runs concurrently and the last task will run when one of these 16 tasks has finished execution.\n\n\nSnappyData uses an optimization method which clubs multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions. \n\n\nIn SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get fair distribution of resources. \n\n\nFor example: In the image below, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently.\n\n\nPending tasks have to wait for completion of the current tasks and are assigned to the core that is first available.\n\n\nWhen you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute.\n\n\n\n\n\n\nMemory Management: Heap and Off-Heap\n\n\nSnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (eg, columns stored as byte arrays).\n\n\nIt is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The \nmemory-size and heap-size\n properties control the off-heap and on-heap sizes of the SnappyData server process. \n\n\n\n\nThe memory pool (off-heap and on-heap) available in SnappyData's cluster is divided into two parts \u2013 Execution and Storage memory pool. The storage memory pool as the name indicates is for the table storage. \n\n\nThe amount of memory that is available for storage is 50% of the total memory but it can grow to 90% (or \neviction-heap-percentage\n store property for heap memory if set) if the execution memory is unused.\nThis can be altered by specifying the \nspark.memory.storageFraction\n property. But, it is recommend to not change this setting. \n\n\nA certain fraction of heap memory is reserved for JVM objects outside of SnappyData storage and eviction. If the \ncritical-heap-percentage\n store property is set then SnappyData uses memory only until that limit is reached and the remaining memory is reserved. If no critical-heap-percentage has been specified then it defaults to 90%. There is no reserved memory for off-heap.\n\n\nSnappyData tables are by default configured for eviction which means, when there is memory pressure, the tables are evicted to disk. This impacts performance to some degree and hence it is recommended to size your VM before you begin. \n\n\n\n\n\n\n\nHA Considerations\n\n\nHigh availability options are available for all the SnappyData components. \n\n\nLead HA\n \n \nSnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of  the secondary lead nodes takes over immediately. \nSetting up the secondary lead node is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted.\n\n\nLocator\n\nSnappyData supports multiple locators in the cluster for high availability. \nIt is recommended to set up multiple locators as, if a locator becomes unavailable, the cluster continues to be available. New members can however not join the cluster.\nWith multiple locators, clients notice nothing and the failover recovery is completely transparent.\n\n\nDataServer\n \nSnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data.  So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. \nHowever, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again. \n\n\nImportant Settings\n\n\n\n\nBuckets\n\n\nBucket is the unit of partitioning for SnappyData tables. The data is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, the buckets are moved around for rebalancing. \n\n\nThe number of buckets should be set according to the table size. By default, there are 113 buckets for a table. \nIf there are more buckets in a table than required, it means there is fewer data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. \nSimilarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently.\nAlso, if the cluster is scaled at a later point of time rebalancing may not be optimal.\n\n\nFor column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data.  \n\n\nmember-timeout\n\n\nSnappyData efficiently uses CPU for running OLAP queries. In such cases, due to the amount of garbage generated, JVMs garbage collection can result in a system pause. These pauses are rare and can also be minimized by setting the off-heap memory. \n\nFor such cases, it is recommended that the \nmember-timeout\n should be increased to a higher value. This ensures that the members are not thrown out of the cluster in case of a system pause.\n\nThe default value of \nmember-timeout\n is: 5 sec. \n\n\nspark.local.dir\n\n\nSnappyData writes table data on disk.  By default, the disk location that SnappyData uses is the directory specified using \n-dir\n option, while starting the member. \nSnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size. \n\nTo achieve better performance, it is recommended to store temporary data on a different disk than the table data. This can be done by setting the \nspark.local.dir\n parameter.\n\n\n\n\nOperating System Settings\n\n\nFor best performance, the following operating system settings are recommended on the lead and server nodes.\n\n\nUlimit\n \n \nSpark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes. \n\n\nA minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K. \n\nTo change the limits of these settings for a user, the /etc/security/limits.conf file needs to be updated. A typical limits.conf used for SnappyData servers and leads looks like: \n\n\nec2-user          hard    nofile      163840 \nec2-user          soft    nofile      16384\nec2-user          hard    nproc       unlimited\nec2-user          soft    nproc       524288\nec2-user          hard    sigpending  unlimited\nec2-user          soft    sigpending  524288\n\n\n\n\n\n\nec2-user\n is the user running SnappyData.    \n\n\n\n\nOS Cache Size\n \nWhen there is lot of disk activity especially during table joins and during eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages.\n \nThe following are the typical configuration to be done on the machines that are running SnappyData processes. \n\n\nsudo sysctl -w vm.dirty_background_ratio=2\nsudo sysctl -w vm.dirty_ratio=4\nsudo sysctl -w vm.dirty_expire_centisecs=2000\nsudo sysctl -w vm.dirty_writeback_centisecs=300\n\n\n\n\nSwap File\n \n \nSince modern operating systems perform lazy allocation, it has been observed that despite setting \n-Xmx\n and \n-Xms\n settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in process going down.\n\nIt is recommended to set swap space on your system using the following commands.\n\n\n# sets a swap space of 32 GB\nsudo dd if=/dev/zero of=/var/swapfile.1 bs=1M count=32768\nsudo chmod 600 /var/swapfile.1\nsudo mkswap /var/swapfile.1\nsudo swapon /var/swapfile.1\n\n\n\n\n\n\nTable Memory Requirements\n\n\nSnappyData column tables encodes data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (i.e. off-heap is enabled), the entire column table is stored in off-heap memory. \n\n\nSnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk.\n\n\n\n\n\n\n\n\nTABLE IS PERSISTED?\n\n\nOVERFLOW IS CONFIGURED?\n\n\nAPPROXIMATE HEAP OVERHEAD\n\n\n\n\n\n\n\n\n\n\nNo\n\n\nNo\n\n\n64 bytes\n\n\n\n\n\n\nYes\n\n\nNo\n\n\n120 bytes\n\n\n\n\n\n\nYes\n\n\nYes\n\n\n152 bytes\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nFor a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.)\n\n\n\n\n\n\n\n\n\n\nTYPE OF INDEX ENTRY\n\n\nAPPROXIMATE HEAP OVERHEAD\n\n\n\n\n\n\n\n\n\n\nNew index entry\n\n\n80 bytes\n\n\n\n\n\n\nFirst non-unique index entry\n\n\n24 bytes\n\n\n\n\n\n\nSubsequent non-unique index entry\n\n\n8 bytes to 24 bytes*\n\n\n\n\n\n\n\n\nIf there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes.\n\n\n\n\nJVM Settings for SnappyData Smart Connector mode and Local mode\n\n\nFor SnappyData Smart Connector mode and local mode, we recommend the following JVM settings for optimal performance:\n\n\n-XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k", 
            "title": "Setting up your Cluster "
        }, 
        {
            "location": "/best_practices/capacity_planning/#overview", 
            "text": "The following topics are covered in this section:    Concurrency and Management of Cores    Memory Management: Heap and Off-Heap    HA Considerations    Important Settings    Operating System Settings    Table Memory Requirements    JVM Settings for SnappyData Smart Connector mode and Local mode", 
            "title": "Overview"
        }, 
        {
            "location": "/best_practices/capacity_planning/#concurrency-and-management-of-cores", 
            "text": "Executing queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job has multiple tasks. The number of tasks is determined by the number of partitions of the underlying data.\nConcurrency in SnappyData is tightly bound with the capacity of the cluster, which means, the number of cores available in the cluster determines the number of concurrent tasks that can be run.   The default setting is  CORES = 2 X number of cores on a machine .  It is recommended to use 2 X number of cores on a machine. If more than one server is running on a machine, the cores should be divided accordingly and specified using the  spark.executor.cores  property. spark.executor.cores  is used to override the number of cores per server.  For example, for a cluster with 2 servers running on two different machines with  4 CPU cores each, a maximum number of tasks that can run concurrently is 16.   \nIf a table has 17 partitions (buckets, for row or column tables), a scan query on this table creates 17 tasks. This means, 16 tasks runs concurrently and the last task will run when one of these 16 tasks has finished execution.  SnappyData uses an optimization method which clubs multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions.   In SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get fair distribution of resources.   For example: In the image below, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently.  Pending tasks have to wait for completion of the current tasks and are assigned to the core that is first available.  When you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute.", 
            "title": "Concurrency and Management of Cores"
        }, 
        {
            "location": "/best_practices/capacity_planning/#memory-management-heap-and-off-heap", 
            "text": "SnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (eg, columns stored as byte arrays).  It is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The  memory-size and heap-size  properties control the off-heap and on-heap sizes of the SnappyData server process.    The memory pool (off-heap and on-heap) available in SnappyData's cluster is divided into two parts \u2013 Execution and Storage memory pool. The storage memory pool as the name indicates is for the table storage.   The amount of memory that is available for storage is 50% of the total memory but it can grow to 90% (or  eviction-heap-percentage  store property for heap memory if set) if the execution memory is unused.\nThis can be altered by specifying the  spark.memory.storageFraction  property. But, it is recommend to not change this setting.   A certain fraction of heap memory is reserved for JVM objects outside of SnappyData storage and eviction. If the  critical-heap-percentage  store property is set then SnappyData uses memory only until that limit is reached and the remaining memory is reserved. If no critical-heap-percentage has been specified then it defaults to 90%. There is no reserved memory for off-heap.  SnappyData tables are by default configured for eviction which means, when there is memory pressure, the tables are evicted to disk. This impacts performance to some degree and hence it is recommended to size your VM before you begin.", 
            "title": "Memory Management: Heap and Off-Heap"
        }, 
        {
            "location": "/best_practices/capacity_planning/#ha-considerations", 
            "text": "High availability options are available for all the SnappyData components.   Lead HA    \nSnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of  the secondary lead nodes takes over immediately. \nSetting up the secondary lead node is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted.  Locator \nSnappyData supports multiple locators in the cluster for high availability. \nIt is recommended to set up multiple locators as, if a locator becomes unavailable, the cluster continues to be available. New members can however not join the cluster.\nWith multiple locators, clients notice nothing and the failover recovery is completely transparent.  DataServer  \nSnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data.  So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. \nHowever, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again.", 
            "title": "HA Considerations"
        }, 
        {
            "location": "/best_practices/capacity_planning/#important-settings", 
            "text": "", 
            "title": "Important Settings"
        }, 
        {
            "location": "/best_practices/capacity_planning/#buckets", 
            "text": "Bucket is the unit of partitioning for SnappyData tables. The data is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, the buckets are moved around for rebalancing.   The number of buckets should be set according to the table size. By default, there are 113 buckets for a table. \nIf there are more buckets in a table than required, it means there is fewer data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. \nSimilarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently.\nAlso, if the cluster is scaled at a later point of time rebalancing may not be optimal.  For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data.", 
            "title": "Buckets"
        }, 
        {
            "location": "/best_practices/capacity_planning/#member-timeout", 
            "text": "SnappyData efficiently uses CPU for running OLAP queries. In such cases, due to the amount of garbage generated, JVMs garbage collection can result in a system pause. These pauses are rare and can also be minimized by setting the off-heap memory.  \nFor such cases, it is recommended that the  member-timeout  should be increased to a higher value. This ensures that the members are not thrown out of the cluster in case of a system pause. \nThe default value of  member-timeout  is: 5 sec.", 
            "title": "member-timeout"
        }, 
        {
            "location": "/best_practices/capacity_planning/#sparklocaldir", 
            "text": "SnappyData writes table data on disk.  By default, the disk location that SnappyData uses is the directory specified using  -dir  option, while starting the member. \nSnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size.  \nTo achieve better performance, it is recommended to store temporary data on a different disk than the table data. This can be done by setting the  spark.local.dir  parameter.", 
            "title": "spark.local.dir"
        }, 
        {
            "location": "/best_practices/capacity_planning/#operating-system-settings", 
            "text": "For best performance, the following operating system settings are recommended on the lead and server nodes.  Ulimit    \nSpark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes.   A minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K.  To change the limits of these settings for a user, the /etc/security/limits.conf file needs to be updated. A typical limits.conf used for SnappyData servers and leads looks like:   ec2-user          hard    nofile      163840 \nec2-user          soft    nofile      16384\nec2-user          hard    nproc       unlimited\nec2-user          soft    nproc       524288\nec2-user          hard    sigpending  unlimited\nec2-user          soft    sigpending  524288   ec2-user  is the user running SnappyData.       OS Cache Size  \nWhen there is lot of disk activity especially during table joins and during eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages.  \nThe following are the typical configuration to be done on the machines that are running SnappyData processes.   sudo sysctl -w vm.dirty_background_ratio=2\nsudo sysctl -w vm.dirty_ratio=4\nsudo sysctl -w vm.dirty_expire_centisecs=2000\nsudo sysctl -w vm.dirty_writeback_centisecs=300  Swap File    \nSince modern operating systems perform lazy allocation, it has been observed that despite setting  -Xmx  and  -Xms  settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in process going down. \nIt is recommended to set swap space on your system using the following commands.  # sets a swap space of 32 GB\nsudo dd if=/dev/zero of=/var/swapfile.1 bs=1M count=32768\nsudo chmod 600 /var/swapfile.1\nsudo mkswap /var/swapfile.1\nsudo swapon /var/swapfile.1", 
            "title": "Operating System Settings"
        }, 
        {
            "location": "/best_practices/capacity_planning/#table-memory-requirements", 
            "text": "SnappyData column tables encodes data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (i.e. off-heap is enabled), the entire column table is stored in off-heap memory.   SnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk.     TABLE IS PERSISTED?  OVERFLOW IS CONFIGURED?  APPROXIMATE HEAP OVERHEAD      No  No  64 bytes    Yes  No  120 bytes    Yes  Yes  152 bytes      Note  For a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.)      TYPE OF INDEX ENTRY  APPROXIMATE HEAP OVERHEAD      New index entry  80 bytes    First non-unique index entry  24 bytes    Subsequent non-unique index entry  8 bytes to 24 bytes*     If there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes.", 
            "title": "Table Memory Requirements"
        }, 
        {
            "location": "/best_practices/capacity_planning/#jvm-settings-for-snappydata-smart-connector-mode-and-local-mode", 
            "text": "For SnappyData Smart Connector mode and local mode, we recommend the following JVM settings for optimal performance:  -XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k", 
            "title": "JVM Settings for SnappyData Smart Connector mode and Local mode"
        }, 
        {
            "location": "/best_practices/design_schema/", 
            "text": "Overview\n\n\nThe following topics are covered in this section:\n\n\n\n\nUsing Column vs Row Table\n\n\nUsing Partitioned vs Replicated Row Table\n\n\nApplying Partitioning Scheme\n\n\nUsing Redundancy\n\n\nOverflow Configuration\n\n\n\n\n\n\nUsing Column vs Row Table\n\n\nA columnar table data is stored in a sequence of columns, whereas, in a row table it stores table records in a sequence of rows.\n\n\n\n\nUsing Column Tables\n\n\nAnalytical Queries\n: A column table has distinct advantages for OLAP queries and therefore large tables involved in such queries are recommended to be created as columnar tables. These tables are rarely mutated (deleted/updated).\nFor a given query on a column table, only the required columns are read (since only the required subset columns are to be scanned), which gives a better scan performance. Thus, aggregation queries execute faster on a column table compared  to a  row table.\n\n\nCompression of Data\n: Another advantage that the column table offers is it allows highly efficient compression of data which reduces the total storage footprint for large tables.\n\n\nColumn tables are not suitable for OLTP scenarios. In this case, row tables are recommended.\n\n\n\n\nUsing Row Tables\n\n\nOLTP Queries\n: Row tables are designed to return the entire row efficiently and are suited for OLTP scenarios when the tables are required to be mutated frequently (when the table rows need to be updated/deleted based on some conditions). In these cases, row tables offer distinct advantages over the column tables.\n\n\nPoint queries\n: Row tables are also suitable for point queries (for example, queries that select only a few records based on certain where clause conditions). \n\n\nSmall Dimension Tables\n: Row tables are also suitable to create small dimension tables as these can be created as replicated tables (table data replicated on all data servers).\n\n\nCreate Index\n: Row tables also allow the creation of an index on certain columns of the table which improves  performance.\n\n\n\n\nNote\n\n\nIn the current release of SnappyData, updates and deletes are not supported on column tables. This feature will be added in a future release.\n\n\n\n\n\n\nUsing Partitioned vs Replicated Row Table\n\n\nIn SnappyData, row tables can be either partitioned across all servers or replicated on every server. For row tables, large fact tables should be partitioned whereas, dimension tables can be replicated.\n\n\nThe SnappyData architecture encourages you to denormalize \u201cdimension\u201d tables into fact tables when possible, and then replicate remaining dimension tables to all datastores in the distributed system.\n\n\nMost databases follow the \nstar schema\n design pattern where large \u201cfact\u201d tables store key information about the events in a system or a business process. For example, a fact table would store rows for events like product sales or bank transactions. Each fact table generally has foreign key relationships to multiple \u201cdimension\u201d tables, which describe further aspects of each row in the fact table.\n\n\nWhen designing a database schema for SnappyData, the main goal with a typical star schema database is to partition the entities in fact tables. Slow-changing dimension tables should then be replicated on each data store that hosts a partitioned fact table. In this way, a join between the fact table and any number of its dimension tables can be executed concurrently on each partition, without requiring multiple network hops to other members in the distributed system.\n\n\n\n\nApplying Partitioning Scheme\n\n\n\n\nCollocated Joins\n\nCollocating frequently joined partitioned tables is the best practice to improve the performance of join queries. When two tables are partitioned on columns and collocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData member. Therefore, in a join query, the join operation is performed on each node's  local data. \n\n\nIf the two tables are not collocated, partitions with same column values for the two tables can be on different nodes thus requiring the data to be shuffled between nodes causing the query performance to degrade.\n\n\nFor an example on collocated joins, refer to \nHow to collocate tables for doing a collocated join\n.\n\n\n\n\nBuckets\n\nThe total number of partitions is fixed for a table by the BUCKETS option. By default, there are 113 buckets. The value should be increased for a large amount of data that also determines the number of Spark RDD partitions that are created for the scan. For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data.\n\nUnit of data movement is a bucket, and buckets of collocated tables move together. When a new server joins, the  \n-rebalance\n option on the startup command-line triggers bucket rebalancing and the new server becomes the primary for some of the buckets (and secondary for some if REDUNDANCY\n0 has been specified). \n\nThere is also a system procedure \ncall sys.rebalance_all_buckets()\n that can be used to trigger rebalance.\nFor more information on BUCKETS, refer to \nBUCKETS\n.\n\n\n\n\nCriteria for Column Partitioning\n\nSnappyData partition is mainly for distributed and collocated joins. It is recommended to use a relevant dimension for partitioning so that all partitions are active and the query are executed concurrently.\n\nIf only a single partition is active and is used largely by queries (especially concurrent queries) it means a significant bottleneck where only a single partition is active all the time, while others are idle. This serializes execution into a single thread handling that partition. Therefore, it is not recommended to use DATE/TIMESTAMP as partitioning.\n\n\n\n\nUsing Redundancy\n\n\nREDUNDANCY clause of \nCREATE TABLE\n specifies the number of secondary copies you want to maintain for your partitioned table. This allows the table data to be highly available even if one of the SnappyData members fails or shuts down. \n\n\nA REDUNDANCY value of 1 is recommended to maintain a secondary copy of the table data. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage.\n\n\nFor an example of the REDUNDANCY clause refer to \nTables in SnappyData\n.\n\n\n\n\nOverflow Configuration\n\n\nIn SnappyData, column tables by default overflow to disk.  For row tables, the use EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria.  \n\n\nThis allows table data to be either evicted or destroyed based on the current memory consumption of the server. Use the \nOVERFLOW\n clause to specify the action to be taken upon the eviction event. \n\n\nFor persistent tables, setting this to 'true' will overflow the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' will cause the evicted rows to be destroyed in case of eviction event.\n\n\nRefer to \nCREATE TABLE\n link to understand how to configure OVERFLOW and EVICTION_BY clauses.\n\n\n\n\nNote\n\n\nThe default action for OVERFLOW is to destroy the evicted rows.", 
            "title": "Designing your Database and Schema"
        }, 
        {
            "location": "/best_practices/design_schema/#overview", 
            "text": "The following topics are covered in this section:   Using Column vs Row Table  Using Partitioned vs Replicated Row Table  Applying Partitioning Scheme  Using Redundancy  Overflow Configuration", 
            "title": "Overview"
        }, 
        {
            "location": "/best_practices/design_schema/#using-column-vs-row-table", 
            "text": "A columnar table data is stored in a sequence of columns, whereas, in a row table it stores table records in a sequence of rows.", 
            "title": "Using Column vs Row Table"
        }, 
        {
            "location": "/best_practices/design_schema/#using-column-tables", 
            "text": "Analytical Queries : A column table has distinct advantages for OLAP queries and therefore large tables involved in such queries are recommended to be created as columnar tables. These tables are rarely mutated (deleted/updated).\nFor a given query on a column table, only the required columns are read (since only the required subset columns are to be scanned), which gives a better scan performance. Thus, aggregation queries execute faster on a column table compared  to a  row table.  Compression of Data : Another advantage that the column table offers is it allows highly efficient compression of data which reduces the total storage footprint for large tables.  Column tables are not suitable for OLTP scenarios. In this case, row tables are recommended.", 
            "title": "Using Column Tables"
        }, 
        {
            "location": "/best_practices/design_schema/#using-row-tables", 
            "text": "OLTP Queries : Row tables are designed to return the entire row efficiently and are suited for OLTP scenarios when the tables are required to be mutated frequently (when the table rows need to be updated/deleted based on some conditions). In these cases, row tables offer distinct advantages over the column tables.  Point queries : Row tables are also suitable for point queries (for example, queries that select only a few records based on certain where clause conditions).   Small Dimension Tables : Row tables are also suitable to create small dimension tables as these can be created as replicated tables (table data replicated on all data servers).  Create Index : Row tables also allow the creation of an index on certain columns of the table which improves  performance.   Note  In the current release of SnappyData, updates and deletes are not supported on column tables. This feature will be added in a future release.", 
            "title": "Using Row Tables"
        }, 
        {
            "location": "/best_practices/design_schema/#using-partitioned-vs-replicated-row-table", 
            "text": "In SnappyData, row tables can be either partitioned across all servers or replicated on every server. For row tables, large fact tables should be partitioned whereas, dimension tables can be replicated.  The SnappyData architecture encourages you to denormalize \u201cdimension\u201d tables into fact tables when possible, and then replicate remaining dimension tables to all datastores in the distributed system.  Most databases follow the  star schema  design pattern where large \u201cfact\u201d tables store key information about the events in a system or a business process. For example, a fact table would store rows for events like product sales or bank transactions. Each fact table generally has foreign key relationships to multiple \u201cdimension\u201d tables, which describe further aspects of each row in the fact table.  When designing a database schema for SnappyData, the main goal with a typical star schema database is to partition the entities in fact tables. Slow-changing dimension tables should then be replicated on each data store that hosts a partitioned fact table. In this way, a join between the fact table and any number of its dimension tables can be executed concurrently on each partition, without requiring multiple network hops to other members in the distributed system.", 
            "title": "Using Partitioned vs Replicated Row Table"
        }, 
        {
            "location": "/best_practices/design_schema/#applying-partitioning-scheme", 
            "text": "Collocated Joins \nCollocating frequently joined partitioned tables is the best practice to improve the performance of join queries. When two tables are partitioned on columns and collocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData member. Therefore, in a join query, the join operation is performed on each node's  local data.   If the two tables are not collocated, partitions with same column values for the two tables can be on different nodes thus requiring the data to be shuffled between nodes causing the query performance to degrade.  For an example on collocated joins, refer to  How to collocate tables for doing a collocated join .   Buckets \nThe total number of partitions is fixed for a table by the BUCKETS option. By default, there are 113 buckets. The value should be increased for a large amount of data that also determines the number of Spark RDD partitions that are created for the scan. For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data. \nUnit of data movement is a bucket, and buckets of collocated tables move together. When a new server joins, the   -rebalance  option on the startup command-line triggers bucket rebalancing and the new server becomes the primary for some of the buckets (and secondary for some if REDUNDANCY 0 has been specified).  \nThere is also a system procedure  call sys.rebalance_all_buckets()  that can be used to trigger rebalance.\nFor more information on BUCKETS, refer to  BUCKETS .   Criteria for Column Partitioning \nSnappyData partition is mainly for distributed and collocated joins. It is recommended to use a relevant dimension for partitioning so that all partitions are active and the query are executed concurrently. \nIf only a single partition is active and is used largely by queries (especially concurrent queries) it means a significant bottleneck where only a single partition is active all the time, while others are idle. This serializes execution into a single thread handling that partition. Therefore, it is not recommended to use DATE/TIMESTAMP as partitioning.", 
            "title": "Applying Partitioning Scheme"
        }, 
        {
            "location": "/best_practices/design_schema/#using-redundancy", 
            "text": "REDUNDANCY clause of  CREATE TABLE  specifies the number of secondary copies you want to maintain for your partitioned table. This allows the table data to be highly available even if one of the SnappyData members fails or shuts down.   A REDUNDANCY value of 1 is recommended to maintain a secondary copy of the table data. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage.  For an example of the REDUNDANCY clause refer to  Tables in SnappyData .", 
            "title": "Using Redundancy"
        }, 
        {
            "location": "/best_practices/design_schema/#overflow-configuration", 
            "text": "In SnappyData, column tables by default overflow to disk.  For row tables, the use EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria.    This allows table data to be either evicted or destroyed based on the current memory consumption of the server. Use the  OVERFLOW  clause to specify the action to be taken upon the eviction event.   For persistent tables, setting this to 'true' will overflow the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' will cause the evicted rows to be destroyed in case of eviction event.  Refer to  CREATE TABLE  link to understand how to configure OVERFLOW and EVICTION_BY clauses.   Note  The default action for OVERFLOW is to destroy the evicted rows.", 
            "title": "Overflow Configuration"
        }, 
        {
            "location": "/aqp/", 
            "text": "Overview of Synopsis Data Engine (SDE)\n\n\nThe SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time. \n\n\nFor instance, in exploratory analytics, a data analyst might be slicing and dicing large data sets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering), while the engineer continues to slice and dice the data sets without any interruptions. \n\n\nWhen accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters.\n\n\nWhile in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records. \n\n\nUnlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a priori known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.\n\n\nHow does it work?\n\n\nThe following diagram provides a simplified view of how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can, however, be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample. \n\n\n\n\nKey Concepts\n\n\nSnappyData SDE relies on two methods for approximations - \nStratified Sampling\n and \nSketching\n. A brief introduction to these concepts is provided below.\n\n\nStratified Sampling\n\n\nSampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying.\n\n\nTake this simple example table that manages AdImpressions. If random sample is created that is a third of the original size two records is picked in random. \nThis is depicted in the following figure:\n\n\n\n\nIf a query is executed, like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample. \n\n\n\n\nBut, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries. \n\n\nStratified sampling, on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata have enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer. \n\n\n\n\nTo understand these concepts in further detail, refer to the \nhandbook\n. It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.\n\n\nOnline Sampling\n\n\nSDE also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark DataFrame APIs to create a uniform random sample over static RDDs. For online sampling, SDE first does \nreservoir sampling\n for each startum in a write-optimized store before flushing it into a read-optimized store for stratified samples. \nThere is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, SnappyData can ensure having enough samples over each 5-minute time window, while still ensuring that all GEOs have good representation in the sample.\n\n\nSketching\n\n\nWhile stratified sampling ensures that data dimensions with low representation are captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, other data structures like a Count-min-sketch are relied on to capture data frequencies in a stream. This is a data structure that requires that it captures how often an element is seen in a stream for the top-N such elements. \nWhile a \nCount-min-sketch\n is well described, SDE extends this with support for providing top-K estimates over time series data. \n\n\nWorking with Stratified Samples\n\n\nCreate Sample Tables\n\n\nYou can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark DataFrames, or sourced from an external data source such as S3 or HDFS. \n\n\nHere is an SQL based example to create a sample on tables locally available in the SnappyData cluster. \n\n\nCREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI \n  OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01') \n  AS (SELECT * FROM NYCTAXI);\n\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  OPTIONS (qcs 'hack_license', fraction '0.01') \n  AS (SELECT * FROM TAXIFARE);\n\n\n\n\nOften your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. \nIn this example below, a sample table is created for an S3 (external) dataset:\n\n\nCREATE EXTERNAL TABLE TAXIFARE USING parquet \n  OPTIONS(path 's3a://\nAWS_SECRET_ACCESS_KEY\n:\nAWS_ACCESS_KEY_ID\n@zeppelindemo/nyctaxifaredata_cleaned');\n//Next, create the sample sourced from this table ..\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  options  (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE);\n\n\n\n\n\nWhen creating a base table, if you have applied the \npartition by\n clause, the clause is also applied to the sample table. The sample table also inherits the \nnumber of buckets\n, \nredundancy\n and \npersistence\n properties from the base table.\n\n\nFor sample tables, the \noverflow\n property is set to \nFalse\n by default. (For column tables the default value is  \nTrue\n). \n\n\nFor example:\n\n\nCREATE TABLE BASETABLENAME \ncolumn details\n \nUSING COLUMN OPTIONS (partition_by '\ncolumn_name_a\n', Buckets '7', Redundancy '1')\n\nCREATE TABLE SAMPLETABLENAME \ncolumn details\n \nUSING COLUMN_SAMPLE OPTIONS (qcs '\ncolumn_name_b\n',fraction '0.05', \nstrataReservoirSize '50', baseTable 'baseTableName')\n// In this case, sample table 'sampleTableName' is partitioned by column 'column_name_a', has 7 buckets and 1 redundancy.\n\n\n\n\n\nQCS (Query Column Set) and Sample Selection\n\n\nFor stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction). \n\n\nQCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like \nhour (pickup_datetime)\n.\n\n\nThe parameter \nfraction\n represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes, you can get pretty accurate answers with a very small fraction. With most data sets that follow a normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. SDE always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. \nFor instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers (a lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement. \n\n\nOne can create multiple sample tables using different sample QCS and sample fraction for a given base table. \n\n\nHere are some general guidelines to use when creating samples:\n\n\n\n\n\n\nNote that samples are only applicable when running aggregation queries. For point lookups or selective queries, the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store.\n\n\n\n\n\n\nStart by identifying the most common columns used in GroupBy/Where and Having clauses. \n\n\n\n\n\n\nThen, identify a subset of these columns where the cardinality is not too large. For instance, in the example above 'hack_license' is picked (one license per driver) as the strata and 1% of the records associated with each driver is sampled. \n\n\n\n\n\n\nAvoid using unique columns or timestamps for your QCS. For instance, in the example above, 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is a possibility that each record in the Dataset has a different timesstamp. Instead, when dealing with time series the 'hour' function is used to capture data for each hour. \n\n\n\n\n\n\nWhen the accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample. \n\n\n\n\n\n\n\n\nNote\n\n\nThe value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.\n\n\n\n\nRunning Queries\n\n\nQueries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause. \n\n\nHere is the syntax:\n\n\n\n\n\n\nSELECT ... FROM .. WHERE .. GROUP BY ...\n\nWITH ERROR \nfraction\n[CONFIDENCE\nfraction\n] [BEHAVIOR \nstring\n]\n\n\n\n\n\n\n\n\nWITH ERROR\n - this is a mandatory clause. The values are  0 \n value(double) \n 1 . \n\n\nCONFIDENCE\n - this is optional clause. The values are confidence 0 \n value(double) \n 1 . The default value is 0.95\n\n\nBEHAVIOR\n - this is an optional clause. The values are \ndo_nothing\n, \nlocal_omit\n, \nstrict\n,  \nrun_on_full_table\n, \npartial_run_on_base_table\n. The default value is \nrun_on_full_table\n   \n\n\n\n\nThese 'behavior' options are fully described in the section below. \n\n\nHere are some examples:\n\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error 0.10 \n// tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error \n// tolerate any error in the answer. Just give me a quick response.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019\n// tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95.\n// If the error for any row is greater than 10% omit the answer. i.e. the row is omitted. \n\n\n\n\nUsing the Spark DataFrame API\n\n\nThe Spark DataFrame API is extended with support for approximate queries. Here is 'withError' API on DataFrames.\n\n\ndef withError(error: Double,\nconfidence: Double = Constant.DEFAULT_CONFIDENCE,\nbehavior: String = \nDO_NOTHING\n): DataFrame\n\n\n\n\nQuery examples using the DataFrame API\n\n\nsnc.table(baseTable).agg(Map(\nArrDelay\n -\n \nsum\n)).orderBy( desc(\nMonth_\n)).withError(0.10) \nsnc.table(baseTable).agg(Map(\nArrDelay\n -\n \nsum\n)).orderBy( desc(\nMonth_\n)).withError(0.10, 0.95, 'local_omit\u2019) \n\n\n\n\nSupporting BI tools or existing Apps\n\n\nTo allow BI tools and existing Apps that say might be generating SQL, SDE also supports specifying these options through your SQL connection or using the Snappy SQLContext. \n\n\nsnContext.sql(s\nspark.sql.aqp.error=$error\n)\nsnContext.sql(s\nspark.sql.aqp.confidence=$confidence\n)\nsnContext.sql(s\nset spark.sql.aqp.behavior=$behavior\n)\n\n\n\n\nThese settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above.\n\n\nApplications or tools using JDBC/ODBC can set the following properties. \nFor example, when using Apache Zeppelin JDBC interpreter or the snappy-sql you can set the values as below:\n\n\nset spark.sql.aqp.error=$error;\nset spark.sql.aqp.confidence=$confidence;\nset spark.sql.aqp.behavior=$behavior;\n\n\n\n\nMore Examples\n\n\nExample 1\n\n\nCreate a sample table with qcs 'medallion' \n\n\nCREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI \n  OPTIONS (buckets '7', qcs 'medallion', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);\n\n\n\n\nSQL Query:\n\n\nselect medallion,avg(trip_distance) as avgTripDist,\n  absolute_error(avgTripDist),relative_error(avgTripDist),\n  lower_bound(avgTripDist),upper_bound(avgTripDist) \n  from nyctaxi group by medallion order by medallion desc limit 100\n  with error;\n  //These built-in error functions is explained in a section below.\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nmedallion\n).agg( avg(\ntrip_distance\n).alias(\navgTripDist\n),\n  absolute_error(\navgTripDist\n),  relative_error(\navgTripDist\n), lower_bound(\navgTripDist\n),\n  upper_bound(\navgTripDist\n)).withError(.6, .90, \ndo_nothing\n).sort(col(\nmedallion\n).desc).limit(100)\n\n\n\n\nExample 2\n\n\nCreate additional sample table with qcs 'hack_license' \n\n\nCREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS\n(buckets '7', qcs 'hack_license', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);\n\n\n\n\nSQL Query:\n\n\nselect  hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error\n// the engine will automitically use the HackLicense sample for a more accurate answer to this query.\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nhack_license\n).count().withError(.6,.90,\ndo_nothing\n).sort(col(\ncount\n).desc).limit(10)\n\n\n\n\nExample 3\n\n\nCreate a sample table using function \"hour(pickup_datetime) as QCS\n\n\nSample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);\n\n\n\n\nSQL Query:\n\n\nselect sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime)\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(hour(col(\npickup_datetime\n))).agg(Map(\ntrip_time_in_secs\n -\n \nsum\n)).withError(0.6,0.90,\ndo_nothing\n).limit(10)\n\n\n\n\nExample 4\n\n\nIf you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns.\n\n\nSample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);\n\n\n\n\nSQL Query:\n\n\nSelect hack_license, sum(trip_distance) as daily_trips from nyctaxi  where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license  order by daily_trips desc\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nhack_license\n,\npickup_datetime\n).agg(Map(\ntrip_distance\n -\n \nsum\n)).alias(\ndaily_trips\n).       filter(year(col(\npickup_datetime\n)).equalTo(2013) and month(col(\npickup_datetime\n)).equalTo(9)).withError(0.6,0.90,\ndo_nothing\n).sort(col(\nsum(trip_distance)\n).desc).limit(10)\n\n\n\n\nSample Selection\n\n\nSample selection logic selects most appropriate sample, based on this relatively simple logic in the current version:\n\n\n\n\n\n\nIf the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else,\n\n\n\n\n\n\nIf query QCS (columns involved in Where/GroupBy/Having matched the sample QCS, then, select that sample\n\n\n\n\n\n\nIf exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used\n\n\n\n\n\n\nIf superset of sample QCS is not available, a sample where the sample QCS is a subset of query QCS is used\n\n\n\n\n\n\nWhen multiple stratified samples with a subset of QCSs match, a sample with most matching columns is used. The largest size of the sample gets selected if multiple such samples are available. \n\n\n\n\n\n\nHigh-level Accuracy Contracts (HAC)\n\n\nSnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts.\n\n\nWhen an error constraint is not met, the action to be taken is defined in the behavior clause. \n\n\nBehavior Clause\n\n\nSynopsis Data Engine has HAC support using the following behavior clause. \n\n\ndo_nothing\n\n\nThe SDE engine returns the estimate as is. \n\n\n\n\nlocal_omit\n\n\nFor aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\". \n\n\n\nstrict\n\n\nIf any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception. \n\n\n\nrun_on_full_table\n\n\nIf any of the single output row exceeds the specified error, then the full query is re-executed on the base table.\n\n\n\n\npartial_run_on_base_table\n\n\nIf the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups.  This result is then merged (without any duplicates) with the result derived from the sample table. \n\n\n\nIn the following example, any one of the above behavior clause can be applied. \n\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_  with error \nfraction\n [CONFIDENCE \nfraction\n] [BEHAVIOR \nbehavior\n]\n\n\n\n\nError Functions\n\n\nIn addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection. \n\n\nThe following four methods are available to be used in query projection when running approximate queries:\n\n\n\n\n\n\nabsolute_error(column alias)\n: Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) \n\n\n\n\n\n\nrelative_error(column alias)\n: Indicates ratio of absolute error to estimate.\n\n\n\n\n\n\nlower_bound(column alias)\n: Lower value of an estimate interval for a given confidence.\n\n\n\n\n\n\nupper_bound(column alias)\n: Upper value of an estimate interval for a given confidence.\n\n\n\n\n\n\nConfidence is the probability that the value of a parameter falls within a specified range of values.\n\n\nFor example:\n\n\nSELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr),\nUniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9\n\n\n\n\n\n\nThe \nabsolute_error\n and \nrelative_error\n function values returns 0 if query is executed on the base table. \n\n\nlower_bound\n and \nupper_bound\n values returns null if query is executed on the base table. \n\n\nThe values are seen in case behavior is set to \nrun_on_full_table\n or\npartial_run_on_base_table\n\n\n\n\nIn addition to using SQL syntax in the queries, you can use data frame API as well. \nFor example, if you have a data frame for the airline table, then the below query can equivalently also be written as :\n\n\nselect AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95\n\n\n\n\nsnc.table(basetable).groupBy(\nYear_\n).agg( avg(\nArrDelay\n).alias(\narrivalDelay), relative_error(\narrivalDelay\n), absolute_error(\narrivalDelay\n), col(\nYear_\n)).withError(0.10, .95).sort(col(\nYear_\n).asc) \n\n\n\n\nReserved Keywords\n\n\nKeywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword \nsample_\n is reserved for SnappyData.\n\n\nIf the aggregate function is aliased in the query as \nsample_\nany string\n, then what you get is true answers on the sample table, and not the estimates of the base table.\n\n\nselect count() rowCount, count() as sample_count from airline with error 0.1\n\n\nrowCount returns estimate of a number of rows in airline table.\nsample_count returns a number of rows (true answer) in sample table of airline table.\n\n\nSketching\n\n\nSynopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A \nBloomFilter\n is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a \nCount-Min-Sketch\n which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.\n\n\nCreating TopK tables\n\n\nTopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query. \nTopK\n queries aim to retrieve, from a potentially very large resultset, only the \nk (k \n= 1)\n best answers.\n\n\nSQL API for creating a TopK table in SnappyData\n\n\nsnsc.sql(\ncreate topK table MostPopularTweets on tweetStreamTable \n +\n        \noptions(key 'hashtag', frequencyCol 'retweets')\n)\n\n\n\n\nThe example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.\n\n\nScala API for creating a TopK table\n\n\nval topKOptionMap = Map(\n    \"epoch\" -\n System.currentTimeMillis().toString,\n    \"timeInterval\" -\n \"1000ms\",\n    \"size\" -\n \"40\",\n    \"frequencyCol\" -\n \"retweets\"\n  )\n  val schema = StructType(List(StructField(\"HashTag\", StringType)))\n  snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"),\n    \"HashTag\", schema, topKOptionMap)\n\n\n\nThe code above shows how to do the same thing using the SnappyData Scala API.\n\n\nQuerying the TopK table\n\n\nselect * from topkTweets order by EstimatedValue desc\n\n\n\nThe example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most re-tweets.\n\n\nApproximate TopK analytics for time series data\n\n\nTime is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the time stamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most re-tweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system.\n\n\nHere is an example of a time-based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData SDE module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals.\n\n\nselect hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where \n    startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' \n    order by EstimatedValue desc\n\n\n\nIf time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.\n\n\nSQL API for creating a TopK table in SnappyData specifying timestampColumn\n\n\nIn the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet.\n\n\nsnsc.sql(\ncreate topK table MostPopularTweets on tweetStreamTable \n +\n        \noptions(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' )\n)\n\n\n\n\nThe example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest re-tweets value in the base table. This works for both static tables and streaming tables\n\n\nScala API for creating a TopK table\n\n\n    val topKOptionMap = Map(\n        \nepoch\n -\n System.currentTimeMillis().toString,\n        \ntimeInterval\n -\n \n1000ms\n,\n        \nsize\n -\n \n40\n,\n        \nfrequencyCol\n -\n \nretweets\n,\n        \ntimeSeriesColumn\n -\n \ntweetTime\n\n      )\n      val schema = StructType(List(StructField(\nHashTag\n, StringType)))\n      snc.createApproxTSTopK(\nMostPopularTweets\n, Some(\ntweetStreamTable\n),\n        \nHashTag\n, schema, topKOptionMap)\n\n\n\n\nThe code above shows how to do the same thing using the SnappyData Scala API.\n\n\nIt is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the \ntimeInterval\n attribute when creating the TopK table.\n\n\nUsing SDE\n\n\nIn the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.", 
            "title": "Synopsis Data Engine (SDE)"
        }, 
        {
            "location": "/aqp/#overview-of-synopsis-data-engine-sde", 
            "text": "The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time.   For instance, in exploratory analytics, a data analyst might be slicing and dicing large data sets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering), while the engineer continues to slice and dice the data sets without any interruptions.   When accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters.  While in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records.   Unlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a priori known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.", 
            "title": "Overview of Synopsis Data Engine (SDE)"
        }, 
        {
            "location": "/aqp/#how-does-it-work", 
            "text": "The following diagram provides a simplified view of how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can, however, be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/aqp/#key-concepts", 
            "text": "SnappyData SDE relies on two methods for approximations -  Stratified Sampling  and  Sketching . A brief introduction to these concepts is provided below.", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/aqp/#stratified-sampling", 
            "text": "Sampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying.  Take this simple example table that manages AdImpressions. If random sample is created that is a third of the original size two records is picked in random. \nThis is depicted in the following figure:   If a query is executed, like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample.    But, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries.   Stratified sampling, on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata have enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer.    To understand these concepts in further detail, refer to the  handbook . It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.", 
            "title": "Stratified Sampling"
        }, 
        {
            "location": "/aqp/#online-sampling", 
            "text": "SDE also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark DataFrame APIs to create a uniform random sample over static RDDs. For online sampling, SDE first does  reservoir sampling  for each startum in a write-optimized store before flushing it into a read-optimized store for stratified samples. \nThere is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, SnappyData can ensure having enough samples over each 5-minute time window, while still ensuring that all GEOs have good representation in the sample.", 
            "title": "Online Sampling"
        }, 
        {
            "location": "/aqp/#sketching", 
            "text": "While stratified sampling ensures that data dimensions with low representation are captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, other data structures like a Count-min-sketch are relied on to capture data frequencies in a stream. This is a data structure that requires that it captures how often an element is seen in a stream for the top-N such elements. \nWhile a  Count-min-sketch  is well described, SDE extends this with support for providing top-K estimates over time series data.", 
            "title": "Sketching"
        }, 
        {
            "location": "/aqp/#working-with-stratified-samples", 
            "text": "", 
            "title": "Working with Stratified Samples"
        }, 
        {
            "location": "/aqp/#create-sample-tables", 
            "text": "You can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark DataFrames, or sourced from an external data source such as S3 or HDFS.   Here is an SQL based example to create a sample on tables locally available in the SnappyData cluster.   CREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI \n  OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01') \n  AS (SELECT * FROM NYCTAXI);\n\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  OPTIONS (qcs 'hack_license', fraction '0.01') \n  AS (SELECT * FROM TAXIFARE);  Often your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. \nIn this example below, a sample table is created for an S3 (external) dataset:  CREATE EXTERNAL TABLE TAXIFARE USING parquet \n  OPTIONS(path 's3a:// AWS_SECRET_ACCESS_KEY : AWS_ACCESS_KEY_ID @zeppelindemo/nyctaxifaredata_cleaned');\n//Next, create the sample sourced from this table ..\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  options  (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE);  When creating a base table, if you have applied the  partition by  clause, the clause is also applied to the sample table. The sample table also inherits the  number of buckets ,  redundancy  and  persistence  properties from the base table.  For sample tables, the  overflow  property is set to  False  by default. (For column tables the default value is   True ).   For example:  CREATE TABLE BASETABLENAME  column details  \nUSING COLUMN OPTIONS (partition_by ' column_name_a ', Buckets '7', Redundancy '1')\n\nCREATE TABLE SAMPLETABLENAME  column details  \nUSING COLUMN_SAMPLE OPTIONS (qcs ' column_name_b ',fraction '0.05', \nstrataReservoirSize '50', baseTable 'baseTableName')\n// In this case, sample table 'sampleTableName' is partitioned by column 'column_name_a', has 7 buckets and 1 redundancy.", 
            "title": "Create Sample Tables"
        }, 
        {
            "location": "/aqp/#qcs-query-column-set-and-sample-selection", 
            "text": "For stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction).   QCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like  hour (pickup_datetime) .  The parameter  fraction  represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes, you can get pretty accurate answers with a very small fraction. With most data sets that follow a normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. SDE always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. \nFor instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers (a lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement.   One can create multiple sample tables using different sample QCS and sample fraction for a given base table.   Here are some general guidelines to use when creating samples:    Note that samples are only applicable when running aggregation queries. For point lookups or selective queries, the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store.    Start by identifying the most common columns used in GroupBy/Where and Having clauses.     Then, identify a subset of these columns where the cardinality is not too large. For instance, in the example above 'hack_license' is picked (one license per driver) as the strata and 1% of the records associated with each driver is sampled.     Avoid using unique columns or timestamps for your QCS. For instance, in the example above, 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is a possibility that each record in the Dataset has a different timesstamp. Instead, when dealing with time series the 'hour' function is used to capture data for each hour.     When the accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample.      Note  The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.", 
            "title": "QCS (Query Column Set) and Sample Selection"
        }, 
        {
            "location": "/aqp/#running-queries", 
            "text": "Queries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause.   Here is the syntax:    SELECT ... FROM .. WHERE .. GROUP BY ... \nWITH ERROR  fraction [CONFIDENCE fraction ] [BEHAVIOR  string ]     WITH ERROR  - this is a mandatory clause. The values are  0   value(double)   1 .   CONFIDENCE  - this is optional clause. The values are confidence 0   value(double)   1 . The default value is 0.95  BEHAVIOR  - this is an optional clause. The values are  do_nothing ,  local_omit ,  strict ,   run_on_full_table ,  partial_run_on_base_table . The default value is  run_on_full_table       These 'behavior' options are fully described in the section below.   Here are some examples:  SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error 0.10 \n// tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error \n// tolerate any error in the answer. Just give me a quick response.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019\n// tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95.\n// If the error for any row is greater than 10% omit the answer. i.e. the row is omitted.", 
            "title": "Running Queries"
        }, 
        {
            "location": "/aqp/#using-the-spark-dataframe-api", 
            "text": "The Spark DataFrame API is extended with support for approximate queries. Here is 'withError' API on DataFrames.  def withError(error: Double,\nconfidence: Double = Constant.DEFAULT_CONFIDENCE,\nbehavior: String =  DO_NOTHING ): DataFrame  Query examples using the DataFrame API  snc.table(baseTable).agg(Map( ArrDelay  -   sum )).orderBy( desc( Month_ )).withError(0.10) \nsnc.table(baseTable).agg(Map( ArrDelay  -   sum )).orderBy( desc( Month_ )).withError(0.10, 0.95, 'local_omit\u2019)", 
            "title": "Using the Spark DataFrame API"
        }, 
        {
            "location": "/aqp/#supporting-bi-tools-or-existing-apps", 
            "text": "To allow BI tools and existing Apps that say might be generating SQL, SDE also supports specifying these options through your SQL connection or using the Snappy SQLContext.   snContext.sql(s spark.sql.aqp.error=$error )\nsnContext.sql(s spark.sql.aqp.confidence=$confidence )\nsnContext.sql(s set spark.sql.aqp.behavior=$behavior )  These settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above.  Applications or tools using JDBC/ODBC can set the following properties. \nFor example, when using Apache Zeppelin JDBC interpreter or the snappy-sql you can set the values as below:  set spark.sql.aqp.error=$error;\nset spark.sql.aqp.confidence=$confidence;\nset spark.sql.aqp.behavior=$behavior;", 
            "title": "Supporting BI tools or existing Apps"
        }, 
        {
            "location": "/aqp/#more-examples", 
            "text": "", 
            "title": "More Examples"
        }, 
        {
            "location": "/aqp/#example-1", 
            "text": "Create a sample table with qcs 'medallion'   CREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI \n  OPTIONS (buckets '7', qcs 'medallion', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);  SQL Query:  select medallion,avg(trip_distance) as avgTripDist,\n  absolute_error(avgTripDist),relative_error(avgTripDist),\n  lower_bound(avgTripDist),upper_bound(avgTripDist) \n  from nyctaxi group by medallion order by medallion desc limit 100\n  with error;\n  //These built-in error functions is explained in a section below.  DataFrame API Query:  snc.table(basetable).groupBy( medallion ).agg( avg( trip_distance ).alias( avgTripDist ),\n  absolute_error( avgTripDist ),  relative_error( avgTripDist ), lower_bound( avgTripDist ),\n  upper_bound( avgTripDist )).withError(.6, .90,  do_nothing ).sort(col( medallion ).desc).limit(100)", 
            "title": "Example 1"
        }, 
        {
            "location": "/aqp/#example-2", 
            "text": "Create additional sample table with qcs 'hack_license'   CREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS\n(buckets '7', qcs 'hack_license', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);  SQL Query:  select  hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error\n// the engine will automitically use the HackLicense sample for a more accurate answer to this query.  DataFrame API Query:  snc.table(basetable).groupBy( hack_license ).count().withError(.6,.90, do_nothing ).sort(col( count ).desc).limit(10)", 
            "title": "Example 2"
        }, 
        {
            "location": "/aqp/#example-3", 
            "text": "Create a sample table using function \"hour(pickup_datetime) as QCS  Sample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);  SQL Query:  select sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime)  DataFrame API Query:  snc.table(basetable).groupBy(hour(col( pickup_datetime ))).agg(Map( trip_time_in_secs  -   sum )).withError(0.6,0.90, do_nothing ).limit(10)", 
            "title": "Example 3"
        }, 
        {
            "location": "/aqp/#example-4", 
            "text": "If you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns.  Sample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);  SQL Query:  Select hack_license, sum(trip_distance) as daily_trips from nyctaxi  where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license  order by daily_trips desc  DataFrame API Query:  snc.table(basetable).groupBy( hack_license , pickup_datetime ).agg(Map( trip_distance  -   sum )).alias( daily_trips ).       filter(year(col( pickup_datetime )).equalTo(2013) and month(col( pickup_datetime )).equalTo(9)).withError(0.6,0.90, do_nothing ).sort(col( sum(trip_distance) ).desc).limit(10)", 
            "title": "Example 4"
        }, 
        {
            "location": "/aqp/#sample-selection", 
            "text": "Sample selection logic selects most appropriate sample, based on this relatively simple logic in the current version:    If the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else,    If query QCS (columns involved in Where/GroupBy/Having matched the sample QCS, then, select that sample    If exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used    If superset of sample QCS is not available, a sample where the sample QCS is a subset of query QCS is used    When multiple stratified samples with a subset of QCSs match, a sample with most matching columns is used. The largest size of the sample gets selected if multiple such samples are available.", 
            "title": "Sample Selection"
        }, 
        {
            "location": "/aqp/#high-level-accuracy-contracts-hac", 
            "text": "SnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts.  When an error constraint is not met, the action to be taken is defined in the behavior clause.", 
            "title": "High-level Accuracy Contracts (HAC)"
        }, 
        {
            "location": "/aqp/#behavior-clause", 
            "text": "Synopsis Data Engine has HAC support using the following behavior clause.", 
            "title": "Behavior Clause"
        }, 
        {
            "location": "/aqp/#do_nothing", 
            "text": "The SDE engine returns the estimate as is.", 
            "title": "&lt;do_nothing&gt;"
        }, 
        {
            "location": "/aqp/#local_omit", 
            "text": "For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\".", 
            "title": "&lt;local_omit&gt;"
        }, 
        {
            "location": "/aqp/#strict", 
            "text": "If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception.", 
            "title": "&lt;strict&gt;"
        }, 
        {
            "location": "/aqp/#run_on_full_table", 
            "text": "If any of the single output row exceeds the specified error, then the full query is re-executed on the base table.", 
            "title": "&lt;run_on_full_table&gt;"
        }, 
        {
            "location": "/aqp/#partial_run_on_base_table", 
            "text": "If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups.  This result is then merged (without any duplicates) with the result derived from the sample table.   In the following example, any one of the above behavior clause can be applied.   SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_  with error  fraction  [CONFIDENCE  fraction ] [BEHAVIOR  behavior ]", 
            "title": "&lt;partial_run_on_base_table&gt;"
        }, 
        {
            "location": "/aqp/#error-functions", 
            "text": "In addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection.   The following four methods are available to be used in query projection when running approximate queries:    absolute_error(column alias) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap)     relative_error(column alias) : Indicates ratio of absolute error to estimate.    lower_bound(column alias) : Lower value of an estimate interval for a given confidence.    upper_bound(column alias) : Upper value of an estimate interval for a given confidence.    Confidence is the probability that the value of a parameter falls within a specified range of values.  For example:  SELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr),\nUniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9   The  absolute_error  and  relative_error  function values returns 0 if query is executed on the base table.   lower_bound  and  upper_bound  values returns null if query is executed on the base table.   The values are seen in case behavior is set to  run_on_full_table  or partial_run_on_base_table   In addition to using SQL syntax in the queries, you can use data frame API as well. \nFor example, if you have a data frame for the airline table, then the below query can equivalently also be written as :  select AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95  snc.table(basetable).groupBy( Year_ ).agg( avg( ArrDelay ).alias( arrivalDelay), relative_error( arrivalDelay ), absolute_error( arrivalDelay ), col( Year_ )).withError(0.10, .95).sort(col( Year_ ).asc)", 
            "title": "Error Functions"
        }, 
        {
            "location": "/aqp/#reserved-keywords", 
            "text": "Keywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword  sample_  is reserved for SnappyData.  If the aggregate function is aliased in the query as  sample_ any string , then what you get is true answers on the sample table, and not the estimates of the base table.  select count() rowCount, count() as sample_count from airline with error 0.1  rowCount returns estimate of a number of rows in airline table.\nsample_count returns a number of rows (true answer) in sample table of airline table.", 
            "title": "Reserved Keywords"
        }, 
        {
            "location": "/aqp/#sketching_1", 
            "text": "Synopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A  BloomFilter  is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a  Count-Min-Sketch  which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.", 
            "title": "Sketching"
        }, 
        {
            "location": "/aqp/#creating-topk-tables", 
            "text": "TopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query.  TopK  queries aim to retrieve, from a potentially very large resultset, only the  k (k  = 1)  best answers.", 
            "title": "Creating TopK tables"
        }, 
        {
            "location": "/aqp/#sql-api-for-creating-a-topk-table-in-snappydata", 
            "text": "snsc.sql( create topK table MostPopularTweets on tweetStreamTable   +\n         options(key 'hashtag', frequencyCol 'retweets') )  The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.", 
            "title": "SQL API for creating a TopK table in SnappyData"
        }, 
        {
            "location": "/aqp/#scala-api-for-creating-a-topk-table", 
            "text": "val topKOptionMap = Map(\n    \"epoch\" -  System.currentTimeMillis().toString,\n    \"timeInterval\" -  \"1000ms\",\n    \"size\" -  \"40\",\n    \"frequencyCol\" -  \"retweets\"\n  )\n  val schema = StructType(List(StructField(\"HashTag\", StringType)))\n  snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"),\n    \"HashTag\", schema, topKOptionMap)  The code above shows how to do the same thing using the SnappyData Scala API.", 
            "title": "Scala API for creating a TopK table"
        }, 
        {
            "location": "/aqp/#querying-the-topk-table", 
            "text": "select * from topkTweets order by EstimatedValue desc  The example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most re-tweets.", 
            "title": "Querying the TopK table"
        }, 
        {
            "location": "/aqp/#approximate-topk-analytics-for-time-series-data", 
            "text": "Time is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the time stamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most re-tweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system.  Here is an example of a time-based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData SDE module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals.  select hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where \n    startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' \n    order by EstimatedValue desc  If time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.", 
            "title": "Approximate TopK analytics for time series data"
        }, 
        {
            "location": "/aqp/#sql-api-for-creating-a-topk-table-in-snappydata-specifying-timestampcolumn", 
            "text": "In the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet.  snsc.sql( create topK table MostPopularTweets on tweetStreamTable   +\n         options(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' ) )  The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest re-tweets value in the base table. This works for both static tables and streaming tables", 
            "title": "SQL API for creating a TopK table in SnappyData specifying timestampColumn"
        }, 
        {
            "location": "/aqp/#scala-api-for-creating-a-topk-table_1", 
            "text": "val topKOptionMap = Map(\n         epoch  -  System.currentTimeMillis().toString,\n         timeInterval  -   1000ms ,\n         size  -   40 ,\n         frequencyCol  -   retweets ,\n         timeSeriesColumn  -   tweetTime \n      )\n      val schema = StructType(List(StructField( HashTag , StringType)))\n      snc.createApproxTSTopK( MostPopularTweets , Some( tweetStreamTable ),\n         HashTag , schema, topKOptionMap)  The code above shows how to do the same thing using the SnappyData Scala API.  It is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the  timeInterval  attribute when creating the TopK table.", 
            "title": "Scala API for creating a TopK table"
        }, 
        {
            "location": "/aqp/#using-sde", 
            "text": "In the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.", 
            "title": "Using SDE"
        }, 
        {
            "location": "/aqp_aws/", 
            "text": "Overview\n\n\niSight-Cloud is a cloud-based service that allows for instant visualization of analytic query results on large datasets. Powered by the SnappyData Synopsis Data Engine (\nSDE\n), users interact with iSight-Cloud to populate the synopsis engine with the right data sets and accelerate SQL queries by using the engine to provide latency bounded responses to large complex aggregate queries. \n\n\niSight-Cloud uses Apache Zeppelin as the front end notebook to display results and allows users to build powerful notebooks representing key elements of their business in a matter of minutes. \n\n\nThe service provides a web URL that spins up a cluster instance on AWS or users can download the iSight-Cloud EC2 script to configure a custom sized cluster, to create and render powerful visualizations of their big data sets with the click of a button. \nWith iSight-Cloud, you can speed up the process of understanding what your data is telling you, and move on to the task of organizing your business around those insights rapidly.\n\n\nIn this document, the features provided by SnappyData for analyzing your data is described. It also provides details for deploying a SnappyData Cloud cluster on AWS using either the CloudFormation service or by using the EC2 scripts.\n\n\nRefer to the examples and guidelines provided in this document to help you create notebooks using which, you can execute SQL queries or data frame API to analyze your data.\n\n\nKey Components\n\n\nThis section provides a brief description of the key terms used in this document. \n\n\n\n\n\n\nAmazon Web Services (AWS\n):  Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).\n\n\n\n\n\n\nSnappyData Cluster\n:  A database cluster which has three main components - Locator, Server and Lead\n\n\n\n\n\n\nApache Zeppelin\n: Apache Zeppelin is a web-based notebook that enables interactive data analytics. It allows you to make data-driven, interactive and collaborative documents with SQL queries or directly use the Spark API to process data.\n\n\n\n\n\n\nInterpreters\n: A software module which is loaded into Apache Zeppelin upon startup. Interpreters allow various third party products including SnappyData to interact with Apache Zeppelin. The SnappyData interpreter gives users the ability to execute SQL queries or use the data frame API to visualize data.\n\n\n\n\n\n\nQuick Start Steps\n\n\nTo understand the product follow these easy steps that can get you started quickly:\n\n\n\n\n\n\nSetting up SnappyData Cloud Cluster\n\n\n\n\n\n\nDeploying SnappyData Cloud Cluster with iSight CloudBuilder\n\n\n\n\n\n\nDeploying SnappyData Cloud Cluster on AWS using Scripts\n\n\n\n\n\n\n\n\n\n\nUsing Apache Zeppelin\n    \n\n\n\n\n\n\nUsing Predefined Notebook\n\n\n\n\n\n\nCreating your own Notebook\n\n\n\n\n\n\n\n\n\n\nLoading Data from AWS S3\n\n\n\n\n\n\nMonitoring SnappyData Cloud Cluster\n\n\n\n\n\n\n\n\nSetting Up SnappyData Cloud Cluster\n\n\nThis section discusses the steps required for setting up and deploying SnappyData Cloud Cluster on AWS using the iSight CloudBuilder and using a script.\n\n\n\n\nDeploying SnappyData Cloud Cluster with iSight CloudBuilder\n\n\nWatch the following  video to learn how easy it is to use iSight CloudBuilder, which generates a SnappyData Cloud Cluster.\n\n\n\n\nPrerequisites\n\n\nBefore you begin:\n\n\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources with CloudFormation\n\n\n\n\n\n\nSign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.\n\n\n\n\n\n\nCreate an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster\n\n\n\n\n\n\nSnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData Cloud cluster. In this release, the configuration supports launching the cluster on a single EC2 instance.\n\n\nIt is recommended that you select an instance type with higher processing power and more memory for this cluster, as it would be running four processes (locator, lead, a data server and an Apache Zeppelin server) on it.\n\n\nThis method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started! \n\n\nConfiguring and Launching the SnappyData Cloud Cluster\n\n\nLaunch the iSight CloudBuilder from \nhttp://www.snappydata.io/cloudbuilder\n. \n\n\n\n\n\n\nEnter the name for your cluster. Each cluster is identified by its unique name. \nThe names and details of the members are automatically derived from the provided cluster name. \n\n\n\n\n\n\n\n\nEnter a name of an existing EC2 KeyPair. This enables SSH access to the cluster.\nRefer to the Amazon documentation for more information on  \ngenerating your own EC2 Key Pair\n.\n\n\n\n\n\n\n\n\nSelect an instance based on the capacity that you require. \n\n\n\n\n\n\n\n\nEnter the size of the EBS storage volume to be attached to the Amazon EC2 instance in the \nEBS Volume Size(gigabytes)\n field.\n\n\n\n\n\n\nNote\n\n\nCurrently only Amazon Elastic Block Storage (EBS) is supported.\n\n\n\n\n\n\n\n\nEnter your email address.  \n\n\n\n\n\n\n\n\nClick \nGenerate\n.\n\n\n\n\n\n\nOn the next page, select the AWS region, and then click \nLaunch Cluster\n to launch your single-node cluster.\n\n\n\n\nNote\n\n\n\n\n\n\nThe region you select must match the EC2 Key Pair you created.\n\n\n\n\n\n\nIf you are not already logged into AWS, you are redirected to the AWS sign-in page.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSelect Template\n page, the URL for the Amazon S3 template is pre-populated. Click \nNext\n to continue.   \n\n\n\n\n\n\n\n\nOn the \nSpecify Details\n page:\n\n\n\n\nStack name\n: You can change the stack name.\n\n\n\n\n\n\nNote\n\n\nThe stack name must contain only letters, numbers, dashes and should start with an alpha character.\n\n\n\n\n\n\nVPCID\n: From the drop-down list, select the Virtual Private Cloud (VPC) ID that is set as default. Your instances will be launched within this VPC.\n\n\n\n\nClick \nNext\n to continue.\n\n\n\n\n\n\n\n\nSpecify the tags (key-value pairs) for resources in your stack or leave the field empty and click \nNext\n.\n\n\n\n\n\n\nOn the \nReview\n page, verify the details and click \nCreate\n to create a stack.\n\n\n\n\nNote\n\n\nThis operation may take a few minutes to complete.\n\n\n\n\n\n\n\n\nThe next page lists the existing stacks. Click \nRefresh\n to view the updated list and the status of the stack creation.\nWhen the cluster has started, the status of the stack changes to \nCREATE_COMPLETE\n. \n\n\n\n\n\n\n\n\n\n\nClick on the \nOutputs\n tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration. \n\n    \n\n\n\n\nNote\n\n\nIf the status of the stack displays \nROLLBACK_IN_PROGRESS\n or \nDELETE_COMPLETE\n, the stack creation may have failed. Some common problems that might have caused the failure are:\n\n\n\n\n\n\nInsufficient Permissions\n: Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.\n\n\n\n\n\n\nInvalid Keypair\n: Verify that the EC2 key pair exists in the region you selected in the iSight CloudBuilder creation steps.\n\n\n\n\n\n\nLimit Exceeded\n: Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nTo stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it.\n\n\n\n\nFor more information, refer to the \nApache Zeppelin\n section or refer to the \nApache Zeppelin documentation\n.\n\n\n\n\nDeploying SnappyData Cloud Cluster on AWS using Scripts\n\n\nSnappyData provides a script that allows you to launch and manage SnappyData clusters on Amazon Elastic Compute Cloud (EC2).\n\n\nDownload the script from the latest \nSnappyData Release page\n.\nThe package is available in compressed files (\nsnappydata-ec2-\nversion\n.tar.gz\n). Extract the contents to a location on your computer.\n\n\nPrerequisites\n\n\nBefore you begin, do the following:\n\n\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources.\n\n\n\n\n\n\nEC2 Key Pair is created in the region where you want to launch the SnappyData cluster.\n\n\n\n\n\n\nUsing the AWS Secret Access Key and the Access Key ID, set the two environment variables, \nAWS_SECRET_ACCESS_KEY\n and \nAWS_ACCESS_KEY_ID\n.\n\n\nIf you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file. You can find this information from the AWS IAM console.\n\n\nFor example:\n\n\n\n\nexport AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112\nexport AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10\n\n\n\n\n\n\n\n\nEnsure Python v 2.7 or later is installed on your local computer.\n\n\n\n\n\n\nLaunching SnappyData Cluster\n\n\nIn the command prompt, go to the directory where the \nsnappydata-ec2-\nversion\n.tar.gz\n is extracted or to the \naws/ec2\n directory where the \nsnappy-cloud-tools\n \nrepository\n is cloned locally.\n\n\nEnter the command in the following format:\n\n\n./snappy-ec2 -k \nyour-key-name\n -i \nyour-keyfile-path\n \naction\n \nyour-cluster-name\n\n\n\n\n\nHere, \nyour-key-name\n refers to the EC2 Key Pair, \nyour-keyfile-path\n refers to the path to the key file and \naction\n refers to the action to be performed (for example, launch, start, stop).\n\n\nBy default, the script starts one instance of a locator, lead,  and server each. \nThe script identifies each cluster by the unique cluster name (you provided) and internally ties members (locators, leads, and stores/servers) of the cluster with EC2 security groups.\n\n\nThe names and details of the members are automatically derived from the provided cluster name. \n\n\nFor example, if you launch a cluster named \nmy-cluster\n, the locator is available in security group named \nmy-cluster-locator\n and the store/server are available in \nmy-cluster-store\n.\n\n\nWhen running the script you can also specify properties like the number of stores and region.\nFor example, using the following command, you can start a SnappyData cluster named \nsnappydata-cluster\n with 2 stores (or servers) in the N. California (us-west-1) region on AWS. It also starts an Apache Zeppelin server on the instance where lead is running.\n\n\nThe examples below assume that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin=embedded --region=us-west-1 launch snappydata-cluster \n\n\n\n\nTo start Apache Zeppelin on a separate instance, use \n--with-zeppelin=non-embedded\n. \n\n\nFor a comprehensive list of command options, simply run \n./snappy-ec2\n in the command prompt.\n\n\n\n\nLoading Data from AWS S3\n\n\nSnappyData provides you with predefined buckets which contain datasets. When data is loaded, the table reads from the files available at the specified external location (AWS S3). \n\n\n\n\nNote\n\n\n\n\n\n\nThe Amazon S3 buckets and files are private by default. Ensure that you set the permissions required to make the data publicly accessible. Please refer to the \ndocumentation provided by Amazon S3\n for detailed information on creating a bucket, adding files and setting required permissions.\n\n\n\n\n\n\nYou can also find AWS related information on the AWS homepage, from the \nAccount\n \n \nSecurity Credentials\n \n \nAccess Credentials\n option.\n\n\n\n\n\n\nInformation related to the Bucket Name and Folder Location can be found on the AWS S3 site.\n\n\n\n\n\n\nIf the Secret Access Key contains a slash, it causes the Spark job to fail. When you create the Secret Access Key, ensure that it only contains alpha-numeric characters. \n\n\n\n\n\n\nWhen reading or writing CSV/Parquet to and from S3, the \nConnectionPoolTimeoutException\n error may be reported. To avoid this error, in the Spark context, set the value of the \nfs.s3a.connection.maximum\n property to a number greater than the possible number of partitions. \n For example, \nsnc.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.maximum\", \"1000\")\n\n\n\n\n\n\nTo access data from AWS S3, copy the \naws-java-sdk-\nversion_number\n and \nhadoop-aws-\nversion_number\n files (available in the Maven repository), to the \njars\n directory (snappydata-\nversion_number\n-bin/jars) in the SnappyData home directory. \n\n\n\n\n\n\n\n\nTo define a table that references the data in AWS S3, create a paragraph in the following format:\n\n\n%sql\nDROP TABLE IF EXISTS \ntable_name\n ;\nCREATE EXTERNAL TABLE \ntable_name\n USING parquet OPTIONS(path 's3a://\nAWS_ACCESS_KEY_ID\n:\nAWS_SECRET_ACCESS_KEY\n@\nbucket_Name\n/\nfolder_name\n');\n\n\n\n\nThe values are:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription/Value\n\n\n\n\n\n\n\n\n\n\ntable_name\n\n\nThe name of the table\n\n\n\n\n\n\nAWS_SECRET_ACCESS_KEY\n:\nAWS_ACCESS_KEY_ID\n\n\nSecurity credentials used to authenticate and authorize calls that you make to AWS.\n\n\n\n\n\n\nbucket_Name\n\n\nThe name of the bucket where the folder is located. Default value: zeppelindemo\n\n\n\n\n\n\nfolder_name\n\n\nThe folder name where the data is stored. Default value: nytaxifaredata\n\n\n\n\n\n\n\n\n\n\nUsing Apache Zeppelin\n\n\nApache Zeppelin provides web-based notebooks for data exploration. A notebook consists of one or more paragraphs, and each paragraph consists of a section each for code and results.\nLaunch Apache Zeppelin from the web browser by accessing the host and port associated with your Apache Zeppelin server. For example, http://\nzeppelin_host\n:\nport_number\n. The welcome page which lists existing notebooks is displayed.\n\nSnappyData provides predefined notebooks which are displayed on the home page after you have logged into Apache Zeppelin. For more information, see \nUsing Predefined Notebooks\n.\n\n\nRefer to the \nApache Zeppelin documentation\n, for more information.\n\n\nFunctioning of the SnappyData Interpreter and SnappyData Cluster\n\n\n\n\n\n\nWhen you execute a paragraph in the Apache Zeppelin UI, the query is sent to the Apache Zeppelin Server.\n\n\n\n\n\n\nThe query is then received by the SnappyData Interpreter which is running on the Lead node in the cluster.\n\n\n\n\n\n\nWhen the query has completed execution, the results are sent from the SnappyData Interpreter (which is running on the Lead node) to the Apache Zeppelin server.\n\n\n\n\n\n\nFinally, the results are displayed in the Zeppelin UI. \n\n\n\n\n\n\nConnecting the SnappyData Interpreter to the SnappyData cluster is represented in the below figure.\n\n\n\n\nUsing the Interpreter\n\n\nSnappyData Interpreter group consists of the interpreters \n%snappydata.spark\n and \n%snappydata.sql\n.\nTo use an interpreter, add the associated interpreter directive with the format, \n%\nInterpreter_name\n at the beginning of a paragraph in your note. In a paragraph, use one of the interpreters, and then enter required commands.\n\n\n\n\nNote\n\n\n\n\nThe SnappyData Interpreter provides a basic auto-completion functionality. Press (Ctrl+.) on the keyboard to view a list of suggestions.\n\n\n\n\n\n\nIt is recommended that you use the SQL interpreter to run queries on the SnappyData cluster, as an out of memory error may be reported with running the Scala interpreter. \n\n\n\n\n\n\nEach paragraph has its own SnappyData context. When you set a property on one paragraph, the property is applicable only to that paragraph and not to other paragraphs in the notebook.\n\n\n\n\n\n\n\n\n\n\nSQL Interpreter\n\n\nThe \n%snappydata.sql\n code specifies the default SQL interpreter. This interpreter is used to execute SQL queries on SnappyData cluster.\n\n\nMulti-Line Statements\n\n\nMulti-line statements ,as well as multiple statements on the same line, are also supported as long as they are separated by a semicolon. However, only the result of the last query is displayed.\n\n\nSnappyData provides a list of connection-specific SQL properties that can be applied to the paragraph that is executed. \n\n\nIn the following example, \nspark.sql.shuffle.partitions\n allows you to specify the number of partitions to be used for this query:\n\n\n%sql\nset spark.sql.shuffle.partitions=6; \nselect medallion,avg(trip_distance) as avgTripDist from nyctaxi group by medallion order by medallion desc limit 100 with error\n\n\n\n\nSnappyData Directives in Apache Zeppelin\n\n\nYou can execute approximate queries on SnappyData cluster by using the \n%sql show-instant-results-first\n directive. \nIn this case, the query is first executed on the sample table and the approximate result is displayed, after which the query is run on the base table. Once the query is complete, the approximate result is replaced with the actual result.\n\n\nIn the following example, you can see that the query is first executed on the sample table, and the time required to execute the query is displayed. \nAt the same time, the query is executed on the base table, and the total time required to execute the query on the base table is displayed.\n\n\n%sql show-instant-results-first\nselect avg(trip_time_in_secs/60) tripTime, hour(pickup_datetime), count(*) howManyTrips, absolute_error(tripTime) from nyctaxi where pickup_latitude \n 40.767588 and pickup_latitude \n 40.749775 and pickup_longitude \n -74.001632 and  pickup_longitude \n -73.974595 and dropoff_latitude \n 40.716800 and  dropoff_latitude \n  40.717776 and dropoff_longitude \n  -74.017682 and dropoff_longitude \n -74.000945 group by hour(pickup_datetime);\n\n\n\n\n\n\n\n\nNote\n\n\nThis directive works only for the SQL interpreter and an error may be displayed for the Scala interpreter.\n\n\n\n\nScala Interpreter\n\n\nThe \n%snappydata.spark\n code specifies the default Scala interpreter. This interpreter is used to write Scala code in the paragraph.\nSnappyContext is injected in this interpreter and can be accessed using variable \nsnc\n.\n\n\n\n\nUsing Predefined Notebooks\n\n\nSnappyData provides you predefined notebooks \nNYCTAXI Analytics\n and \nAirline Data Analytics\n which contains definitions that are stored in a single file. \n\n\nWhen you launch Apache Zeppelin in the browser, the welcome page displays the existing notebooks. Open a notebook and run any of the paragraphs to analyze data and view the result. \n\n\n\n\nCreating Notebooks - Try it Yourself!\n\n\n\n\n\n\nLog on to Apache Zeppelin, create a notebook and insert a new paragraph.\n\n\n\n\n\n\nUse \n%snappydata.spark\n for SnappyData interpreter or use \n%snappydata.sql\n for SQL interpreter.\n\n\n\n\n\n\nDownload a dataset you want to use and create tables as mentioned below\n\n\n\n\n\n\nExamples of Queries and Results\n\n\nThis section provides you with examples you can use in a paragraph.\n\n\n\n\nIn this example, you can create tables using an external Dataset from AWS S3.\n\n\n\n\n\n\n\n\nIn this example, you can execute a query on a base table using the SQL interpreter. It returns the number of rides per week. \n\n\n\n\n\n\n\n\nIn this example, you can execute a query on a sample table using the SQL interpreter. It returns the number of rides per week\n\n\n\n\n\n\n\n\n\n\nIn this example, you are processing data using the SnappyData Scala interpreter.\n\n\n\n\n\n\n\nApache Zeppelin allows you to dynamically create input fields. To create a text input field, use \n${fieldname}\n.\nIn this example, the input forms are, \n${taxiin=60} or taxiout \n ${taxiout=60}\n\n\n\n\n\n\n\n\n\n\nMonitoring the SnappyData Cloud Cluster\n\n\nYou can monitor the SnappyData cluster using the Apache Spark Console. This enables you to observe and record the performance and the activities on the SnappyData cluster.\n\n\n\n\n\nThe Apache Spark Console displays useful information about SnappyData. This includes a list of scheduler stages and tasks, summary of tables and memory usage.\n\n\nAccessing the Console\n\n\nTo access the Apache Spark console from the Apache Zeppelin notebook: \n\n\n\n\nClick on the \nSpark UI\n \nlink provided in the paragraph.\n    \n\n\n\n\nOnce you have logged in, you can start monitoring the SnappyData cluster. \n\n\n\n\nThe Technology Powering iSight Cloud\n\n\niSight Cloud uses the SnappyData Synopsis Engine to deliver blazing fast responses to queries that have long processing times. Analytic queries typically aim to provide aggregate information and involve full table or partial table scans. The cost of these queries is directly proportional to the amount of data that needs to be scanned. Analytics queries also often involve distributed joins of a dimension table with one or more fact tables. The cost of pruning these queries down to the final result is directly proportional to the size of the data involved. Distributed joins involve lots of data movement making such queries extremely expensive in traditional systems that process the entire data set.\n\n\nThe Synopsis Data Engine offers a breakthrough solution to these problems by building out stratified samples of the most common columns used in queries, as well as other probabilistic data structures like count-min-sketch, bloom filters etc. The use of these structures, along with extensions to the querying engine allow users to get almost-perfect answers to complex queries in a fraction of the time it used to take to answer these queries.\n\n\nFor more information on SDE and sampling techniques used by SnappyData, refer to the \nSDE documentation\n.", 
            "title": "Using iSight-Cloud"
        }, 
        {
            "location": "/aqp_aws/#overview", 
            "text": "iSight-Cloud is a cloud-based service that allows for instant visualization of analytic query results on large datasets. Powered by the SnappyData Synopsis Data Engine ( SDE ), users interact with iSight-Cloud to populate the synopsis engine with the right data sets and accelerate SQL queries by using the engine to provide latency bounded responses to large complex aggregate queries.   iSight-Cloud uses Apache Zeppelin as the front end notebook to display results and allows users to build powerful notebooks representing key elements of their business in a matter of minutes.   The service provides a web URL that spins up a cluster instance on AWS or users can download the iSight-Cloud EC2 script to configure a custom sized cluster, to create and render powerful visualizations of their big data sets with the click of a button. \nWith iSight-Cloud, you can speed up the process of understanding what your data is telling you, and move on to the task of organizing your business around those insights rapidly.  In this document, the features provided by SnappyData for analyzing your data is described. It also provides details for deploying a SnappyData Cloud cluster on AWS using either the CloudFormation service or by using the EC2 scripts.  Refer to the examples and guidelines provided in this document to help you create notebooks using which, you can execute SQL queries or data frame API to analyze your data.", 
            "title": "Overview"
        }, 
        {
            "location": "/aqp_aws/#key-components", 
            "text": "This section provides a brief description of the key terms used in this document.     Amazon Web Services (AWS ):  Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).    SnappyData Cluster :  A database cluster which has three main components - Locator, Server and Lead    Apache Zeppelin : Apache Zeppelin is a web-based notebook that enables interactive data analytics. It allows you to make data-driven, interactive and collaborative documents with SQL queries or directly use the Spark API to process data.    Interpreters : A software module which is loaded into Apache Zeppelin upon startup. Interpreters allow various third party products including SnappyData to interact with Apache Zeppelin. The SnappyData interpreter gives users the ability to execute SQL queries or use the data frame API to visualize data.", 
            "title": "Key Components"
        }, 
        {
            "location": "/aqp_aws/#quick-start-steps", 
            "text": "To understand the product follow these easy steps that can get you started quickly:    Setting up SnappyData Cloud Cluster    Deploying SnappyData Cloud Cluster with iSight CloudBuilder    Deploying SnappyData Cloud Cluster on AWS using Scripts      Using Apache Zeppelin         Using Predefined Notebook    Creating your own Notebook      Loading Data from AWS S3    Monitoring SnappyData Cloud Cluster", 
            "title": "Quick Start Steps"
        }, 
        {
            "location": "/aqp_aws/#setting-up-snappydata-cloud-cluster", 
            "text": "This section discusses the steps required for setting up and deploying SnappyData Cloud Cluster on AWS using the iSight CloudBuilder and using a script.", 
            "title": "Setting Up SnappyData Cloud Cluster"
        }, 
        {
            "location": "/aqp_aws/#deploying-snappydata-cloud-cluster-with-isight-cloudbuilder", 
            "text": "Watch the following  video to learn how easy it is to use iSight CloudBuilder, which generates a SnappyData Cloud Cluster.", 
            "title": "Deploying SnappyData Cloud Cluster with iSight CloudBuilder"
        }, 
        {
            "location": "/aqp_aws/#prerequisites", 
            "text": "Before you begin:    Ensure that you have an existing AWS account with required permissions to launch EC2 resources with CloudFormation    Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.    Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster    SnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData Cloud cluster. In this release, the configuration supports launching the cluster on a single EC2 instance.  It is recommended that you select an instance type with higher processing power and more memory for this cluster, as it would be running four processes (locator, lead, a data server and an Apache Zeppelin server) on it.  This method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started!", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aqp_aws/#configuring-and-launching-the-snappydata-cloud-cluster", 
            "text": "Launch the iSight CloudBuilder from  http://www.snappydata.io/cloudbuilder .     Enter the name for your cluster. Each cluster is identified by its unique name. \nThe names and details of the members are automatically derived from the provided cluster name.      Enter a name of an existing EC2 KeyPair. This enables SSH access to the cluster.\nRefer to the Amazon documentation for more information on   generating your own EC2 Key Pair .     Select an instance based on the capacity that you require.      Enter the size of the EBS storage volume to be attached to the Amazon EC2 instance in the  EBS Volume Size(gigabytes)  field.    Note  Currently only Amazon Elastic Block Storage (EBS) is supported.     Enter your email address.       Click  Generate .    On the next page, select the AWS region, and then click  Launch Cluster  to launch your single-node cluster.   Note    The region you select must match the EC2 Key Pair you created.    If you are not already logged into AWS, you are redirected to the AWS sign-in page.           On the  Select Template  page, the URL for the Amazon S3 template is pre-populated. Click  Next  to continue.        On the  Specify Details  page:   Stack name : You can change the stack name.    Note  The stack name must contain only letters, numbers, dashes and should start with an alpha character.    VPCID : From the drop-down list, select the Virtual Private Cloud (VPC) ID that is set as default. Your instances will be launched within this VPC.   Click  Next  to continue.     Specify the tags (key-value pairs) for resources in your stack or leave the field empty and click  Next .    On the  Review  page, verify the details and click  Create  to create a stack.   Note  This operation may take a few minutes to complete.     The next page lists the existing stacks. Click  Refresh  to view the updated list and the status of the stack creation.\nWhen the cluster has started, the status of the stack changes to  CREATE_COMPLETE .       Click on the  Outputs  tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration.  \n       Note  If the status of the stack displays  ROLLBACK_IN_PROGRESS  or  DELETE_COMPLETE , the stack creation may have failed. Some common problems that might have caused the failure are:    Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.    Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the iSight CloudBuilder creation steps.    Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.*        Note  To stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it.   For more information, refer to the  Apache Zeppelin  section or refer to the  Apache Zeppelin documentation .", 
            "title": "Configuring and Launching the SnappyData Cloud Cluster"
        }, 
        {
            "location": "/aqp_aws/#deploying-snappydata-cloud-cluster-on-aws-using-scripts", 
            "text": "SnappyData provides a script that allows you to launch and manage SnappyData clusters on Amazon Elastic Compute Cloud (EC2).  Download the script from the latest  SnappyData Release page .\nThe package is available in compressed files ( snappydata-ec2- version .tar.gz ). Extract the contents to a location on your computer.", 
            "title": "Deploying SnappyData Cloud Cluster on AWS using Scripts"
        }, 
        {
            "location": "/aqp_aws/#prerequisites_1", 
            "text": "Before you begin, do the following:    Ensure that you have an existing AWS account with required permissions to launch EC2 resources.    EC2 Key Pair is created in the region where you want to launch the SnappyData cluster.    Using the AWS Secret Access Key and the Access Key ID, set the two environment variables,  AWS_SECRET_ACCESS_KEY  and  AWS_ACCESS_KEY_ID .  If you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file. You can find this information from the AWS IAM console.  For example:   export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112\nexport AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10     Ensure Python v 2.7 or later is installed on your local computer.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aqp_aws/#launching-snappydata-cluster", 
            "text": "In the command prompt, go to the directory where the  snappydata-ec2- version .tar.gz  is extracted or to the  aws/ec2  directory where the  snappy-cloud-tools   repository  is cloned locally.  Enter the command in the following format:  ./snappy-ec2 -k  your-key-name  -i  your-keyfile-path   action   your-cluster-name   Here,  your-key-name  refers to the EC2 Key Pair,  your-keyfile-path  refers to the path to the key file and  action  refers to the action to be performed (for example, launch, start, stop).  By default, the script starts one instance of a locator, lead,  and server each. \nThe script identifies each cluster by the unique cluster name (you provided) and internally ties members (locators, leads, and stores/servers) of the cluster with EC2 security groups.  The names and details of the members are automatically derived from the provided cluster name.   For example, if you launch a cluster named  my-cluster , the locator is available in security group named  my-cluster-locator  and the store/server are available in  my-cluster-store .  When running the script you can also specify properties like the number of stores and region.\nFor example, using the following command, you can start a SnappyData cluster named  snappydata-cluster  with 2 stores (or servers) in the N. California (us-west-1) region on AWS. It also starts an Apache Zeppelin server on the instance where lead is running.  The examples below assume that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin=embedded --region=us-west-1 launch snappydata-cluster   To start Apache Zeppelin on a separate instance, use  --with-zeppelin=non-embedded .   For a comprehensive list of command options, simply run  ./snappy-ec2  in the command prompt.", 
            "title": "Launching SnappyData Cluster"
        }, 
        {
            "location": "/aqp_aws/#loading-data-from-aws-s3", 
            "text": "SnappyData provides you with predefined buckets which contain datasets. When data is loaded, the table reads from the files available at the specified external location (AWS S3).    Note    The Amazon S3 buckets and files are private by default. Ensure that you set the permissions required to make the data publicly accessible. Please refer to the  documentation provided by Amazon S3  for detailed information on creating a bucket, adding files and setting required permissions.    You can also find AWS related information on the AWS homepage, from the  Account     Security Credentials     Access Credentials  option.    Information related to the Bucket Name and Folder Location can be found on the AWS S3 site.    If the Secret Access Key contains a slash, it causes the Spark job to fail. When you create the Secret Access Key, ensure that it only contains alpha-numeric characters.     When reading or writing CSV/Parquet to and from S3, the  ConnectionPoolTimeoutException  error may be reported. To avoid this error, in the Spark context, set the value of the  fs.s3a.connection.maximum  property to a number greater than the possible number of partitions.   For example,  snc.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.maximum\", \"1000\")    To access data from AWS S3, copy the  aws-java-sdk- version_number  and  hadoop-aws- version_number  files (available in the Maven repository), to the  jars  directory (snappydata- version_number -bin/jars) in the SnappyData home directory.      To define a table that references the data in AWS S3, create a paragraph in the following format:  %sql\nDROP TABLE IF EXISTS  table_name  ;\nCREATE EXTERNAL TABLE  table_name  USING parquet OPTIONS(path 's3a:// AWS_ACCESS_KEY_ID : AWS_SECRET_ACCESS_KEY @ bucket_Name / folder_name ');  The values are:     Property  Description/Value      table_name  The name of the table    AWS_SECRET_ACCESS_KEY : AWS_ACCESS_KEY_ID  Security credentials used to authenticate and authorize calls that you make to AWS.    bucket_Name  The name of the bucket where the folder is located. Default value: zeppelindemo    folder_name  The folder name where the data is stored. Default value: nytaxifaredata", 
            "title": "Loading Data from AWS S3"
        }, 
        {
            "location": "/aqp_aws/#using-apache-zeppelin", 
            "text": "Apache Zeppelin provides web-based notebooks for data exploration. A notebook consists of one or more paragraphs, and each paragraph consists of a section each for code and results.\nLaunch Apache Zeppelin from the web browser by accessing the host and port associated with your Apache Zeppelin server. For example, http:// zeppelin_host : port_number . The welcome page which lists existing notebooks is displayed. \nSnappyData provides predefined notebooks which are displayed on the home page after you have logged into Apache Zeppelin. For more information, see  Using Predefined Notebooks .  Refer to the  Apache Zeppelin documentation , for more information.", 
            "title": "Using Apache Zeppelin"
        }, 
        {
            "location": "/aqp_aws/#functioning-of-the-snappydata-interpreter-and-snappydata-cluster", 
            "text": "When you execute a paragraph in the Apache Zeppelin UI, the query is sent to the Apache Zeppelin Server.    The query is then received by the SnappyData Interpreter which is running on the Lead node in the cluster.    When the query has completed execution, the results are sent from the SnappyData Interpreter (which is running on the Lead node) to the Apache Zeppelin server.    Finally, the results are displayed in the Zeppelin UI.     Connecting the SnappyData Interpreter to the SnappyData cluster is represented in the below figure.", 
            "title": "Functioning of the SnappyData Interpreter and SnappyData Cluster"
        }, 
        {
            "location": "/aqp_aws/#using-the-interpreter", 
            "text": "SnappyData Interpreter group consists of the interpreters  %snappydata.spark  and  %snappydata.sql .\nTo use an interpreter, add the associated interpreter directive with the format,  % Interpreter_name  at the beginning of a paragraph in your note. In a paragraph, use one of the interpreters, and then enter required commands.   Note   The SnappyData Interpreter provides a basic auto-completion functionality. Press (Ctrl+.) on the keyboard to view a list of suggestions.    It is recommended that you use the SQL interpreter to run queries on the SnappyData cluster, as an out of memory error may be reported with running the Scala interpreter.     Each paragraph has its own SnappyData context. When you set a property on one paragraph, the property is applicable only to that paragraph and not to other paragraphs in the notebook.", 
            "title": "Using the Interpreter"
        }, 
        {
            "location": "/aqp_aws/#sql-interpreter", 
            "text": "The  %snappydata.sql  code specifies the default SQL interpreter. This interpreter is used to execute SQL queries on SnappyData cluster.", 
            "title": "SQL Interpreter"
        }, 
        {
            "location": "/aqp_aws/#multi-line-statements", 
            "text": "Multi-line statements ,as well as multiple statements on the same line, are also supported as long as they are separated by a semicolon. However, only the result of the last query is displayed.  SnappyData provides a list of connection-specific SQL properties that can be applied to the paragraph that is executed.   In the following example,  spark.sql.shuffle.partitions  allows you to specify the number of partitions to be used for this query:  %sql\nset spark.sql.shuffle.partitions=6; \nselect medallion,avg(trip_distance) as avgTripDist from nyctaxi group by medallion order by medallion desc limit 100 with error", 
            "title": "Multi-Line Statements"
        }, 
        {
            "location": "/aqp_aws/#snappydata-directives-in-apache-zeppelin", 
            "text": "You can execute approximate queries on SnappyData cluster by using the  %sql show-instant-results-first  directive. \nIn this case, the query is first executed on the sample table and the approximate result is displayed, after which the query is run on the base table. Once the query is complete, the approximate result is replaced with the actual result.  In the following example, you can see that the query is first executed on the sample table, and the time required to execute the query is displayed. \nAt the same time, the query is executed on the base table, and the total time required to execute the query on the base table is displayed.  %sql show-instant-results-first\nselect avg(trip_time_in_secs/60) tripTime, hour(pickup_datetime), count(*) howManyTrips, absolute_error(tripTime) from nyctaxi where pickup_latitude   40.767588 and pickup_latitude   40.749775 and pickup_longitude   -74.001632 and  pickup_longitude   -73.974595 and dropoff_latitude   40.716800 and  dropoff_latitude    40.717776 and dropoff_longitude    -74.017682 and dropoff_longitude   -74.000945 group by hour(pickup_datetime);    Note  This directive works only for the SQL interpreter and an error may be displayed for the Scala interpreter.", 
            "title": "SnappyData Directives in Apache Zeppelin"
        }, 
        {
            "location": "/aqp_aws/#scala-interpreter", 
            "text": "The  %snappydata.spark  code specifies the default Scala interpreter. This interpreter is used to write Scala code in the paragraph.\nSnappyContext is injected in this interpreter and can be accessed using variable  snc .", 
            "title": "Scala Interpreter"
        }, 
        {
            "location": "/aqp_aws/#using-predefined-notebooks", 
            "text": "SnappyData provides you predefined notebooks  NYCTAXI Analytics  and  Airline Data Analytics  which contains definitions that are stored in a single file.   When you launch Apache Zeppelin in the browser, the welcome page displays the existing notebooks. Open a notebook and run any of the paragraphs to analyze data and view the result.", 
            "title": "Using Predefined Notebooks"
        }, 
        {
            "location": "/aqp_aws/#creating-notebooks-try-it-yourself", 
            "text": "Log on to Apache Zeppelin, create a notebook and insert a new paragraph.    Use  %snappydata.spark  for SnappyData interpreter or use  %snappydata.sql  for SQL interpreter.    Download a dataset you want to use and create tables as mentioned below", 
            "title": "Creating Notebooks - Try it Yourself!"
        }, 
        {
            "location": "/aqp_aws/#examples-of-queries-and-results", 
            "text": "This section provides you with examples you can use in a paragraph.   In this example, you can create tables using an external Dataset from AWS S3.     In this example, you can execute a query on a base table using the SQL interpreter. It returns the number of rides per week.      In this example, you can execute a query on a sample table using the SQL interpreter. It returns the number of rides per week      In this example, you are processing data using the SnappyData Scala interpreter.    Apache Zeppelin allows you to dynamically create input fields. To create a text input field, use  ${fieldname} .\nIn this example, the input forms are,  ${taxiin=60} or taxiout   ${taxiout=60}", 
            "title": "Examples of Queries and Results"
        }, 
        {
            "location": "/aqp_aws/#monitoring-the-snappydata-cloud-cluster", 
            "text": "You can monitor the SnappyData cluster using the Apache Spark Console. This enables you to observe and record the performance and the activities on the SnappyData cluster.   The Apache Spark Console displays useful information about SnappyData. This includes a list of scheduler stages and tasks, summary of tables and memory usage.", 
            "title": "Monitoring the SnappyData Cloud Cluster"
        }, 
        {
            "location": "/aqp_aws/#accessing-the-console", 
            "text": "To access the Apache Spark console from the Apache Zeppelin notebook:    Click on the  Spark UI   link provided in the paragraph.\n       Once you have logged in, you can start monitoring the SnappyData cluster.", 
            "title": "Accessing the Console"
        }, 
        {
            "location": "/aqp_aws/#the-technology-powering-isight-cloud", 
            "text": "iSight Cloud uses the SnappyData Synopsis Engine to deliver blazing fast responses to queries that have long processing times. Analytic queries typically aim to provide aggregate information and involve full table or partial table scans. The cost of these queries is directly proportional to the amount of data that needs to be scanned. Analytics queries also often involve distributed joins of a dimension table with one or more fact tables. The cost of pruning these queries down to the final result is directly proportional to the size of the data involved. Distributed joins involve lots of data movement making such queries extremely expensive in traditional systems that process the entire data set.  The Synopsis Data Engine offers a breakthrough solution to these problems by building out stratified samples of the most common columns used in queries, as well as other probabilistic data structures like count-min-sketch, bloom filters etc. The use of these structures, along with extensions to the querying engine allow users to get almost-perfect answers to complex queries in a fraction of the time it used to take to answer these queries.  For more information on SDE and sampling techniques used by SnappyData, refer to the  SDE documentation .", 
            "title": "The Technology Powering iSight Cloud"
        }, 
        {
            "location": "/monitoring/monitoring/", 
            "text": "SnappyData Pulse\n\n\nSnappyData Pulse is a monitoring system that gives you a high-level overview of the status and performance of the cluster. It provides a simple widget based view which helps you easily navigate and monitor your cluster.\n\n\nTo access the SnappyData Pulse, start your cluster and open http:\nleadhost\n:5050/dashboard/ in your web browser. \n\n\nleadhost\n is the hostname or IP of the lead node in your cluster.\n\n\nThe Dashboard also displays the \nLast Updated Date\n and \nTime of statistics\n on the top-left side of the page.\n\n\nThe Dashboard\n\n\nThe Dashboard offers the following capabilities and benefits:\n\n\n\n\nCluster Statistics\n\n\n\n\n\n\n\n\nCluster Status\n\n    Displays the current status of the cluster. \n\n\n\n\n\n\n\n\nStatus\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNormal\n\n\nAll nodes in the cluster are running\n\n\n\n\n\n\nWarning\n\n\nSome nodes in the cluster are stopped or unavailable\n\n\n\n\n\n\n\n\n\n\n\n\nCPU Usage\n \n\n   Displays the average CPU utilization of all the nodes present in the cluster.\n\n\n\n\n\n\nMemory Usage\n\n   Displays the collective usage of on-heap and off-heap memory by all nodes in the cluster.\n\n\n\n\n\n\nJVM Heap Usage\n\n   Displays the collective JVM Heap usage by all nodes in the cluster.\n\n\n\n\n\n\nMember Statistics\n\n\n\n\n\n\n\n\nMembers Count\n\n   Displays the total number of members (leads, locators and data servers) that exist in the cluster. The tooltip displays the count for each member.\n\n\n\n\n\n\nMembers Status\n\n   Displays the status of the members, which can be either Running or Stopped.\n\n\n\n\n\n\nDescription\n\n    A brief description of the member is displayed in Member column. You can view the detailed description for the member by clicking on the arrow next to the member name.\n\n    The description provides details of the member host, working directory, and process ID.\n\n\n\n\n\n\nType\n\n   Displays the type of member, which can be lead, locator or data server.\n\n\n\n\n\n\nCPU Usage\n\n   The CPU utilized by the member's host.\n\n\n\n\n\n\nMemory Usage\n\n   Members collective Heap and Off-Heap Memory utilization along with Total Memory.\n\n\n\n\n\n\nHeap Memory\n\n   Displays the total available heap memory and the used heap memory.\n \n   You can view the detailed description of the member's heap storage, heap execution memory, utilizations along with JVM Heap utilization by clicking on the arrow next to the member name.\n\n\n\n\n\n\nOff-Heap Memory Usage\n\n   Displays the members total off-heap memory and used off-heap memory.\n You can also view the member's off-heap storage and off-heap execution memory and utilizations by clicking on the arrow next to the member name.\n\n\n\n\n\n\nTable Statistics\n\n\n\n\n\n\n\n\nTables Count\n\n   Displays the total number of data tables present in the cluster. The tooltip displays the count for the row and column tables.\n\n\n\n\n\n\nName\n\n  Displays the name of the data table.\n\n\n\n\n\n\nStorage Model\n\n   Displays the data storage model of the data table. Possible models are ROW and COLUMN.\n\n\n\n\n\n\nDistribution Type\n\n   Displays the data distribution type for the table. Possible values are PARTITION, PARTITION_PERSISTENT, PARTITION_REDUNDANT, PARTITION_OVERFLOW, REPLICATE, REPLICATE_PERSISTENT, REPLICATE_OVERFLOW etc.\n\n\n\n\n\n\nRow Count\n\n   Displays the row count, which is the number of records present in the data table.\n\n\n\n\n\n\nMemory Size\n\n   Displays the heap memory used by data table to store its data/records.\n\n\n\n\n\n\nTotal Size\n\n   Displays the collective physical memory and disk overflow space used by the data table to store its data/records.", 
            "title": "Monitoring - SnappyData Pulse"
        }, 
        {
            "location": "/monitoring/monitoring/#snappydata-pulse", 
            "text": "SnappyData Pulse is a monitoring system that gives you a high-level overview of the status and performance of the cluster. It provides a simple widget based view which helps you easily navigate and monitor your cluster.  To access the SnappyData Pulse, start your cluster and open http: leadhost :5050/dashboard/ in your web browser.   leadhost  is the hostname or IP of the lead node in your cluster.  The Dashboard also displays the  Last Updated Date  and  Time of statistics  on the top-left side of the page.", 
            "title": "SnappyData Pulse"
        }, 
        {
            "location": "/monitoring/monitoring/#the-dashboard", 
            "text": "The Dashboard offers the following capabilities and benefits:", 
            "title": "The Dashboard"
        }, 
        {
            "location": "/monitoring/monitoring/#cluster-statistics", 
            "text": "Cluster Status \n    Displays the current status of the cluster.      Status  Description      Normal  All nodes in the cluster are running    Warning  Some nodes in the cluster are stopped or unavailable       CPU Usage   \n   Displays the average CPU utilization of all the nodes present in the cluster.    Memory Usage \n   Displays the collective usage of on-heap and off-heap memory by all nodes in the cluster.    JVM Heap Usage \n   Displays the collective JVM Heap usage by all nodes in the cluster.", 
            "title": "Cluster Statistics"
        }, 
        {
            "location": "/monitoring/monitoring/#member-statistics", 
            "text": "Members Count \n   Displays the total number of members (leads, locators and data servers) that exist in the cluster. The tooltip displays the count for each member.    Members Status \n   Displays the status of the members, which can be either Running or Stopped.    Description \n    A brief description of the member is displayed in Member column. You can view the detailed description for the member by clicking on the arrow next to the member name. \n    The description provides details of the member host, working directory, and process ID.    Type \n   Displays the type of member, which can be lead, locator or data server.    CPU Usage \n   The CPU utilized by the member's host.    Memory Usage \n   Members collective Heap and Off-Heap Memory utilization along with Total Memory.    Heap Memory \n   Displays the total available heap memory and the used heap memory.  \n   You can view the detailed description of the member's heap storage, heap execution memory, utilizations along with JVM Heap utilization by clicking on the arrow next to the member name.    Off-Heap Memory Usage \n   Displays the members total off-heap memory and used off-heap memory.  You can also view the member's off-heap storage and off-heap execution memory and utilizations by clicking on the arrow next to the member name.", 
            "title": "Member Statistics"
        }, 
        {
            "location": "/monitoring/monitoring/#table-statistics", 
            "text": "Tables Count \n   Displays the total number of data tables present in the cluster. The tooltip displays the count for the row and column tables.    Name \n  Displays the name of the data table.    Storage Model \n   Displays the data storage model of the data table. Possible models are ROW and COLUMN.    Distribution Type \n   Displays the data distribution type for the table. Possible values are PARTITION, PARTITION_PERSISTENT, PARTITION_REDUNDANT, PARTITION_OVERFLOW, REPLICATE, REPLICATE_PERSISTENT, REPLICATE_OVERFLOW etc.    Row Count \n   Displays the row count, which is the number of records present in the data table.    Memory Size \n   Displays the heap memory used by data table to store its data/records.    Total Size \n   Displays the collective physical memory and disk overflow space used by the data table to store its data/records.", 
            "title": "Table Statistics"
        }, 
        {
            "location": "/reference/", 
            "text": "Reference Guides\n\n\nThe SnappyData Reference provides a list and description of command-line utilities, APIs, and Standard Query Language (SQL) implementation.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nSQL Reference Guide\n\n\n\n\n\n\nBuilt-in System Procedures\n\n\n\n\n\n\nSystem Tables\n\n\n\n\n\n\nCommand Line Utilities\n\n\n\n\n\n\nSnappy-SQL Shell Interactive Commands\n\n\n\n\n\n\nConfiguration Parameters", 
            "title": "Reference Guides"
        }, 
        {
            "location": "/reference/#reference-guides", 
            "text": "The SnappyData Reference provides a list and description of command-line utilities, APIs, and Standard Query Language (SQL) implementation.  The following topics are covered in this section:    SQL Reference Guide    Built-in System Procedures    System Tables    Command Line Utilities    Snappy-SQL Shell Interactive Commands    Configuration Parameters", 
            "title": "Reference Guides"
        }, 
        {
            "location": "/sql_reference/", 
            "text": "SQL Reference Guide\n\n\nThis section provides a complete description of the Structured Query Language (SQL) used to manage information in SnappyData. It includes syntax, usage, keywords, and examples of the SQL statements used on SnappyData.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nCREATE STATEMENTS\n\n\n\n\n\n\nCREATE DISKSTORE\n\n\n\n\n\n\nCREATE FUNCTION\n\n\n\n\n\n\nCREATE INDEX\n\n\n\n\n\n\nCREATE SCHEMA\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nCREATE EXTERNAL TABLE\n\n\n\n\n\n\nCREATE TEMPORARY TABLE\n\n\n\n\n\n\nCREATE SAMPLE TABLE\n\n\n\n\n\n\nCREATE STREAM TABLE\n \n\n\n\n\n\n\n\n\n\n\nDELETE\n\n\n\n\n\n\nDROP STATEMENTS\n\n\n\n\n\n\nDROP DISKSTORE\n\n\n\n\n\n\nDROP FUNCTION\n\n\n\n\n\n\nDROP INDEX\n\n\n\n\n\n\nDROP TABLE\n\n\n\n\n\n\n\n\n\n\nINSERT\n\n\n\n\n\n\nPUT INTO\n\n\n\n\n\n\nSELECT\n\n\n\n\n\n\nSET SCHEMA\n\n\n\n\n\n\nTRUNCATE TABLE\n\n\n\n\n\n\nUPDATE", 
            "title": "SQL Reference Guide"
        }, 
        {
            "location": "/sql_reference/#sql-reference-guide", 
            "text": "This section provides a complete description of the Structured Query Language (SQL) used to manage information in SnappyData. It includes syntax, usage, keywords, and examples of the SQL statements used on SnappyData.  The following topics are covered in this section:    CREATE STATEMENTS    CREATE DISKSTORE    CREATE FUNCTION    CREATE INDEX    CREATE SCHEMA    CREATE TABLE    CREATE EXTERNAL TABLE    CREATE TEMPORARY TABLE    CREATE SAMPLE TABLE    CREATE STREAM TABLE        DELETE    DROP STATEMENTS    DROP DISKSTORE    DROP FUNCTION    DROP INDEX    DROP TABLE      INSERT    PUT INTO    SELECT    SET SCHEMA    TRUNCATE TABLE    UPDATE", 
            "title": "SQL Reference Guide"
        }, 
        {
            "location": "/reference/sql_reference/create-statements/", 
            "text": "CREATE Statements\n\n\nUse Create statements to create functions, indexes, procedures, schemas, synonyms, tables, triggers, and views.\n\n\n\n\n\n\nCREATE DISKSTORE\n\n\n\n\n\n\nCREATE FUNCTION\n\n\n\n\n\n\nCREATE INDEX\n\n\n\n\n\n\nCREATE SCHEMA\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nCREATE EXTERNAL TABLE\n\n\n\n\n\n\nCREATE SAMPLE TABLE\n\n\n\n\n\n\nCREATE STREAM TABLE", 
            "title": "CREATE Statements"
        }, 
        {
            "location": "/reference/sql_reference/create-statements/#create-statements", 
            "text": "Use Create statements to create functions, indexes, procedures, schemas, synonyms, tables, triggers, and views.    CREATE DISKSTORE    CREATE FUNCTION    CREATE INDEX    CREATE SCHEMA    CREATE TABLE    CREATE EXTERNAL TABLE    CREATE SAMPLE TABLE    CREATE STREAM TABLE", 
            "title": "CREATE Statements"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/", 
            "text": "CREATE DISKSTORE\n\n\nDisk stores provide disk storage for tables that need to overflow or persist.\n\n\nCREATE DISKSTORE diskstore_name\n\n    [ MAXLOGSIZE max-log-size-in-mb ]\n    [ AUTOCOMPACT boolean-constant ]\n    [ ALLOWFORCECOMPACTION boolean-constant ]\n    [ COMPACTIONTHRESHOLD garbage-threshold ]\n    [ TIMEINTERVAL time-after-which-data-is-flused-to-disk ]\n    [ WRITEBUFFERSIZE buffer-size-in-mb ]\n    [ QUEUESIZE max-row-operations-to-disk ]\n    [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]\n\n\n\n\nDescription\n\n\nSnappyData attempts to preallocate oplog files when you execute the CREATE DISKSTORE command. \n\n\n\n\n\nAll tables that target the same disk store share that disk store's persistence attributes. A table that does not target a named disk store uses the default disk store for overflow or persistence. By default, SnappyData uses the working directory of the member as the default disk store.\n\n\nMAXLOGSIZE\n\n\nSnappyData records DML statements in an operation log (oplog) files. This option sets the maximum size in megabytes that the oplog can become before SnappyData automatically rolls to a new file. This size is the combined sizes of the oplog files. When SnappyData creates an oplog file, it immediately reserves this amount of file space. SnappyData only truncates the unused space on a clean shutdown (for example, \nsnappy server stop\n or \n./sbin/snappy-stop-all\n).\n\n\nThe default value is 1 GB.\n\n\nAUTOCOMPACT\n\n\nSet this option to \"true\" (the default) to automatically compact disk files. Set the option to \"false\" if compaction is not needed or if you intend to manually compact disk files using the \nsnappy\n utility.\n\n\nSnappyData performs compaction by removing \"garbage\" data that DML statements generate in the oplog file.\n\n\nALLOWFORCECOMPACTION\n\n\nSet this option to \"true\" to enable online compaction of oplog files using the \nsnappy\n utility. By default, this option is set to \"false\" (disabled).\n\n\nCOMPACTIONTHRESHOLD\n\n\nSets the threshold for the amount of \"garbage\" data that can exist in the oplog before SnappyData initiates automatic compaction. Garbage data is created as DML operations create, update, and delete rows in a table. The threshold is defined as a percentage (an integer from 0\u2013100). The default is 50. When the amount of \"garbage\" data exceeds this percentage, the disk store becomes eligible for auto-compaction if AUTOCOMPACT is enabled.\n\n\nTIMEINTERVAL\n\n\nSets the number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. TIMEINTERVAL is only used for tables that were created using the \nasynchronous\n option in the persistence clause of the CREATE TABLE statement. See \nCREATE TABLE\n. The default value is 1000 milliseconds (1 second).\n\n\nWRITEBUFFERSIZE\n\n\nSets the buffer size in bytes to use when persisting data to disk. The default is 32768 bytes.\n\n\nQUEUESIZE\n\n\nSets the maximum number of row operations that SnappyData asynchronously queues to disk. After this number of asynchronous operations are queued, additional asynchronous operations block until existing writes are flushed to disk. A single DML operation may affect multiple rows, and each row modification, insertion, and deletion are considered a separate operation. The default QUEUESIZE value is 0, which specifies no limit.\n\n\ndir-name\n\n\nThe optional \ndir-name\n entry defines a specific host system directory to use for the disk store. You can include one or more \ndir-name\n entries using the syntax:\n\n\n[ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]\n\n\n\n\nIn each entry:\n\n\n\n\n\n\ndir-name\n specifies the name of a directory to use for the disk store. The disk store directory is created on each member if necessary. If you do not specify an absolute path, then SnappyData creates or uses the named directory in each member's working directory (or in the value specified by the \nsys-disk-dir\n boot property, if defined). If you specify an absolute path, then all parent directories in the path must exist at the time you execute the command.\n\n\n\n\nNote\n\n\nSnappyData uses a \"shared nothing\" disk store design, and you cannot use a single disk store directory to store oplog files from multiple SnappyData members. \n\n\n\n\n\n\n\n\ndisk-space-in-mb\n optionally specifies the maximum amount of space, in megabytes, to use for the disk store in that directory. The space used is calculated as the combined sizes of all oplog files in the directory.\n\n\nIf you do not specify the \ndisk-space-in-mb\n value, then SnappyData does not impose a limit on the amount of space used by disk store files in that directory. If you do specify a limit, the size must be large enough to accommodate the disk store oplog files (the \nMAXLOGSIZE\n value, or 1 GB by default) and leave enough free space in the directory to avoid low disk space warnings. If you specify a size that cannot accommodate the oplog files and maintain enough free space, SnappyData fails to create the disk store with SQLState error XOZ33: Cannot create oplogs with size {0}MB which is greater than the maximum size {1}MB for store directory ''{2}''.\n\n\n\n\n\n\nYou can specify any number of \ndir-name\n entries in a \nCREATE DISKSTORE\n statement. The data is spread evenly among the active disk files in the directories, keeping within any limits you set.\n\n\nExample\n\n\nThis example uses the default base directory and parameter values to create a named disk store:\n\n\nsnappy\n CREATE DISKSTORE STORE1;\n\n\n\n\nThis example configures disk store parameters and specifies a storage directory:\n\n\nsnappy\n CREATE DISKSTORE STORE1\n      MAXLOGSIZE 1024 \n      AUTOCOMPACT TRUE\n      ALLOWFORCECOMPACTION  FALSE \n      COMPACTIONTHRESHOLD  80\n      TIMEINTERVAL  223344\n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 10240);\n\n\n\n\nThis example specifies multiple storage directories and directory sizes for oplog files:\n\n\nsnappy\n CREATE DISKSTORE STORE1 \n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 456 , 'dir2', 'dir3' 532 );", 
            "title": "CREATE DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/#create-diskstore", 
            "text": "Disk stores provide disk storage for tables that need to overflow or persist.  CREATE DISKSTORE diskstore_name\n\n    [ MAXLOGSIZE max-log-size-in-mb ]\n    [ AUTOCOMPACT boolean-constant ]\n    [ ALLOWFORCECOMPACTION boolean-constant ]\n    [ COMPACTIONTHRESHOLD garbage-threshold ]\n    [ TIMEINTERVAL time-after-which-data-is-flused-to-disk ]\n    [ WRITEBUFFERSIZE buffer-size-in-mb ]\n    [ QUEUESIZE max-row-operations-to-disk ]\n    [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]", 
            "title": "CREATE DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/#description", 
            "text": "SnappyData attempts to preallocate oplog files when you execute the CREATE DISKSTORE command.    All tables that target the same disk store share that disk store's persistence attributes. A table that does not target a named disk store uses the default disk store for overflow or persistence. By default, SnappyData uses the working directory of the member as the default disk store.  MAXLOGSIZE  SnappyData records DML statements in an operation log (oplog) files. This option sets the maximum size in megabytes that the oplog can become before SnappyData automatically rolls to a new file. This size is the combined sizes of the oplog files. When SnappyData creates an oplog file, it immediately reserves this amount of file space. SnappyData only truncates the unused space on a clean shutdown (for example,  snappy server stop  or  ./sbin/snappy-stop-all ).  The default value is 1 GB.  AUTOCOMPACT  Set this option to \"true\" (the default) to automatically compact disk files. Set the option to \"false\" if compaction is not needed or if you intend to manually compact disk files using the  snappy  utility.  SnappyData performs compaction by removing \"garbage\" data that DML statements generate in the oplog file.  ALLOWFORCECOMPACTION  Set this option to \"true\" to enable online compaction of oplog files using the  snappy  utility. By default, this option is set to \"false\" (disabled).  COMPACTIONTHRESHOLD  Sets the threshold for the amount of \"garbage\" data that can exist in the oplog before SnappyData initiates automatic compaction. Garbage data is created as DML operations create, update, and delete rows in a table. The threshold is defined as a percentage (an integer from 0\u2013100). The default is 50. When the amount of \"garbage\" data exceeds this percentage, the disk store becomes eligible for auto-compaction if AUTOCOMPACT is enabled.  TIMEINTERVAL  Sets the number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. TIMEINTERVAL is only used for tables that were created using the  asynchronous  option in the persistence clause of the CREATE TABLE statement. See  CREATE TABLE . The default value is 1000 milliseconds (1 second).  WRITEBUFFERSIZE  Sets the buffer size in bytes to use when persisting data to disk. The default is 32768 bytes.  QUEUESIZE  Sets the maximum number of row operations that SnappyData asynchronously queues to disk. After this number of asynchronous operations are queued, additional asynchronous operations block until existing writes are flushed to disk. A single DML operation may affect multiple rows, and each row modification, insertion, and deletion are considered a separate operation. The default QUEUESIZE value is 0, which specifies no limit.  dir-name  The optional  dir-name  entry defines a specific host system directory to use for the disk store. You can include one or more  dir-name  entries using the syntax:  [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]  In each entry:    dir-name  specifies the name of a directory to use for the disk store. The disk store directory is created on each member if necessary. If you do not specify an absolute path, then SnappyData creates or uses the named directory in each member's working directory (or in the value specified by the  sys-disk-dir  boot property, if defined). If you specify an absolute path, then all parent directories in the path must exist at the time you execute the command.   Note  SnappyData uses a \"shared nothing\" disk store design, and you cannot use a single disk store directory to store oplog files from multiple SnappyData members.      disk-space-in-mb  optionally specifies the maximum amount of space, in megabytes, to use for the disk store in that directory. The space used is calculated as the combined sizes of all oplog files in the directory.  If you do not specify the  disk-space-in-mb  value, then SnappyData does not impose a limit on the amount of space used by disk store files in that directory. If you do specify a limit, the size must be large enough to accommodate the disk store oplog files (the  MAXLOGSIZE  value, or 1 GB by default) and leave enough free space in the directory to avoid low disk space warnings. If you specify a size that cannot accommodate the oplog files and maintain enough free space, SnappyData fails to create the disk store with SQLState error XOZ33: Cannot create oplogs with size {0}MB which is greater than the maximum size {1}MB for store directory ''{2}''.    You can specify any number of  dir-name  entries in a  CREATE DISKSTORE  statement. The data is spread evenly among the active disk files in the directories, keeping within any limits you set.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/#example", 
            "text": "This example uses the default base directory and parameter values to create a named disk store:  snappy  CREATE DISKSTORE STORE1;  This example configures disk store parameters and specifies a storage directory:  snappy  CREATE DISKSTORE STORE1\n      MAXLOGSIZE 1024 \n      AUTOCOMPACT TRUE\n      ALLOWFORCECOMPACTION  FALSE \n      COMPACTIONTHRESHOLD  80\n      TIMEINTERVAL  223344\n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 10240);  This example specifies multiple storage directories and directory sizes for oplog files:  snappy  CREATE DISKSTORE STORE1 \n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 456 , 'dir2', 'dir3' 532 );", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-function/", 
            "text": "Create Function\n\n\nCREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'\n\n\n\n\nDescription\n\n\nCreates a function. Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.\n\n\nYou can extend any one of the interfaces in the package \norg.apache.spark.sql.api.java\n. These interfaces can be included in your client application by adding \nsnappy-spark-sql_2.11-2.0.3-2.jar\n to your classpath.\n\n\n\n\nNote\n\n\nFor input/output types: \n The framework always returns the Java types to the UDFs. So, if you are writing \nscala.math.BigDecimal\n as an input type or output type, an exception is reported. You can use \njava.math.BigDecimal\n in the SCALA code.\n\n\n\n\nReturn Types to UDF Program Type Mapping\n\n\n\n\n\n\n\n\nSnappyData Type\n\n\nUDF Type\n\n\n\n\n\n\n\n\n\n\nSTRING\n\n\njava.lang.String\n\n\n\n\n\n\nINTEGER\n\n\njava.lang.Integer\n\n\n\n\n\n\nLONG\n\n\njava.lang.Long\n\n\n\n\n\n\nDOUBLE\n\n\njava.lang.Double\n\n\n\n\n\n\nDECIMAL\n\n\njava.math.BigDecimal\n\n\n\n\n\n\nDATE\n\n\njava.sql.Date\n\n\n\n\n\n\nTIMESTAMP\n\n\njava.sql.Timestamp\n\n\n\n\n\n\nFLOAT\n\n\njava.lang.Float\n\n\n\n\n\n\nBOOLEAN\n\n\njava.lang.Boolean\n\n\n\n\n\n\nSHORT\n\n\njava.lang.Short\n\n\n\n\n\n\nBYTE\n\n\njava.lang.Byte\n\n\n\n\n\n\nCHAR\n\n\njava.lang.String\n\n\n\n\n\n\nVARCHAR\n\n\njava.lang.String\n\n\n\n\n\n\n\n\nExample\n\n\nCREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'\n\n\n\n\nYou can write a JAVA or SCALA class to write an UDF implementation.", 
            "title": "CREATE FUNCTION"
        }, 
        {
            "location": "/reference/sql_reference/create-function/#create-function", 
            "text": "CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'", 
            "title": "Create Function"
        }, 
        {
            "location": "/reference/sql_reference/create-function/#description", 
            "text": "Creates a function. Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.  You can extend any one of the interfaces in the package  org.apache.spark.sql.api.java . These interfaces can be included in your client application by adding  snappy-spark-sql_2.11-2.0.3-2.jar  to your classpath.   Note  For input/output types:   The framework always returns the Java types to the UDFs. So, if you are writing  scala.math.BigDecimal  as an input type or output type, an exception is reported. You can use  java.math.BigDecimal  in the SCALA code.   Return Types to UDF Program Type Mapping     SnappyData Type  UDF Type      STRING  java.lang.String    INTEGER  java.lang.Integer    LONG  java.lang.Long    DOUBLE  java.lang.Double    DECIMAL  java.math.BigDecimal    DATE  java.sql.Date    TIMESTAMP  java.sql.Timestamp    FLOAT  java.lang.Float    BOOLEAN  java.lang.Boolean    SHORT  java.lang.Short    BYTE  java.lang.Byte    CHAR  java.lang.String    VARCHAR  java.lang.String", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-function/#example", 
            "text": "CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'  You can write a JAVA or SCALA class to write an UDF implementation.", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-index/", 
            "text": "CREATE INDEX\n\n\nCreates an index on one or more columns of a table.\n\n\nCREATE INDEX index_name\n    ON table-name (\n    column-name \n    [ , column-name] * ) \n\n\n\n\nDescription\n\n\nThe \nCREATE INDEX\n statement creates an index on one or more columns of a table. Indexes can speed up queries that use those columns for filtering data, or can also enforce a unique constraint on the indexed columns.\n\n\n\n\nNote\n\n\nCREATE INDEX\n is currently under development for column tables and does not work if the data is updated.\n\n\n\n\nExample\n\n\nCreate an index on two columns:\n\n\nCREATE INDEX idx ON FLIGHTS (flight_id, segment_number);", 
            "title": "CREATE INDEX"
        }, 
        {
            "location": "/reference/sql_reference/create-index/#create-index", 
            "text": "Creates an index on one or more columns of a table.  CREATE INDEX index_name\n    ON table-name (\n    column-name \n    [ , column-name] * )", 
            "title": "CREATE INDEX"
        }, 
        {
            "location": "/reference/sql_reference/create-index/#description", 
            "text": "The  CREATE INDEX  statement creates an index on one or more columns of a table. Indexes can speed up queries that use those columns for filtering data, or can also enforce a unique constraint on the indexed columns.   Note  CREATE INDEX  is currently under development for column tables and does not work if the data is updated.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-index/#example", 
            "text": "Create an index on two columns:  CREATE INDEX idx ON FLIGHTS (flight_id, segment_number);", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/", 
            "text": "CREATE SCHEMA\n\n\nCreates a schema with the given name which provides a mechanism to logically group objects.\n\n\nCREATE SCHEMA schema-name;\n\n\n\n\nDescription\n\n\nThis creates a schema with the given name which provides a mechanism to logically group objects by providing a namespace for objects. This can then be used by other CREATE statements as the namespace prefix. \n\n\nFor example, CREATE TABLE SCHEMA1.TABLE1 ( ... ) creates a table TABLE1 in the schema SCHEMA1. \n\n\n\n\n\nExample\n\n\nCREATE SCHEMA myschema;", 
            "title": "CREATE SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/#create-schema", 
            "text": "Creates a schema with the given name which provides a mechanism to logically group objects.  CREATE SCHEMA schema-name;", 
            "title": "CREATE SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/#description", 
            "text": "This creates a schema with the given name which provides a mechanism to logically group objects by providing a namespace for objects. This can then be used by other CREATE statements as the namespace prefix.   For example, CREATE TABLE SCHEMA1.TABLE1 ( ... ) creates a table TABLE1 in the schema SCHEMA1.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/#example", 
            "text": "CREATE SCHEMA myschema;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-table/", 
            "text": "CREATE TABLE\n\n\nTo Create Row/Column Table:\n\n\nCREATE TABLE [IF NOT EXISTS] table_name {\n    ( column-definition [ , column-definition  ] * )\n    }\n    USING row | column \n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    PARTITION_BY 'column-name', // If not specified it will be a replicated table.\n    BUCKETS  'num-partitions', // Default 113. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time_to_live_in_seconds',\n    COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table.\n    COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer. Only for column table.\n    )\n    [AS select_statement];\n\n\n\n\nRefer to these sections for more information on \nCreating Sample Table\n, \nCreating External Table\n, \nCreating Temporary Table\n, \nCreating Stream Table\n.\n\n\nThe column definition defines the name of a column and its data type.\n\n\n\n\ncolumn-definition\n (for Column Table)\n\n\ncolumn-definition: column-name column-data-type [NOT NULL]\n\ncolumn-name: 'unique column name'\n\n\n\n\n\n\ncolumn-definition\n (for Row Table)\n\n\ncolumn-definition: column-definition-for-row-table | table-constraint\n\ncolumn-definition-for-row-table: column-name column-data-type [ column-constraint ] *\n    [ [ WITH ] DEFAULT { constant-expression | NULL } \n      | [ GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY\n          [ ( START WITH start-value-as-integer [, INCREMENT BY step-value-as-integer ] ) ] ] ]\n    [ column-constraint ] *\n\n\n\n\nRefer to the \nidentity\n section for more information on GENERATED.\n\nRefer to the \nconstraint\n section for more information on table-constraint and column-constraint.\n\n\ncolumn-data-type\n\n\ncolumn-data-type: \n    STRING | \n    INTEGER | \n    INT | \n    BIGINT | \n    LONG |  \n    DOUBLE |  \n    DECIMAL | \n    NUMERIC | \n    DATE | \n    TIMESTAMP | \n    FLOAT | \n    REAL | \n    BOOLEAN | \n    CLOB | \n    BLOB | \n    BINARY | \n    VARBINARY | \n    SMALLINT | \n    SHORT | \n    TINYINT | \n    BYTE | \n    CHAR | \n    VARCHAR  \n\n\n\n\nColumn tables can also use ARRAY, MAP and STRUCT types.\n\nDecimal and numeric has default precision of 38 and scale of 18.\n\nCHAR and VARCHAR expect size from the user.\nIn this release, LONG is supported only for column tables. It is recommended to use BEGIN fo row tables instead.\n\n\n\n\nCOLOCATE_WITH\n\nThe COLOCATE_WITH clause specifies a partitioned table to collocate with. The referenced table must already exist. \n\n\nPARTITION_BY\n\nUse the PARTITION_BY {COLUMN} clause to provide a set of column names that determines the partitioning. \nIf not specified, it is a replicated table.\n Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally on a generated scheme.\n The default number of storage partitions (BUCKETS) is 113 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.\n\n\nBUCKETS\n \n\nThe optional BUCKETS attribute specifies the fixed number of \"buckets\" to use for the partitioned row or column tables. Each data server JVM manages one or more buckets. A bucket is a container of data and is the smallest unit of partitioning and migration in the system. For instance, in a cluster of 5 nodes and bucket count of 25 would result in 5 buckets on each node. But, if you configured the reverse - 25 nodes and a bucket count of 5, only 5 data servers will host all the data for this table. If not specified, the number of buckets defaults to 113.\n\n\nREDUNDANCY\n\nUse the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. It is important to note that a redundancy of '1' implies two physical copies of data. By default, REDUNDANCY is set to 0 (zero).\n\n\nEVICTION_BY\n\nUse the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store. It is important to note that all column tables (expected to host larger data sets) overflow to disk, by default. \n\n\nPERSISTENCE\n\nWhen you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member. \n\n\n\n\nNote\n\n\n\n\n\n\nBy default, both row and column tables are persistent.\n\n\n\n\n\n\nThe option \nPERSISTENT\n has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use \nPERSISTENCE\n instead.\n\n\n\n\n\n\n\n\nDISKSTORE\n\nThe disk directories where you want to persist the table data. By default, SnappyData creates a \"default\" disk store on each member node. You can use this option to control the location where data will be stored. For instance, you may decide to use a network file system or specify multiple disk mount points to uniformly scatter the data across disks. For more information, \nrefer to this document\n.\n\n\nOVERFLOW\n \nUse the OVERFLOW clause to specify the action to be taken upon the eviction event. For persistent tables, setting this to 'true' will overflow the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' will cause the evicted rows to be destroyed in case of eviction event.\n\n\nEXPIRE\n\nUse the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured \ntime_to_live_in_seconds\n.\n\n\nCOLUMN_BATCH_SIZE\n\nThe default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M)\n\n\nCOLUMN_MAX_DELTA_ROWS\n\nThe maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by COLUMN_BATCH_SIZE (and thus limits the size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The default value is 10000. \n\n\n\n\nNote\n\n\nThe following corresponding SQLConf properties for \nCOLUMN_BATCH_SIZE\n and \nCOLUMN_MAX_DELTA_ROWS\n are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL): \n\n\n\n\nsnappydata.column.batchSize\n - Explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit \nCOLUMN_BATCH_SIZE\n specification, then this is inherited for that table property. \n\n\nsnappydata.column.maxDeltaRows\n - The maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit COLUMN_MAX_DELTA_ROWS specification, then this is inherited for that table property.\n\n\n\n\n\n\nTables created using the standard SQL syntax without any of SnappyData specific extensions are created as row-oriented replicated tables. Thus, each data server node in the cluster hosts a consistent replica of the table. All tables are also registered in the Spark catalog and hence visible as DataFrames.\n\n\nFor example, \ncreate table if not exists Table1 (a int)\n is equivalent to \ncreate table if not exists Table1 (a int) using row\n.\n\n\nExamples\n\n\nExample: Column Table Partitioned on a Single Column\n\n\nsnappy\nCREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL))\n    USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY');\n\n\n\n\nExample: Column Table Partitioned with 10 Buckets and Persistence Enabled\n\n\nsnappy\nCREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL))\n    USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY', PERSISTENCE 'SYNCHRONOUS');\n\n\n\n\nExample: Replicated, Persistent Row Table\n\n\nsnappy\nCREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (PERSISTENCE 'ASYNCHRONOUS');\n\n\n\n\nExample: Row Table Partitioned with 10 Buckets and Overflow Enabled\n\n\nsnappy\nCREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (BUCKETS '10',\n      PARTITION_BY 'S_SUPPKEY',\n      PERSISTENCE 'ASYNCHRONOUS',\n      EVICTION_BY 'LRUCOUNT 3',\n      OVERFLOW 'true');\n\n\n\n\nExample: Create Table using Select Query\n\n\nCREATE TABLE CUSTOMER_STAGING USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') AS SELECT * FROM CUSTOMER ;\n\n\n\n\nWith this alternate form of the CREATE TABLE statement, you specify the column names and/or the column data types with a query. The columns in the query result are used as a model for creating the columns in the new table.\n\n\nIf no column names are specified for the new table, then all the columns in the result of the query expression are used to create same-named columns in the new table, of the corresponding data type(s). If one or more column names are specified for the new table, the same number of columns must be present in the result of the query expression; the data types of those columns are used for the corresponding columns of the new table.\n\n\nNote that only the column names and datatypes from the queried table are used when creating the new table. Additional settings in the queried table, such as partitioning, replication, and persistence, are not duplicated. You can optionally specify partitioning, replication, and persistence configuration settings for the new table and those settings need not match the settings of the queried table.\n\n\nExample: Create Table using Spark DataFrame API\n\n\nFor information on using the Apache Spark API, refer to \nUsing the Spark DataFrame API\n.\n\n\n\n\nConstraint (only for Row Tables)\n\n\nA CONSTRAINT clause is an optional part of a CREATE TABLE \n statement that defines a rule to which table data must conform.\n\n\n\n\n\nThere are two types of constraints:\n\n\nColumn-level constraints\n: Refer to a single column in the table and do not specify a column name (except check constraints). They refer to the column that they follow.\n\n\nTable-level constraints\n: Refer to one or more columns in the table. Table-level constraints specify the names of the columns to which they apply. Table-level CHECK constraints can refer to 0 or more columns in the table.\n\n\nColumn and table constraints include:\n\n\n\n\n\n\nNOT NULL\u2014 Specifies that a column cannot hold NULL values (constraints of this type are not nameable).\n\n\n\n\n\n\nPRIMARY KEY\u2014 Specifies a column (or multiple columns if specified in a table constraint) that uniquely identifies a row in the table. The identified columns must be defined as NOT NULL.\n\n\n\n\n\n\nUNIQUE\u2014 Specifies that values in the column must be unique. NULL values are not allowed.\n\n\n\n\n\n\nFOREIGN KEY\u2014 Specifies that the values in the columns must correspond to values in referenced primary key or unique columns or that they are NULL. \nIf the foreign key consists of multiple columns and any column is NULL, then the whole key is considered NULL. SnappyData permits the insert no matter what is in the non-null columns.\n\n\n\n\n\n\n\nCHECK\u2014 Specifies rules for values in a column, or specifies a wide range of rules for values when included as a table constraint. The CHECK constraint has the same format and restrictions for column and table constraints.\nColumn constraints and table constraints have the same function; the difference is where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition. \n\n\n\n\nColumn-level constraints (except for check constraints) refer to only one column.\nIf you do not specify a name for a column or table constraint, then SnappyData generates a unique name.\n\n\n\n\nIdentity Columns (only for Row Tables)\n\n\n\nSnappyData supports both GENERATED ALWAYS and GENERATED BY DEFAULT identity columns only for BIGINT and INTEGER data types. The START WITH and INCREMENT BY clauses are supported only for GENERATED BY DEFAULT identity columns. \n\n\n\n\n\nFor a GENERATED ALWAYS identity column, SnappyData increments the default value on every insertion, and stores the incremented value in the column. You cannot insert a value directly into a GENERATED ALWAYS identity column, and you cannot update a value in a GENERATED ALWAYS identity column. Instead, you must either specify the DEFAULT keyword when inserting data into the table or you must leave the identity column out of the insertion column list.\n\n\nConsider a table with the following column definition:\n\n\ncreate table greetings (i int generated always as identity, ch char(50)) using row;\n\n\n\n\nYou can insert rows into the table using either the DEFAULT keyword or by omitting the identity column from the INSERT statement:\n\n\ninsert into greetings values (DEFAULT, 'hello');\n\n\n\n\ninsert into greetings(ch) values ('hi');\n\n\n\n\nThe values that SnappyData automatically generates for a GENERATED ALWAYS identity column are unique.\n\n\nFor a GENERATED BY DEFAULT identity column, SnappyData increments and uses a default value for an INSERT only when no explicit value is given. To use the generated default value, either specify the DEFAULT keyword when inserting into the identity column or leave the identity column out of the INSERT column list.\n\n\nIn contrast to GENERATED ALWAYS identity columns, with a GENERATED BY DEFAULT column you can specify an identity value to use instead of the generated default value. To specify a value, include it in the INSERT statement.\n\n\nFor example, consider a table created using the statement:\n\n\ncreate table greetings (i int generated by default as identity, ch char(50)); \n\n\n\n\nThe following statement specifies the value \u201c1\u201d for the identity column:\n\n\ninsert into greetings values (1, 'hi'); \n\n\n\n\nThese statements both use generated default values:\n\n\ninsert into greetings values (DEFAULT, 'hello');\ninsert into greetings(ch) values ('bye');\n\n\n\n\nAlthough the automatically-generated values in a GENERATED BY DEFAULT identity column are unique, a GENERATED BY DEFAULT column does not guarantee unique identity values for all rows in the table. For example, in the above statements, the rows containing \u201chi\u201d and \u201chello\u201d both have an identity value of \u201c1.\u201d This occurs because the generated column starts at \u201c1\u201d and the user-specified value was also \u201c1.\u201d\n\n\nTo avoid duplicating identity values (for example, during an import operation), you can use the START WITH clause to specify the first identity value that SnappyData should assign and increment. Or, you can use a primary key or a unique constraint on the GENERATED BY DEFAULT identity column to check for and disallow duplicates.\n\n\nBy default, the initial value of a GENERATED BY DEFAULT identity column is 1, and the value is incremented by 1 for each INSERT. Use the optional START WITH clause to specify a new initial value. Use the optional INCREMENT BY clause to change the increment value used during each INSERT.", 
            "title": "CREATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#create-table", 
            "text": "To Create Row/Column Table:  CREATE TABLE [IF NOT EXISTS] table_name {\n    ( column-definition [ , column-definition  ] * )\n    }\n    USING row | column \n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    PARTITION_BY 'column-name', // If not specified it will be a replicated table.\n    BUCKETS  'num-partitions', // Default 113. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time_to_live_in_seconds',\n    COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table.\n    COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer. Only for column table.\n    )\n    [AS select_statement];  Refer to these sections for more information on  Creating Sample Table ,  Creating External Table ,  Creating Temporary Table ,  Creating Stream Table .  The column definition defines the name of a column and its data type.   column-definition  (for Column Table)  column-definition: column-name column-data-type [NOT NULL]\n\ncolumn-name: 'unique column name'   column-definition  (for Row Table)  column-definition: column-definition-for-row-table | table-constraint\n\ncolumn-definition-for-row-table: column-name column-data-type [ column-constraint ] *\n    [ [ WITH ] DEFAULT { constant-expression | NULL } \n      | [ GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY\n          [ ( START WITH start-value-as-integer [, INCREMENT BY step-value-as-integer ] ) ] ] ]\n    [ column-constraint ] *  Refer to the  identity  section for more information on GENERATED. \nRefer to the  constraint  section for more information on table-constraint and column-constraint.  column-data-type  column-data-type: \n    STRING | \n    INTEGER | \n    INT | \n    BIGINT | \n    LONG |  \n    DOUBLE |  \n    DECIMAL | \n    NUMERIC | \n    DATE | \n    TIMESTAMP | \n    FLOAT | \n    REAL | \n    BOOLEAN | \n    CLOB | \n    BLOB | \n    BINARY | \n    VARBINARY | \n    SMALLINT | \n    SHORT | \n    TINYINT | \n    BYTE | \n    CHAR | \n    VARCHAR    Column tables can also use ARRAY, MAP and STRUCT types. \nDecimal and numeric has default precision of 38 and scale of 18. \nCHAR and VARCHAR expect size from the user.\nIn this release, LONG is supported only for column tables. It is recommended to use BEGIN fo row tables instead.   COLOCATE_WITH \nThe COLOCATE_WITH clause specifies a partitioned table to collocate with. The referenced table must already exist.   PARTITION_BY \nUse the PARTITION_BY {COLUMN} clause to provide a set of column names that determines the partitioning.  If not specified, it is a replicated table.  Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally on a generated scheme.  The default number of storage partitions (BUCKETS) is 113 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.  BUCKETS   \nThe optional BUCKETS attribute specifies the fixed number of \"buckets\" to use for the partitioned row or column tables. Each data server JVM manages one or more buckets. A bucket is a container of data and is the smallest unit of partitioning and migration in the system. For instance, in a cluster of 5 nodes and bucket count of 25 would result in 5 buckets on each node. But, if you configured the reverse - 25 nodes and a bucket count of 5, only 5 data servers will host all the data for this table. If not specified, the number of buckets defaults to 113.  REDUNDANCY \nUse the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. It is important to note that a redundancy of '1' implies two physical copies of data. By default, REDUNDANCY is set to 0 (zero).  EVICTION_BY \nUse the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store. It is important to note that all column tables (expected to host larger data sets) overflow to disk, by default.   PERSISTENCE \nWhen you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.    Note    By default, both row and column tables are persistent.    The option  PERSISTENT  has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use  PERSISTENCE  instead.     DISKSTORE \nThe disk directories where you want to persist the table data. By default, SnappyData creates a \"default\" disk store on each member node. You can use this option to control the location where data will be stored. For instance, you may decide to use a network file system or specify multiple disk mount points to uniformly scatter the data across disks. For more information,  refer to this document .  OVERFLOW  \nUse the OVERFLOW clause to specify the action to be taken upon the eviction event. For persistent tables, setting this to 'true' will overflow the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' will cause the evicted rows to be destroyed in case of eviction event.  EXPIRE \nUse the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured  time_to_live_in_seconds .  COLUMN_BATCH_SIZE \nThe default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M)  COLUMN_MAX_DELTA_ROWS \nThe maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by COLUMN_BATCH_SIZE (and thus limits the size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The default value is 10000.    Note  The following corresponding SQLConf properties for  COLUMN_BATCH_SIZE  and  COLUMN_MAX_DELTA_ROWS  are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL):    snappydata.column.batchSize  - Explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit  COLUMN_BATCH_SIZE  specification, then this is inherited for that table property.   snappydata.column.maxDeltaRows  - The maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit COLUMN_MAX_DELTA_ROWS specification, then this is inherited for that table property.    Tables created using the standard SQL syntax without any of SnappyData specific extensions are created as row-oriented replicated tables. Thus, each data server node in the cluster hosts a consistent replica of the table. All tables are also registered in the Spark catalog and hence visible as DataFrames.  For example,  create table if not exists Table1 (a int)  is equivalent to  create table if not exists Table1 (a int) using row .", 
            "title": "CREATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-column-table-partitioned-on-a-single-column", 
            "text": "snappy CREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL))\n    USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY');", 
            "title": "Example: Column Table Partitioned on a Single Column"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-column-table-partitioned-with-10-buckets-and-persistence-enabled", 
            "text": "snappy CREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL))\n    USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY', PERSISTENCE 'SYNCHRONOUS');", 
            "title": "Example: Column Table Partitioned with 10 Buckets and Persistence Enabled"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-replicated-persistent-row-table", 
            "text": "snappy CREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (PERSISTENCE 'ASYNCHRONOUS');", 
            "title": "Example: Replicated, Persistent Row Table"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-row-table-partitioned-with-10-buckets-and-overflow-enabled", 
            "text": "snappy CREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (BUCKETS '10',\n      PARTITION_BY 'S_SUPPKEY',\n      PERSISTENCE 'ASYNCHRONOUS',\n      EVICTION_BY 'LRUCOUNT 3',\n      OVERFLOW 'true');", 
            "title": "Example: Row Table Partitioned with 10 Buckets and Overflow Enabled"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-create-table-using-select-query", 
            "text": "CREATE TABLE CUSTOMER_STAGING USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') AS SELECT * FROM CUSTOMER ;  With this alternate form of the CREATE TABLE statement, you specify the column names and/or the column data types with a query. The columns in the query result are used as a model for creating the columns in the new table.  If no column names are specified for the new table, then all the columns in the result of the query expression are used to create same-named columns in the new table, of the corresponding data type(s). If one or more column names are specified for the new table, the same number of columns must be present in the result of the query expression; the data types of those columns are used for the corresponding columns of the new table.  Note that only the column names and datatypes from the queried table are used when creating the new table. Additional settings in the queried table, such as partitioning, replication, and persistence, are not duplicated. You can optionally specify partitioning, replication, and persistence configuration settings for the new table and those settings need not match the settings of the queried table.", 
            "title": "Example: Create Table using Select Query"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-create-table-using-spark-dataframe-api", 
            "text": "For information on using the Apache Spark API, refer to  Using the Spark DataFrame API .", 
            "title": "Example: Create Table using Spark DataFrame API"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#constraint-only-for-row-tables", 
            "text": "A CONSTRAINT clause is an optional part of a CREATE TABLE   statement that defines a rule to which table data must conform.   There are two types of constraints:  Column-level constraints : Refer to a single column in the table and do not specify a column name (except check constraints). They refer to the column that they follow.  Table-level constraints : Refer to one or more columns in the table. Table-level constraints specify the names of the columns to which they apply. Table-level CHECK constraints can refer to 0 or more columns in the table.  Column and table constraints include:    NOT NULL\u2014 Specifies that a column cannot hold NULL values (constraints of this type are not nameable).    PRIMARY KEY\u2014 Specifies a column (or multiple columns if specified in a table constraint) that uniquely identifies a row in the table. The identified columns must be defined as NOT NULL.    UNIQUE\u2014 Specifies that values in the column must be unique. NULL values are not allowed.    FOREIGN KEY\u2014 Specifies that the values in the columns must correspond to values in referenced primary key or unique columns or that they are NULL.  If the foreign key consists of multiple columns and any column is NULL, then the whole key is considered NULL. SnappyData permits the insert no matter what is in the non-null columns.    CHECK\u2014 Specifies rules for values in a column, or specifies a wide range of rules for values when included as a table constraint. The CHECK constraint has the same format and restrictions for column and table constraints.\nColumn constraints and table constraints have the same function; the difference is where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition.    Column-level constraints (except for check constraints) refer to only one column.\nIf you do not specify a name for a column or table constraint, then SnappyData generates a unique name.", 
            "title": "Constraint (only for Row Tables)"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#identity-columns-only-for-row-tables", 
            "text": "SnappyData supports both GENERATED ALWAYS and GENERATED BY DEFAULT identity columns only for BIGINT and INTEGER data types. The START WITH and INCREMENT BY clauses are supported only for GENERATED BY DEFAULT identity columns.    For a GENERATED ALWAYS identity column, SnappyData increments the default value on every insertion, and stores the incremented value in the column. You cannot insert a value directly into a GENERATED ALWAYS identity column, and you cannot update a value in a GENERATED ALWAYS identity column. Instead, you must either specify the DEFAULT keyword when inserting data into the table or you must leave the identity column out of the insertion column list.  Consider a table with the following column definition:  create table greetings (i int generated always as identity, ch char(50)) using row;  You can insert rows into the table using either the DEFAULT keyword or by omitting the identity column from the INSERT statement:  insert into greetings values (DEFAULT, 'hello');  insert into greetings(ch) values ('hi');  The values that SnappyData automatically generates for a GENERATED ALWAYS identity column are unique.  For a GENERATED BY DEFAULT identity column, SnappyData increments and uses a default value for an INSERT only when no explicit value is given. To use the generated default value, either specify the DEFAULT keyword when inserting into the identity column or leave the identity column out of the INSERT column list.  In contrast to GENERATED ALWAYS identity columns, with a GENERATED BY DEFAULT column you can specify an identity value to use instead of the generated default value. To specify a value, include it in the INSERT statement.  For example, consider a table created using the statement:  create table greetings (i int generated by default as identity, ch char(50));   The following statement specifies the value \u201c1\u201d for the identity column:  insert into greetings values (1, 'hi');   These statements both use generated default values:  insert into greetings values (DEFAULT, 'hello');\ninsert into greetings(ch) values ('bye');  Although the automatically-generated values in a GENERATED BY DEFAULT identity column are unique, a GENERATED BY DEFAULT column does not guarantee unique identity values for all rows in the table. For example, in the above statements, the rows containing \u201chi\u201d and \u201chello\u201d both have an identity value of \u201c1.\u201d This occurs because the generated column starts at \u201c1\u201d and the user-specified value was also \u201c1.\u201d  To avoid duplicating identity values (for example, during an import operation), you can use the START WITH clause to specify the first identity value that SnappyData should assign and increment. Or, you can use a primary key or a unique constraint on the GENERATED BY DEFAULT identity column to check for and disallow duplicates.  By default, the initial value of a GENERATED BY DEFAULT identity column is 1, and the value is incremented by 1 for each INSERT. Use the optional START WITH clause to specify a new initial value. Use the optional INCREMENT BY clause to change the increment value used during each INSERT.", 
            "title": "Identity Columns (only for Row Tables)"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/", 
            "text": "CREATE SAMPLE TABLE\n\n\nMode 1\n\n\nCREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )\n    USING column_sample\n    OPTIONS (\n    baseTable 'baseTableName',\n    BUCKETS  'num-partitions', // Default 113. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    [AS select_statement];\n\n\n\n\nMode 2\n\n\nCREATE SAMPLE TABLE table_name ON base_table_name\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    BUCKETS  'num-partitions', // Default 113. Must be an integer.\n    REDUNDANCY        'num-redundant-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    AS select_statement\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nWhen creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. The sample table also inherits the number of buckets, redundancy and persistence properties from the base table.\nFor sample tables, the overflow property is set to \nFalse\n by default. For column tables the default value is \nTrue\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating External Table\n, \nCreating Temporary Table\n and \nCreating Stream Table\n.\n\n\nDescription\n\n\n\n\n\n\nQCS\n: Query Column Set. These columns are used for stratification in stratified sampling. \n\n\n\n\n\n\nFRACTION\n: This represents the fraction of the full population (base table) that is managed in the sample. \n\n\n\n\n\n\nSTRATARESERVOIRSIZE\n: The initial capacity of each stratum.\n\n\n\n\n\n\nbaseTable\n: Table on which sampling is done.\n\n\n\n\n\n\nExamples:\n\n\nMode 1 Example\n\n\nsnappy\nCREATE TABLE CUSTOMER_SAMPLE ( \n      C_CUSTKEY     INTEGER NOT NULL,\n      C_NAME        VARCHAR(25) NOT NULL,\n      C_ADDRESS     VARCHAR(40) NOT NULL,\n      C_NATIONKEY   INTEGER NOT NULL,\n      C_PHONE       VARCHAR(15) NOT NULL,\n      C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n      C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n      C_COMMENT     VARCHAR(117) NOT NULL)\n      USING COLUMN_SAMPLE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50', baseTable 'CUSTOMER_BASE');\n\n\n\n\nMode 2 Example\n\n\nsnappy\nCREATE SAMPLE TABLE CUSTOMER_SAMPLE on CUSTOMER_BASE\n      OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50') AS (SELECT * FROM CUSTOMER_BASE);\n\n\n\n\n\n\nNote\n\n\nRefer to \ncreate sample tables in SDE section\n for more information on creating sample tables on datasets that can be sourced from any source supported in Spark/SnappyData.", 
            "title": "CREATE SAMPLE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#create-sample-table", 
            "text": "Mode 1  CREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )\n    USING column_sample\n    OPTIONS (\n    baseTable 'baseTableName',\n    BUCKETS  'num-partitions', // Default 113. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    [AS select_statement];  Mode 2  CREATE SAMPLE TABLE table_name ON base_table_name\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    BUCKETS  'num-partitions', // Default 113. Must be an integer.\n    REDUNDANCY        'num-redundant-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    AS select_statement  For more information on column-definition, refer to  Column Definition For Column Table .  When creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. The sample table also inherits the number of buckets, redundancy and persistence properties from the base table.\nFor sample tables, the overflow property is set to  False  by default. For column tables the default value is  True .  Refer to these sections for more information on  Creating Table ,  Creating External Table ,  Creating Temporary Table  and  Creating Stream Table .", 
            "title": "CREATE SAMPLE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#description", 
            "text": "QCS : Query Column Set. These columns are used for stratification in stratified sampling.     FRACTION : This represents the fraction of the full population (base table) that is managed in the sample.     STRATARESERVOIRSIZE : The initial capacity of each stratum.    baseTable : Table on which sampling is done.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#examples", 
            "text": "", 
            "title": "Examples:"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#mode-1-example", 
            "text": "snappy CREATE TABLE CUSTOMER_SAMPLE ( \n      C_CUSTKEY     INTEGER NOT NULL,\n      C_NAME        VARCHAR(25) NOT NULL,\n      C_ADDRESS     VARCHAR(40) NOT NULL,\n      C_NATIONKEY   INTEGER NOT NULL,\n      C_PHONE       VARCHAR(15) NOT NULL,\n      C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n      C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n      C_COMMENT     VARCHAR(117) NOT NULL)\n      USING COLUMN_SAMPLE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50', baseTable 'CUSTOMER_BASE');", 
            "title": "Mode 1 Example"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#mode-2-example", 
            "text": "snappy CREATE SAMPLE TABLE CUSTOMER_SAMPLE on CUSTOMER_BASE\n      OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50') AS (SELECT * FROM CUSTOMER_BASE);   Note  Refer to  create sample tables in SDE section  for more information on creating sample tables on datasets that can be sourced from any source supported in Spark/SnappyData.", 
            "title": "Mode 2 Example"
        }, 
        {
            "location": "/reference/sql_reference/create-external-table/", 
            "text": "CREATE EXTERNAL TABLE\n\n\nCREATE EXTERNAL TABLE [IF NOT EXISTS] [schema_name.]table_name\n    ( column-definition [ , column-definition  ] * )\n    USING datasource\n    [OPTIONS (key1=val1, key2=val2, ...)]\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating Sample Table\n, \nCreating Temporary Table\n and \nCreating Stream Table\n.\n\n\nEXTERNAL\n\nExternal tables point to external data sources. SnappyData supports all the data sources supported by Spark. You should use external tables to load data in parallel from any of the external sources. The table definition is persisted in the catalog and visible across all sessions. \n\n\nUSING \n\nSpecify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. Note that most of the prominent datastores provide an implementation of 'DataSource' and accessible as a table. For instance, you can use the Cassandra spark package to create external tables pointing to Cassandra tables and directly run queries on them. You can mix any external table and SnappyData managed tables in your queries. \n\n\nExample\n\n\nCreate an external table using PARQUET data source\n\n\nsnappy\n CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData');\n\n\n\n\nCreate an external table using CSV data source\n\n\nCREATE EXTERNAL TABLE IF NOT EXISTS CUSTOMER_STAGING USING csv OPTIONS(path '../../quickstart/src/main/resources/customer.csv');\n\n\n\n\nCREATE EXTERNAL TABLE CUSTOMER_STAGING_1 (C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, \nC_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, \nC_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) \nUSING csv OPTIONS (path '../../quickstart/src/main/resources/customer.csv');\n\n\n\n\nYou can also load data from AWS S3, as given in the example below:\n\n\nCREATE EXTERNAL TABLE NYCTAXI USINg parquet OPTIONS(path 's3a://\nAWS_SECRET_KEY\n:\nAWS_SECRET_ID\n@\nfolder\n/\ndata\n');\n\n\n\n\nFor more information on loading data from AWS, refer \nLoading Data from AWS S3\n.", 
            "title": "CREATE EXTERNAL TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-external-table/#create-external-table", 
            "text": "CREATE EXTERNAL TABLE [IF NOT EXISTS] [schema_name.]table_name\n    ( column-definition [ , column-definition  ] * )\n    USING datasource\n    [OPTIONS (key1=val1, key2=val2, ...)]  For more information on column-definition, refer to  Column Definition For Column Table .  Refer to these sections for more information on  Creating Table ,  Creating Sample Table ,  Creating Temporary Table  and  Creating Stream Table .  EXTERNAL \nExternal tables point to external data sources. SnappyData supports all the data sources supported by Spark. You should use external tables to load data in parallel from any of the external sources. The table definition is persisted in the catalog and visible across all sessions.   USING  \nSpecify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. Note that most of the prominent datastores provide an implementation of 'DataSource' and accessible as a table. For instance, you can use the Cassandra spark package to create external tables pointing to Cassandra tables and directly run queries on them. You can mix any external table and SnappyData managed tables in your queries.", 
            "title": "CREATE EXTERNAL TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-external-table/#example", 
            "text": "Create an external table using PARQUET data source  snappy  CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData');  Create an external table using CSV data source  CREATE EXTERNAL TABLE IF NOT EXISTS CUSTOMER_STAGING USING csv OPTIONS(path '../../quickstart/src/main/resources/customer.csv');  CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 (C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, \nC_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, \nC_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) \nUSING csv OPTIONS (path '../../quickstart/src/main/resources/customer.csv');  You can also load data from AWS S3, as given in the example below:  CREATE EXTERNAL TABLE NYCTAXI USINg parquet OPTIONS(path 's3a:// AWS_SECRET_KEY : AWS_SECRET_ID @ folder / data ');  For more information on loading data from AWS, refer  Loading Data from AWS S3 .", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/", 
            "text": "CREATE TEMPORARY TABLE\n\n\nCREATE TEMPORARY TABLE [schema_name.]table_name\n    ( column-definition [ , column-definition  ] * )\n    USING datasource\n    [OPTIONS (key1=val1, key2=val2, ...)]\n    [AS select_statement]\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating Sample Table\n, \nCreating External Table\n and \nCreating Stream Table\n.\n\n\nTEMPORARY\n\n\nTemporary tables are scoped to SQL connection or the Snappy Spark session that creates it. This table does not appear in the system catalog nor visible to other connections or sessions.\n\n\nUSING \n\n\nSpecify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister.\n\n\nAS \n\nPopulate the table with input data from the select statement. \n\n\nExamples\n\n\nCreate a Temporary Table\n\n\nsnappy\n CREATE TEMPORARY TABLE STAGING_AIRLINEREF USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData');\n\n\n\n\nsnappy\n CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP (CODE2 string, DESCRIPTION2 String) AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;\n\n\n\n\nsnappy\n CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;", 
            "title": "CREATE TEMPORARY TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#create-temporary-table", 
            "text": "CREATE TEMPORARY TABLE [schema_name.]table_name\n    ( column-definition [ , column-definition  ] * )\n    USING datasource\n    [OPTIONS (key1=val1, key2=val2, ...)]\n    [AS select_statement]  For more information on column-definition, refer to  Column Definition For Column Table .  Refer to these sections for more information on  Creating Table ,  Creating Sample Table ,  Creating External Table  and  Creating Stream Table .  TEMPORARY  Temporary tables are scoped to SQL connection or the Snappy Spark session that creates it. This table does not appear in the system catalog nor visible to other connections or sessions.  USING   Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister.  AS  \nPopulate the table with input data from the select statement.", 
            "title": "CREATE TEMPORARY TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#examples", 
            "text": "Create a Temporary Table  snappy  CREATE TEMPORARY TABLE STAGING_AIRLINEREF USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData');  snappy  CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP (CODE2 string, DESCRIPTION2 String) AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;  snappy  CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/", 
            "text": "CREATE STREAM TABLE\n\n\nTo Create Stream Table:\n\n\n// DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n    ( column-definition [ , column-definition  ] * )\n    USING kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream\n    OPTIONS (\n    // multiple stream source specific options\n      storagelevel 'cache-data-option',\n      rowConverter 'rowconverter-class-name',\n      topics 'comma-seperated-topic-name',\n      kafkaParams 'kafka-related-params',\n      consumerKey 'consumer-key',\n      consumerSecret 'consumer-secret',\n      accessToken 'access-token',\n      accessTokenSecret 'access-token-secret',\n      hostname 'socket-streaming-hostname',\n      port 'socket-streaming-port-number',\n      directory 'file-streaming-directory'\n    )\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating Sample Table\n, \nCreating External Table\n and \nCreating Temporary Table\n.\n\n\nDescription\n\n\nCreate a stream table using a stream data source. If a table with the same name already exists in the database, an exception will be thrown.\n\n\nUSING \ndata source\n \n\nSpecify the streaming source to be used for this table.\n\n\nstorageLevel\n\nProvides different trade-offs between memory usage and CPU efficiency.\n\n\nrowConverter\n\nConverts the unstructured streaming data to a set of rows.\n\n\ntopics\n\nSubscribed Kafka topics.\n\n\nkafkaParams\n\nKafka configuration parameters such as \nmetadata.broker.list\n, \nbootstrap.servers\n etc.\n\n\ndirectory\n\nHDFS directory to monitor for the new file.\n\n\nhostname\n\nHostname to connect to, for receiving data.\n\n\nport\n\nPort to connect to, for receiving data.\n\n\nconsumerKey\n\nConsumer Key (API Key) for your Twitter account.\n\n\nconsumerSecret\n\nConsumer Secret key for your Twitter account.\n\n\naccessToken\n\nAccess token for your Twitter account.\n\n\naccessTokenSecret\n\nAccess token secret for your Twitter account.\n\n\n\n\nNote\n\n\nYou need to register to \nhttps://apps.twitter.com/\n to get the \nconsumerKey\n, \nconsumerSecret\n, \naccessToken\n and \naccessTokenSecret\n credentials.\n\n\n\n\nExample\n\n\n//create a connection\nsnappy\n connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy\n streaming init 2secs;\n\n// Create a stream table\nsnappy\n create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy\n streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy\n select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy\n drop table streamTable;\n\n// Stop the streaming\nsnappy\n streaming stop;", 
            "title": "CREATE STREAM TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/#create-stream-table", 
            "text": "To Create Stream Table:  // DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n    ( column-definition [ , column-definition  ] * )\n    USING kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream\n    OPTIONS (\n    // multiple stream source specific options\n      storagelevel 'cache-data-option',\n      rowConverter 'rowconverter-class-name',\n      topics 'comma-seperated-topic-name',\n      kafkaParams 'kafka-related-params',\n      consumerKey 'consumer-key',\n      consumerSecret 'consumer-secret',\n      accessToken 'access-token',\n      accessTokenSecret 'access-token-secret',\n      hostname 'socket-streaming-hostname',\n      port 'socket-streaming-port-number',\n      directory 'file-streaming-directory'\n    )  For more information on column-definition, refer to  Column Definition For Column Table .  Refer to these sections for more information on  Creating Table ,  Creating Sample Table ,  Creating External Table  and  Creating Temporary Table .", 
            "title": "CREATE STREAM TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/#description", 
            "text": "Create a stream table using a stream data source. If a table with the same name already exists in the database, an exception will be thrown.  USING  data source   \nSpecify the streaming source to be used for this table.  storageLevel \nProvides different trade-offs between memory usage and CPU efficiency.  rowConverter \nConverts the unstructured streaming data to a set of rows.  topics \nSubscribed Kafka topics.  kafkaParams \nKafka configuration parameters such as  metadata.broker.list ,  bootstrap.servers  etc.  directory \nHDFS directory to monitor for the new file.  hostname \nHostname to connect to, for receiving data.  port \nPort to connect to, for receiving data.  consumerKey \nConsumer Key (API Key) for your Twitter account.  consumerSecret \nConsumer Secret key for your Twitter account.  accessToken \nAccess token for your Twitter account.  accessTokenSecret \nAccess token secret for your Twitter account.   Note  You need to register to  https://apps.twitter.com/  to get the  consumerKey ,  consumerSecret ,  accessToken  and  accessTokenSecret  credentials.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/#example", 
            "text": "//create a connection\nsnappy  connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy  streaming init 2secs;\n\n// Create a stream table\nsnappy  create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy  streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy  select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy  drop table streamTable;\n\n// Stop the streaming\nsnappy  streaming stop;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/delete/", 
            "text": "DELETE\n\n\nDelete rows from a table.\n\n\n{\n    DELETE FROM table-name [ ]\n        [ WHERE ]\n}\n\n\n\n\nDescription\n\n\nThis form is called a searched delete, removes all rows identified by the table name and WHERE clause.\n\n\nExample\n\n\n-- Delete rows from the CUSTOMERS table where the CID is equal to 10.\nDELETE FROM TRADE.CUSTOMERS WHERE CID = 10;\n\n-- Delete all rows from table T.\nDELETE FROM T;\n\n\n\n\n\n\nNote\n\n\nThis statement is currently supported only for row tables.", 
            "title": "DELETE"
        }, 
        {
            "location": "/reference/sql_reference/delete/#delete", 
            "text": "Delete rows from a table.  {\n    DELETE FROM table-name [ ]\n        [ WHERE ]\n}", 
            "title": "DELETE"
        }, 
        {
            "location": "/reference/sql_reference/delete/#description", 
            "text": "This form is called a searched delete, removes all rows identified by the table name and WHERE clause.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/delete/#example", 
            "text": "-- Delete rows from the CUSTOMERS table where the CID is equal to 10.\nDELETE FROM TRADE.CUSTOMERS WHERE CID = 10;\n\n-- Delete all rows from table T.\nDELETE FROM T;   Note  This statement is currently supported only for row tables.", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-statements/", 
            "text": "Drop Statements\n\n\nDrop statements are used to drop functions, indexes, procedures, and tables.\n\n\n\n\n\n\nDROP DISKSTORE\n\n\n\n\n\n\nDROP FUNCTION\n\n\n\n\n\n\nDROP INDEX\n\n\n\n\n\n\nDROP TABLE", 
            "title": "DROP Statements"
        }, 
        {
            "location": "/reference/sql_reference/drop-statements/#drop-statements", 
            "text": "Drop statements are used to drop functions, indexes, procedures, and tables.    DROP DISKSTORE    DROP FUNCTION    DROP INDEX    DROP TABLE", 
            "title": "Drop Statements"
        }, 
        {
            "location": "/reference/sql_reference/drop-diskstore/", 
            "text": "DROP DISKSTORE\n\n\nRemoves a disk store configuration from the SnappyData cluster.\n\n\nDROP DISKSTORE [ IF EXISTS ] store-name\n\n\n\n\nIF EXISTS\n\nInclude the \nIF EXISTS\n clause to execute the statement only if the specified disk store exists in SnappyData.\n\n\nstore-name\n\nUser-defined name of the disk store configuration that you want to remove. The available names are stored in the \nSYSDISKSTORES\n system table.\n\n\nExample\n\n\nThis command removes the disk store \"STORE1\" from the cluster:\n\n\nDROP DISKSTORE store1;", 
            "title": "DROP DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/drop-diskstore/#drop-diskstore", 
            "text": "Removes a disk store configuration from the SnappyData cluster.  DROP DISKSTORE [ IF EXISTS ] store-name  IF EXISTS \nInclude the  IF EXISTS  clause to execute the statement only if the specified disk store exists in SnappyData.  store-name \nUser-defined name of the disk store configuration that you want to remove. The available names are stored in the  SYSDISKSTORES  system table.", 
            "title": "DROP DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/drop-diskstore/#example", 
            "text": "This command removes the disk store \"STORE1\" from the cluster:  DROP DISKSTORE store1;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/", 
            "text": "Drop Function\n\n\nDROP FUNCTION IF EXISTS udf_name\n\n\n\n\nDescription\n\n\nDrops an existing function. If the function to drop does not exist, an exception is reported.\n\n\nExample\n\n\nDROP FUNCTION IF EXISTS app.strnglen", 
            "title": "DROP FUNCTION"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/#drop-function", 
            "text": "DROP FUNCTION IF EXISTS udf_name", 
            "title": "Drop Function"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/#description", 
            "text": "Drops an existing function. If the function to drop does not exist, an exception is reported.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/#example", 
            "text": "DROP FUNCTION IF EXISTS app.strnglen", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/", 
            "text": "DROP INDEX\n\n\nDrops the specified index.\n\n\nDROP INDEX [ IF EXISTS ] [schema-name.]index-name\n\n\n\n\nDescription\n\n\nDrops the index in the given schema (or current schema if none is provided). Include the \nIF EXISTS\n clause to execute the statement only if the specified index exists in SnappyData.\n\n\nExample\n\n\nDROP INDEX IF EXISTS app.idx", 
            "title": "DROP INDEX"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/#drop-index", 
            "text": "Drops the specified index.  DROP INDEX [ IF EXISTS ] [schema-name.]index-name", 
            "title": "DROP INDEX"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/#description", 
            "text": "Drops the index in the given schema (or current schema if none is provided). Include the  IF EXISTS  clause to execute the statement only if the specified index exists in SnappyData.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/#example", 
            "text": "DROP INDEX IF EXISTS app.idx", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/", 
            "text": "DROP TABLE\n\n\nRemoves the specified table.\n\n\nDROP TABLE [ IF EXISTS ] [schema-name.]table-name\n\n\n\n\nDescription\n\n\nRemoves the specified table. Include the \nIF EXISTS\n clause to execute the statement only if the specified table exists in SnappyData. The \nschema-name.\n prefix is optional if you are currently using the schema that contains the table.\n\n\nExample\n\n\nDROP TABLE IF EXISTS app.customer", 
            "title": "DROP TABLE"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/#drop-table", 
            "text": "Removes the specified table.  DROP TABLE [ IF EXISTS ] [schema-name.]table-name", 
            "title": "DROP TABLE"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/#description", 
            "text": "Removes the specified table. Include the  IF EXISTS  clause to execute the statement only if the specified table exists in SnappyData. The  schema-name.  prefix is optional if you are currently using the schema that contains the table.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/#example", 
            "text": "DROP TABLE IF EXISTS app.customer", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/insert/", 
            "text": "INSERT\n\n\nAn INSERT statement creates a row or rows and stores them in the named table. The number of values assigned in an INSERT statement must be the same as the number of specified or implied columns.\n\n\nINSERT INTO table-name\n    [ ( simple-column-name [ , simple-column-name ]* ) ]\n   Query\n\n\n\n\nDescription\n\n\nThe query can be:\n\n\n\n\na VALUES list\n\n\na multiple-row VALUES expression\n\n\n\n\n\n\nNote\n\n\nSnappyData does not support an INSERT with a subselect query if any subselect query requires aggregation. \n\n\n\n\nSingle-row and multiple-row lists can include the keyword DEFAULT. Specifying DEFAULT for a column inserts the column's default value into the column. Another way to insert the default value into the column is to omit the column from the column list and only insert values into other columns in the table.\n\n\nFor more information, refer to \nSELECT\n.\n\n\nExample\n\n\nINSERT INTO TRADE.CUSTOMERS\n      VALUES (1, 'User 1', '07-06-2002', 'SnappyData', 1);\n\n-- Insert a new customer into the CUSTOMERS  table,\n-- but do not assign  value to  'SINCE'  column\nINSERT INTO TRADE.CUSTOMERS(CID ,CUST_NAME , ADDR ,TID)\n VALUES (1, 'User 1', 'SnappyData', 1);\n\n-- Insert two new customers using one statement \n-- into the CUSTOMER table as in the previous example, \n-- but do not assign  value to 'SINCE'  field of the new customer.\nINSERT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID)\n VALUES (1, 'User 1' , 'SnappyData', 1),\n (2, 'User 2' , 'SnappyData', 1);\n\n-- Insert the DEFAULT value for the LOCATION column\nINSERT INTO TRADE.CUSTOMERS\n      VALUES (1, 'User 1', DEFAULT, 'SnappyData',1);\n\n-- Insert using a select statement.\nINSERT INTO TRADE.NEWCUSTOMERS\n     SELECT * from TRADE.CUSTOMERS WHERE TID=1;", 
            "title": "INSERT"
        }, 
        {
            "location": "/reference/sql_reference/insert/#insert", 
            "text": "An INSERT statement creates a row or rows and stores them in the named table. The number of values assigned in an INSERT statement must be the same as the number of specified or implied columns.  INSERT INTO table-name\n    [ ( simple-column-name [ , simple-column-name ]* ) ]\n   Query", 
            "title": "INSERT"
        }, 
        {
            "location": "/reference/sql_reference/insert/#description", 
            "text": "The query can be:   a VALUES list  a multiple-row VALUES expression    Note  SnappyData does not support an INSERT with a subselect query if any subselect query requires aggregation.    Single-row and multiple-row lists can include the keyword DEFAULT. Specifying DEFAULT for a column inserts the column's default value into the column. Another way to insert the default value into the column is to omit the column from the column list and only insert values into other columns in the table.  For more information, refer to  SELECT .", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/insert/#example", 
            "text": "INSERT INTO TRADE.CUSTOMERS\n      VALUES (1, 'User 1', '07-06-2002', 'SnappyData', 1);\n\n-- Insert a new customer into the CUSTOMERS  table,\n-- but do not assign  value to  'SINCE'  column\nINSERT INTO TRADE.CUSTOMERS(CID ,CUST_NAME , ADDR ,TID)\n VALUES (1, 'User 1', 'SnappyData', 1);\n\n-- Insert two new customers using one statement \n-- into the CUSTOMER table as in the previous example, \n-- but do not assign  value to 'SINCE'  field of the new customer.\nINSERT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID)\n VALUES (1, 'User 1' , 'SnappyData', 1),\n (2, 'User 2' , 'SnappyData', 1);\n\n-- Insert the DEFAULT value for the LOCATION column\nINSERT INTO TRADE.CUSTOMERS\n      VALUES (1, 'User 1', DEFAULT, 'SnappyData',1);\n\n-- Insert using a select statement.\nINSERT INTO TRADE.NEWCUSTOMERS\n     SELECT * from TRADE.CUSTOMERS WHERE TID=1;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/put-into/", 
            "text": "PUT INTO\n\n\nPUT INTO table-name\n     VALUES ( column-value [ , column-value ]* ) \n\n\n\n\nPUT INTO table-name\n    ( simple-column-name [ , simple-column-name ]* )\n   Query\n\n\n\n\nDescription\n\n\nPUT INTO operates like standard \nINSERT\n statement.\n\n\nFor Row Tables\n\n\nPUT INTO uses a syntax similar to the INSERT statement, but SnappyData does not check existing primary key values before executing the PUT INTO command. If a row with the same primary key exists in the table, PUT INTO simply overwrites the older row value. If no rows with the same primary key exist, PUT INTO operates like a standard INSERT. This behavior ensures that only the last primary key value inserted or updated remains in the system, which preserves the primary key constraint. Removing the primary key check speeds execution when importing bulk data.\n\n\nThe PUT INTO statement is similar to the \"UPSERT\" command or capability provided by other RDBMS to relax primary key checks. By default, the PUT INTO statement ignores only primary key constraints. \n\n\n\n\nNote\n\n\nSnappyData does not support a PUT INTO with a subselect query if any subselect query requires aggregation.\n\n\n\n\nExample\n\n\nPUT INTO TRADE.CUSTOMERS\n      VALUES (1, 'User 1', '07-06-2002', 'SnappyData', 1);\n\n\n\n\nPUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID)\n VALUES (1, 'User 1' , 'SnappyData', 1),\n (2, 'User 2' , 'SnappyData', 1);", 
            "title": "PUT INTO"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#put-into", 
            "text": "PUT INTO table-name\n     VALUES ( column-value [ , column-value ]* )   PUT INTO table-name\n    ( simple-column-name [ , simple-column-name ]* )\n   Query", 
            "title": "PUT INTO"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#description", 
            "text": "PUT INTO operates like standard  INSERT  statement.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#for-row-tables", 
            "text": "PUT INTO uses a syntax similar to the INSERT statement, but SnappyData does not check existing primary key values before executing the PUT INTO command. If a row with the same primary key exists in the table, PUT INTO simply overwrites the older row value. If no rows with the same primary key exist, PUT INTO operates like a standard INSERT. This behavior ensures that only the last primary key value inserted or updated remains in the system, which preserves the primary key constraint. Removing the primary key check speeds execution when importing bulk data.  The PUT INTO statement is similar to the \"UPSERT\" command or capability provided by other RDBMS to relax primary key checks. By default, the PUT INTO statement ignores only primary key constraints.    Note  SnappyData does not support a PUT INTO with a subselect query if any subselect query requires aggregation.", 
            "title": "For Row Tables"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#example", 
            "text": "PUT INTO TRADE.CUSTOMERS\n      VALUES (1, 'User 1', '07-06-2002', 'SnappyData', 1);  PUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID)\n VALUES (1, 'User 1' , 'SnappyData', 1),\n (2, 'User 2' , 'SnappyData', 1);", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/select/", 
            "text": "SELECT\n\n\nSELECT [DISTINCT] named_expression[, named_expression, ...]\n    FROM relation[, relation, ...]\n    [WHERE boolean_expression]\n    [aggregation [HAVING boolean_expression]]\n    [ORDER BY sort_expressions]\n    [CLUSTER BY expressions]\n    [DISTRIBUTE BY expressions]\n    [SORT BY sort_expressions]\n    [WINDOW named_window[, WINDOW named_window, ...]]\n    [LIMIT num_rows]\n\nnamed_expression:\n    : expression [AS alias]\n\nrelation:\n    | join_relation\n    | (table_name|query) [sample] [AS alias]\n\nexpressions:\n    : expression[, expression, ...]\n\nsort_expressions:\n    : expression [ASC|DESC][, expression [ASC|DESC], ...]\n\n\n\n\n\nFor information on executing queries on Synopsis Data Engine, refer to \nSDE\n.\n\n\nDescription\n\n\nOutput data from one or more relations.\n\n\nA relation here refers to any source of input data. It could be the contents of an existing table (or view), the joined result of two existing tables, or a subquery (the result of another select statement).\n\n\nDISTINCT\n\nSelect all matching rows from the relation then remove duplicate results.\n\n\nWHERE\n\nFilter rows by a predicate.\n\n\nHAVING\n\nFilter grouped result by a predicate.\n\n\nORDER BY\n\nImpose total ordering on a set of expressions. Default sort direction is ascending. This may not be used with SORT BY, CLUSTER BY, or DISTRIBUTE BY.\n\n\nDISTRIBUTE BY\n\nRepartition rows in the relation based on a set of expressions. Rows with the same expression values will be hashed to the same worker. This may not be used with ORDER BY or CLUSTER BY.\n\n\nSORT BY\n\nImpose ordering on a set of expressions within each partition. Default sort direction is ascending. This may not be used with ORDER BY or CLUSTER BY.\n\n\nCLUSTER BY\n\nRepartition rows in the relation based on a set of expressions and sort the rows in ascending order based on the expressions. In other words, this is a shorthand for DISTRIBUTE BY and SORT BY where all expressions are sorted in ascending order. This may not be used with ORDER BY, DISTRIBUTE BY, or SORT BY.\n\n\nWINDOW\n\nAssign an identifier to a window specification.\n\n\nLIMIT\n\nLimit the number of rows returned.\n\n\nExample\n\n\n    SELECT * FROM boxes\n    SELECT width, length FROM boxes WHERE height=3\n    SELECT DISTINCT width, length FROM boxes WHERE height=3 LIMIT 2\n    SELECT * FROM VALUES (1, 2, 3) AS (width, length, height)\n    SELECT * FROM VALUES (1, 2, 3), (2, 3, 4) AS (width, length, height)\n    SELECT * FROM boxes ORDER BY width\n    SELECT * FROM boxes DISTRIBUTE BY width SORT BY width\n    SELECT * FROM boxes CLUSTER BY length\n\n\n\n\nJOINS\n\n\n    join_relation:\n        | relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...]))\n        : relation NATURAL join_type JOIN relation\n    join_type:\n        | INNER\n        | (LEFT|RIGHT) SEMI\n        | (LEFT|RIGHT|FULL) [OUTER]\n        : [LEFT] ANTI\n\n\n\n\nINNER JOIN\n\nSelect all rows from both relations where there is match.\n\n\nOUTER JOIN\n\nSelect all rows from both relations, filling with null values on the side that does not have a match.\n\n\nSEMI JOIN\n\nSelect only rows from the side of the SEMI JOIN where there is a match. If one row matches multiple rows, only the first match is returned.\n\n\nLEFT ANTI JOIN\n\nSelect only rows from the left side that match no rows on the right side.\n\n\nExample\n:\n\n\n    SELECT * FROM boxes INNER JOIN rectangles ON boxes.width = rectangles.width\n    SELECT * FROM boxes FULL OUTER JOIN rectangles USING (width, length)\n    SELECT * FROM boxes NATURAL JOIN rectangles\n\n\n\n\nAGGREGATION\n\n\n    aggregation:\n        : GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))]\n\n\n\n\nGroup by a set of expressions using one or more aggregate functions. Common built-in aggregate functions include count, avg, min, max, and sum.\n\n\nROLLUP\n\nCreate a grouping set at each hierarchical level of the specified expressions. For instance, For instance, GROUP BY a, b, c WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (a), ()). The total number of grouping sets will be N + 1, where N is the number of group expressions.\n\n\nCUBE\n\nCreate a grouping set for each possible combination of a set of the specified expressions. For instance, GROUP BY a, b, c WITH CUBE is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ()). The total number of grouping sets will be 2^N, where N is the number of group expressions.\n\n\nGROUPING SETS\n\nPerform a group by for each subset of the group expressions specified in the grouping sets. For instance, GROUP BY x, y GROUPING SETS (x, y) is equivalent to the result of GROUP BY x unioned with that of GROUP BY y.\n\n\nExample\n:\n\n\n    SELECT height, COUNT(*) AS num_rows FROM boxes GROUP BY height\n    SELECT width, AVG(length) AS average_length FROM boxes GROUP BY width\n    SELECT width, length, height FROM boxes GROUP BY width, length, height WITH ROLLUP\n    SELECT width, length, avg(height) FROM boxes GROUP BY width, length GROUPING SETS (width, length)\n\n\n\n\nWindow Functions\n\n\nwindow_expression:\n    : expression OVER window_spec\n\nnamed_window:\n    : window_identifier AS window_spec\n\nwindow_spec:\n    | window_identifier\n    : ((PARTITION|DISTRIBUTE) BY expressions\n          [(ORDER|SORT) BY sort_expressions] [window_frame])\n\nwindow_frame:\n    | (RANGE|ROWS) frame_bound\n    : (RANGE|ROWS) BETWEEN frame_bound AND frame_bound\n\nframe_bound:\n    | CURRENT ROW\n    | UNBOUNDED (PRECEDING|FOLLOWING)\n    : expression (PRECEDING|FOLLOWING)\n\n\n\n\nCompute a result over a range of input rows. A windowed expression is specified using the OVER keyword, which is followed by either an identifier to the window (defined using the WINDOW keyword) or the specification of a window.\n\n\nPARTITION BY\n\nSpecify which rows will be in the same partition, aliased by DISTRIBUTE BY.\n\n\nORDER BY\n\nSpecify how rows within a window partition are ordered, aliased by SORT BY.\n\n\nRANGE bound\n\nExpress the size of the window in terms of a value range for the expression.\n\n\nROWS bound\n\nExpress the size of the window in terms of the number of rows before and/or after the current row.\n\n\nCURRENT ROW\n\nUse the current row as a bound.\n\n\nUNBOUNDED\n\nUse negative infinity as the lower bound or infinity as the upper bound.\n\n\nPRECEDING\n\nIf used with a RANGE bound, this defines the lower bound of the value range. If used with a ROWS bound, this determines the number of rows before the current row to keep in the window.\n\n\nFOLLOWING\n\nIf used with a RANGE bound, this defines the upper bound of the value range. If used with a ROWS bound, this determines the number of rows after the current row to keep in the window.", 
            "title": "SELECT"
        }, 
        {
            "location": "/reference/sql_reference/select/#select", 
            "text": "SELECT [DISTINCT] named_expression[, named_expression, ...]\n    FROM relation[, relation, ...]\n    [WHERE boolean_expression]\n    [aggregation [HAVING boolean_expression]]\n    [ORDER BY sort_expressions]\n    [CLUSTER BY expressions]\n    [DISTRIBUTE BY expressions]\n    [SORT BY sort_expressions]\n    [WINDOW named_window[, WINDOW named_window, ...]]\n    [LIMIT num_rows]\n\nnamed_expression:\n    : expression [AS alias]\n\nrelation:\n    | join_relation\n    | (table_name|query) [sample] [AS alias]\n\nexpressions:\n    : expression[, expression, ...]\n\nsort_expressions:\n    : expression [ASC|DESC][, expression [ASC|DESC], ...]  For information on executing queries on Synopsis Data Engine, refer to  SDE .", 
            "title": "SELECT"
        }, 
        {
            "location": "/reference/sql_reference/select/#description", 
            "text": "Output data from one or more relations.  A relation here refers to any source of input data. It could be the contents of an existing table (or view), the joined result of two existing tables, or a subquery (the result of another select statement).  DISTINCT \nSelect all matching rows from the relation then remove duplicate results.  WHERE \nFilter rows by a predicate.  HAVING \nFilter grouped result by a predicate.  ORDER BY \nImpose total ordering on a set of expressions. Default sort direction is ascending. This may not be used with SORT BY, CLUSTER BY, or DISTRIBUTE BY.  DISTRIBUTE BY \nRepartition rows in the relation based on a set of expressions. Rows with the same expression values will be hashed to the same worker. This may not be used with ORDER BY or CLUSTER BY.  SORT BY \nImpose ordering on a set of expressions within each partition. Default sort direction is ascending. This may not be used with ORDER BY or CLUSTER BY.  CLUSTER BY \nRepartition rows in the relation based on a set of expressions and sort the rows in ascending order based on the expressions. In other words, this is a shorthand for DISTRIBUTE BY and SORT BY where all expressions are sorted in ascending order. This may not be used with ORDER BY, DISTRIBUTE BY, or SORT BY.  WINDOW \nAssign an identifier to a window specification.  LIMIT \nLimit the number of rows returned.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/select/#example", 
            "text": "SELECT * FROM boxes\n    SELECT width, length FROM boxes WHERE height=3\n    SELECT DISTINCT width, length FROM boxes WHERE height=3 LIMIT 2\n    SELECT * FROM VALUES (1, 2, 3) AS (width, length, height)\n    SELECT * FROM VALUES (1, 2, 3), (2, 3, 4) AS (width, length, height)\n    SELECT * FROM boxes ORDER BY width\n    SELECT * FROM boxes DISTRIBUTE BY width SORT BY width\n    SELECT * FROM boxes CLUSTER BY length", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/select/#joins", 
            "text": "join_relation:\n        | relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...]))\n        : relation NATURAL join_type JOIN relation\n    join_type:\n        | INNER\n        | (LEFT|RIGHT) SEMI\n        | (LEFT|RIGHT|FULL) [OUTER]\n        : [LEFT] ANTI  INNER JOIN \nSelect all rows from both relations where there is match.  OUTER JOIN \nSelect all rows from both relations, filling with null values on the side that does not have a match.  SEMI JOIN \nSelect only rows from the side of the SEMI JOIN where there is a match. If one row matches multiple rows, only the first match is returned.  LEFT ANTI JOIN \nSelect only rows from the left side that match no rows on the right side.  Example :      SELECT * FROM boxes INNER JOIN rectangles ON boxes.width = rectangles.width\n    SELECT * FROM boxes FULL OUTER JOIN rectangles USING (width, length)\n    SELECT * FROM boxes NATURAL JOIN rectangles", 
            "title": "JOINS"
        }, 
        {
            "location": "/reference/sql_reference/select/#aggregation", 
            "text": "aggregation:\n        : GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))]  Group by a set of expressions using one or more aggregate functions. Common built-in aggregate functions include count, avg, min, max, and sum.  ROLLUP \nCreate a grouping set at each hierarchical level of the specified expressions. For instance, For instance, GROUP BY a, b, c WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (a), ()). The total number of grouping sets will be N + 1, where N is the number of group expressions.  CUBE \nCreate a grouping set for each possible combination of a set of the specified expressions. For instance, GROUP BY a, b, c WITH CUBE is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ()). The total number of grouping sets will be 2^N, where N is the number of group expressions.  GROUPING SETS \nPerform a group by for each subset of the group expressions specified in the grouping sets. For instance, GROUP BY x, y GROUPING SETS (x, y) is equivalent to the result of GROUP BY x unioned with that of GROUP BY y.  Example :      SELECT height, COUNT(*) AS num_rows FROM boxes GROUP BY height\n    SELECT width, AVG(length) AS average_length FROM boxes GROUP BY width\n    SELECT width, length, height FROM boxes GROUP BY width, length, height WITH ROLLUP\n    SELECT width, length, avg(height) FROM boxes GROUP BY width, length GROUPING SETS (width, length)", 
            "title": "AGGREGATION"
        }, 
        {
            "location": "/reference/sql_reference/select/#window-functions", 
            "text": "window_expression:\n    : expression OVER window_spec\n\nnamed_window:\n    : window_identifier AS window_spec\n\nwindow_spec:\n    | window_identifier\n    : ((PARTITION|DISTRIBUTE) BY expressions\n          [(ORDER|SORT) BY sort_expressions] [window_frame])\n\nwindow_frame:\n    | (RANGE|ROWS) frame_bound\n    : (RANGE|ROWS) BETWEEN frame_bound AND frame_bound\n\nframe_bound:\n    | CURRENT ROW\n    | UNBOUNDED (PRECEDING|FOLLOWING)\n    : expression (PRECEDING|FOLLOWING)  Compute a result over a range of input rows. A windowed expression is specified using the OVER keyword, which is followed by either an identifier to the window (defined using the WINDOW keyword) or the specification of a window.  PARTITION BY \nSpecify which rows will be in the same partition, aliased by DISTRIBUTE BY.  ORDER BY \nSpecify how rows within a window partition are ordered, aliased by SORT BY.  RANGE bound \nExpress the size of the window in terms of a value range for the expression.  ROWS bound \nExpress the size of the window in terms of the number of rows before and/or after the current row.  CURRENT ROW \nUse the current row as a bound.  UNBOUNDED \nUse negative infinity as the lower bound or infinity as the upper bound.  PRECEDING \nIf used with a RANGE bound, this defines the lower bound of the value range. If used with a ROWS bound, this determines the number of rows before the current row to keep in the window.  FOLLOWING \nIf used with a RANGE bound, this defines the upper bound of the value range. If used with a ROWS bound, this determines the number of rows after the current row to keep in the window.", 
            "title": "Window Functions"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/", 
            "text": "SET SCHEMA\n\n\nSet or change the default schema for a connection's session.\n\n\nSET SCHEMA schema-name\n\n\n\n\nDescription\n\n\nThe SET SCHEMA statement sets or changes the default schema for a connection's session to the provided schema. This is then used as the schema for all statements issued from the connection that does not explicitly specify a schema name. \n\n\nThe default schema is APP.\n\n\nExample\n\n\n-- below are equivalent assuming a TRADE schema\nSET SCHEMA TRADE;\nSET SCHEMA trade;", 
            "title": "SET SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/#set-schema", 
            "text": "Set or change the default schema for a connection's session.  SET SCHEMA schema-name", 
            "title": "SET SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/#description", 
            "text": "The SET SCHEMA statement sets or changes the default schema for a connection's session to the provided schema. This is then used as the schema for all statements issued from the connection that does not explicitly specify a schema name.   The default schema is APP.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/#example", 
            "text": "-- below are equivalent assuming a TRADE schema\nSET SCHEMA TRADE;\nSET SCHEMA trade;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/", 
            "text": "TRUNCATE TABLE\n\n\nRemove all content from a table and return it to its initial, empty state. TRUNCATE TABLE clears all in-memory data for the specified table as well as any data that was persisted to SnappyData disk stores. \n\n\nTRUNCATE TABLE table-name\n\n\n\n\nDescription\n\n\nTo truncate a table, you must be the table's owner. You cannot use this command to truncate system tables.\n\n\nExample\n\n\nTo truncate the \"flights\" table in the current schema:\n\n\nTRUNCATE TABLE flights;", 
            "title": "TRUNCATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/#truncate-table", 
            "text": "Remove all content from a table and return it to its initial, empty state. TRUNCATE TABLE clears all in-memory data for the specified table as well as any data that was persisted to SnappyData disk stores.   TRUNCATE TABLE table-name", 
            "title": "TRUNCATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/#description", 
            "text": "To truncate a table, you must be the table's owner. You cannot use this command to truncate system tables.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/#example", 
            "text": "To truncate the \"flights\" table in the current schema:  TRUNCATE TABLE flights;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/update/", 
            "text": "UPDATE\n\n\nUpdate the value of one or more columns.\n\n\n\n\nNote\n\n\nCurrently not supported on column tables.\n\n\n\n\n{ UPDATE table-name [ [ AS ] correlation-name]\n        SET column-name = value\n        [, column-name = value} ]*\n        [ WHERE ]    \n}\n\n\n\n\nDescription\n\n\nThis form of the UPDATE statement is called a searched update. It updates the value of one or more columns for all rows of the table for which the WHERE clause evaluates to TRUE. Specifying DEFAULT for the update value sets the value of the column to the default defined for that table.\n\n\nThe UPDATE statement returns the number of rows that were updated.\n\n\n\n\nNote\n\n\nUpdates on partitioning column and primary key column are not supported. \n\n\n\n\nvalue\n\n\nexpression | DEFAULT\n\n\n\n\nExample\n\n\n-- Change the ADDRESS and SINCE  fields to null for all customers with ID greater than 10.\nUPDATE TRADE.CUSTOMERS\n  SET ADDR=NULL, SINCE=NULL\n  WHERE CID \n 10;\n\n-- Set the ADDR of all customers to 'SnappyData' where the current address is NULL.\nUPDATE TRADE.CUSTOMERS\n SET ADDR = 'Snappydata'\n WHERE ADDR IS NULL;\n\n// Increase the  QTY field by 10 for all rows of SELLORDERS table.\nUPDATE TRADE.SELLORDERS \n SET QTY = QTY+10;\n\n-- Change the  STATUS  field of  all orders of SELLORDERS table to DEFAULT  ( 'open') , for customer ID = 10\nUPDATE TRADE.SELLORDERS\n  SET STATUS = DEFAULT\n  WHERE CID = 10;", 
            "title": "UPDATE"
        }, 
        {
            "location": "/reference/sql_reference/update/#update", 
            "text": "Update the value of one or more columns.   Note  Currently not supported on column tables.   { UPDATE table-name [ [ AS ] correlation-name]\n        SET column-name = value\n        [, column-name = value} ]*\n        [ WHERE ]    \n}", 
            "title": "UPDATE"
        }, 
        {
            "location": "/reference/sql_reference/update/#description", 
            "text": "This form of the UPDATE statement is called a searched update. It updates the value of one or more columns for all rows of the table for which the WHERE clause evaluates to TRUE. Specifying DEFAULT for the update value sets the value of the column to the default defined for that table.  The UPDATE statement returns the number of rows that were updated.   Note  Updates on partitioning column and primary key column are not supported.    value  expression | DEFAULT", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/update/#example", 
            "text": "-- Change the ADDRESS and SINCE  fields to null for all customers with ID greater than 10.\nUPDATE TRADE.CUSTOMERS\n  SET ADDR=NULL, SINCE=NULL\n  WHERE CID   10;\n\n-- Set the ADDR of all customers to 'SnappyData' where the current address is NULL.\nUPDATE TRADE.CUSTOMERS\n SET ADDR = 'Snappydata'\n WHERE ADDR IS NULL;\n\n// Increase the  QTY field by 10 for all rows of SELLORDERS table.\nUPDATE TRADE.SELLORDERS \n SET QTY = QTY+10;\n\n-- Change the  STATUS  field of  all orders of SELLORDERS table to DEFAULT  ( 'open') , for customer ID = 10\nUPDATE TRADE.SELLORDERS\n  SET STATUS = DEFAULT\n  WHERE CID = 10;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/system-procedures/", 
            "text": "Procedures\n\n\nSnappyData provides built-in system procedures to help you manage the distributed system. For example, you can use system procedures to install required JAR files.\n\n\n\n\n\nThe following built-in procedures are available:\n\n\n\n\n\n\nSYS.DUMP_STACKS\n\n\n\n\n\n\nSYS.REBALANCE_ALL_BUCKETS\n\n\n\n\n\n\nSYS.SET_CRITICAL_HEAP_PERCENTAGE\n\n\n\n\n\n\nSYS.SET_TRACE_FLAG", 
            "title": "Built-in System Procedures"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/system-procedures/#procedures", 
            "text": "SnappyData provides built-in system procedures to help you manage the distributed system. For example, you can use system procedures to install required JAR files.   The following built-in procedures are available:    SYS.DUMP_STACKS    SYS.REBALANCE_ALL_BUCKETS    SYS.SET_CRITICAL_HEAP_PERCENTAGE    SYS.SET_TRACE_FLAG", 
            "title": "Procedures"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/", 
            "text": "SYS.DUMP_STACKS\n\n\nWrites thread stacks, locks, and transaction states to the SnappyData log file. You can write stack information either for the current SnappyData member or for all SnappyData members in the distributed system.\n\n\n\n\n\nSyntax\n\n\nSYS.DUMP_STACKS (\nIN ALL BOOLEAN\n)\n\n\n\n\nALL\n \n\nSpecifies boolean value: \ntrue\n or \n1\n to log stack trace information for all SnappyData members, or \nfalse\n or \n0\n to log information only for the local SnappyData member.\n\n\nExample\n\n\nThis command writes thread stack information only for the local SnappyData member. The stack information is written to the SnappyData log file (by default snappyserver.log in the member startup directory):\n\n\nsnappy\n call sys.dump_stacks('false');\nStatement executed.", 
            "title": "DUMP_STACKS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/#sysdump_stacks", 
            "text": "Writes thread stacks, locks, and transaction states to the SnappyData log file. You can write stack information either for the current SnappyData member or for all SnappyData members in the distributed system.", 
            "title": "SYS.DUMP_STACKS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/#syntax", 
            "text": "SYS.DUMP_STACKS (\nIN ALL BOOLEAN\n)  ALL   \nSpecifies boolean value:  true  or  1  to log stack trace information for all SnappyData members, or  false  or  0  to log information only for the local SnappyData member.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/#example", 
            "text": "This command writes thread stack information only for the local SnappyData member. The stack information is written to the SnappyData log file (by default snappyserver.log in the member startup directory):  snappy  call sys.dump_stacks('false');\nStatement executed.", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/", 
            "text": "SYS.REBALANCE_ALL_BUCKETS\n\n\nRebalance partitioned table data on available SnappyData members.\n\n\nSyntax\n\n\nSYS.REBALANCE_ALL_BUCKETS()\n\n\n\n\nRebalancing is a SnappyData member operation that affects partitioned tables created in the cluster. Rebalancing performs two tasks:\n\n\n\n\n\n\nIf the partitioned table's redundancy setting is not satisfied, rebalancing does what it can to recover redundancy. \n\n\n\n\n\n\nRebalancing moves the partitioned table's data buckets between host members as needed to establish the best balance of data across the distributed system.\n\n\n\n\n\n\nFor efficiency, when starting multiple members, trigger the rebalance a single time, after you have added all members.\n\n\n\nExample\n\n\nsnappy\n call sys.rebalance_all_buckets();", 
            "title": "REBALANCE_ALL_BUCKETS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/#sysrebalance_all_buckets", 
            "text": "Rebalance partitioned table data on available SnappyData members.", 
            "title": "SYS.REBALANCE_ALL_BUCKETS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/#syntax", 
            "text": "SYS.REBALANCE_ALL_BUCKETS()  Rebalancing is a SnappyData member operation that affects partitioned tables created in the cluster. Rebalancing performs two tasks:    If the partitioned table's redundancy setting is not satisfied, rebalancing does what it can to recover redundancy.     Rebalancing moves the partitioned table's data buckets between host members as needed to establish the best balance of data across the distributed system.    For efficiency, when starting multiple members, trigger the rebalance a single time, after you have added all members.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/#example", 
            "text": "snappy  call sys.rebalance_all_buckets();", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/", 
            "text": "SYS.SET_CRITICAL_HEAP_PERCENTAGE\n\n\nSets the percentage threshold of Java heap memory usage that triggers \nLowMemoryException\ns on a SnappyData data store. This procedure executes only on the local SnappyData data store member.\n\n\nThis procedure sets the percentage threshold of critical Java heap memory usage on the local SnappyData data store. If the amount of heap memory being used exceeds the percentage, the member will report LowMemoryExceptions during local or client put operations into heap tables. The member will also inform other members in the distributed system that it has reached the critical threshold. When a data store is started with the \n-heap-size\n option, the default critical threshold is 90%.\n\n\nSyntax\n\n\nSYS.SET_CRITICAL_HEAP_PERCENTAGE (\nIN PERCENTAGE REAL NOT NULL\n)\n\n\n\n\nPERCENTAGE\n \n\nThe percentage of used heap space that triggers \nLowMemoryException\ns on the local SnappyData data store.\n\n\nExample\n\n\nThis command sets the critical threshold for heap memory usage on the local SnappyData member to 99.9%:\n\n\ncall sys.set_critical_heap_percentage (99.9);", 
            "title": "SET_CRITICAL_HEAP_PERCENTAGE"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/#sysset_critical_heap_percentage", 
            "text": "Sets the percentage threshold of Java heap memory usage that triggers  LowMemoryException s on a SnappyData data store. This procedure executes only on the local SnappyData data store member.  This procedure sets the percentage threshold of critical Java heap memory usage on the local SnappyData data store. If the amount of heap memory being used exceeds the percentage, the member will report LowMemoryExceptions during local or client put operations into heap tables. The member will also inform other members in the distributed system that it has reached the critical threshold. When a data store is started with the  -heap-size  option, the default critical threshold is 90%.", 
            "title": "SYS.SET_CRITICAL_HEAP_PERCENTAGE"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/#syntax", 
            "text": "SYS.SET_CRITICAL_HEAP_PERCENTAGE (\nIN PERCENTAGE REAL NOT NULL\n)  PERCENTAGE   \nThe percentage of used heap space that triggers  LowMemoryException s on the local SnappyData data store.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/#example", 
            "text": "This command sets the critical threshold for heap memory usage on the local SnappyData member to 99.9%:  call sys.set_critical_heap_percentage (99.9);", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/", 
            "text": "SYS.SET_TRACE_FLAG\n\n\nEnables a \nsnappydata.debug.true\n trace flag on all members of the distributed system, including locators.\n\n\nThis procedure enables or disables a specific trace flag for the distributed system as a whole. You must be a system user to execute this procedure. \n\n\nSyntax\n\n\nSYS.SET_TRACE_FLAG (\nIN TRACE_FLAG VARCHAR(256),\nIN ON BOOLEAN\n)\n\n\n\n\nTRACE_FLAG \n\nSpecifies name of the trace flag to enable or disable.\n\n\nON \n\nSpecifies boolean value: \ntrue\n or \n1\n to enable the trace flag, or \nfalse\n or \n0\n to disable it.\n\n\nExample\n\n\nThis command traces all JAR installation, update, and removal operations in the SnappyData distributed system:\n\n\ncall sys.set_trace_flag ('TraceJars', 'true');", 
            "title": "SET_TRACE_FLAG"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/#sysset_trace_flag", 
            "text": "Enables a  snappydata.debug.true  trace flag on all members of the distributed system, including locators.  This procedure enables or disables a specific trace flag for the distributed system as a whole. You must be a system user to execute this procedure.", 
            "title": "SYS.SET_TRACE_FLAG"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/#syntax", 
            "text": "SYS.SET_TRACE_FLAG (\nIN TRACE_FLAG VARCHAR(256),\nIN ON BOOLEAN\n)  TRACE_FLAG  \nSpecifies name of the trace flag to enable or disable.  ON  \nSpecifies boolean value:  true  or  1  to enable the trace flag, or  false  or  0  to disable it.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/#example", 
            "text": "This command traces all JAR installation, update, and removal operations in the SnappyData distributed system:  call sys.set_trace_flag ('TraceJars', 'true');", 
            "title": "Example"
        }, 
        {
            "location": "/reference/system_tables/system_tables/", 
            "text": "System Tables\n\n\nTables that SnappyData uses to manage the distributed system and its data. All system tables are part of the SYS schema.\n\n\nThe following system tables are available:\n\n\n\n\n\n\nMEMBERS\n\n\n\n\n\n\nMEMORYANALYTICS\n\n\n\n\n\n\nSYSDISKSTORES", 
            "title": "System Tables"
        }, 
        {
            "location": "/reference/system_tables/system_tables/#system-tables", 
            "text": "Tables that SnappyData uses to manage the distributed system and its data. All system tables are part of the SYS schema.  The following system tables are available:    MEMBERS    MEMORYANALYTICS    SYSDISKSTORES", 
            "title": "System Tables"
        }, 
        {
            "location": "/reference/system_tables/members/", 
            "text": "MEMBERS\n\n\n\n\nNote\n\n\nSnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.\n\n\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nID\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe unique ID of the member. This ID has the format: \nhostname(process_id);member_number;:udp_port/tcp_port\nFor example:\n10.0.1.31(66878);v0;:41715/63386\n\n\n\n\n\n\nKIND\n\n\nVARCHAR\n\n\n24\n\n\nNo\n\n\nSpecifies the type of SnappyData member process: \n * datastore\u2014A member that hosts data.\n * peer\u2014A member that does not host data.\n * locator\u2014Provides discovery services for a cluster.\n Member types can also be qualified with additional keywords \n  * normal\u2014The member can communicate with other members in a cluster. \n * loner\u2014The member is standalone and cannot communicate with other members. Loners use no locators for discovery.\n * admin\u2014The member also acts as a JMX manager node.\n\n\n\n\n\n\nHOSTDATA\n\n\nBOOLEAN\n\n\n\n\nYes\n\n\nA value of \u20181\u2019 indicates that this member is a data store and can host data. Otherwise, the member is a peer client with no hosted data.\n\n\n\n\n\n\nISELDER\n\n\nBOOLEAN\n\n\n\n\nNo\n\n\nIs this the eldest member of the distributed system. Typically, this is the member who first joins the cluster.\n\n\n\n\n\n\nIPADDRESS\n\n\nVARCHAR\n\n\n64\n\n\nYes\n\n\nThe fully-qualified hostname/IP address of the member.\n\n\n\n\n\n\nHOST\n\n\nVARCHAR\n\n\n128\n\n\nYes\n\n\nThe fully-qualified hostname of the member.\n\n\n\n\n\n\nPID\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe member process ID.\n\n\n\n\n\n\nPORT\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe member UDP port.\n\n\n\n\n\n\nROLES\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nNot used.\n\n\n\n\n\n\nNETSERVERS\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nHost and port information for Network Servers that are running on SnappyData members.\n\n\n\n\n\n\nLOCATOR\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nHost and port information for locator members.\n\n\n\n\n\n\nSERVERGROUPS\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nA comma-separated list of server groups of which this member is a part. \n \nNote\n: SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.\n\n\n\n\n\n\nSYSTEMPROPS\n\n\nCLOB\n\n\n2147483647\n\n\nNo\n\n\nA list of all system properties used to start this member. This includes properties such as the classpath, JVM version, and so forth.\n\n\n\n\n\n\nGEMFIREPROPS\n\n\nCLOB\n\n\n2147483647\n\n\nNo\n\n\nThe names and values of GemFire core system properties that the member uses.\n\n\n\n\n\n\nBOOTPROPS\n\n\nCLOB\n\n\n2147483647\n\n\nNo\n\n\nAll of the SnappyData boot properties names and values that a member uses.", 
            "title": "MEMBERS"
        }, 
        {
            "location": "/reference/system_tables/members/#members", 
            "text": "Note  SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.      Column Name  Type  Length  Nullable  Contents      ID  VARCHAR  128  No  The unique ID of the member. This ID has the format:  hostname(process_id);member_number;:udp_port/tcp_port For example: 10.0.1.31(66878);v0;:41715/63386    KIND  VARCHAR  24  No  Specifies the type of SnappyData member process:   * datastore\u2014A member that hosts data.  * peer\u2014A member that does not host data.  * locator\u2014Provides discovery services for a cluster.  Member types can also be qualified with additional keywords    * normal\u2014The member can communicate with other members in a cluster.   * loner\u2014The member is standalone and cannot communicate with other members. Loners use no locators for discovery.  * admin\u2014The member also acts as a JMX manager node.    HOSTDATA  BOOLEAN   Yes  A value of \u20181\u2019 indicates that this member is a data store and can host data. Otherwise, the member is a peer client with no hosted data.    ISELDER  BOOLEAN   No  Is this the eldest member of the distributed system. Typically, this is the member who first joins the cluster.    IPADDRESS  VARCHAR  64  Yes  The fully-qualified hostname/IP address of the member.    HOST  VARCHAR  128  Yes  The fully-qualified hostname of the member.    PID  INTEGER  10  No  The member process ID.    PORT  INTEGER  10  No  The member UDP port.    ROLES  VARCHAR  128  No  Not used.    NETSERVERS  VARCHAR  32672  No  Host and port information for Network Servers that are running on SnappyData members.    LOCATOR  VARCHAR  32672  No  Host and port information for locator members.    SERVERGROUPS  VARCHAR  32672  No  A comma-separated list of server groups of which this member is a part.    Note : SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.    SYSTEMPROPS  CLOB  2147483647  No  A list of all system properties used to start this member. This includes properties such as the classpath, JVM version, and so forth.    GEMFIREPROPS  CLOB  2147483647  No  The names and values of GemFire core system properties that the member uses.    BOOTPROPS  CLOB  2147483647  No  All of the SnappyData boot properties names and values that a member uses.", 
            "title": "MEMBERS"
        }, 
        {
            "location": "/reference/system_tables/memoryanalytics/", 
            "text": "MEMORYANALYTICS\n\n\nA SnappyData virtual table that provides information about the overhead and memory usage of user tables and indexes.\n\n\nYou can query the SYS.MEMORYANALYTICS table to obtain details about individual tables and indexes\n\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nTABLE_NAME\n\n\nVARCHAR\n\n\n64\n\n\nNo\n\n\nThe full name of the table using the format \nschema_name\n.\ntable_name\n.\n\n\n\n\n\n\nINDEX_NAME\n\n\nVARCHAR\n\n\n64\n\n\nYes\n\n\nName of the index associated with the table.\n\n\n\n\n\n\nINDEX_TYPE\n\n\nVARCHAR\n\n\n32\n\n\nYes\n\n\nDescription of the type of index associated with the table-- local or a global hash index, and whether the index is sorted.\n\n\n\n\n\n\nID\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nMember ID of the member hosting the table.\n\n\n\n\n\n\nHOST\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe SnappyData member to which the memory values apply.\n\n\n\n\n\n\nCONSTANT_OVERHEAD\n\n\nREAL\n\n\n0\n\n\nNo\n\n\nOne-time memory overhead cost due to artifacts produced when a blank table is created.\n\n\n\n\n\n\nENTRY_SIZE\n\n\nREAL\n\n\n0\n\n\nNo\n\n\nEntry overhead, in kilobytes. Only reflects the amount of memory required to hold the table row in memory but not including the memory to hold its key and value. (Excludes KEY_SIZE, CONSTANT_OVERHEAD, VALUE_SIZE and VALUE_SIZE_OFFHEAP below.)\n\n\n\n\n\n\nKEY_SIZE\n\n\nREAL\n\n\n0\n\n\nNo\n\n\nKey overhead, in kilobytes. Note that this column will only display a non-zero value when the table is set to overflow to disk and the complete row (in other words, the row value) is no longer held in memory.\n\n\n\n\n\n\nVALUE_SIZE\n\n\nREAL\n\n\n0\n\n\nNo\n\n\nThe size, in kilobytes, of the table row data stored in the JVM heap. (This includes the Entry Size overhead.)\n\n\n\n\n\n\nVALUE_SIZE_OFFHEAP\n\n\nREAL\n\n\n0\n\n\nNo\n\n\nThe size, in kilobytes, of the table row data stored in off-heap memory.\n\n\n\n\n\n\nTOTAL_SIZE\n\n\nREAL\n\n\n0\n\n\nNo\n\n\nTotal size is the sum, in kilobytes, of the following columns:\n * CONSTANT_OVERHEAD\n * ENTRY_SIZE\n * KEY_SIZE\n * VALUE_SIZE\n * VALUE_SIZE_OFFHEAP\n\n\n\n\n\n\nNUM_ROWS\n\n\nBIGINT\n\n\n0\n\n\nNo\n\n\nThe total number of rows stored on the local SnappyData member. For a partitioned table, this includes all buckets for the table, as well as primary and secondary replicas.\n\n\n\n\n\n\nNUM_KEYS_IN_MEMORY\n\n\nBIGINT\n\n\n0\n\n\nNo\n\n\nThe total number of keys stored in the heap for the table. Note that this column will only display a non-zero value when the table is set to overflow to disk and the complete row (in other words, the row's value) is no longer held in memory.\n\n\n\n\n\n\nNUM_VALUES_IN_MEMORY\n\n\nBIGINT\n\n\n0\n\n\nNo\n\n\nThe total number of row values stored in the heap for the table.\n\n\n\n\n\n\nNUM_VALUES_IN_OFFHEAP\n\n\nBIGINT\n\n\n0\n\n\nNo\n\n\nThe total number of row values stored in off-heap memory.\n\n\n\n\n\n\nMEMORY\n\n\nLONG VARCHAR\n\n\n2147483647\n\n\nNo\n\n\nPlaceholder for future use.", 
            "title": "MEMORYANALYTICS"
        }, 
        {
            "location": "/reference/system_tables/memoryanalytics/#memoryanalytics", 
            "text": "A SnappyData virtual table that provides information about the overhead and memory usage of user tables and indexes.  You can query the SYS.MEMORYANALYTICS table to obtain details about individual tables and indexes     Column Name  Type  Length  Nullable  Contents      TABLE_NAME  VARCHAR  64  No  The full name of the table using the format  schema_name . table_name .    INDEX_NAME  VARCHAR  64  Yes  Name of the index associated with the table.    INDEX_TYPE  VARCHAR  32  Yes  Description of the type of index associated with the table-- local or a global hash index, and whether the index is sorted.    ID  VARCHAR  128  No  Member ID of the member hosting the table.    HOST  VARCHAR  128  No  The SnappyData member to which the memory values apply.    CONSTANT_OVERHEAD  REAL  0  No  One-time memory overhead cost due to artifacts produced when a blank table is created.    ENTRY_SIZE  REAL  0  No  Entry overhead, in kilobytes. Only reflects the amount of memory required to hold the table row in memory but not including the memory to hold its key and value. (Excludes KEY_SIZE, CONSTANT_OVERHEAD, VALUE_SIZE and VALUE_SIZE_OFFHEAP below.)    KEY_SIZE  REAL  0  No  Key overhead, in kilobytes. Note that this column will only display a non-zero value when the table is set to overflow to disk and the complete row (in other words, the row value) is no longer held in memory.    VALUE_SIZE  REAL  0  No  The size, in kilobytes, of the table row data stored in the JVM heap. (This includes the Entry Size overhead.)    VALUE_SIZE_OFFHEAP  REAL  0  No  The size, in kilobytes, of the table row data stored in off-heap memory.    TOTAL_SIZE  REAL  0  No  Total size is the sum, in kilobytes, of the following columns:  * CONSTANT_OVERHEAD  * ENTRY_SIZE  * KEY_SIZE  * VALUE_SIZE  * VALUE_SIZE_OFFHEAP    NUM_ROWS  BIGINT  0  No  The total number of rows stored on the local SnappyData member. For a partitioned table, this includes all buckets for the table, as well as primary and secondary replicas.    NUM_KEYS_IN_MEMORY  BIGINT  0  No  The total number of keys stored in the heap for the table. Note that this column will only display a non-zero value when the table is set to overflow to disk and the complete row (in other words, the row's value) is no longer held in memory.    NUM_VALUES_IN_MEMORY  BIGINT  0  No  The total number of row values stored in the heap for the table.    NUM_VALUES_IN_OFFHEAP  BIGINT  0  No  The total number of row values stored in off-heap memory.    MEMORY  LONG VARCHAR  2147483647  No  Placeholder for future use.", 
            "title": "MEMORYANALYTICS"
        }, 
        {
            "location": "/reference/system_tables/sysdiskstores/", 
            "text": "SYSDISKSTORES\n\n\nContains information about all disk stores created in the SnappyData distributed system.\n\n\nSee \nCREATE DISKSTORE\n.\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nNAME\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe unique identifier of the disk store.\n\n\n\n\n\n\nMAXLOGSIZE\n\n\nBIGINT\n\n\n10\n\n\nNo\n\n\nThe maximum size, in megabytes, of a single oplog file in the disk store.\n\n\n\n\n\n\nAUTOCOMPACT\n\n\nCHAR\n\n\n6\n\n\nNo\n\n\nSpecifies whether SnappyData automatically compacts log files in this disk store.\n\n\n\n\n\n\nALLOWFORCECOMPACTION\n\n\nCHAR\n\n\n6\n\n\nNo\n\n\nSpecifies whether the disk store permits online compaction of log files using the \nsnappy\n utility.\n\n\n\n\n\n\nCOMPACTIONTHRESHOLD\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe threshold after which an oplog file is eligible for compaction. Specified as a percentage value from 0\u2013100.\n\n\n\n\n\n\nTIMEINTERVAL\n\n\nBIGINT\n\n\n10\n\n\nNo\n\n\nThe maximum number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk.\n\n\n\n\n\n\nWRITEBUFFERSIZE\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe size of the buffer SnappyData uses to store operations when writing to the disk store.\n\n\n\n\n\n\nQUEUESIZE\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe maximum number of row operations that SnappyData can asynchronously queue for writing to the disk store.\n\n\n\n\n\n\nDIR_PATH_SIZE\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nThe directory names that hold disk store oplog files, and the maximum size in megabytes that each directory can store.", 
            "title": "SYSDISKSTORES"
        }, 
        {
            "location": "/reference/system_tables/sysdiskstores/#sysdiskstores", 
            "text": "Contains information about all disk stores created in the SnappyData distributed system.  See  CREATE DISKSTORE .     Column Name  Type  Length  Nullable  Contents      NAME  VARCHAR  128  No  The unique identifier of the disk store.    MAXLOGSIZE  BIGINT  10  No  The maximum size, in megabytes, of a single oplog file in the disk store.    AUTOCOMPACT  CHAR  6  No  Specifies whether SnappyData automatically compacts log files in this disk store.    ALLOWFORCECOMPACTION  CHAR  6  No  Specifies whether the disk store permits online compaction of log files using the  snappy  utility.    COMPACTIONTHRESHOLD  INTEGER  10  No  The threshold after which an oplog file is eligible for compaction. Specified as a percentage value from 0\u2013100.    TIMEINTERVAL  BIGINT  10  No  The maximum number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk.    WRITEBUFFERSIZE  INTEGER  10  No  The size of the buffer SnappyData uses to store operations when writing to the disk store.    QUEUESIZE  INTEGER  10  No  The maximum number of row operations that SnappyData can asynchronously queue for writing to the disk store.    DIR_PATH_SIZE  VARCHAR  32672  No  The directory names that hold disk store oplog files, and the maximum size in megabytes that each directory can store.", 
            "title": "SYSDISKSTORES"
        }, 
        {
            "location": "/reference/command_line_utilities/store-launcher/", 
            "text": "Command Line Utilites\n\n\nUse the \nsnappy\n command-line utility to launch SnappyData utilities.\n\n\nTo display a full list of snappy commands and options:\n\n\nsnappy --help\n\n\n\n\nThe command form to display a particular utility's usage is:\n\n\nsnappy \nutility\n --help\n\n\n\n\nWith no arguments, \nsnappy\n starts an \ninteractive SQL command shell\n:\n\n\nsnappy\n\n\n\n\nTo specify a system property for an interactive \nsnappy\n session, you must define the JAVA_ARGS environment variable before starting \nsnappy\n. For example, \nsnappy\n uses the \ngfxd.history\n system property to define the file that stores a list of the commands that are executed during an interactive session. To change this property, you would define it as part of the JAVA_ARGS variable:\n\n\n$ export JAVA_ARGS=\n-Dgfxd.history=/Users/user1/snappystore-history.sql\n \n$ snappy\n\n\n\n\nTo launch and exit a \nsnappy\n utility (rather than start an interactive \nsnappy\n shell) use the syntax:\n\n\nsnappy \nutility\n \narguments for specified utility\n\n\n\n\n\nTo specify a system property when launching a \nsnappy\n utility, use -J-D\nproperty_name\n=\nproperty_value\n argument.\n\n\nIn addition to launching various utilities provided with SnappyData, when launched without any arguments, \nsnappy\n starts an interactive command shell that you can use to connect to a SnappyData system and execute various commands, including SQL statements.\n\n\n\n\n\n\n\n\n\n\n\n\n\nbackup\n\n    Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.\n\n\n\n\n\n\nlist-missing-disk-stores\n\n    Lists all disk stores with the most recent data for which other members are waiting.\n\n\n\n\n\n\nrevoke-missing-disk-store\n\n    Instruct SnappyData members to stop waiting for a disk store to become available.\n\n\n\n\n\n\nrun\n\n    Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive SnappyData shell.\n\n\n\n\n\n\nversion\n\n    Prints information about the SnappyData product version", 
            "title": "Command Line Utilities"
        }, 
        {
            "location": "/reference/command_line_utilities/store-launcher/#command-line-utilites", 
            "text": "Use the  snappy  command-line utility to launch SnappyData utilities.  To display a full list of snappy commands and options:  snappy --help  The command form to display a particular utility's usage is:  snappy  utility  --help  With no arguments,  snappy  starts an  interactive SQL command shell :  snappy  To specify a system property for an interactive  snappy  session, you must define the JAVA_ARGS environment variable before starting  snappy . For example,  snappy  uses the  gfxd.history  system property to define the file that stores a list of the commands that are executed during an interactive session. To change this property, you would define it as part of the JAVA_ARGS variable:  $ export JAVA_ARGS= -Dgfxd.history=/Users/user1/snappystore-history.sql  \n$ snappy  To launch and exit a  snappy  utility (rather than start an interactive  snappy  shell) use the syntax:  snappy  utility   arguments for specified utility   To specify a system property when launching a  snappy  utility, use -J-D property_name = property_value  argument.  In addition to launching various utilities provided with SnappyData, when launched without any arguments,  snappy  starts an interactive command shell that you can use to connect to a SnappyData system and execute various commands, including SQL statements.      backup \n    Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.    list-missing-disk-stores \n    Lists all disk stores with the most recent data for which other members are waiting.    revoke-missing-disk-store \n    Instruct SnappyData members to stop waiting for a disk store to become available.    run \n    Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive SnappyData shell.    version \n    Prints information about the SnappyData product version", 
            "title": "Command Line Utilites"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/", 
            "text": "backup\n\n\nCreates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.\n\n\n\n\nNote\n\n\nSnappyData does not support backing up disk stores on systems with live transactions, or when concurrent DML statements are being executed. \n\n\n\n\n\n\n\n\nSyntax\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\nPrerequisites and Best Practices\n\n\n\n\n\n\nSpecifying the Backup Directory\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nOutput Messages from snappy backup\n\n\n\n\n\n\nBackup Directory Structure and Its Contents\n\n\n\n\n\n\nRestoring an Online Backup\n\n\n\n\n\n\n\n\nSyntax\n\n\nUse the -locator option, on the command line to connect to the SnappyData cluster.\n\n\nsnappy backup [-baseline=\nbaseline directory\n] \ntarget directory\n [-J-D\nvmprop\n=\nprop-value\n]\n [-locators=\naddresses\n] [-bind-address=\naddress\n] [-\nprop-name\n=\nprop-value\n]*\n\n\n\n\nAlternatively, you can specify these and other distributed system properties in a gemfirexd.properties file that is available in the directory where you run the \nsnappy\n command.\n\n\nThe table describes options for snappy backup.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-baseline\n\n\nThe directory that contains a baseline backup used for comparison during an incremental backup. The baseline directory corresponds to the date when the original backup command was performed, rather than the backup location you specified (for example, a valid baseline directory might resemble /export/fileServerDirectory/gemfireXDBackupLocation/2012-10-01-12-30).\nAn incremental backup operation backs up any data that is not already present in the specified \n-baseline\n directory. If the member cannot find previously backed up data or if the previously backed up data is corrupt, then command performs a full backup on that member. (The command also performs a full backup if you omit the \n-baseline\n option.\n\n\n\n\n\n\ntarget-directory\n\n\nThe directory in which SnappyData stores the backup content. See \nSpecifying the Backup Directory\n.\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\n-J-D\n=\n\n\nSets Java system property to the specified value.\n\n\n\n\n\n\n\n\n\n\nDescription\n\n\nAn online backup saves the following:\n\n\n\n\n\n\nFor each member with persistent data, the backup includes disk store files for all stores containing persistent table data.\n\n\n\n\n\n\nConfiguration files from the member startup.\n\n\n\n\n\n\ngemfirexd.properties, with the properties the member was started with.\n\n\n\n\n\n\nA restore script, written for the member's operating system, that copies the files back to their original locations. For example, in Windows, the file is restore.bat and in Linux, it is restore.sh.\n\n\n\n\n\n\n\n\nPrerequisites and Best Practices\n\n\n\n\n\n\nRun this command during a period of low activity in your system. The backup does not block system activities, but it uses file system resources on all hosts in your distributed system and can affect performance.\n\n\n\n\n\n\nDo not try to create backup files from a running system using file copy commands. You will get incomplete and unusable copies.\n\n\n\n\n\n\nMake sure the target backup directory exists and has the proper permissions for your members to write to it and create subdirectories. \n\n\n\n\n\n\nYou might want to compact your disk store before running the backup.\n\n\n\n\n\n\nMake sure that those SnappyData members that host persistent data are running in the distributed system. Offline members cannot back up their disk stores. (A complete backup can still be performed if all table data is available in the running members.)\n\n\n\n\n\n\n\n\nSpecifying the Backup Directory\n\n\nThe directory you specify for backup can be used multiple times. Each backup first creates a top level directory for the backup, under the directory you specify, identified to the minute. You can use one of two formats:\n\n\n\n\n\n\nUse a single physical location, such as a network file server. (For example, /export/fileServerDirectory/snappyStoreBackupLocation).\n\n\n\n\n\n\nUse a directory that is local to all host machines in the system. (For example, ./snappyStoreBackupLocation).\n\n\n\n\n\n\n\n\nExample\n\n\nUsing a backup directory that is local to all host machines in the system:\n\n\nsnappy backup  ./snappyStoreBackupLocation\n  -locators=localhost:10334\n\n\n\n\n\n\n\nTo perform an incremental backup at a later time:\n\n\nsnappy backup -baseline=./snappyStoreBackupLocation/2012-10-01-12-30 ./snappyStoreBackupLocation \n  -locators=localhost:10334\n\n\n\n\n\n\nNote\n\n\nSnappyData does not support taking incremental backups on systems with live transactions, or when concurrent DML statements are being executed.\n\n\n\n\n\n\nOutput Messages from snappy backup\n\n\nWhen you run \nsnappy backup\n, it reports on the outcome of the operation.\n\n\nIf any members were offline when you run \nsnappy backup\n, you get this message:\n\n\nThe backup may be incomplete. The following disk\nstores are not online: 93050768-514d-4b20-99e5-c8c9c0156ae9 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/.]  \n\n\n\n\nA complete backup can still be performed if all table data is available in the running members.\n\n\nThe tool reports on the success of the operation. If the operation is successful, you see a message like this:\n\n\nConnecting to distributed system: -locators=localhost:10334\nThe following disk stores were backed up:\n93050768-514d-4b20-99e5-c8c9c0156ae9 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/.]\n    4860bb01-4b0a-4025-80c3-17708770d933 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/./datadictionary]\n    9c656fdd-aa7b-4f08-926b-768424ec672a [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/./datadictionary]\n    45ddc031-ae2a-4e8a-9b28-dad4fa1fd4cf [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/.]\nBackup successful.\n\n\n\n\nIf the operation does not succeed at backing up all known members, you see a message like this:\n\n\nConnecting to distributed system: -locators=localhost:10334\nThe following disk stores were backed up:\n93050768-514d-4b20-99e5-c8c9c0156ae9 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/.]\n    4860bb01-4b0a-4025-80c3-17708770d933 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/./datadictionary]\n    9c656fdd-aa7b-4f08-926b-768424ec672a [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/./datadictionary]\n    45ddc031-ae2a-4e8a-9b28-dad4fa1fd4cf [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/.]\nThe backup may be incomplete. The following disk stores are not online:\n\n\n\n\nA member that fails to complete its backup is noted in this ending status message and leaves the file INCOMPLETE_BACKUP in its highest level backup directory.\n\n\n\n\nBackup Directory Structure and Its Contents\n\n\nBelow is the structure of files and directories backed up in a distributed system:\n\n\n 2011-05-02-18-10 /:\npc15_8933_v10_10761_54522\n2011-05-02-18-10/pc15_8933_v10_10761_54522:\nconfig diskstores README.txt restore.sh\n2011-05-02-18-10/pc15_8933_v10_10761_54522/config:\ngemfirexd.properties\n2011-05-02-18-10/pc15_8933_v10_10761_54522/diskstores:\nGFXD_DD_DISKSTORE\n2011-05-02-18-10/pc15_8933_v10_10761_54522/diskstores/GFXD_DD_DISKSTORE:\ndir0\n2011-05-02-18-10/pc15_8933_v10_10761_54522/diskstores/GFXD_DD_DISKSTORE/dir0:\nBACKUPGFXD-DD-DISKSTORE_1.crf\nBACKUPGFXD-DD-DISKSTORE_1.drf BACKUPGFXD-DD-DISKSTORE_2.crf\nBACKUPGFXD-DD-DISKSTORE_2.drf BACKUPGFXD-DD-DISKSTORE.if\n\n\n\n\n\n\nRestoring Files\n\n\nThe restore script (restore.sh restore.bat) copies files back to their original locations. You can do this manually if you wish:\n\n\n\n\n\n\nRestore your disk stores when your members are offline and the system is down.\n\n\n\n\n\n\nRead the restore scripts to see where they the files are placed and make sure the destination locations are ready. The restore scripts does not copy over files with the same names.\n\n\n\n\n\n\nRun the restore scripts. Run each script on the host where the backup originated.\n\n\n\n\n\n\nThe restore operation copies the files back to their original location.", 
            "title": "backup"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#backup", 
            "text": "Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.   Note  SnappyData does not support backing up disk stores on systems with live transactions, or when concurrent DML statements are being executed.      Syntax    Description    Prerequisites and Best Practices    Specifying the Backup Directory    Example    Output Messages from snappy backup    Backup Directory Structure and Its Contents    Restoring an Online Backup", 
            "title": "backup"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#syntax", 
            "text": "Use the -locator option, on the command line to connect to the SnappyData cluster.  snappy backup [-baseline= baseline directory ]  target directory  [-J-D vmprop = prop-value ]\n [-locators= addresses ] [-bind-address= address ] [- prop-name = prop-value ]*  Alternatively, you can specify these and other distributed system properties in a gemfirexd.properties file that is available in the directory where you run the  snappy  command.  The table describes options for snappy backup.     Option  Description      -baseline  The directory that contains a baseline backup used for comparison during an incremental backup. The baseline directory corresponds to the date when the original backup command was performed, rather than the backup location you specified (for example, a valid baseline directory might resemble /export/fileServerDirectory/gemfireXDBackupLocation/2012-10-01-12-30). An incremental backup operation backs up any data that is not already present in the specified  -baseline  directory. If the member cannot find previously backed up data or if the previously backed up data is corrupt, then command performs a full backup on that member. (The command also performs a full backup if you omit the  -baseline  option.    target-directory  The directory in which SnappyData stores the backup content. See  Specifying the Backup Directory .    -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  Any other SnappyData distributed system property.    -J-D =  Sets Java system property to the specified value.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#description", 
            "text": "An online backup saves the following:    For each member with persistent data, the backup includes disk store files for all stores containing persistent table data.    Configuration files from the member startup.    gemfirexd.properties, with the properties the member was started with.    A restore script, written for the member's operating system, that copies the files back to their original locations. For example, in Windows, the file is restore.bat and in Linux, it is restore.sh.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#prerequisites-and-best-practices", 
            "text": "Run this command during a period of low activity in your system. The backup does not block system activities, but it uses file system resources on all hosts in your distributed system and can affect performance.    Do not try to create backup files from a running system using file copy commands. You will get incomplete and unusable copies.    Make sure the target backup directory exists and has the proper permissions for your members to write to it and create subdirectories.     You might want to compact your disk store before running the backup.    Make sure that those SnappyData members that host persistent data are running in the distributed system. Offline members cannot back up their disk stores. (A complete backup can still be performed if all table data is available in the running members.)", 
            "title": "Prerequisites and Best Practices"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#specifying-the-backup-directory", 
            "text": "The directory you specify for backup can be used multiple times. Each backup first creates a top level directory for the backup, under the directory you specify, identified to the minute. You can use one of two formats:    Use a single physical location, such as a network file server. (For example, /export/fileServerDirectory/snappyStoreBackupLocation).    Use a directory that is local to all host machines in the system. (For example, ./snappyStoreBackupLocation).", 
            "title": "Specifying the Backup Directory"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#example", 
            "text": "Using a backup directory that is local to all host machines in the system:  snappy backup  ./snappyStoreBackupLocation\n  -locators=localhost:10334   To perform an incremental backup at a later time:  snappy backup -baseline=./snappyStoreBackupLocation/2012-10-01-12-30 ./snappyStoreBackupLocation \n  -locators=localhost:10334   Note  SnappyData does not support taking incremental backups on systems with live transactions, or when concurrent DML statements are being executed.", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#output-messages-from-snappy-backup", 
            "text": "When you run  snappy backup , it reports on the outcome of the operation.  If any members were offline when you run  snappy backup , you get this message:  The backup may be incomplete. The following disk\nstores are not online: 93050768-514d-4b20-99e5-c8c9c0156ae9 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/.]    A complete backup can still be performed if all table data is available in the running members.  The tool reports on the success of the operation. If the operation is successful, you see a message like this:  Connecting to distributed system: -locators=localhost:10334\nThe following disk stores were backed up:\n93050768-514d-4b20-99e5-c8c9c0156ae9 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/.]\n    4860bb01-4b0a-4025-80c3-17708770d933 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/./datadictionary]\n    9c656fdd-aa7b-4f08-926b-768424ec672a [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/./datadictionary]\n    45ddc031-ae2a-4e8a-9b28-dad4fa1fd4cf [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/.]\nBackup successful.  If the operation does not succeed at backing up all known members, you see a message like this:  Connecting to distributed system: -locators=localhost:10334\nThe following disk stores were backed up:\n93050768-514d-4b20-99e5-c8c9c0156ae9 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/.]\n    4860bb01-4b0a-4025-80c3-17708770d933 [localhost:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-locator-1/./datadictionary]\n    9c656fdd-aa7b-4f08-926b-768424ec672a [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/./datadictionary]\n    45ddc031-ae2a-4e8a-9b28-dad4fa1fd4cf [snappy-sgoel:/home/user1/snappydata/build-artifacts/scala-2.11/snappy/work1/localhost-server-1/.]\nThe backup may be incomplete. The following disk stores are not online:  A member that fails to complete its backup is noted in this ending status message and leaves the file INCOMPLETE_BACKUP in its highest level backup directory.", 
            "title": "Output Messages from snappy backup"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#backup-directory-structure-and-its-contents", 
            "text": "Below is the structure of files and directories backed up in a distributed system:   2011-05-02-18-10 /:\npc15_8933_v10_10761_54522\n2011-05-02-18-10/pc15_8933_v10_10761_54522:\nconfig diskstores README.txt restore.sh\n2011-05-02-18-10/pc15_8933_v10_10761_54522/config:\ngemfirexd.properties\n2011-05-02-18-10/pc15_8933_v10_10761_54522/diskstores:\nGFXD_DD_DISKSTORE\n2011-05-02-18-10/pc15_8933_v10_10761_54522/diskstores/GFXD_DD_DISKSTORE:\ndir0\n2011-05-02-18-10/pc15_8933_v10_10761_54522/diskstores/GFXD_DD_DISKSTORE/dir0:\nBACKUPGFXD-DD-DISKSTORE_1.crf\nBACKUPGFXD-DD-DISKSTORE_1.drf BACKUPGFXD-DD-DISKSTORE_2.crf\nBACKUPGFXD-DD-DISKSTORE_2.drf BACKUPGFXD-DD-DISKSTORE.if", 
            "title": "Backup Directory Structure and Its Contents"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#restoring-files", 
            "text": "The restore script (restore.sh restore.bat) copies files back to their original locations. You can do this manually if you wish:    Restore your disk stores when your members are offline and the system is down.    Read the restore scripts to see where they the files are placed and make sure the destination locations are ready. The restore scripts does not copy over files with the same names.    Run the restore scripts. Run each script on the host where the backup originated.    The restore operation copies the files back to their original location.", 
            "title": "Restoring Files"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/", 
            "text": "list-missing-disk-stores\n\n\nLists all disk stores with the most recent data for which other members are waiting.\n\n\nSyntax\n\n\nsnappy list-missing-disk-stores \n  [-locators=\naddresses\n] [-bind-address=\naddress\n] [-\nprop-name\n=\nprop-value\n]*\n\n\n\n\nIf no locator option is specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.\n\n\nThe table describes options for snappy list-missing-disk-stores.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\nsnappy list-missing-disk-stores \n-locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\n1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary]", 
            "title": "list-missing-disk-stores"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/#list-missing-disk-stores", 
            "text": "Lists all disk stores with the most recent data for which other members are waiting.", 
            "title": "list-missing-disk-stores"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/#syntax", 
            "text": "snappy list-missing-disk-stores \n  [-locators= addresses ] [-bind-address= address ] [- prop-name = prop-value ]*  If no locator option is specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.  The table describes options for snappy list-missing-disk-stores.     Option  Description      -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  Any other SnappyData distributed system property.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/#example", 
            "text": "snappy list-missing-disk-stores \n-locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\n1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary]", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/", 
            "text": "revoke-missing-disk-store\n\n\nInstruct SnappyData members to stop waiting for a disk store to become available.\n\n\nSyntax\n\n\nsnappy revoke-missing-disk-store \ndisk-store-id\n\n  [-locators=\naddresses\n] \n        [-bind-address=\naddress\n] \n  [-\nprop-name\n=\nprop-value\n]*\n\n\n\n\nThe table describes options and arguments for snappy revoke-missing-disk-store. If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndisk-store-id\n\n\n(Required.) Specifies the unique ID of the disk store to revoke.\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\nExample\n\n\nThis command first lists the missing disk store:\n\n\nsnappy list-missing-disk-stores -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\n1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary]\n\n\n\n\nNext, \nsnappy\n revokes the missing disk store if more recent data is available:\n\n\nsnappy revoke-missing-disk-store 1f811502-f126-4ce4-9839-9549335b734d -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\nrevocation was successful and no disk stores are now missing\n\n\n\n\nFinally, \nsnappy\n verifies that no disk stores are missing:\n\n\nsnappy list-missing-disk-stores -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\nThe distributed system did not have any missing disk stores", 
            "title": "revoke-missing-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#revoke-missing-disk-store", 
            "text": "Instruct SnappyData members to stop waiting for a disk store to become available.", 
            "title": "revoke-missing-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#syntax", 
            "text": "snappy revoke-missing-disk-store  disk-store-id \n  [-locators= addresses ] \n        [-bind-address= address ] \n  [- prop-name = prop-value ]*  The table describes options and arguments for snappy revoke-missing-disk-store. If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.     Option  Description      disk-store-id  (Required.) Specifies the unique ID of the disk store to revoke.    -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  Any other SnappyData distributed system property.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#description", 
            "text": "", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#example", 
            "text": "This command first lists the missing disk store:  snappy list-missing-disk-stores -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\n1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary]  Next,  snappy  revokes the missing disk store if more recent data is available:  snappy revoke-missing-disk-store 1f811502-f126-4ce4-9839-9549335b734d -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\nrevocation was successful and no disk stores are now missing  Finally,  snappy  verifies that no disk stores are missing:  snappy list-missing-disk-stores -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\nThe distributed system did not have any missing disk stores", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/", 
            "text": "run\n\n\nConnects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.\n\n\nSyntax\n\n\nsnappy run -file=\npath or URL\n\n     [-auth-provider=\nname\n]\n     [-client-bind-address=\naddress\n]\n     [-client-port=\nport\n]\n     [-encoding=\ncharset\n]\n     [-extra-conn-props=\nproperties\n] \n     [-help] \n     [-ignore-errors]\n     [-J-D\nproperty=value\n]\n     [-password[=\npassword\n]]\n     [-path=\npath\n]\n     [-user=\nusername\n]\n\n\n\n\nThis table describes options for the \nsnappy run\n command. Default values are used if you do not specify an option.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-file\n\n\nThe local path of a SQL command file to execute, or a URL that links to the SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.\nThis argument is required.\n\n\n\n\n\n\n-auth-provider\n\n\nSets the authentication provider to use for peer-to-peer connections as well as client-server connections. Valid values are BUILTIN and LDAP. All other members of the SnappyData distributed system must use the same authentication provider and user definitions. If you omit this option, the connection uses no authentication mechanism.\n\n\n\n\n\n\n-client-bind-address\n\n\nThe hostname or IP address on which a SnappyData locator listens for client connections. The default is \"localhost\" \nUse this option with \n-client-port\n to attach to a SnappyData cluster as a thin client and perform the command.\n\n\n\n\n\n\n-client-port\n\n\nThe port on which a SnappyData locator listens for client connections. The default is 1527.\nUse this option with \n-client-bind-address\n to attach to a SnappyData cluster as a thin client and perform the command.\n\n\n\n\n\n\n-encoding\n\n\nThe character set encoding of the SQL script file (\n-file\n argument). The default is UTF-8. Other possible values are: US-ASCII, ISO-8859-1, UTF-8, UTF-16BE, UTF-16LE, UTF-16. See the \njava.nio.charset.Charset\n reference for more information.\n\n\n\n\n\n\n-extra-conn-props\n\n\nA semicolon-separated list of properties to use when connecting to the SnappyData distributed system.\n\n\n\n\n\n\n-help, --help\n\n\nDisplay the help message for this snappy command.\n\n\n\n\n\n\n-ignore-errors\n\n\nInclude this option to ignore any errors that may occur while executing statements in the file, and continue executing the remaining statements. If you omit this option, then snappy immediately terminates the script's execution if an exception occurs.\n\n\n\n\n\n\n-J-D;property=value;\n\n\nSets Java system property to the specified value.\n\n\n\n\n\n\n-password\n\n\nIf the servers or locators have been configured to use authentication, this option specifies the password for the user (specified with the -user option) to use for booting the server and joining the distributed system.\nThe password value is optional. If you omit the password, you are prompted to enter a password from the console.\n\n\n\n\n\n\n-path\n\n\nConfigures the working directory for any other SQL command files executed from within the script. The \n-path\n entry is prepended to any SQL script file name executed that the script executes in a \nrun\n command.\n\n\n\n\n\n\n-user\n\n\nIf the servers or locators have been configured to use authentication, this option specifies the user name to use for booting the server and joining the distributed system.\n\n\n\n\n\n\n\n\nDescription\n\n\nSpecify the below command to connect to a SnappyData Distributed system and execute a SQL command file:\n\n\nUse both \n-client-bind-address\n and \n-client-port\n to connect to a SnappyData cluster as a thin client and perform the command.\n\n\nThe \n-file\n argument specifies the location of the SQL script file to execute. If the script file itself calls other script files using \nrun 'filename'\n, also consider using the \n-path\n option to specify the location of the embedded script files. If an exception occurs while executing the script, SnappyData immediately stops executing script commands, unless you include the \n-ignore-errors\n option.\n\n\nExamples\n\n\nThis command connects to a SnappyData network server running on localhost:1527 and executes commands in the create_and_load_column_table.sql file:\n\n\n\nsnappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 \n\n\n\n\nIf the script calls for dependent scripts (for example load_countries.sql, load_cities.sql) and if the command is executed outside the directory in which the dependent scripts are located, specify the working directory using the \n-path\n option.\n\n\nsnappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -path=/home/user1/snappydata/examples/quickstart -client-bind-address=localhost -client-port=1527", 
            "title": "run"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#run", 
            "text": "Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.", 
            "title": "run"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#syntax", 
            "text": "snappy run -file= path or URL \n     [-auth-provider= name ]\n     [-client-bind-address= address ]\n     [-client-port= port ]\n     [-encoding= charset ]\n     [-extra-conn-props= properties ] \n     [-help] \n     [-ignore-errors]\n     [-J-D property=value ]\n     [-password[= password ]]\n     [-path= path ]\n     [-user= username ]  This table describes options for the  snappy run  command. Default values are used if you do not specify an option.     Option  Description      -file  The local path of a SQL command file to execute, or a URL that links to the SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell. This argument is required.    -auth-provider  Sets the authentication provider to use for peer-to-peer connections as well as client-server connections. Valid values are BUILTIN and LDAP. All other members of the SnappyData distributed system must use the same authentication provider and user definitions. If you omit this option, the connection uses no authentication mechanism.    -client-bind-address  The hostname or IP address on which a SnappyData locator listens for client connections. The default is \"localhost\"  Use this option with  -client-port  to attach to a SnappyData cluster as a thin client and perform the command.    -client-port  The port on which a SnappyData locator listens for client connections. The default is 1527. Use this option with  -client-bind-address  to attach to a SnappyData cluster as a thin client and perform the command.    -encoding  The character set encoding of the SQL script file ( -file  argument). The default is UTF-8. Other possible values are: US-ASCII, ISO-8859-1, UTF-8, UTF-16BE, UTF-16LE, UTF-16. See the  java.nio.charset.Charset  reference for more information.    -extra-conn-props  A semicolon-separated list of properties to use when connecting to the SnappyData distributed system.    -help, --help  Display the help message for this snappy command.    -ignore-errors  Include this option to ignore any errors that may occur while executing statements in the file, and continue executing the remaining statements. If you omit this option, then snappy immediately terminates the script's execution if an exception occurs.    -J-D;property=value;  Sets Java system property to the specified value.    -password  If the servers or locators have been configured to use authentication, this option specifies the password for the user (specified with the -user option) to use for booting the server and joining the distributed system. The password value is optional. If you omit the password, you are prompted to enter a password from the console.    -path  Configures the working directory for any other SQL command files executed from within the script. The  -path  entry is prepended to any SQL script file name executed that the script executes in a  run  command.    -user  If the servers or locators have been configured to use authentication, this option specifies the user name to use for booting the server and joining the distributed system.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#description", 
            "text": "Specify the below command to connect to a SnappyData Distributed system and execute a SQL command file:  Use both  -client-bind-address  and  -client-port  to connect to a SnappyData cluster as a thin client and perform the command.  The  -file  argument specifies the location of the SQL script file to execute. If the script file itself calls other script files using  run 'filename' , also consider using the  -path  option to specify the location of the embedded script files. If an exception occurs while executing the script, SnappyData immediately stops executing script commands, unless you include the  -ignore-errors  option.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#examples", 
            "text": "This command connects to a SnappyData network server running on localhost:1527 and executes commands in the create_and_load_column_table.sql file:  \nsnappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527   If the script calls for dependent scripts (for example load_countries.sql, load_cities.sql) and if the command is executed outside the directory in which the dependent scripts are located, specify the working directory using the  -path  option.  snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -path=/home/user1/snappydata/examples/quickstart -client-bind-address=localhost -client-port=1527", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/", 
            "text": "version\n\n\nPrints information about the SnappyData product version.\n\n\nSyntax\n\n\nsnappy version\n\n\n\n\nExample\n\n\nbin/snappy version\nSnappyData Platform Version 0.9 \n    SnappyData RowStore 1.5.4 \n    SnappyData Column Store 0.9 \n    SnappyData AQP 0.9", 
            "title": "version"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/#version", 
            "text": "Prints information about the SnappyData product version.", 
            "title": "version"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/#syntax", 
            "text": "snappy version", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/#example", 
            "text": "bin/snappy version\nSnappyData Platform Version 0.9 \n    SnappyData RowStore 1.5.4 \n    SnappyData Column Store 0.9 \n    SnappyData AQP 0.9", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/store_command_reference/", 
            "text": "Snappy-SQL Shell Interactive Commands\n\n\nsnappy\n implements an interactive command-line tool that is based on the Apache Derby \nij\n tool. Use \nsnappy\n to run scripts or interactive queries against a SnappyData cluster.\n\n\nStart the interactive \nsnappy\n command prompt by using the snappy script without supplying any other options:\n\n\nsnappy\n\n\n\n\nThe system property \ngfxd.history\n specifies a file in which to store all of the commands executed during an interactive \nsnappy\n session. For example:\n\n\n$ export JAVA_ARGS=\n-Dgfxd.history=/Users/user1/snappydata-history.sql\n\n$ snappy\n\n\n\n\nBy default the history file is named .gfxd.history, and it is stored in the current user's home directory.\n\n\nsnappy\n accepts several commands to control its use of JDBC. It recognizes a semicolon as the end of a \nsnappy\n or SQL command. It treats semicolons within SQL comments, strings, and delimited identifiers as part of those constructs and not as the end of the command. Semicolons are required at the end of a \nsnappy\n or SQL statement.\n\n\nAll \nsnappy\n commands, identifiers, and keywords are case-insensitive.\n\n\nCommands can span multiple lines without using any special escape character for ends of lines. This means that if a string spans a line, the new line contents show up in the value in the string.\n\n\nsnappy\n treats any command that it does not recognize as a SQL command that is passed to the underlying connection. This means that any syntactic errors in \nsnappy\n commands are handed to the SQL engine and generally result in SQL parsing errors.\n\n\n\n\n\n\nconnect client\n\n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the \nhost:port\n values.\n\n\n\n\n\n\nconnect\n\n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the \nhost:port\n values.\n\n\n\n\n\n\ndescribe\n\n    Provides a description of the specified table or view.\n\n\n\n\n\n\ndisconnect\n\n    Disconnects from the database.\n\n\n\n\n\n\nelapsedtime\n\n    Displays the total time elapsed during statement execution.\n\n\n\n\n\n\nexit\n\n    Completes the \nsnappy\n application and halts processing.\n\n\n\n\n\n\nMaximumDisplayWidth\n\n    Sets the largest display width for columns to the specified value.\n\n\n\n\n\n\nrun\n\n    Treats the value of the string as a valid file name, and redirects \nsnappy\n processing to read from that file until it ends or an exit command is executed.\n\n\n\n\n\n\nset connection\n\n    Specifies which connection to make current when more than one connection is open.\n\n\n\n\n\n\nshow\n\n    Displays information about active connections and database objects.", 
            "title": "Snappy-SQL Shell Interactive Commands"
        }, 
        {
            "location": "/reference/interactive_commands/store_command_reference/#snappy-sql-shell-interactive-commands", 
            "text": "snappy  implements an interactive command-line tool that is based on the Apache Derby  ij  tool. Use  snappy  to run scripts or interactive queries against a SnappyData cluster.  Start the interactive  snappy  command prompt by using the snappy script without supplying any other options:  snappy  The system property  gfxd.history  specifies a file in which to store all of the commands executed during an interactive  snappy  session. For example:  $ export JAVA_ARGS= -Dgfxd.history=/Users/user1/snappydata-history.sql \n$ snappy  By default the history file is named .gfxd.history, and it is stored in the current user's home directory.  snappy  accepts several commands to control its use of JDBC. It recognizes a semicolon as the end of a  snappy  or SQL command. It treats semicolons within SQL comments, strings, and delimited identifiers as part of those constructs and not as the end of the command. Semicolons are required at the end of a  snappy  or SQL statement.  All  snappy  commands, identifiers, and keywords are case-insensitive.  Commands can span multiple lines without using any special escape character for ends of lines. This means that if a string spans a line, the new line contents show up in the value in the string.  snappy  treats any command that it does not recognize as a SQL command that is passed to the underlying connection. This means that any syntactic errors in  snappy  commands are handed to the SQL engine and generally result in SQL parsing errors.    connect client \n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the  host:port  values.    connect \n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the  host:port  values.    describe \n    Provides a description of the specified table or view.    disconnect \n    Disconnects from the database.    elapsedtime \n    Displays the total time elapsed during statement execution.    exit \n    Completes the  snappy  application and halts processing.    MaximumDisplayWidth \n    Sets the largest display width for columns to the specified value.    run \n    Treats the value of the string as a valid file name, and redirects  snappy  processing to read from that file until it ends or an exit command is executed.    set connection \n    Specifies which connection to make current when more than one connection is open.    show \n    Displays information about active connections and database objects.", 
            "title": "Snappy-SQL Shell Interactive Commands"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/", 
            "text": "connect client\n\n\nSyntax\n\n\nCONNECT CLIENT 'host:port[;property=value]*' [ AS connectionName ]\n\n\n\n\nDescription\n\n\nUses the JDBC SnappyData thin client driver to connect to a SnappyData member indicated by the \nhost:port\n values. You can specify an optional name for your connection. Use the \nset connection\n to switch between multiple connections. If you do not name a connection, the system generates a name automatically.\n\n\nIf the connection requires a user name and password, supply those with the optional properties.\n\n\nIf the connect succeeds, the connection becomes the current one and \nsnappy\n displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.\n\n\nAll further commands are processed against the new, current connection.\n\n\nExample\n\n\nSnappyData version 0.9\nsnappy\n connect client 'localhost:1527' as clientConnection;\nsnappy\n show connections;\nCLIENTCONNECTION* -     jdbc:snappydata://localhost:1527/\n* = current connection", 
            "title": "connect client"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#connect-client", 
            "text": "", 
            "title": "connect client"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#syntax", 
            "text": "CONNECT CLIENT 'host:port[;property=value]*' [ AS connectionName ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#description", 
            "text": "Uses the JDBC SnappyData thin client driver to connect to a SnappyData member indicated by the  host:port  values. You can specify an optional name for your connection. Use the  set connection  to switch between multiple connections. If you do not name a connection, the system generates a name automatically.  If the connection requires a user name and password, supply those with the optional properties.  If the connect succeeds, the connection becomes the current one and  snappy  displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.  All further commands are processed against the new, current connection.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#example", 
            "text": "SnappyData version 0.9\nsnappy  connect client 'localhost:1527' as clientConnection;\nsnappy  show connections;\nCLIENTCONNECTION* -     jdbc:snappydata://localhost:1527/\n* = current connection", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/connect/", 
            "text": "connect\n\n\nConnects to the database indicated by the \nConnectionURLString\n.\n\n\nSyntax\n\n\nCONNECT ConnectionURLString [ PROTOCOL Identifier ]\n    [ AS Identifier ]\n\n\n\n\nDescription\n\n\nConnects to the database indicated by the \nConnectionURLString\n. You have the option of specifying a name for your connection. Use the \nSet Connection\n command to switch between connections. If you do not name a connection, the system generates a name automatically.\n\n\n\n\n\n\n\nNote\n\n\nIf the connection requires a user name and password, supply those in the connection URL string, as shown in the example. \n\n\n\n\nIf the connect succeeds, the connection becomes the current one and \nsnappy\n displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.\n\n\nAll further commands are processed against the new, current connection.\n\n\nExample\n\n\nsnappy\n protocol 'jdbc:derby:';\nsnappy\n connect '//armenia:29303/myDB;user=a;password=a' as db5Connection; \nsnappy\n show connections;\nDB5CONNECTION* -        jdbc:derby://armenia:29303/myDB\n* = current connection", 
            "title": "connect"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#connect", 
            "text": "Connects to the database indicated by the  ConnectionURLString .", 
            "title": "connect"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#syntax", 
            "text": "CONNECT ConnectionURLString [ PROTOCOL Identifier ]\n    [ AS Identifier ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#description", 
            "text": "Connects to the database indicated by the  ConnectionURLString . You have the option of specifying a name for your connection. Use the  Set Connection  command to switch between connections. If you do not name a connection, the system generates a name automatically.    Note  If the connection requires a user name and password, supply those in the connection URL string, as shown in the example.    If the connect succeeds, the connection becomes the current one and  snappy  displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.  All further commands are processed against the new, current connection.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#example", 
            "text": "snappy  protocol 'jdbc:derby:';\nsnappy  connect '//armenia:29303/myDB;user=a;password=a' as db5Connection; \nsnappy  show connections;\nDB5CONNECTION* -        jdbc:derby://armenia:29303/myDB\n* = current connection", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/describe/", 
            "text": "describe\n\n\nProvides a description of the specified table or view.\n\n\nSyntax\n\n\nDESCRIBE { table-Name | view-Name }\n\n\n\n\nDescription\n\n\nProvides a description of the specified table or view. For a list of tables in the current schema, use the Show Tables command. For a list of views in the current schema, use the Show Views command. For a list of available schemas, use the Show Schemas command.\n\n\nIf the table or view is in a particular schema, qualify it with the schema name. If the table or view name is case-sensitive, enclose it in single quotes. You can display all the columns from all the tables and views in a single schema in a single display by using the wildcard character '*'. See the examples below.\n\n\nExample\n\n\nsnappy\n describe maps;\nCOLUMN_NAME         |TYPE_NAME|DEC\n|NUM\n|COLUM\n|COLUMN_DEF|CHAR_OCTE\n|IS_NULL\n\n------------------------------------------------------------------------------\nMAP_ID              |INTEGER  |0   |10  |10    |AUTOINCRE\n|NULL      |NO\nMAP_NAME            |VARCHAR  |NULL|NULL|24    |NULL      |48        |NO\nREGION              |VARCHAR  |NULL|NULL|26    |NULL      |52        |YES\nAREA                |DECIMAL  |4   |10  |8     |NULL      |NULL      |NO\nPHOTO_FORMAT        |VARCHAR  |NULL|NULL|26    |NULL      |52        |NO\nPICTURE             |BLOB     |NULL|NULL|102400|NULL      |NULL      |YES\n\n6 rows selected\nsnappy", 
            "title": "describe"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#describe", 
            "text": "Provides a description of the specified table or view.", 
            "title": "describe"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#syntax", 
            "text": "DESCRIBE { table-Name | view-Name }", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#description", 
            "text": "Provides a description of the specified table or view. For a list of tables in the current schema, use the Show Tables command. For a list of views in the current schema, use the Show Views command. For a list of available schemas, use the Show Schemas command.  If the table or view is in a particular schema, qualify it with the schema name. If the table or view name is case-sensitive, enclose it in single quotes. You can display all the columns from all the tables and views in a single schema in a single display by using the wildcard character '*'. See the examples below.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#example", 
            "text": "snappy  describe maps;\nCOLUMN_NAME         |TYPE_NAME|DEC |NUM |COLUM |COLUMN_DEF|CHAR_OCTE |IS_NULL \n------------------------------------------------------------------------------\nMAP_ID              |INTEGER  |0   |10  |10    |AUTOINCRE |NULL      |NO\nMAP_NAME            |VARCHAR  |NULL|NULL|24    |NULL      |48        |NO\nREGION              |VARCHAR  |NULL|NULL|26    |NULL      |52        |YES\nAREA                |DECIMAL  |4   |10  |8     |NULL      |NULL      |NO\nPHOTO_FORMAT        |VARCHAR  |NULL|NULL|26    |NULL      |52        |NO\nPICTURE             |BLOB     |NULL|NULL|102400|NULL      |NULL      |YES\n\n6 rows selected\nsnappy", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/", 
            "text": "disconnect\n\n\nDisconnects from the database.\n\n\nSyntax\n\n\nDISCONNECT [ ALL | CURRENT | ConnectionIdentifier ]\n\n\n\n\nDescription\n\n\nDisconnects from the database. Specifically, issues a \njava.sql.Connection.close\n request against the connection indicated on the command line. There must be a current connection at the time the request is made.\n\n\nIf ALL is specified, all known connections are closed and there will be no current connection.\n\n\nDisconnect CURRENT is the same as Disconnect without indicating a connection; the default connection is closed.\n\n\nIf a connection name is specified with an identifier, the command disconnects the named connection. The name must be the name of a connection in the current session provided with the \nConnect\n command.\n\n\nIf the \nConnect\n command without the AS clause was used, you can supply the name the system generated for the connection. If the current connection is the named connection when the command completes, there will be no current connection and you must issue a \nConnect\n command.\n\n\nExample\n\n\nsnappy\n disconnect peerclient;\nsnappy", 
            "title": "disconnect"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#disconnect", 
            "text": "Disconnects from the database.", 
            "title": "disconnect"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#syntax", 
            "text": "DISCONNECT [ ALL | CURRENT | ConnectionIdentifier ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#description", 
            "text": "Disconnects from the database. Specifically, issues a  java.sql.Connection.close  request against the connection indicated on the command line. There must be a current connection at the time the request is made.  If ALL is specified, all known connections are closed and there will be no current connection.  Disconnect CURRENT is the same as Disconnect without indicating a connection; the default connection is closed.  If a connection name is specified with an identifier, the command disconnects the named connection. The name must be the name of a connection in the current session provided with the  Connect  command.  If the  Connect  command without the AS clause was used, you can supply the name the system generated for the connection. If the current connection is the named connection when the command completes, there will be no current connection and you must issue a  Connect  command.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#example", 
            "text": "snappy  disconnect peerclient;\nsnappy", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/", 
            "text": "elapsedtime\n\n\nDisplays the total time elapsed during statement execution.\n\n\nSyntax\n\n\nELAPSEDTIME { ON | OFF }\n\n\n\n\nDescription\n\n\nWhen \nelapsedtime\n is turned on, \nsnappy\n displays the total time elapsed during statement execution. The default value is OFF.\n\n\nExample\n\n\nsnappy\n elapsedtime on;\nsnappy\n select * from airlines;\nA\n|AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT\n|ECONOMY_SE\n|BUSINESS_S\n|FIRSTCLASS\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\n\n3 rows selected\nELAPSED TIME = 2 milliseconds", 
            "title": "elapsedtime"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#elapsedtime", 
            "text": "Displays the total time elapsed during statement execution.", 
            "title": "elapsedtime"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#syntax", 
            "text": "ELAPSEDTIME { ON | OFF }", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#description", 
            "text": "When  elapsedtime  is turned on,  snappy  displays the total time elapsed during statement execution. The default value is OFF.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#example", 
            "text": "snappy  elapsedtime on;\nsnappy  select * from airlines;\nA |AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT |ECONOMY_SE |BUSINESS_S |FIRSTCLASS \n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\n\n3 rows selected\nELAPSED TIME = 2 milliseconds", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/exit/", 
            "text": "exit\n\n\nCompletes the \nsnappy\n application and halts processing.\n\n\nSyntax\n\n\nEXIT\n\n\n\n\nDescription\n\n\nCauses the \nsnappy\n application to complete and processing to halt. Issuing this command from within a file started with the \nRun\n command or on the command line causes the outermost input loop to halt.\n\n\nsnappy\n exits when the Exit command is entered or if given a command file on the Java invocation line, when the end of the command file is reached.\n\n\nExample\n\n\nsnappy\n disconnect peerclient;\nsnappy\n exit;", 
            "title": "exit"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#exit", 
            "text": "Completes the  snappy  application and halts processing.", 
            "title": "exit"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#syntax", 
            "text": "EXIT", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#description", 
            "text": "Causes the  snappy  application to complete and processing to halt. Issuing this command from within a file started with the  Run  command or on the command line causes the outermost input loop to halt.  snappy  exits when the Exit command is entered or if given a command file on the Java invocation line, when the end of the command file is reached.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#example", 
            "text": "snappy  disconnect peerclient;\nsnappy  exit;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/", 
            "text": "MaximumDisplayWidth\n\n\nSets the largest display width for columns to the specified value.\n\n\nSyntax\n\n\nMAXIMUMDISPLAYWIDTH integer_value\n\n\n\n\nDescription\n\n\nSets the largest display width for columns to the specified value. You generally use this command to increase the default value in order to display large blocks of text.\n\n\nExample\n\n\nsnappy\n insert into airlineref values('A-1', 'NOW IS THE TIME');\n1 row inserted/updated/deleted\nsnappy\n maximumdisplaywidth 4;\nsnappy\n select * from AIRLINEREF where code='A-1';\nCODE|DES\n\n---------\nA-1 |NOW\n\n\n1 row selected\nsnappy\n  maximumdisplaywidth 30;\nsnappy\n select * from AIRLINEREF where code='A-1';\nCODE |DESCRIPTION                   \n------------------------------------\nA-1  |NOW IS THE TIME               \n\n1 row selected", 
            "title": "MaximumDisplayWidth"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#maximumdisplaywidth", 
            "text": "Sets the largest display width for columns to the specified value.", 
            "title": "MaximumDisplayWidth"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#syntax", 
            "text": "MAXIMUMDISPLAYWIDTH integer_value", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#description", 
            "text": "Sets the largest display width for columns to the specified value. You generally use this command to increase the default value in order to display large blocks of text.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#example", 
            "text": "snappy  insert into airlineref values('A-1', 'NOW IS THE TIME');\n1 row inserted/updated/deleted\nsnappy  maximumdisplaywidth 4;\nsnappy  select * from AIRLINEREF where code='A-1';\nCODE|DES \n---------\nA-1 |NOW \n\n1 row selected\nsnappy   maximumdisplaywidth 30;\nsnappy  select * from AIRLINEREF where code='A-1';\nCODE |DESCRIPTION                   \n------------------------------------\nA-1  |NOW IS THE TIME               \n\n1 row selected", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/run/", 
            "text": "run\n\n\nTreats the value of the string as a valid file name, and redirects \nsnappy\n processing to read from that file until it ends or an exit command is executed.\n\n\nSyntax\n\n\nRUN String\n\n\n\n\nDescription\n\n\nTreats the value of the string as a valid file name, and redirects \nsnappy\n processing to read from that file until it ends or an \nExit\n command is executed. If the end of the file is reached without \nsnappy\n exiting, reading continues from the previous input source once the end of the file is reached. Files can contain Run commands.\n\n\nsnappy\n prints out the statements in the file as it executes them.\n\n\nAny changes made to the \nsnappy\n environment by the file are visible in the environment when processing resumes.\n\n\nExample\n\n\nsnappy\n run 'ToursDB_schema.sql';", 
            "title": "run"
        }, 
        {
            "location": "/reference/interactive_commands/run/#run", 
            "text": "Treats the value of the string as a valid file name, and redirects  snappy  processing to read from that file until it ends or an exit command is executed.", 
            "title": "run"
        }, 
        {
            "location": "/reference/interactive_commands/run/#syntax", 
            "text": "RUN String", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/run/#description", 
            "text": "Treats the value of the string as a valid file name, and redirects  snappy  processing to read from that file until it ends or an  Exit  command is executed. If the end of the file is reached without  snappy  exiting, reading continues from the previous input source once the end of the file is reached. Files can contain Run commands.  snappy  prints out the statements in the file as it executes them.  Any changes made to the  snappy  environment by the file are visible in the environment when processing resumes.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/run/#example", 
            "text": "snappy  run 'ToursDB_schema.sql';", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/", 
            "text": "set connection\n\n\nSpecifies which connection to make current when more than one connection is open.\n\n\nSyntax\n\n\nSET CONNECTION Identifier\n\n\n\n\nDescription\n\n\nAllows you to specify which connection to make current when you have more than one connection open. Use the \nShow Connections\n command to display open connections.\n\n\nIf there is no such connection, an error results and the current connection is unchanged.\n\n\nExample\n\n\nsnappy\n set connection clientConnection;\nsnappy(CLIENTCONNECTION)\n show connections;\nCLIENTCONNECTION* -     jdbc:snappydata://localhost:1527/\nPEERCLIENT -    jdbc:snappydata:\n* = current connection\nsnappy(CLIENTCONNECTION)", 
            "title": "set connection"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#set-connection", 
            "text": "Specifies which connection to make current when more than one connection is open.", 
            "title": "set connection"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#syntax", 
            "text": "SET CONNECTION Identifier", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#description", 
            "text": "Allows you to specify which connection to make current when you have more than one connection open. Use the  Show Connections  command to display open connections.  If there is no such connection, an error results and the current connection is unchanged.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#example", 
            "text": "snappy  set connection clientConnection;\nsnappy(CLIENTCONNECTION)  show connections;\nCLIENTCONNECTION* -     jdbc:snappydata://localhost:1527/\nPEERCLIENT -    jdbc:snappydata:\n* = current connection\nsnappy(CLIENTCONNECTION)", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/show/", 
            "text": "show\n\n\nDisplays information about active connections and database objects.\n\n\nSyntax\n\n\nSHOW\n{\n   CONNECTIONS |\n   IMPORTEDKEYS [ IN schemaName | FROM table-Name ] |\n   INDEXES [ IN schemaName | FROM table-Name ] |\n   PROCEDURES [ IN schemaName ] |\n   SCHEMAS |\n   SYNONYMS [ IN schemaName ] |\n   TABLES [ IN schemaName ] |\n   VIEWS [ IN schemaName ] |\n\n}\n\n\n\n\nDescription\n\n\nDisplays information about active connections and database objects.\n\n\nSHOW CONNECTIONS\n\n\nIf there are no connections, the SHOW CONNECTIONS command returns \"No connections available\".\n\n\nOtherwise, the command displays a list of connection names and the URLs used to connect to them. The currently active connection is marked with an * after its name.\n\n\nExample\n\n\nsnappy(CLIENTCONNECTION)\n show connections;\nCLIENTCONNECTION* -     jdbc:snappydata://localhost:1527/\nPEERCLIENT -    jdbc:snappydata:\n* = current connection\nsnappy(CLIENTCONNECTION)\n\n\n\n\n\nSHOW IMPORTEDKEYS\n\n\nSHOW IMPORTEDKEYS displays all foreign keys in the specified schema or table. If you omit the schema and table clauses, SnappyData displays all foreign keys for all tables in the current schema.\n\n\nExample\n\n\nsnappy\n show importedkeys;\nKTABLE_SCHEM |PKTABLE_NAME|PKCOLUMN_NAME   |PK_NAME     |FKTABLE_SCHEM|FKTABLE_NAME      |FKCOLUMN_NAME   |FK_NAME     |KEY_SEQ\n-------------------------------------------------------------------------------------------------------------------------------\nAPP          |COUNTRIES   |COUNTRY_ISO_CODE|COUNTRIES_PK|APP          |CITIES            |COUNTRY_ISO_CODE|COUNTRIES_FK|1      \nAPP          |FLIGHTS     |FLIGHT_ID       |FLIGHTS_PK  |APP          |FLIGHTAVAILABILITY|FLIGHT_ID       |FLIGHTS_FK2 |1      \nAPP          |FLIGHTS     |SEGMENT_NUMBER  |FLIGHTS_PK  |APP          |FLIGHTAVAILABILITY|SEGMENT_NUMBER  |FLIGHTS_FK2 |2      \n\n3 rows selected\nsnappy\n show importedkeys from flightavailability;\nPKTABLE_NAME|PKCOLUMN_NAME |PK_NAME   |FKTABLE_SCHEM|FKTABLE_NAME      |FKCOLUMN_NAME |FK_NAME    |KEY_SEQ\n----------------------------------------------------------------------------------------------------------\nFLIGHTS     |FLIGHT_ID     |FLIGHTS_PK|APP          |FLIGHTAVAILABILITY|FLIGHT_ID     |FLIGHTS_FK2|1      \nFLIGHTS     |SEGMENT_NUMBER|FLIGHTS_PK|APP          |FLIGHTAVAILABILITY|SEGMENT_NUMBER|FLIGHTS_FK2|2      \n\n2 rows selected\n\n\n\n\nSHOW INDEXES\n\n\nSHOW INDEXES displays all the indexes in the database.\n\n\nIf \nIN schemaName\n is specified, only the indexes in the specified schema are displayed.\n\n\nIf \nFROM table-Name\n is specified, only the indexes on the specified table are displayed.\n\n\nExample\n\n\nsnappy\n show indexes in app;\nTABLE_NAME          |COLUMN_NAME         |NON_U\n|TYPE|ASC\n|CARDINA\n|PAGES\n----------------------------------------------------------------------------\nAIRLINES            |AIRLINE             |false |3   |A   |NULL    |NULL\nCITIES              |CITY_ID             |false |3   |A   |NULL    |NULL\nCITIES              |COUNTRY_ISO_CODE    |true  |3   |A   |NULL    |NULL\nCOUNTRIES           |COUNTRY_ISO_CODE    |false |3   |A   |NULL    |NULL\nCOUNTRIES           |COUNTRY             |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_DATE         |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_ID           |true  |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |SEGMENT_NUMBER      |true  |3   |A   |NULL    |NULL\nFLIGHTS             |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTS             |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTS             |DEST_AIRPORT        |true  |3   |A   |NULL    |NULL\nFLIGHTS             |ORIG_AIRPORT        |true  |3   |A   |NULL    |NULL\nMAPS                |MAP_ID              |false |3   |A   |NULL    |NULL\nMAPS                |MAP_NAME            |false |3   |A   |NULL    |NULL\n\n16 rows selected\nsnappy\n show indexes from flights;\nTABLE_NAME          |COLUMN_NAME         |NON_U\n|TYPE|ASC\n|CARDINA\n|PAGES\n----------------------------------------------------------------------------\nFLIGHTS             |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTS             |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTS             |DEST_AIRPORT        |true  |3   |A   |NULL    |NULL\nFLIGHTS             |ORIG_AIRPORT        |true  |3   |A   |NULL    |NULL\n\n4 rows selected\nsnappy\n\n\n\n\n\nSHOW PROCEDURES\n\n\nSHOW PROCEDURES displays all the procedures in the database that have been created with the CREATE PROCEDURE statement, as well as system procedures.\n\n\nIf \nIN schemaName\n is specified, only procedures in the specified schema are displayed.\n\n\nExample\n\n\nsnappy\n show procedures in syscs_util;\nPROCEDURE_SCHEM     |PROCEDURE_NAME                |REMARKS\n------------------------------------------------------------------------\nSYSCS_UTIL          |BACKUP_DATABASE               |com.pivotal.snappydata.\n\nSYSCS_UTIL          |BACKUP_DATABASE_AND_ENABLE_LO\n|com.pivotal.snappydata.\n\nSYSCS_UTIL          |BACKUP_DATABASE_AND_ENABLE_LO\n|com.pivotal.snappydata.\n\nSYSCS_UTIL          |BACKUP_DATABASE_NOWAIT        |com.pivotal.snappydata.\n\nSYSCS_UTIL          |BULK_INSERT                   |com.pivotal.snappydata.\n\nSYSCS_UTIL          |CHECKPOINT_DATABASE           |com.pivotal.snappydata.\n\nSYSCS_UTIL          |COMPRESS_TABLE                |com.pivotal.snappydata.\n\nSYSCS_UTIL          |DISABLE_LOG_ARCHIVE_MODE      |com.pivotal.snappydata.\n\nSYSCS_UTIL          |EMPTY_STATEMENT_CACHE         |com.pivotal.snappydata.\n\nSYSCS_UTIL          |EXPORT_QUERY                  |com.pivotal.snappydata.\n\nSYSCS_UTIL          |EXPORT_QUERY_LOBS_TO_EXTFILE  |com.pivotal.snappydata.\n\nSYSCS_UTIL          |EXPORT_TABLE                  |com.pivotal.snappydata.\n\nSYSCS_UTIL          |EXPORT_TABLE_LOBS_TO_EXTFILE  |com.pivotal.snappydata.\n\nSYSCS_UTIL          |FREEZE_DATABASE               |com.pivotal.snappydata.\n\nSYSCS_UTIL          |IMPORT_DATA                   |com.pivotal.snappydata.\n\nSYSCS_UTIL          |IMPORT_DATA_LOBS_FROM_EXTFILE |com.pivotal.snappydata.\n\nSYSCS_UTIL          |IMPORT_TABLE                  |com.pivotal.snappydata.\n\nSYSCS_UTIL          |IMPORT_TABLE_LOBS_FROM_EXTFILE|com.pivotal.snappydata.\n\nSYSCS_UTIL          |INPLACE_COMPRESS_TABLE        |com.pivotal.snappydata.\n\nSYSCS_UTIL          |RELOAD_SECURITY_POLICY        |com.pivotal.snappydata.\n\nSYSCS_UTIL          |SET_DATABASE_PROPERTY         |com.pivotal.snappydata.\n\nSYSCS_UTIL          |SET_EXPLAIN_CONNECTION        |com.pivotal.snappydata.\n\nSYSCS_UTIL          |SET_STATISTICS_TIMING         |com.pivotal.snappydata.\n\nSYSCS_UTIL          |SET_USER_ACCESS               |com.pivotal.snappydata.\n\nSYSCS_UTIL          |UNFREEZE_DATABASE             |com.pivotal.snappydata.\n\n\n27 rows selected\nsnappy\n         \n\n\n\n\nSHOW SCHEMAS\n\n\nSHOW SCHEMAS displays all of the schemas in the current connection.\n\n\nExample\n\n\nsnappy\n create schema sample;\n0 rows inserted/updated/deleted\nsnappy\n show schemas;\nTABLE_SCHEM\n------------------------------\nAPP\nNULLID\nSAMPLE\nSQLJ\nSYS\nSYSCAT\nSYSCS_DIAG\nSYSCS_UTIL\nSYSFUN\nSYSIBM\nSYSPROC\nSYSSTAT\n\n12 rows selected\n\n\n\n\nSHOW SYNONYMS\n\n\nSHOW SYNONYMS displays all synonyms in the database that have been created with the CREATE SYNONYMS statement.\n\n\nIf \nIN schemaName\n is specified, only synonyms in the specified schema are displayed.\n\n\nExample\n\n\nsnappy\n SHOW SYNONYMS;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\n\n0 rows selected\nsnappy\n CREATE SYNONYM myairline FOR airlines;\n0 rows inserted/updated/deleted\nsnappy\n SHOW SYNONYMS;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\nAPP                 |MYAIRLINE                     |\n\n1 row selected\nsnappy\n\n\n\n\n\nSHOW TABLES\n\n\nSHOW TABLES displays all of the tables in the current schema.\n\n\nIf \nIN schemaName\n is specified, the tables in the given schema are displayed.\n\n\nExample\n\n\nsnappy\n show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\nAPP                 |AIRLINES                      |\nAPP                 |CITIES                        |\nAPP                 |COUNTRIES                     |\nAPP                 |FLIGHTAVAILABILITY            |\nAPP                 |FLIGHTS                       |\nAPP                 |FLIGHTS_HISTORY               |\nAPP                 |MAPS                          |\n\n7 rows selected\nsnappy\n\n\n\n\n\nSHOW VIEWS\n\n\nSHOW VIEWS displays all of the views in the current schema.\n\n\nIf \nIN schemaName\n is specified, the views in the given schema are displayed.\n\n\nExample\n\n\nsnappy\n create view v1 as select * from maps;\n0 rows inserted/updated/deleted\nsnappy\n show views;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\nAPP                 |V1                            |\n\n1 row selected", 
            "title": "show"
        }, 
        {
            "location": "/reference/interactive_commands/show/#show", 
            "text": "Displays information about active connections and database objects.", 
            "title": "show"
        }, 
        {
            "location": "/reference/interactive_commands/show/#syntax", 
            "text": "SHOW\n{\n   CONNECTIONS |\n   IMPORTEDKEYS [ IN schemaName | FROM table-Name ] |\n   INDEXES [ IN schemaName | FROM table-Name ] |\n   PROCEDURES [ IN schemaName ] |\n   SCHEMAS |\n   SYNONYMS [ IN schemaName ] |\n   TABLES [ IN schemaName ] |\n   VIEWS [ IN schemaName ] |\n\n}", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/show/#description", 
            "text": "Displays information about active connections and database objects.  SHOW CONNECTIONS  If there are no connections, the SHOW CONNECTIONS command returns \"No connections available\".  Otherwise, the command displays a list of connection names and the URLs used to connect to them. The currently active connection is marked with an * after its name.  Example  snappy(CLIENTCONNECTION)  show connections;\nCLIENTCONNECTION* -     jdbc:snappydata://localhost:1527/\nPEERCLIENT -    jdbc:snappydata:\n* = current connection\nsnappy(CLIENTCONNECTION)   SHOW IMPORTEDKEYS  SHOW IMPORTEDKEYS displays all foreign keys in the specified schema or table. If you omit the schema and table clauses, SnappyData displays all foreign keys for all tables in the current schema.  Example  snappy  show importedkeys;\nKTABLE_SCHEM |PKTABLE_NAME|PKCOLUMN_NAME   |PK_NAME     |FKTABLE_SCHEM|FKTABLE_NAME      |FKCOLUMN_NAME   |FK_NAME     |KEY_SEQ\n-------------------------------------------------------------------------------------------------------------------------------\nAPP          |COUNTRIES   |COUNTRY_ISO_CODE|COUNTRIES_PK|APP          |CITIES            |COUNTRY_ISO_CODE|COUNTRIES_FK|1      \nAPP          |FLIGHTS     |FLIGHT_ID       |FLIGHTS_PK  |APP          |FLIGHTAVAILABILITY|FLIGHT_ID       |FLIGHTS_FK2 |1      \nAPP          |FLIGHTS     |SEGMENT_NUMBER  |FLIGHTS_PK  |APP          |FLIGHTAVAILABILITY|SEGMENT_NUMBER  |FLIGHTS_FK2 |2      \n\n3 rows selected\nsnappy  show importedkeys from flightavailability;\nPKTABLE_NAME|PKCOLUMN_NAME |PK_NAME   |FKTABLE_SCHEM|FKTABLE_NAME      |FKCOLUMN_NAME |FK_NAME    |KEY_SEQ\n----------------------------------------------------------------------------------------------------------\nFLIGHTS     |FLIGHT_ID     |FLIGHTS_PK|APP          |FLIGHTAVAILABILITY|FLIGHT_ID     |FLIGHTS_FK2|1      \nFLIGHTS     |SEGMENT_NUMBER|FLIGHTS_PK|APP          |FLIGHTAVAILABILITY|SEGMENT_NUMBER|FLIGHTS_FK2|2      \n\n2 rows selected  SHOW INDEXES  SHOW INDEXES displays all the indexes in the database.  If  IN schemaName  is specified, only the indexes in the specified schema are displayed.  If  FROM table-Name  is specified, only the indexes on the specified table are displayed.  Example  snappy  show indexes in app;\nTABLE_NAME          |COLUMN_NAME         |NON_U |TYPE|ASC |CARDINA |PAGES\n----------------------------------------------------------------------------\nAIRLINES            |AIRLINE             |false |3   |A   |NULL    |NULL\nCITIES              |CITY_ID             |false |3   |A   |NULL    |NULL\nCITIES              |COUNTRY_ISO_CODE    |true  |3   |A   |NULL    |NULL\nCOUNTRIES           |COUNTRY_ISO_CODE    |false |3   |A   |NULL    |NULL\nCOUNTRIES           |COUNTRY             |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_DATE         |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_ID           |true  |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |SEGMENT_NUMBER      |true  |3   |A   |NULL    |NULL\nFLIGHTS             |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTS             |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTS             |DEST_AIRPORT        |true  |3   |A   |NULL    |NULL\nFLIGHTS             |ORIG_AIRPORT        |true  |3   |A   |NULL    |NULL\nMAPS                |MAP_ID              |false |3   |A   |NULL    |NULL\nMAPS                |MAP_NAME            |false |3   |A   |NULL    |NULL\n\n16 rows selected\nsnappy  show indexes from flights;\nTABLE_NAME          |COLUMN_NAME         |NON_U |TYPE|ASC |CARDINA |PAGES\n----------------------------------------------------------------------------\nFLIGHTS             |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTS             |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTS             |DEST_AIRPORT        |true  |3   |A   |NULL    |NULL\nFLIGHTS             |ORIG_AIRPORT        |true  |3   |A   |NULL    |NULL\n\n4 rows selected\nsnappy   SHOW PROCEDURES  SHOW PROCEDURES displays all the procedures in the database that have been created with the CREATE PROCEDURE statement, as well as system procedures.  If  IN schemaName  is specified, only procedures in the specified schema are displayed.  Example  snappy  show procedures in syscs_util;\nPROCEDURE_SCHEM     |PROCEDURE_NAME                |REMARKS\n------------------------------------------------------------------------\nSYSCS_UTIL          |BACKUP_DATABASE               |com.pivotal.snappydata. \nSYSCS_UTIL          |BACKUP_DATABASE_AND_ENABLE_LO |com.pivotal.snappydata. \nSYSCS_UTIL          |BACKUP_DATABASE_AND_ENABLE_LO |com.pivotal.snappydata. \nSYSCS_UTIL          |BACKUP_DATABASE_NOWAIT        |com.pivotal.snappydata. \nSYSCS_UTIL          |BULK_INSERT                   |com.pivotal.snappydata. \nSYSCS_UTIL          |CHECKPOINT_DATABASE           |com.pivotal.snappydata. \nSYSCS_UTIL          |COMPRESS_TABLE                |com.pivotal.snappydata. \nSYSCS_UTIL          |DISABLE_LOG_ARCHIVE_MODE      |com.pivotal.snappydata. \nSYSCS_UTIL          |EMPTY_STATEMENT_CACHE         |com.pivotal.snappydata. \nSYSCS_UTIL          |EXPORT_QUERY                  |com.pivotal.snappydata. \nSYSCS_UTIL          |EXPORT_QUERY_LOBS_TO_EXTFILE  |com.pivotal.snappydata. \nSYSCS_UTIL          |EXPORT_TABLE                  |com.pivotal.snappydata. \nSYSCS_UTIL          |EXPORT_TABLE_LOBS_TO_EXTFILE  |com.pivotal.snappydata. \nSYSCS_UTIL          |FREEZE_DATABASE               |com.pivotal.snappydata. \nSYSCS_UTIL          |IMPORT_DATA                   |com.pivotal.snappydata. \nSYSCS_UTIL          |IMPORT_DATA_LOBS_FROM_EXTFILE |com.pivotal.snappydata. \nSYSCS_UTIL          |IMPORT_TABLE                  |com.pivotal.snappydata. \nSYSCS_UTIL          |IMPORT_TABLE_LOBS_FROM_EXTFILE|com.pivotal.snappydata. \nSYSCS_UTIL          |INPLACE_COMPRESS_TABLE        |com.pivotal.snappydata. \nSYSCS_UTIL          |RELOAD_SECURITY_POLICY        |com.pivotal.snappydata. \nSYSCS_UTIL          |SET_DATABASE_PROPERTY         |com.pivotal.snappydata. \nSYSCS_UTIL          |SET_EXPLAIN_CONNECTION        |com.pivotal.snappydata. \nSYSCS_UTIL          |SET_STATISTICS_TIMING         |com.pivotal.snappydata. \nSYSCS_UTIL          |SET_USER_ACCESS               |com.pivotal.snappydata. \nSYSCS_UTIL          |UNFREEZE_DATABASE             |com.pivotal.snappydata. \n\n27 rows selected\nsnappy            SHOW SCHEMAS  SHOW SCHEMAS displays all of the schemas in the current connection.  Example  snappy  create schema sample;\n0 rows inserted/updated/deleted\nsnappy  show schemas;\nTABLE_SCHEM\n------------------------------\nAPP\nNULLID\nSAMPLE\nSQLJ\nSYS\nSYSCAT\nSYSCS_DIAG\nSYSCS_UTIL\nSYSFUN\nSYSIBM\nSYSPROC\nSYSSTAT\n\n12 rows selected  SHOW SYNONYMS  SHOW SYNONYMS displays all synonyms in the database that have been created with the CREATE SYNONYMS statement.  If  IN schemaName  is specified, only synonyms in the specified schema are displayed.  Example  snappy  SHOW SYNONYMS;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\n\n0 rows selected\nsnappy  CREATE SYNONYM myairline FOR airlines;\n0 rows inserted/updated/deleted\nsnappy  SHOW SYNONYMS;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\nAPP                 |MYAIRLINE                     |\n\n1 row selected\nsnappy   SHOW TABLES  SHOW TABLES displays all of the tables in the current schema.  If  IN schemaName  is specified, the tables in the given schema are displayed.  Example  snappy  show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\nAPP                 |AIRLINES                      |\nAPP                 |CITIES                        |\nAPP                 |COUNTRIES                     |\nAPP                 |FLIGHTAVAILABILITY            |\nAPP                 |FLIGHTS                       |\nAPP                 |FLIGHTS_HISTORY               |\nAPP                 |MAPS                          |\n\n7 rows selected\nsnappy   SHOW VIEWS  SHOW VIEWS displays all of the views in the current schema.  If  IN schemaName  is specified, the views in the given schema are displayed.  Example  snappy  create view v1 as select * from maps;\n0 rows inserted/updated/deleted\nsnappy  show views;\nTABLE_SCHEM         |TABLE_NAME                    |REMARKS\n------------------------------------------------------------------------\nAPP                 |V1                            |\n\n1 row selected", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/", 
            "text": "Configuration Properties\n\n\nYou use JDBC connection properties, connection boot properties, and Java system properties to configure SnappyData members and connections.\n\n\n\n\n\n\nProperty Types\n\n\n\n\n\n\nProperty Prefixes\n\n\n\n\n\n\nUsing Non-ASCII Strings in SnappyData Property Files\n\n\n\n\n\n\nList of Property Names\n\n\n\n\n\n\n\n\nProperty Types\n\n\nSnappyData configuration properties are divided into the following property types:\n\n\n\n\n\n\nConnection properties\n. Connection properties configure the features of a SnappyData member or a SnappyData client connection when you start or connect to a SnappyData member. You can define connection properties directly in the JDBC connection URL (or in the \"connect\" command in an interactive SnappyData session). You can also define connection properties in the \ngemfirexd.properties\n file or as Java system properties. For example, use -J-D\nproperty_name\n=\nproperty_value\n when you execute a \nsnappy\n utility. Or, use the JAVA_ARGS environment variable to define a Java system property for an interactive \nsnappy\n session (JAVA_ARGS=\"-D\nproperty_name\n=\nproperty_value\n\"). \n\n\n\n\nNote\n\n\nYou must add a prefix to certain connection property names in order to specify those properties as Java system properties. See \nProperty Prefixes\n.\n\n\n\n\nConnection properties can be further categorized as either \nboot properties\n or \nclient properties\n:\n\n\n\n\n\n\nBoot properties\n. A boot connection property configures features of a SnappyData member, and can only be applied along with the first connection that starts a SnappyData member. Boot properties have no effect when they are specified on connections to a member after the member has started. Boot properties have no effect when they are specified on a thin client connection.\n\n\n\n\n\n\nClient properties\n. A client connection property configures features of the client connection itself and can be used with the JDBC thin client drive (for example, using a JDBC thin client connection URL or the \nconnect client\n command from an interactive \nsnappy\n session).\n\n\n\n\n\n\n\n\n\n\nSystem properties\n. Certain SnappyData configuration properties \nmust\n be specified either as Java system properties (using -J-D\nproperty_name\n=\nproperty_value\n with a \nsnappy\n utility or setting JAVA_ARGS=\"-D\nproperty_name\n=\nproperty_value\n\" for an interactive \nsnappy\n session). You cannot define these properties in a JDBC URL connection. Many of SnappyData system properties affect features of the SnappyData member at boot time and can be optionally defined in the gemfirexd.properties file. See the property description to determine whether or not a system property can be defined in gemfirexd.properties.\n\n\nThe names of SnappyData system properties always include the \nsnappydata.\n prefix. For example, all properties that configure LDAP server information for user authentication must be specified as Java system properties, rather than JDBC properties, when you boot a server.\n\n\n\n\n\n\nCertain properties have additional behaviors or restrictions. See the individual property descriptions for more information.\n\n\n\n\nProperty Prefixes\n\n\nYou must add a prefix to connection and boot property names when you define those properties as Java system properties. The \nPrefix\n row in each property table lists a prefix value (\nsnappydata.\n or \ngemfire.\n) when one is required. Do not use an indicated prefix when you specify the property in a connection string.\n\n\nIf no prefix is specified, use only the indicated property name in all circumstances. For example, use \"host-data\" whether you define this property in gemfirexd.properties, as a Java system property, or as a property definition for FabricServer.\n\n\n\n\nUsing Non-ASCII Strings in SnappyData Property Files\n\n\nYou can specify Unicode (non-ASCII) characters in SnappyData property files by using a \n\\uXXXX\n escape sequence. For a supplementary character, you need two escape sequences, one for each of the two UTF-16 code units. The XXXX denotes the 4 hexadecimal digits for the value of the UTF-16 code unit. For example, a properties file might have the following entries:\n\n\ns1=hello there\ns2=\\u3053\\u3093\\u306b\\u3061\\u306f\n\n\n\n\nFor example, in \ngemfirexd.properties\n, you might write:\n\n\nlog-file=my\\u00df.log\n\n\n\n\nto indicate the desired property definition of \nlog-file=my.log\n.\n\n\nIf you have edited and saved the file in a non-ASCII encoding, you can convert it to ASCII with the \nnative2ascii\n tool included in your Oracle Java distribution. For example, you might want to do this when editing a properties file in Shift_JIS, a popular Japanese encoding.\n\n\n\n\nList of Property Names\n\n\nBelow is the list of all the configuration properties and links for each property reference page.\n\n\n\n\n\n\nack-severe-alert-threshold\n\n\n\n\n\n\nack-wait-threshold\n\n\n\n\n\n\narchive-disk-space-limit\n\n\n\n\n\n\narchive-file-size-limit\n\n\n\n\n\n\nbind-address\n\n\n\n\n\n\nenable-network-partition-detection\n\n\n\n\n\n\nenable-stats\n\n\n\n\n\n\nenable-time-statistics\n\n\n\n\n\n\nenforce-unique-host\n\n\n\n\n\n\ninit-scripts\n\n\n\n\n\n\nlocators\n\n\n\n\n\n\nlog-file\n\n\n\n\n\n\nlog-level\n\n\n\n\n\n\nmember-timeout\n\n\n\n\n\n\nredundancy-zone\n\n\n\n\n\n\nstart-locator\n\n\n\n\n\n\nstatistic-archive-file\n\n\n\n\n\n\nstatistic-sample-rate\n\n\n\n\n\n\nstatistic-sampling-enabled\n\n\n\n\n\n\nsys-disk-dir\n\n\n\n\n\n\nuser", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#configuration-properties", 
            "text": "You use JDBC connection properties, connection boot properties, and Java system properties to configure SnappyData members and connections.    Property Types    Property Prefixes    Using Non-ASCII Strings in SnappyData Property Files    List of Property Names", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#property-types", 
            "text": "SnappyData configuration properties are divided into the following property types:    Connection properties . Connection properties configure the features of a SnappyData member or a SnappyData client connection when you start or connect to a SnappyData member. You can define connection properties directly in the JDBC connection URL (or in the \"connect\" command in an interactive SnappyData session). You can also define connection properties in the  gemfirexd.properties  file or as Java system properties. For example, use -J-D property_name = property_value  when you execute a  snappy  utility. Or, use the JAVA_ARGS environment variable to define a Java system property for an interactive  snappy  session (JAVA_ARGS=\"-D property_name = property_value \").    Note  You must add a prefix to certain connection property names in order to specify those properties as Java system properties. See  Property Prefixes .   Connection properties can be further categorized as either  boot properties  or  client properties :    Boot properties . A boot connection property configures features of a SnappyData member, and can only be applied along with the first connection that starts a SnappyData member. Boot properties have no effect when they are specified on connections to a member after the member has started. Boot properties have no effect when they are specified on a thin client connection.    Client properties . A client connection property configures features of the client connection itself and can be used with the JDBC thin client drive (for example, using a JDBC thin client connection URL or the  connect client  command from an interactive  snappy  session).      System properties . Certain SnappyData configuration properties  must  be specified either as Java system properties (using -J-D property_name = property_value  with a  snappy  utility or setting JAVA_ARGS=\"-D property_name = property_value \" for an interactive  snappy  session). You cannot define these properties in a JDBC URL connection. Many of SnappyData system properties affect features of the SnappyData member at boot time and can be optionally defined in the gemfirexd.properties file. See the property description to determine whether or not a system property can be defined in gemfirexd.properties.  The names of SnappyData system properties always include the  snappydata.  prefix. For example, all properties that configure LDAP server information for user authentication must be specified as Java system properties, rather than JDBC properties, when you boot a server.    Certain properties have additional behaviors or restrictions. See the individual property descriptions for more information.", 
            "title": "Property Types"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#property-prefixes", 
            "text": "You must add a prefix to connection and boot property names when you define those properties as Java system properties. The  Prefix  row in each property table lists a prefix value ( snappydata.  or  gemfire. ) when one is required. Do not use an indicated prefix when you specify the property in a connection string.  If no prefix is specified, use only the indicated property name in all circumstances. For example, use \"host-data\" whether you define this property in gemfirexd.properties, as a Java system property, or as a property definition for FabricServer.", 
            "title": "Property Prefixes"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#using-non-ascii-strings-in-snappydata-property-files", 
            "text": "You can specify Unicode (non-ASCII) characters in SnappyData property files by using a  \\uXXXX  escape sequence. For a supplementary character, you need two escape sequences, one for each of the two UTF-16 code units. The XXXX denotes the 4 hexadecimal digits for the value of the UTF-16 code unit. For example, a properties file might have the following entries:  s1=hello there\ns2=\\u3053\\u3093\\u306b\\u3061\\u306f  For example, in  gemfirexd.properties , you might write:  log-file=my\\u00df.log  to indicate the desired property definition of  log-file=my.log .  If you have edited and saved the file in a non-ASCII encoding, you can convert it to ASCII with the  native2ascii  tool included in your Oracle Java distribution. For example, you might want to do this when editing a properties file in Shift_JIS, a popular Japanese encoding.", 
            "title": "Using Non-ASCII Strings in SnappyData Property Files"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#list-of-property-names", 
            "text": "Below is the list of all the configuration properties and links for each property reference page.    ack-severe-alert-threshold    ack-wait-threshold    archive-disk-space-limit    archive-file-size-limit    bind-address    enable-network-partition-detection    enable-stats    enable-time-statistics    enforce-unique-host    init-scripts    locators    log-file    log-level    member-timeout    redundancy-zone    start-locator    statistic-archive-file    statistic-sample-rate    statistic-sampling-enabled    sys-disk-dir    user", 
            "title": "List of Property Names"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/", 
            "text": "ack-severe-alert-threshold\n\n\nDescription\n\n\nThe number of seconds the distributed system waits after the \nack-wait-threshold\n for a message to be acknowledged before it issues an alert at a severe level. A value of zero disables this feature.\n\n\nDefault Value\n\n\n0 (disabled)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "ack-severe-alert-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#ack-severe-alert-threshold", 
            "text": "", 
            "title": "ack-severe-alert-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#description", 
            "text": "The number of seconds the distributed system waits after the  ack-wait-threshold  for a message to be acknowledged before it issues an alert at a severe level. A value of zero disables this feature.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#default-value", 
            "text": "0 (disabled)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/", 
            "text": "ack-wait-threshold\n\n\nDescription\n\n\nThe number of seconds a distributed message waits for an acknowledgment before it sends an alert to signal that something might be wrong with the system member that is unresponsive. After sending this alert the waiter continues to wait. The alerts are logged in the system members log as warnings.\n\n\nValid values are in the range 0...2147483647\n\n\nDefault Value\n\n\n15\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "ack-wait-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#ack-wait-threshold", 
            "text": "", 
            "title": "ack-wait-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#description", 
            "text": "The number of seconds a distributed message waits for an acknowledgment before it sends an alert to signal that something might be wrong with the system member that is unresponsive. After sending this alert the waiter continues to wait. The alerts are logged in the system members log as warnings.  Valid values are in the range 0...2147483647", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#default-value", 
            "text": "15", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/", 
            "text": "archive-disk-space-limit\n\n\nDescription\n\n\nThe maximum size in megabytes of all inactive statistic archive files combined. If this limit is exceeded, inactive archive files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited.\n\n\nDefault Value\n\n\n0 (unlimited)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "archive-disk-space-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#archive-disk-space-limit", 
            "text": "", 
            "title": "archive-disk-space-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#description", 
            "text": "The maximum size in megabytes of all inactive statistic archive files combined. If this limit is exceeded, inactive archive files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#default-value", 
            "text": "0 (unlimited)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/", 
            "text": "archive-file-size-limit\n\n\nDescription\n\n\nThe maximum size in megabytes of a single statistic archive file. Once this limit is exceeded, a new statistic archive file is created, and the current archive file becomes inactive. If set to zero, the file size is unlimited.\n\n\nDefault Value\n\n\n0 (unlimited)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "archive-file-size-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#archive-file-size-limit", 
            "text": "", 
            "title": "archive-file-size-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#description", 
            "text": "The maximum size in megabytes of a single statistic archive file. Once this limit is exceeded, a new statistic archive file is created, and the current archive file becomes inactive. If set to zero, the file size is unlimited.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#default-value", 
            "text": "0 (unlimited)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/", 
            "text": "bind-address\n\n\nDescription\n\n\n\n\nNote\n\n\nThis setting is relevant only for multi-homed hosts (machines with multiple network interface cards). \n\n\n\n\nAdapter card the cache binds to for peer-to-peer communication. It also specifies the default location for SnappyData servers to listen on, which is used unless overridden by the \nserver-bind-address\n.\n\n\nSpecify the IP address, not the hostname, because each network card may not have a unique hostname. An empty string (the default) causes the member to listen on the default card for the machine.\n\n\nThis attribute is a machine-wide attribute used for system member and client/server communication. It has no effect on the locator location unless the locator is embedded in a member process.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "bind-address"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#bind-address", 
            "text": "", 
            "title": "bind-address"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#description", 
            "text": "Note  This setting is relevant only for multi-homed hosts (machines with multiple network interface cards).    Adapter card the cache binds to for peer-to-peer communication. It also specifies the default location for SnappyData servers to listen on, which is used unless overridden by the  server-bind-address .  Specify the IP address, not the hostname, because each network card may not have a unique hostname. An empty string (the default) causes the member to listen on the default card for the machine.  This attribute is a machine-wide attribute used for system member and client/server communication. It has no effect on the locator location unless the locator is embedded in a member process.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/", 
            "text": "enable-network-partition-detection\n\n\nDescription\n\n\nBoolean instructing the system to detect and handle splits in the distributed system, typically caused by a partitioning of the network (split brain) where the distributed system is running. \n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enable-network-partition-detection"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#enable-network-partition-detection", 
            "text": "", 
            "title": "enable-network-partition-detection"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#description", 
            "text": "Boolean instructing the system to detect and handle splits in the distributed system, typically caused by a partitioning of the network (split brain) where the distributed system is running.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/", 
            "text": "enable-stats\n\n\nDescription\n\n\nThis property can only be used with a peer client connection; you cannot use it from a thin client.\n\n\nEnables statistics collection at the statement level for the associated connection. \n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nsnappydata.", 
            "title": "enable-stats"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#enable-stats", 
            "text": "", 
            "title": "enable-stats"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#description", 
            "text": "This property can only be used with a peer client connection; you cannot use it from a thin client.  Enables statistics collection at the statement level for the associated connection.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#prefix", 
            "text": "snappydata.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/", 
            "text": "enable-time-statistics\n\n\nDescription\n\n\nBoolean instructing the system to track time-based statistics for the distributed system. Disabled by default for performance.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enable-time-statistics"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#enable-time-statistics", 
            "text": "", 
            "title": "enable-time-statistics"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#description", 
            "text": "Boolean instructing the system to track time-based statistics for the distributed system. Disabled by default for performance.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/", 
            "text": "enforce-unique-host\n\n\nDescription\n\n\nDetermines whether SnappyData puts redundant copies of the same data in different members running on the same physical machine. By default, SnappyData tries to put redundant copies on different machines, but it puts them on the same machine if no other machines are available. \n\nSetting this property to \ntrue\n prevents this and requires different machines for redundant copies.\n\n\nUsage\n\n\nhost1 -gemfire.enforce-unique-host=true\nhost2\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enforce-unique-host"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#enforce-unique-host", 
            "text": "", 
            "title": "enforce-unique-host"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#description", 
            "text": "Determines whether SnappyData puts redundant copies of the same data in different members running on the same physical machine. By default, SnappyData tries to put redundant copies on different machines, but it puts them on the same machine if no other machines are available.  \nSetting this property to  true  prevents this and requires different machines for redundant copies.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#usage", 
            "text": "host1 -gemfire.enforce-unique-host=true\nhost2", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/", 
            "text": "init-scripts\n\n\nDescription\n\n\nOne or more SQL script files to execute after loading DDL from the data dictionary. Use a comma-separated list of files to supply multiple values.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nsnappydata.", 
            "title": "init-scripts"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#init-scripts", 
            "text": "", 
            "title": "init-scripts"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#description", 
            "text": "One or more SQL script files to execute after loading DDL from the data dictionary. Use a comma-separated list of files to supply multiple values.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#prefix", 
            "text": "snappydata.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/", 
            "text": "locators\n\n\nDescription\n\n\nList of locators used by system members. The list must be configured consistently for every member of the distributed system. If the list is empty, locators are not used.\n\n\nFor each locator, provide a host name and/or address (separated by '@', if you use both), followed by a port number in brackets. Examples:\n\n\nlocators=address1[port1],address2[port2]\n\n\n\n\nlocators=hostName1@address1[port1],name2@address2[port2]\n\n\n\n\nlocators=hostName1[port1],name2[port2]\n\n\n\n\n\n\nNote\n\n\nOn multi-homed hosts, this last notation uses the default address. If you use bind addresses for your locators, explicitly specify the addresses in the locator's list - do not use just the hostname. \n\n\n\n\nUsage\n\n\nTo start multiple locators in a cluster modify the following files: \n\n\n\n\n\n\nconf/locators\n\n\nlocalhost -peer-discovery-address=localhost -peer-discovery-port=3241 -locators=localhost:3242\nlocalhost -peer-discovery-address=localhost -peer-discovery-port=3242 -locators=localhost:3241\n\n\n\n\n\n\n\nconf/servers\n\n\nlocalhost -locators=localhost:3241,localhost:3242\n\n\n\n\n\n\n\nconf/leads\n\n\nlocalhost -locators=localhost:3241,localhost:3242\n\n\n\n\n\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "locators"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#locators", 
            "text": "", 
            "title": "locators"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#description", 
            "text": "List of locators used by system members. The list must be configured consistently for every member of the distributed system. If the list is empty, locators are not used.  For each locator, provide a host name and/or address (separated by '@', if you use both), followed by a port number in brackets. Examples:  locators=address1[port1],address2[port2]  locators=hostName1@address1[port1],name2@address2[port2]  locators=hostName1[port1],name2[port2]   Note  On multi-homed hosts, this last notation uses the default address. If you use bind addresses for your locators, explicitly specify the addresses in the locator's list - do not use just the hostname.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#usage", 
            "text": "To start multiple locators in a cluster modify the following files:     conf/locators  localhost -peer-discovery-address=localhost -peer-discovery-port=3241 -locators=localhost:3242\nlocalhost -peer-discovery-address=localhost -peer-discovery-port=3242 -locators=localhost:3241    conf/servers  localhost -locators=localhost:3241,localhost:3242    conf/leads  localhost -locators=localhost:3241,localhost:3242", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/", 
            "text": "log-file\n\n\nDescription\n\n\nFile to use for writing log messages. If this property is not set, the default is used.\n\n\nEach member type has its own default output:\n\n\n\n\nleader: \nsnappyleader.log\n\n\nlocator: \nsnappylocator.log\n\n\nserver: \nsnappyserver.log\n\n\n\n\nUse the snappydata. prefix for SnappyData members, or use the snappydata.client prefix for client-side logging.\n\n\nUsage\n\n\nlocalhost -log-file=/home/supriya/snappy/server/snappy-server.log\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nsnappydata. or snappydata.client.", 
            "title": "log-file"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#log-file", 
            "text": "", 
            "title": "log-file"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#description", 
            "text": "File to use for writing log messages. If this property is not set, the default is used.  Each member type has its own default output:   leader:  snappyleader.log  locator:  snappylocator.log  server:  snappyserver.log   Use the snappydata. prefix for SnappyData members, or use the snappydata.client prefix for client-side logging.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#usage", 
            "text": "localhost -log-file=/home/supriya/snappy/server/snappy-server.log", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#prefix", 
            "text": "snappydata. or snappydata.client.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/", 
            "text": "log-level\n\n\nDescription\n\n\nThe level of detail of the messages written to the system member's log. Setting log-level to one of the ordered levels causes all messages of that level and greater severity to be printed.\n\n\nValid values from lowest to highest are fine, config, info, warning, error, severe, and none.\n\n\nUsage\n\n\nlocalhost -log-level=fine\n\n\nDefault Value\n\n\nconfig\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "log-level"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#log-level", 
            "text": "", 
            "title": "log-level"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#description", 
            "text": "The level of detail of the messages written to the system member's log. Setting log-level to one of the ordered levels causes all messages of that level and greater severity to be printed.  Valid values from lowest to highest are fine, config, info, warning, error, severe, and none.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#usage", 
            "text": "localhost -log-level=fine", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#default-value", 
            "text": "config", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/", 
            "text": "member-timeout\n\n\nDescription\n\n\nInterval, in milliseconds, between two attempts to determine whether another system member is alive. When another member appears to be gone, SnappyData tries to contact it twice before quitting.\n\n\nValid values are in the range 1000..600000.\n\n\nDefault Value\n\n\n5000\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "member-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#member-timeout", 
            "text": "", 
            "title": "member-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#description", 
            "text": "Interval, in milliseconds, between two attempts to determine whether another system member is alive. When another member appears to be gone, SnappyData tries to contact it twice before quitting.  Valid values are in the range 1000..600000.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#default-value", 
            "text": "5000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/", 
            "text": "redundancy-zone\n\n\nDescription\n\n\nDefines this member's redundancy zone. Used to separate members into different groups for satisfying partitioned table redundancy. If this property is set, SnappyData does not put redundant copies of data in members with the same redundancy zone setting.\n\n\nFor example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. \n\n\nYou set one redundancy zone in the \nconf/server\n file for all members that run on one rack:\n\n\n-gemfire.redundancy-zone=rack1\n\n\n\n\nYou can also set another redundancy zone in the \nconf/server\n file for all members that run on another rack:\n\n\n-gemfire.redundancy-zone=rack2\n\n\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.\n\n\nExample\n\n\nlocalhost1 -gemfire.redundancy-zone=rack1\nlocalhost1 -gemfire.redundancy-zone=rack1\nlocalhost2 -gemfire.redundancy-zone=rack2\nlocalhost2 -gemfire.redundancy-zone=rack2", 
            "title": "redundancy-zone"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#redundancy-zone", 
            "text": "", 
            "title": "redundancy-zone"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#description", 
            "text": "Defines this member's redundancy zone. Used to separate members into different groups for satisfying partitioned table redundancy. If this property is set, SnappyData does not put redundant copies of data in members with the same redundancy zone setting.  For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack.   You set one redundancy zone in the  conf/server  file for all members that run on one rack:  -gemfire.redundancy-zone=rack1  You can also set another redundancy zone in the  conf/server  file for all members that run on another rack:  -gemfire.redundancy-zone=rack2", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#example", 
            "text": "localhost1 -gemfire.redundancy-zone=rack1\nlocalhost1 -gemfire.redundancy-zone=rack1\nlocalhost2 -gemfire.redundancy-zone=rack2\nlocalhost2 -gemfire.redundancy-zone=rack2", 
            "title": "Example"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/", 
            "text": "start-locator\n\n\nDescription\n\n\nIf set, automatically starts a locator in the current process when the member connects to the distributed system and stops the locator when the member disconnects.\n\n\nTo use, specify the locator with an optional address or host specification and a required port number, in one of these formats:\n\n\nstart-locator=address[port1] \n\n\n\n\nstart-locator=port1\n\n\n\n\n\n\n\n\nIf you only specify the port, the address assigned to the member is used for the locator.\n\n\n\n\n\n\nIf not already there, this locator is automatically added to the list of locators in this set of SnappyData properties.\n\n\n\n\n\n\n\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "start-locator"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#start-locator", 
            "text": "", 
            "title": "start-locator"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#description", 
            "text": "If set, automatically starts a locator in the current process when the member connects to the distributed system and stops the locator when the member disconnects.  To use, specify the locator with an optional address or host specification and a required port number, in one of these formats:  start-locator=address[port1]   start-locator=port1    If you only specify the port, the address assigned to the member is used for the locator.    If not already there, this locator is automatically added to the list of locators in this set of SnappyData properties.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/", 
            "text": "statistic-archive-file\n\n\nDescription\n\n\nThe file to which the running system member writes statistic samples. An empty string disables archiving. Adding .gz suffix to the file name causes it to be compressed. This property is commonly used with \narchive-disk-space-limit\n and \narchive-file-size-limit\n.\n\n\nDefault Value\n\n\nnot set (this disables statistic archiving)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "statistic-archive-file"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#statistic-archive-file", 
            "text": "", 
            "title": "statistic-archive-file"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#description", 
            "text": "The file to which the running system member writes statistic samples. An empty string disables archiving. Adding .gz suffix to the file name causes it to be compressed. This property is commonly used with  archive-disk-space-limit  and  archive-file-size-limit .", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#default-value", 
            "text": "not set (this disables statistic archiving)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/", 
            "text": "statistic-sample-rate\n\n\nDescription\n\n\nBoot property that specifies how often to sample statistics, in milliseconds.\n\n\nValid values are in the range 1000..60000.\n\n\n\n\nNote\n\n\nIf the value is set to less than 1000, the rate will be set to 1000 because the VSD tool does not support sub-second sampling.\n\n\n\n\nDefault Value\n\n\n1000\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "statistic-sample-rate"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#statistic-sample-rate", 
            "text": "", 
            "title": "statistic-sample-rate"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#description", 
            "text": "Boot property that specifies how often to sample statistics, in milliseconds.  Valid values are in the range 1000..60000.   Note  If the value is set to less than 1000, the rate will be set to 1000 because the VSD tool does not support sub-second sampling.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#default-value", 
            "text": "1000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/", 
            "text": "statistic-sampling-enabled\n\n\nDescription\n\n\nDetermines whether to collect and archive statistics on the member.\n\n\nTurning statistics sampling off saves on resources, but it also takes away potentially valuable information for ongoing system tuning and about unexpected system problems.\n\n\nDefault Value\n\n\ntrue\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "statistic-sampling-enabled"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#statistic-sampling-enabled", 
            "text": "", 
            "title": "statistic-sampling-enabled"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#description", 
            "text": "Determines whether to collect and archive statistics on the member.  Turning statistics sampling off saves on resources, but it also takes away potentially valuable information for ongoing system tuning and about unexpected system problems.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#default-value", 
            "text": "true", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/", 
            "text": "sys-disk-dir\n\n\nDescription\n\n\nSpecifies the base path of the default disk store. This directory also holds the data dictionary subdirectory, which stores the persistent data dictionary.\n\n\nOther SnappyData features also use this directory for storing files. For example, gateway queue overflow and overflow tables use this attribute by default. You can override \nsys-disk-dir\n for table overflow using options in a table's \nCREATE TABLE\n statement.\n\n\nUsage\n\n\n-spark.snappydata.store.sys-disk-dir=\n\n\nDefault Value\n\n\nThe SnappyData working directory.\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nsnappydata.", 
            "title": "sys-disk-dir"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#sys-disk-dir", 
            "text": "", 
            "title": "sys-disk-dir"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#description", 
            "text": "Specifies the base path of the default disk store. This directory also holds the data dictionary subdirectory, which stores the persistent data dictionary.  Other SnappyData features also use this directory for storing files. For example, gateway queue overflow and overflow tables use this attribute by default. You can override  sys-disk-dir  for table overflow using options in a table's  CREATE TABLE  statement.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#usage", 
            "text": "-spark.snappydata.store.sys-disk-dir=", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#default-value", 
            "text": "The SnappyData working directory.", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#prefix", 
            "text": "snappydata.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/user/", 
            "text": "user\n\n\nDescription\n\n\nThe user name for the member or connection. A valid username and password are required when user authentication is turned on.\n\n\nUse this attribute in conjunction with the \npassword\n attribute.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "user"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#user", 
            "text": "", 
            "title": "user"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#description", 
            "text": "The user name for the member or connection. A valid username and password are required when user authentication is turned on.  Use this attribute in conjunction with the  password  attribute.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/apidocsintro/", 
            "text": "API Documentation\n\n\nAPI reference for SnappyData can be found \nhere\n.", 
            "title": "API Documentation"
        }, 
        {
            "location": "/apidocsintro/#api-documentation", 
            "text": "API reference for SnappyData can be found  here .", 
            "title": "API Documentation"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/", 
            "text": "Setting Up SnappyData ODBC Driver and Tableau Desktop\n\n\n\n\nNote\n\n\nThis is currently tested and supported only on Windows 10 (32-Bit \n 64-Bit systems). Although versions other than Windows 10 may work, SnappyData is not claiming full testing on other versions.\n\n\n\n\nStep 1 and 2:\n\n\nEnsure that you do the following:\n\n\n\n\n\n\nInstall Visual C++ Redistributable for Visual Studio 2015\n\n\n\n\n\n\nInstall SnappyData ODBC Driver\n\n\n\n\n\n\nStep 3: Create SnappyData DSN from ODBC Data Sources 64-bit/32-bit\n\n\nTo create SnappyData DSN from ODBC Data Sources:\n\n\n\n\n\n\nOpen the \nODBC Data Source Administrator\n window:\n\n\na. On the \nStart\n page, type ODBC Data Sources, and select Set up ODBC data sources from the list or Select  \nODBC Data Sources\n in the \nAdministrative Tools\n. \n\n\nb. Based on your Windows installation, open \nODBC Data Sources (64-bit)\n or \nODBC Data Sources (32-bit)\n\n\n\n\n\n\nIn the\n ODBC Data Source Administrator\n window, select either the \nUser DSN\n or \nSystem DSN\n tab. \n\n\n\n\n\n\nClick \nAdd\n to view the list of installed ODBC Drivers on your machine.\n\n\n\n\n\n\nFrom the list of drivers, select \nSnappyData ODBC Driver\n and click \nFinish\n.\n\n\n\n\n\n\nThe \nSnappyData ODBC Configuration Dialog\n is displayed. Enter the following details to create DSN:\n\n\n\n\n\n\nData Source Name\n: Name of the Data Source. For example, snappydsn.  \n\n\n\n\n\n\nServer (Hostname or IP)\n: IP address of the data server which is running in the SnappyData cluster.\n\n\n\n\n\n\nPort\n: Port number of the server. By default, it is \n1528\n for the first data server in the cluster.\n\n\n\n\n\n\nLogin ID\n: The login ID required to connect to the server. Fir example, \napp\n\n\n\n\n\n\nPassword\n: The password required to connect to the server. For example, \napp\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nEnsure that you provide the IP Address/Host Name and Port number of the data server. If you provide the details of the locator, the connection fails. \n\n\n\n\nStep 4. Install Tableau Desktop (10.1 or Higher)\n\n\nTo install Tableau desktop:\n\n\n\n\n\n\nDownload Tableau Desktop\n.\n\n\n\n\n\n\nDepending on your Windows installation, download the 32-bit or 64-bit version of the installer. \n\n\n\n\n\n\nFollow the steps to complete the installation.\n\n\n\n\n\n\nStep 5. Connecting Tableau Desktop to SnappyData Server\n\n\nBefore using Tableau with SnappyData ODBC Driver for the first time, you must add the \nodbc-snappydata.tdc\n configuration file in the Tableau data sources directory. Follow below steps to do so.\n\n\nTo connect the Tableau Desktop to SnappyData Server:\n\n\n\n\n\n\nDownload \nodbc-snappydata.tdc\n configuration file from the \nSnappyData Release page\n.\n\n\n\n\n\n\nCopy the downloaded odbc-snappydata.tdc file to  the \nUser_Home_Path\n\\Documents\\My Tableau Repository\\Datasources directory.\n\n\n\n\n\n\nOpen the Tableau Desktop application\n\n\n\n\n\n\nOn the start page, under \nConnect\n \n \nTo a Server\n, and then click, \nOther Databases (ODBC)\n.\nThe \nOther Databases (ODBC)\n window is displayed. \n\n\n\n\n\n\nIn the \nConnect Using \n DSN\n drop-down, select the data source name provided earlier. (eg. snappydsn).\n\n\n\n\n\n\nClick \nConnect\n.\n\n\n\n\n\n\nWhen a connection to SnappyData Server is established, the \nSign In\n option is enabled. \n\n\n\n\n\n\nClick \nSign In\n to log into Tableau.\n\n\n\n\n\n\nFrom the \nSchema\n drop-down list, select a schema.\n\n\n\n\n\n\nAll Tables from the selected schema is listed.\n\n\n\n\n\n\nSelect the required table(s) and drag it to the canvas. A view generated using the selected tables is displayed. \nIf you make changes to the table, click \nUpdate Now\n to see your changes.\n\n\n\n\n\n\nClick the \nWorksheets\n tab \n \nsheet\n to start the analysis.\n \n\n\n\n\n\n\nOn this screen, you can click and drag a field from the \nDimensions\n area to\n Rows\n or \nColumns\n. Refer to the Tableau documentation for more information on data visualization.", 
            "title": "Setting Up SnappyData ODBC Driver and Tableau Desktop"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#setting-up-snappydata-odbc-driver-and-tableau-desktop", 
            "text": "Note  This is currently tested and supported only on Windows 10 (32-Bit   64-Bit systems). Although versions other than Windows 10 may work, SnappyData is not claiming full testing on other versions.", 
            "title": "Setting Up SnappyData ODBC Driver and Tableau Desktop"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#step-1-and-2", 
            "text": "Ensure that you do the following:    Install Visual C++ Redistributable for Visual Studio 2015    Install SnappyData ODBC Driver", 
            "title": "Step 1 and 2:"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#step-3-create-snappydata-dsn-from-odbc-data-sources-64-bit32-bit", 
            "text": "To create SnappyData DSN from ODBC Data Sources:    Open the  ODBC Data Source Administrator  window:  a. On the  Start  page, type ODBC Data Sources, and select Set up ODBC data sources from the list or Select   ODBC Data Sources  in the  Administrative Tools .   b. Based on your Windows installation, open  ODBC Data Sources (64-bit)  or  ODBC Data Sources (32-bit)    In the  ODBC Data Source Administrator  window, select either the  User DSN  or  System DSN  tab.     Click  Add  to view the list of installed ODBC Drivers on your machine.    From the list of drivers, select  SnappyData ODBC Driver  and click  Finish .    The  SnappyData ODBC Configuration Dialog  is displayed. Enter the following details to create DSN:    Data Source Name : Name of the Data Source. For example, snappydsn.      Server (Hostname or IP) : IP address of the data server which is running in the SnappyData cluster.    Port : Port number of the server. By default, it is  1528  for the first data server in the cluster.    Login ID : The login ID required to connect to the server. Fir example,  app    Password : The password required to connect to the server. For example,  app       Note  Ensure that you provide the IP Address/Host Name and Port number of the data server. If you provide the details of the locator, the connection fails.", 
            "title": "Step 3: Create SnappyData DSN from ODBC Data Sources 64-bit/32-bit"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#step-4-install-tableau-desktop-101-or-higher", 
            "text": "To install Tableau desktop:    Download Tableau Desktop .    Depending on your Windows installation, download the 32-bit or 64-bit version of the installer.     Follow the steps to complete the installation.", 
            "title": "Step 4. Install Tableau Desktop (10.1 or Higher)"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#step-5-connecting-tableau-desktop-to-snappydata-server", 
            "text": "Before using Tableau with SnappyData ODBC Driver for the first time, you must add the  odbc-snappydata.tdc  configuration file in the Tableau data sources directory. Follow below steps to do so.  To connect the Tableau Desktop to SnappyData Server:    Download  odbc-snappydata.tdc  configuration file from the  SnappyData Release page .    Copy the downloaded odbc-snappydata.tdc file to  the  User_Home_Path \\Documents\\My Tableau Repository\\Datasources directory.    Open the Tableau Desktop application    On the start page, under  Connect     To a Server , and then click,  Other Databases (ODBC) .\nThe  Other Databases (ODBC)  window is displayed.     In the  Connect Using   DSN  drop-down, select the data source name provided earlier. (eg. snappydsn).    Click  Connect .    When a connection to SnappyData Server is established, the  Sign In  option is enabled.     Click  Sign In  to log into Tableau.    From the  Schema  drop-down list, select a schema.    All Tables from the selected schema is listed.    Select the required table(s) and drag it to the canvas. A view generated using the selected tables is displayed.  If you make changes to the table, click  Update Now  to see your changes.    Click the  Worksheets  tab    sheet  to start the analysis.      On this screen, you can click and drag a field from the  Dimensions  area to  Rows  or  Columns . Refer to the Tableau documentation for more information on data visualization.", 
            "title": "Step 5. Connecting Tableau Desktop to SnappyData Server"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "RowStore User's Guide\n\n\nThe RowStore User's Guide is intended for existing SQLFire or GemFire XD users who want to upgrade to SnappyData RowStore. This guide contains the migration steps for such users along with other product details.\n\n\nThe latest document is available at \nhttp://rowstore.docs.snappydata.io\n.\n\n\nSnappyData Resources\n\n\nYou can view videos, presentations, slideshows and refer to important resources about SnappyData at \nhttp://www.snappydata.io/resources\n.", 
            "title": "Additional Information"
        }, 
        {
            "location": "/additional_docs/#rowstore-users-guide", 
            "text": "The RowStore User's Guide is intended for existing SQLFire or GemFire XD users who want to upgrade to SnappyData RowStore. This guide contains the migration steps for such users along with other product details.  The latest document is available at  http://rowstore.docs.snappydata.io .", 
            "title": "RowStore User's Guide"
        }, 
        {
            "location": "/additional_docs/#snappydata-resources", 
            "text": "You can view videos, presentations, slideshows and refer to important resources about SnappyData at  http://www.snappydata.io/resources .", 
            "title": "SnappyData Resources"
        }, 
        {
            "location": "/techsupport/", 
            "text": "Mail Us\n\n\nYou can contact the SnappyData support team using:\n\n\n\n\nchomp@snappydata.io\n\n\nsuppport@snappydata.io\n \n\n\n\n\nCommunity\n\n\nThe following channels are monitored for comments/questions:\n\n\n\n\n\n\nStackoverflow\n\n\n\n\n\n\nSlack\n\n\n\n\n\n\nGitter\n \n\n\n\n\n\n\nIRC\n \n\n\n\n\n\n\nReddit\n \n\n\n\n\n\n\nJIRA", 
            "title": "Contact and Support"
        }, 
        {
            "location": "/techsupport/#mail-us", 
            "text": "You can contact the SnappyData support team using:   chomp@snappydata.io  suppport@snappydata.io", 
            "title": "Mail Us"
        }, 
        {
            "location": "/techsupport/#community", 
            "text": "The following channels are monitored for comments/questions:    Stackoverflow    Slack    Gitter      IRC      Reddit      JIRA", 
            "title": "Community"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "LICENSE\n\n\n                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n(c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\n\n\nCopyright 2016 SnappyData Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/LICENSE/#license", 
            "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION    Definitions.  \"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.  \"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.  \"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.  \"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.  \"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.  \"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.  \"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).  \"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.  \"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"  \"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.    Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.    Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.    Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:  (a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and  (b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and  (c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and  (d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.  You may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.    Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.    Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.    Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.    Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.    Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.    END OF TERMS AND CONDITIONS  APPENDIX: How to apply the Apache License to your work.    To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.  Copyright 2016 SnappyData Inc.  Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "LICENSE"
        }
    ]
}