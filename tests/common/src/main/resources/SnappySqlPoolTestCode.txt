/*
 * Copyright (c) 2017-2022 TIBCO Software Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */


/*
This is a script file used as in input to
io.snappydata.cluster.SplitClusterDUnitTest.testSparkShell and testSparkShellCurrent
*/
import java.math.BigDecimal
import org.apache.spark.sql.Row
import com.gemstone.gemfire.internal.shared.ClientSharedUtils.toHexString
import io.snappydata.sql.implicits._

println("Querying row and column tables")

// set default schema to current user if any
val sparkConf = sc.getConf
var userName = "app"
var password = "app"
val hostPort = sparkConf.get("spark.snappydata.connection").split(":")
val host = hostPort(0)
val port = hostPort(1).toInt
val PREFIXES = Array("spark.snappydata.store.", "spark.snappydata.")
PREFIXES.find(p => sparkConf.contains(p + "user")) match {
  case None =>
  case Some(prefix) =>
    userName = sparkConf.get(prefix + "user")
    password = sparkConf.get(prefix + "password")
    if (spark.sql(s"show databases like '$userName'").collect().length == 0) {
      spark.sql(s"create database if not exists $userName")
    }
    spark.sql(s"use $userName").collect()
}

// test a mix of explicit user/password/host/port and nothing (where all are auto-resolved from SparkConf)

// create the tables and insert couple of rows
spark.snappyExecute("create table testTable1 (id long, data string)", userName, password)
// check for JdbcWriter implicit
val insertDf = spark.range(2).selectExpr("ID", "concat('col', cast(id as string)) as DATA")
insertDf.write.snappy("testTable1")
assert(spark.snappyQuery("select * from testTable1").collect().length == 2)

spark.snappyExecute("create table testTable2 (id long, data1 varchar(40), " +
    "data2 decimal(12, 4), data3 array<int>, data4 map<long, string>, " +
    "data5 struct<id: int, s: varchar(20), dec: decimal(10, 4)>) using column", host, port)
assert(spark.snappyExecute("insert into testTable2 select 1, 'one', 1.2, array(3, 4), " +
    "map(3, 'three', 5, 'five'), struct(6, 'six', 7.1)", host, port, userName, password) == 1)
assert(spark.snappyExecute("insert into testTable2 select 2, 'two', 2.3, array(5, 6), " +
    "map(6, 'six', 8, 'eight'), struct(9, 'nine', 8.2)") == 1)

// simple row and column table scan queries
val allRows1 = Seq(Row(0, "col0"), Row(1, "col1"))
var rs = spark.snappyQuery("select * from testTable1").collect()
assert(rs.length == 2)
assert(rs.toSeq.sortBy(_.getLong(0)) == allRows1)
// try with explicit user/password
rs = spark.snappyQuery("select id, data from testTable1", userName, password).collect()
assert(rs.length == 2)
assert(rs.toSeq.sortBy(_.getLong(0)) == allRows1)

// some group by queries (try with explicit host/port)
rs = spark.snappyQuery("select id, last(data) data from testTable1 group by id", host, port).collect()
assert(rs.length == allRows1.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == allRows1)
// complex return type
rs = spark.snappyQuery("select id, collect_list(data) dataList from testTable1 group by id").collect()
assert(rs.length == allRows1.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == Seq(Row(0, """{"col_0":["col0"]}"""),
                                            Row(1, """{"col_0":["col1"]}""")))

// test table having complex types

val allRows2 = Seq(
  Row(1, "one", new BigDecimal("1.2000"), """{"col_0":[3,4]}""",
      """{"col_1":{"3":"three","5":"five"}}""", """{"col_2":{"id":6,"s":"six","dec":7.1000}}"""),
  Row(2, "two", new BigDecimal("2.3000"), """{"col_0":[5,6]}""",
      """{"col_1":{"6":"six","8":"eight"}}""", """{"col_2":{"id":9,"s":"nine","dec":8.2000}}"""))

rs = spark.snappyQuery("select * from testTable2").collect()
assert(rs.length == allRows2.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == allRows2)
rs = spark.snappyQuery("select id, data1, data2, data3, data4, data5 from testTable2").collect()
assert(rs.length == allRows2.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == allRows2)

// group by, array index access, map key access and struct field access
// this tests scenarios where inner sub-queries need to resolve types as proper
// complex types but top-level result projection will return complex types as JSON/binary

// full projection using FIRST/LAST with group by
rs = spark.snappyQuery("""select id, d2, d3, d4, d5 from (select id, last(data2) d2,
       first(data3) d3, last(data4) d4, last(data5) d5 from testTable2 group by id)""").collect()
assert(rs.length == allRows2.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == allRows2.map(r => Row(r.get(0), r.get(2), r.get(3),
       r.get(4), r.get(5))))

// aggregate values at an array index, collect strings with group by
rs = spark.snappyQuery("""select id, collect_list(data1) d1, max(data3[0]) d3,
       last(data4) d4, collect_list(data5.s) d5 from testTable2 group by id""").collect()
val expected2_1 = Seq(
  Row(1, """{"col_0":["one"]}""", 3, """{"col_1":{"3":"three","5":"five"}}""", """{"col_2":["six"]}"""),
  Row(2, """{"col_0":["two"]}""", 5, """{"col_1":{"6":"six","8":"eight"}}""", """{"col_2":["nine"]}"""))
assert(rs.length == expected2_1.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == expected2_1)

// map key access, struct field access in projection (try with explicit host/port and user/password)
rs = spark.snappyQuery("""select id, d1, d3[1], IfNull(d4[5], d4[6]) d4, d5.dec from
       (select id, collect_list(data1) d1, first(data3) d3, last(data4) d4,
        first(data5) d5 from testTable2 group by id)""", host, port, userName, password).collect()
val expected2_2 = Seq(
  Row(1, """{"col_0":["one"]}""", 4, "five", new BigDecimal("7.1")),
  Row(2, """{"col_0":["two"]}""", 6, "six", new BigDecimal("8.2")))
assert(rs.length == expected2_2.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == expected2_2)

// test complexTypeAsJson(0) hint to return binary value
rs = spark.snappyQuery("""select id, d2, d3, d4, d5.s from --+ complexTypeAsJson(0)
       (select id, avg(data2) d2, last(data3[0]) d3, last(data4) d4,
        first(data5) d5 from testTable2 group by id)""").collect()
rs = rs.map { row =>
  val b = row.get(3).asInstanceOf[Array[Byte]]
  Row(row.get(0), row.get(1), row.get(2), toHexString(b, 0, b.length), row.get(4))
}
val expected2_3 = Seq(
  Row(1, new BigDecimal("1.2"), 3,
    "20000000000000000200000000000000000000000000000003000000000000000500000000000000020000000000000000000000000000000500000020000000040000002800000074687265650000006669766500000000",
    "six"),
  Row(2, new BigDecimal("2.3"), 5,
    "20000000000000000200000000000000000000000000000006000000000000000800000000000000020000000000000000000000000000000300000020000000050000002800000073697800000000006569676874000000",
    "nine"))
assert(rs.length == expected2_3.length)
assert(rs.toSeq.sortBy(_.getLong(0)) == expected2_3)


// Tests using direct jdbc driver with pool URL rather than the implicits.
// These will result in double executions with first being with LIMIT 1 to determine
// schema which can be double the cost for some pushdown queries
// (e.g. SELECT AVG(C) FROM T is same cost with LIMIT 1 at top-level)

val JDBC_URL=s"jdbc:snappydata:pool://$host:$port"
// Client Pool Driver Test - Start
val connProps = new java.util.Properties()
connProps.setProperty("pool.maxTotal", "50")
connProps.setProperty("pool.maxIdle", "20")
connProps.setProperty("pool.initialSize", "50")
connProps.setProperty("pool.minActive", "50")
connProps.setProperty("pool.minIdle", "10")
connProps.setProperty("driver", "io.snappydata.jdbc.ClientPoolDriver")
connProps.setProperty("user", userName)
connProps.setProperty("password", password)

val df = spark.read.jdbc(JDBC_URL, "testTable1", connProps)
val result = df.select("id").groupBy("id").avg("id").orderBy("id").collect()
assert(result.length == 2) // expected rows
assert(result(0).getAs("id") == 0)// expected result

val df1 = spark.read.jdbc(JDBC_URL, "testTable1", connProps).select("id", "data").where("id = '1'").collect()
assert(df1.length == 1) // expected rows
assert(df1(0).getAs("id") == 1) // expected result

val df2 = spark.read.jdbc(url = JDBC_URL, table = "testTable2", columnName = "id",
lowerBound = 0, upperBound = 100, numPartitions = 16, connectionProperties = connProps).collect()
assert(df2.length == 2, "expected length is 2") // expected rows
assert(df2.toSeq.sortBy(_.getLong(0)) == allRows2)

val pushdown_query = s"(select * from testTable1 where id=0) test_alias"
val df3 = spark.read.jdbc(url = JDBC_URL, table = pushdown_query, properties = connProps).collect()
assert(df3.length == 1) // expected rows
val row3 = Array(Row(0, "col0"))
assert(df3(0).get(0) == 0 ) // expected result
// Client Pool Driver Test - End

System.exit(0)
