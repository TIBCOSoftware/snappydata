#Do not change the way of queries, sparkProperties, sparkSqlProperties. Just change the values inside strings

#queries to be run
queries="q1,q2,q3,q4,q5,q6,q7,q8,q9,q10,q11,q12,q13,q14a,q14b,q15,q16,q17,q18,q19,q20,q21,q22,q23a,q23b,q24a,q24b,q25,q26,q27,q28,q29,q30,q31,q32,q33,q34,q35,q36,q37,q38,q39a,q39b,q40,q41,q42,q43,q44,q45,q46,q47,q48,q49,q50,q51,q52,q53,q54,q55,q56,q57,q58,q59,q60,q61,q62,q63,q64,q65,q66,q67,q68,q69,q70,q71,q72,q73,q74,q75,q76,q77,q78,q79,q80,q81,q82,q83,q84,q85,q86,q87,q88,q89,q90,q91,q92,q93,q94,q95,q96,q97,q98,q99"

#spark Properties spefied in lead
#spark-executor-cores has nothing to do with CPU cores available.
#sparkProperties="-J-Xmx2g -spark.network.timeout=300s -spark.driver.maxResultSize=2g -spark.shuffle.sort.bypassMergeThreshold=28"
sparkProperties="--driver-memory 2g --conf spark.executor.memory=3g"

#Spark Sql properties are specified while executing query
#spark.sql.shuffle.partitions=${shufflePartitions},spark.sql.inMemoryColumnarStorage.compressed=${inMemoryColumnarStorageCompressed}
#spark.sql.inMemoryColumnarStorage.compressed=false
sparkSqlProperties="\"\""

#location of checkout
#SnappyData=$HOME/SNAPPY/CHECKOUT/snappydata/build-artifacts/scala-2.11/snappy
SnappyData=$HOME/SNAPPY/CHECKOUT/spark-2.1.1-bin-hadoop2.7
#SnappyData=$HOME/SNAPPY/CHECKOUT/snappydata-1.1.2-bin

#Whether to repartition dataframe
rePartition=true

#number of buckets for order and lineitem tables
buckets_Order_Lineitem=113

#number of buckets for Customer, Part and PartSupplier tables
buckets_Cust_Part_PartSupp=113

#Are Nation, Region, Supplier tables column tables?
IsSupplierColumnTable=false

#number of buckets for Nation, Region, Supplier
buckets_Supplier=113

#Loading from parquet or csv file
Parquet=false

#From how many files data should be loaded in table
#NumberOfLoadStages=10
NumberOfLoadStages=10

#Machine Setup
master=$HOSTNAME
slaves=($HOSTNAME)
client=$HOSTNAME

#represent whether query should use dynamic paramters or static
IsDynamic=true

#Whether to collect results.For performance testing this should be false.
ResultCollection=false

#warmUpIterations
WarmupRuns=2
#Actual runs whose average will be taken and repordted as performance
AverageRuns=3

# location of jar which has TPCH related class files
TPCHJar=$HOME/SNAPPY/CHECKOUT/snappydata/cluster/build-artifacts/scala-2.11/libs/snappydata-cluster_2.11-1.1.-tests.jar

#Size of the TPCH data. Do not chage format
dataSize=1GB_Stages
#dataSize=1GB

#Location of the TPCH Data. Make sure directory name is same as the dataSize specified above
dataDir=$HOME/SNAPPY/DATA/TPCH/$dataSize

#Location where final output will be collected
outputLocation=$HOME/SNAPPY/OUTPUT/TPCH/SNAPPY_RESULT

queryPath=$HOME/code/snappydata/spark/sql/core/src/test/resources/tpcds

# Whether to cache tables in spark cache or query the temp view (underlying data source)
cacheTables=true

