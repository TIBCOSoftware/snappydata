#Do not change the way of queries, sparkProperties, sparkSqlProperties. Just change the values inside strings

#queries to be run
queries="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-22-21"
#queries=10
#queries="\"1-3-14\""


#spark Properties spefied in lead
#spark-executor-cores has nothing to do with CPU cores available.
#sparkProperties="-J-Xmx2g -spark.network.timeout=300s -spark.driver.maxResultSize=2g -spark.shuffle.sort.bypassMergeThreshold=28"
sparkProperties="--driver-memory 2g --conf spark.executor.memory=3g"

#Spark Sql properties are specified while executing query
#spark.sql.shuffle.partitions=${shufflePartitions},spark.sql.inMemoryColumnarStorage.compressed=${inMemoryColumnarStorageCompressed}
#spark.sql.inMemoryColumnarStorage.compressed=false
sparkSqlProperties="\"\""

#location of checkout
SnappyData=/home/kishor/SNAPPY/CHECKOUT/snappydata/build-artifacts/scala-2.11/snappy
#SnappyData=/home/kishor/SNAPPY/CHECKOUT/spark-2.1.1-bin-hadoop2.7
#SnappyData=/home/kishor/SNAPPY/CHECKOUT/snappydata-0.7-bin

#Whether to repartition dataframe
rePartition=true

#number of buckets for order and lineitem tables
buckets_Order_Lineitem=113

#number of buckets for Customer, Part and PartSupplier tables
buckets_Cust_Part_PartSupp=113

#Are Nation, Region, Supplier tables column tables?
IsSupplierColumnTable=false

#number of buckets for Nation, Region, Supplier
buckets_Supplier=113

#Loading from parquet or csv file
Parquet=false

#From how many files data should be loaded in table
#NumberOfLoadStages=10
NumberOfLoadStages=10

#Machine Setup
master=localhost
slaves=(localhost)
client=localhost

#represent whether query should use dynamic paramters or static
IsDynamic=true

#Whether to collect results.For performance testing this should be false.
ResultCollection=false

#warmUpIterations
WarmupRuns=2
#Actual runs whose average will be taken and repordted as performance
AverageRuns=3

# location of jar which has TPCH related class files
TPCHJar=/home/kishor/SNAPPY/CHECKOUT/snappydata/cluster/build-artifacts/scala-2.11/libs/snappydata-cluster_2.11-1.3.1-tests.jar

#Size of the TPCH data. Do not chage format
#dataSize=1GB_Stages
dataSize=1GB

#Location of the TPCH Data. Make sure directory name is same as the dataSize specified above
dataDir=/home/kishor/SNAPPY/DATA/TPCH/$dataSize

#Location where final output will be collected
outputLocation=/home/kishor/SNAPPY/OUTPUT/TPCH/SNAPPY_RESULT

