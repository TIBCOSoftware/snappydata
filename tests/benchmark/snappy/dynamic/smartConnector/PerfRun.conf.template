#Do not change the way of queries, sparkProperties, sparkSqlProperties. Just change the values inside strings

#queries to be run
queries="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22"
#queries="\"1-3-14\""

#location of checkout
SNAPPY_SRC=$HOME/code/snappydata

# Apache Spark as SPARK_HOME
SPARK_HOME=$HOME/opt/spark-2.1.1-bin-hadoop2.7
# Snappy Spark as SPARK_HOME
#SPARK_HOME=$SNAPPY_SRC/build-artifacts/scala-2.11/snappy

#Location of snappydata binary (built from source or binary distribution)
SNAPPY_HOME=$SNAPPY_SRC/build-artifacts/scala-2.11/snappy

#spark Properties specified in lead
#spark-executor-cores has nothing to do with CPU cores available.
#sparkProperties="-J-Xmx2g -spark.network.timeout=300s -spark.driver.maxResultSize=2g -spark.shuffle.sort.bypassMergeThreshold=28"
# As specicifed in the docs, the packages parameter is to fetch the Snappydata jars from mavn central repo
#sparkProperties="--driver-memory 2g --conf spark.executor.memory=2g --packages SnappyDataInc:snappydata:1.1.0-s_2.11"
# Alternatively, the --jars parameter is used (instead of --packages) to download an unreleased version in development. 
# The core jar is built with the mavn target `./gradlew distpRoduct`. The name is different for OSS and enterprise versions. 
#sparkProperties="--driver-memory 2g --conf spark.executor.memory=2g --jars $SNAPPY_HOME/../distributions/snappydata-core_2.11-1.1.0.jar"
sparkProperties="--driver-memory 2g --conf spark.executor.memory=2g --jars $SNAPPY_HOME/../distributions/TIB_compute-core_2.11-1.1.0.jar"
#sparkProperties="--driver-memory 2g --conf spark.executor.memory=2g"

leadProperties="-heap-size=2g -conserve-sockets=false"

locatorProperties="-conserve-sockets=false"

serverProperties="-heap-size=3g -memory-size=3g -conserve-sockets=false"

#Machine Setup
# Spark standalone cluster master URI doesn't accept localhost or ip address as the master. Requires the host name. 
master=$(hostname)
slave=(localhost)

locator=localhost
leads=localhost
servers=(localhost)

#Log Directories
locatorDir=$HOME/snappy/output/tpch/locator
leadDir=$HOME/snappy/output/tpch/lead
serverDir=$HOME/snappy/output/tpch/server

#Spark Sql properties are specified while executing query
#spark.sql.shuffle.partitions=${shufflePartitions},spark.sql.inMemoryColumnarStorage.compressed=${inMemoryColumnarStorageCompressed}
#spark.sql.inMemoryColumnarStorage.compressed=false
sparkSqlProperties="\"\""


#Whether to repartition dataframe
rePartition=false

#number of buckets for order and lineitem tables
buckets_Order_Lineitem=32

#number of buckets for Customer, Part and PartSupplier tables
buckets_Cust_Part_PartSupp=32

#Are Nation, Region, Supplier tables column tables?
IsSupplierColumnTable=false

#number of buckets for Nation, Region, Supplier
buckets_Supplier=32

#Redundancy Level
Redundancy=1

#Whether region is persistent
Persistence=true

#Type of Persistence
Persistence_Type=SYNCHRONOUS

#Loading from parquet or csv file
Parquet=false

#From how many files data should be loaded in table
#NumberOfLoadStages=10
NumberOfLoadStages=1

#represent whether query should use dynamic paramters or static
IsDynamic=true

#Whether to collect results.For performance testing this should be false.
ResultCollection=false

#warmUpIterations
WarmupRuns=2
#Actual runs whose average will be taken and repordted as performance
AverageRuns=3

# location of jar which has TPCH related class files - it is not part of the binary distribution. Build by running the task './$SNAPPY_SRC/gradlew buildAll'
TPCHJar=$SNAPPY_SRC/cluster/build-artifacts/scala-2.11/libs/snappydata-cluster_2.11-1.1.0-tests.jar
#Size of the TPCH data. Do not chage format
#dataSize=1GB_Stages
dataSize=1GB
#dataSize=1GB

#Location of the TPCH Data. Make sure directory name is same as the dataSize specified above
#SnappyData source includes 1 GB test/ reference data in tests/benchmark/data
dataDir=$HOME/code/snappy/master/tests/benchmark/data/$dataSize

#Location where final output will be collected
outputLocation=$HOME/snappy/output/tpch/snappy_result

# Trace events to get granular timing info and report in loadPerfStream
traceEvents=false

# seed value for random number used in generating query parameter data
randomSeed=42

threadNumber=1 
