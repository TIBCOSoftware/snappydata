/*
 * Copyright (c) 2018 SnappyData, Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */

import org.gradle.api.tasks.testing.logging.*
import org.gradle.internal.logging.*

buildscript {
  repositories {
    maven { url 'https://plugins.gradle.org/m2' }
    mavenCentral()
  }
  dependencies {
    classpath 'io.snappydata:gradle-scalatest:0.23'
    classpath 'org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.11:0.9.0'
    classpath 'com.github.jengelman.gradle.plugins:shadow:4.0.3'
    classpath 'de.undercouch:gradle-download-task:3.4.3'
    classpath 'net.rdrei.android.buildtimetracker:gradle-plugin:0.11.+'
    classpath 'com.netflix.nebula:gradle-ospackage-plugin:5.2.+'
  }
}

apply plugin: 'wrapper'
apply plugin: 'distribution'
apply plugin: 'nebula.ospackage-base'
apply plugin: "nebula.ospackage"

allprojects {
  // We want to see all test results.  This is equivalent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  tasks.withType(Javadoc) {
    options.addStringOption('Xdoclint:none', '-quiet')
    /*
    if (javax.tools.ToolProvider.getSystemDocumentationTool().isSupportedOption("--allow-script-in-comments") == 0) {
      options.addBooleanOption("-allow-script-in-comments", true)
    }
    */
  }

  repositories {
    mavenCentral()
    maven { url 'https://dl.bintray.com/big-data/maven' }
    maven { url "https://repo.spring.io/libs-release" }
    maven { url "https://oss.sonatype.org/content/repositories/snapshots" }
    // maven { url 'http://repository.snappydata.io/repository/internal' }
    // maven { url 'http://repository.snappydata.io/repository/snapshots' }
    maven { url 'https://app.camunda.com/nexus/content/repositories/public' }
  }

  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'
  apply plugin: 'com.github.johnrengelman.shadow'
  apply plugin: 'idea'
  apply plugin: "build-time-tracker"

  group = 'io.snappydata'
  version = '1.0.2.1'

  // apply compiler options
  tasks.withType(JavaCompile) {
    options.encoding = 'UTF-8'
    options.incremental = true
    options.compilerArgs << '-Xlint:-serial,-path,-deprecation,-unchecked,-rawtypes'
    options.compilerArgs << '-XDignore.symbol.file'
    options.fork = true
    options.forkOptions.javaHome = file(System.properties['java.home'])
    options.forkOptions.jvmArgs = [ '-J-Xmx2g', '-J-Xms2g', '-J-XX:ReservedCodeCacheSize=512m', '-J-Djava.net.preferIPv4Stack=true' ]
  }
  tasks.withType(ScalaCompile) {
    options.fork = true
    options.forkOptions.jvmArgs = [ '-Xmx2g', '-Xms2g', '-XX:ReservedCodeCacheSize=512m', '-Djava.net.preferIPv4Stack=true' ]
    // scalaCompileOptions.optimize = true
    // scalaCompileOptions.useAnt = false
    scalaCompileOptions.deprecation = false
    scalaCompileOptions.additionalParameters = [ '-feature' ]
    options.encoding = 'UTF-8'
  }

  jar.duplicatesStrategy = DuplicatesStrategy.EXCLUDE

  javadoc.options.charSet = 'UTF-8'

  ext {
    productName = 'SnappyData'
    vendorName = 'SnappyData, Inc.'
    scalaBinaryVersion = '2.11'
    scalaVersion = scalaBinaryVersion + '.8'
    sparkVersion = '2.1.1'
    snappySparkVersion = '2.1.1.4'
    sparkDistName = "spark-${sparkVersion}-bin-hadoop2.7"
    sparkCurrentVersion = '2.3.2'
    sparkCurrentDistName = "spark-${sparkCurrentVersion}-bin-hadoop2.7"
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.25'
    junitVersion = '4.12'
    mockitoVersion = '1.10.19'
    hadoopVersion = '2.7.7'
    scalatestVersion = '2.2.6'
    jettyVersion = '9.2.26.v20180806'
    guavaVersion = '14.0.1'
    kryoVersion = '4.0.1'
    thriftVersion = '0.9.3'
    metricsVersion = '4.0.3'
    metrics2Version = '2.2.0'
    janinoVersion = '3.0.8'
    derbyVersion = '10.14.2.0'
    parboiledVersion = '2.1.5'
    tomcatJdbcVersion = '8.5.37'
    hikariCPVersion = '2.7.9'
    twitter4jVersion = '4.0.7'
    objenesisVersion = '3.0.1'
    rabbitMqVersion = '4.9.1'
    akkaVersion = '2.3.16'
    sprayVersion = '1.3.4'
    sprayJsonVersion = '1.3.5'
    sprayShapelessVersion = '1.3.3'
    sprayTestkitVersion = '1.3.4'
    jodaVersion = '2.1.2'
    jodaTimeVersion = '2.10.1'
    slickVersion = '2.1.0'
    h2Version = '1.3.176'
    commonsIoVersion = '2.6'
    commonsPoolVersion = '1.6'
    dbcpVersion = '1.4'
    shiroVersion = '1.2.6'
    flywayVersion = '3.2.1'
    typesafeConfigVersion = '1.3.3'
    mssqlVersion = '7.0.0.jre8'
    antlr2Version = '2.7.7'

    pegdownVersion = '1.6.0'
    snappyStoreVersion = '1.6.2.1'
    snappydataVersion = version
    pulseVersion = '1.5.1'
    zeppelinInterpreterVersion = '0.7.3.4'

    buildFlags = ''
    createdBy = System.getProperty('user.name')
    osArch = System.getProperty('os.arch')
    osName = org.gradle.internal.os.OperatingSystem.current()
    osVersion = System.getProperty('os.version')
    buildDate = new Date().format('yyyy-MM-dd HH:mm:ss Z')
    buildNumber = new Date().format('MMddyy')
    jdkVersion = System.getProperty('java.version')
    sparkJobServerVersion = '0.6.2.8'
    eclipseCollectionsVersion = '9.2.0'
    fastutilVersion = '8.2.2'

    gitCmd = "git --git-dir=${rootDir}/.git --work-tree=${rootDir}"
    gitBranch = "${gitCmd} rev-parse --abbrev-ref HEAD".execute().text.trim()
    commitId = "${gitCmd} rev-parse HEAD".execute().text.trim()
    sourceDate = "${gitCmd} log -n 1 --format=%ai".execute().text.trim()
    buildIdPrefix = System.env.USER + ' '

    sparkDistDir = "${project.gradle.gradleUserHomeDir}/sparkDist"
    sparkProductDir = "${sparkDistDir}/${sparkDistName}"
    sparkCurrentProductDir = "${sparkDistDir}/${sparkCurrentDistName}"
  }

  if (!buildRoot.isEmpty()) {
    buildDir = new File(buildRoot, 'scala-' + scalaBinaryVersion + '/' +  project.path.replace(':', '/'))
  } else {
    // default output directory suffix like in sbt/maven
    buildDir = 'build-artifacts/scala-' + scalaBinaryVersion
  }
  if (rootProject.hasProperty('enablePublish')) {
    buildIdPrefix = "${vendorName} "
  }
  if (rootProject.hasProperty('sparkDistDir')) {
    sparkDistDir = rootProject.property('sparkDistDir')
    sparkProductDir = "${sparkDistDir}/${sparkDistName}"
    sparkCurrentProductDir = "${sparkDistDir}/${sparkCurrentDistName}"
  }

  ext {
    testResultsBase = "${rootProject.buildDir}/tests/snappy"
    snappyProductDir = "${rootProject.buildDir}/snappy"
  }

  // force same output directory for IDEA and gradle
  idea {
    module {
      def projOutDir = file("${projectDir}/src/main/scala").exists()
        ? "${project.sourceSets.main.java.outputDir}/../../scala/main"
        : project.sourceSets.main.java.outputDir
      def projTestOutDir = file("${projectDir}/src/test/scala").exists()
        ? "${project.sourceSets.test.java.outputDir}/../../scala/test"
        : project.sourceSets.test.java.outputDir
      outputDir file(projOutDir)
      testOutputDir file(projTestOutDir)
    }
  }
}


def isEnterpriseProduct = rootProject.hasProperty('snappydata.enterprise')
def hasAqpProject = new File(rootDir, 'aqp/build.gradle').exists()
def aqpProject = isEnterpriseProduct ? project(":snappy-aqp_${scalaBinaryVersion}") : null
def hasJdbcConnectorProject = new File(rootDir, 'snappy-connectors/jdbc-stream-connector/build.gradle').exists()
def hasGemFireConnectorProject = new File(rootDir, 'snappy-connectors/gemfire-connector/build.gradle').exists()

if (isEnterpriseProduct) {
  if (!hasJdbcConnectorProject || !hasGemFireConnectorProject) {
    throw new GradleException('Project repository snappy-connectors not found.')
  }
}

def getProcessId() {
  String name = java.lang.management.ManagementFactory.getRuntimeMXBean().getName()
  return name[0..name.indexOf('@') - 1]
}

def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

// Skip snappy-spark, snappy-aqp and spark-jobserver that have their own
// scalaStyle configuration. Skip snappy-store that will not use it.
configure(subprojects.findAll {!(it.name ==~ /snappy-spark.*/ ||
      it.name ==~ /snappy-store.*/ ||
      it.name ==~ /snappy-aqp.*/ ||
      it.name ==~ /spark-jobserver.*/)}) {
  scalaStyle {
    configLocation = "${rootProject.projectDir}/scalastyle-config.xml"
    inputEncoding = 'UTF-8'
    outputEncoding = 'UTF-8'
    outputFile = "${buildDir}/scalastyle-output.xml"
    includeTestSourceDirectory = true
    source = 'src/main/scala'
    testSource = 'src/test/scala'
    failOnViolation = true
    failOnWarning = false
  }
}

def cleanIntermediateFiles(def projectName) {
  def projDir = "${project(projectName).projectDir}"
  delete "${projDir}/metastore_db"
  delete "${projDir}/warehouse"
  delete "${projDir}/datadictionary"
  delete fileTree(projDir) {
    include 'BACKUPGFXD-DEFAULT-DISKSTORE**', 'locator*.dat'
  }
}

def now() {
  return new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
}

task cleanScalaTest { doLast {
  String workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanJUnit { doLast {
  String workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanDUnit { doLast {
  String workingDir = "${testResultsBase}/dunit"
  delete workingDir
  file(workingDir).mkdirs()
  // clean spark cluster directories
  delete "${snappyProductDir}/work", "${snappyProductDir}/logs"
  delete "${sparkProductDir}/work", "${sparkProductDir}/logs"
  delete "${sparkCurrentProductDir}/work", "${sparkCurrentProductDir}/logs"
} }
task cleanSecurityDUnit { doLast {
  String workingDir = "${testResultsBase}/dunit-security"
  delete workingDir
  file(workingDir).mkdirs()
  // clean spark cluster directories
  delete "${snappyProductDir}/work", "${snappyProductDir}/logs"
  delete "${sparkProductDir}/work", "${sparkProductDir}/logs"
  delete "${sparkCurrentProductDir}/work", "${sparkCurrentProductDir}/logs"
} }
task cleanAllReports { doLast {
  String workingDir = "${testResultsBase}/combined-reports"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanQuickstart { doLast {
  String workingDir = "${testResultsBase}/quickstart"
  delete workingDir
  file(workingDir).mkdirs()
} }

subprojects {
  // the run task for a selected sub-project
  task run(type:JavaExec) {
    if (!project.hasProperty('mainClass')) {
      main = 'io.snappydata.app.SparkSQLTest'
    } else {
      main = mainClass
    }
    if (project.hasProperty('params')) {
      args = params.split(',') as List
    }
    classpath = sourceSets.main.runtimeClasspath + sourceSets.test.runtimeClasspath
    jvmArgs '-Xmx2g', '-Xms2g'
  }

  task scalaTest(type: Test) {
    def factory = new com.github.maiflai.BackwardsCompatibleJavaExecActionFactory(gradle.gradleVersion)
    actions = [ new com.github.maiflai.ScalaTestAction(factory) ]
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    minHeapSize '4g'
    maxHeapSize '4g'
    jvmArgs '-ea', '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:MaxNewSize=1g',
            '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled'
    // for benchmarking
    // minHeapSize '12g'
    // maxHeapSize '12g'
    // jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:MaxNewSize=2g',
    //        '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled'

    testLogging.exceptionFormat = TestExceptionFormat.FULL
    testLogging.events = TestLogEvent.values() as Set

    extensions.add(com.github.maiflai.ScalaTestAction.TAGS, new org.gradle.api.tasks.util.PatternSet())
    List<String> suites = []
    extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
    extensions.add('suite', { String name -> suites.add(name) } )
    extensions.add('suites', { String... name -> suites.addAll(name) } )

    def result = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTRESULT, result)
    extensions.add('testResult', { String name -> result.setLength(0); result.append(name) } )

    def output = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTOUTPUT, output)
    extensions.add('testOutput', { String name -> output.setLength(0); output.append(name) })

    def errorOutput = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTERROR, errorOutput)
    extensions.add('testError', { String name -> errorOutput.setLength(0); errorOutput.append(name) })

    // running a single scala suite
    if (rootProject.hasProperty('singleSuite')) {
      suite singleSuite
    }
    workingDir = "${testResultsBase}/scalatest"

    // testResult '/dev/tty'
    testOutput "${workingDir}/output.txt"
    testError "${workingDir}/error.txt"
    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test {
    maxParallelForks = Runtime.getRuntime().availableProcessors()
    maxHeapSize '2g'
    jvmArgs '-ea', '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC',
            '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled'
    testLogging.exceptionFormat = TestExceptionFormat.FULL

    def single = System.getProperty('junit.single')
    if (single == null || single.length() == 0) {
      single = rootProject.hasProperty('junit.single') ?
          rootProject.property('junit.single') : null
    }
    if (single == null || single.length() == 0) {
      include '**/*.class'
      exclude '**/*TestBase.class'
      exclude '**/*DUnit*.class'
    } else {
      include single
    }

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      progress << "${eol}${now} ========== STARTING JUNIT TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
  }
  task dunitTest(type: Test) {
    dependsOn ':cleanDUnit'
    dependsOn ':product'
    dependsOn ':copyShadowJars'
    maxParallelForks = 1
    minHeapSize '1536m'
    maxHeapSize '1536m'

    // limit netty buffer arenas to avoid occasional OOMEs with 1.5g heap
    int numArenas = Math.min(8, Runtime.getRuntime().availableProcessors())
    jvmArgs = ['-XX:+HeapDumpOnOutOfMemoryError',
               '-XX:+UseParNewGC', '-XX:+UseConcMarkSweepGC',
               '-XX:CMSInitiatingOccupancyFraction=50',
               '-XX:+CMSClassUnloadingEnabled', '-ea',
               '-Dspark.sql.codegen.cacheSize=1000',
               '-Dspark.ui.retainedStages=500',
               '-Dspark.ui.retainedJobs=500',
               '-Dspark.sql.ui.retainedExecutions=500',
               '-Dio.netty.allocator.pageSize=8192',
               '-Dio.netty.allocator.maxOrder=10',
               "-Dio.netty.allocator.numHeapArenas=${numArenas}",
               "-Dio.netty.allocator.numDirectArenas=${numArenas}"]

    workingDir = "${testResultsBase}/dunit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    systemProperties 'java.net.preferIPv4Stack': 'true',
                     'SNAPPY_HOME': snappyProductDir

    int numTestClasses = 0
    def testCount = new java.util.concurrent.atomic.AtomicInteger(0)

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      numTestClasses = getCandidateClassFiles().getFiles().size()
      progress << "${eol}${now} ========== STARTING DUNIT TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
    beforeSuite { desc ->
      if (desc.className != null) {
        def count = testCount.incrementAndGet()
        println "${now()} Start ${desc.className} ($count/$numTestClasses)"
      }
    }
    afterSuite { desc, result ->
      if (desc.className != null) {
        println "${now()} END ${desc.className}"
      }
    }
  }

  task dunitSecurityTest(type: Test) {
    dependsOn ':cleanSecurityDUnit'
    dependsOn ':product'
    dependsOn ':copyShadowJars'
    maxParallelForks = 1
    minHeapSize '1536m'
    maxHeapSize '1536m'

    // limit netty buffer arenas to avoid occasional OOMEs with 1.5g heap
    int numArenas = Math.min(8, Runtime.getRuntime().availableProcessors())
    jvmArgs = ['-XX:+HeapDumpOnOutOfMemoryError',
               '-XX:+UseParNewGC', '-XX:+UseConcMarkSweepGC',
               '-XX:CMSInitiatingOccupancyFraction=50',
               '-XX:+CMSClassUnloadingEnabled', '-ea',
               '-Dspark.sql.codegen.cacheSize=1000',
               '-Dspark.ui.retainedStages=500',
               '-Dspark.ui.retainedJobs=500',
               '-Dspark.sql.ui.retainedExecutions=500',
               '-Dio.netty.allocator.pageSize=8192',
               '-Dio.netty.allocator.maxOrder=10',
               "-Dio.netty.allocator.numHeapArenas=${numArenas}",
               "-Dio.netty.allocator.numDirectArenas=${numArenas}"]

    workingDir = "${testResultsBase}/dunit-security"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    systemProperties 'java.net.preferIPv4Stack': 'true',
                     'SNAPPY_HOME': snappyProductDir

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      progress << "${eol}${now} ========== STARTING SECURITY DUNIT TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
  }

  // apply default manifest
  if (rootProject.hasProperty('enablePublish')) {
    createdBy = 'SnappyData Build Team'
  }
  jar {
    manifest {
      attributes(
        'Manifest-Version'  : '1.0',
        'Created-By'        : createdBy,
        'Title'             : rootProject.name,
        'Version'           : version,
        'Vendor'            : vendorName
      )
    }
  }

  configurations {
    testOutput {
      extendsFrom testCompile
      description 'a dependency that exposes test artifacts'
    }
    /*
    all {
      resolutionStrategy {
        // fail eagerly on version conflict (includes transitive dependencies)
        // e.g. multiple different versions of the same dependency (group and name are equal)
        failOnVersionConflict()
      }
    }
    */
  }

  // force versions for some dependencies that get pulled multiple times
  configurations.all {
    resolutionStrategy.force "com.google.guava:guava:${guavaVersion}",
      "org.apache.derby:derby:${derbyVersion}",
      "org.apache.hadoop:hadoop-annotations:${hadoopVersion}",
      "org.apache.hadoop:hadoop-auth:${hadoopVersion}",
      "org.apache.hadoop:hadoop-client:${hadoopVersion}",
      "org.apache.hadoop:hadoop-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-hdfs:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-app:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-core:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-jobclient:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-shuffle:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-api:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-client:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-server-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-server-nodemanager:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-server-web-proxy:${hadoopVersion}"
    exclude(group: 'org.mortbay.jetty', module: 'servlet-api')
  }
  configurations.testRuntime {
    // below is included indirectly by hadoop deps and conflicts with embedded 1.5.7 apacheds
    exclude(group: 'org.apache.directory.server', module: 'apacheds-kerberos-codec')
  }

  task packageTests(type: Jar, dependsOn: testClasses) {
    description 'Assembles a jar archive of test classes.'
    classifier = 'tests'
  }
  artifacts {
    testOutput packageTests
  }

  dependencies {
    compile 'log4j:log4j:' + log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
  }
}

// maven publish tasks
subprojects {

  apply plugin: 'signing'

  task packageSources(type: Jar, dependsOn: classes) {
    classifier = 'sources'
    from sourceSets.main.allSource
  }
  task packageDocs(type: Jar, dependsOn: javadoc) {
    classifier = 'javadoc'
    from javadoc
  }
  if (rootProject.hasProperty('enablePublish')) {
    signing {
      sign configurations.archives
    }

    uploadArchives {
      repositories {
        mavenDeployer {
          beforeDeployment { MavenDeployment deployment -> signing.signPom(deployment) }

          repository(url: 'https://oss.sonatype.org/service/local/staging/deploy/maven2/') {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }
          snapshotRepository(url: 'https://oss.sonatype.org/content/repositories/snapshots/') {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }

          pom.project {
            name 'SnappyData'
            packaging 'jar'
            // optionally artifactId can be defined here
            description 'SnappyData distributed data store and execution engine'
            url 'http://www.snappydata.io'

            scm {
              connection 'scm:git:https://github.com/SnappyDataInc/snappydata.git'
              developerConnection 'scm:git:https://github.com/SnappyDataInc/snappydata.git'
              url 'https://github.com/SnappyDataInc/snappydata'
            }

            licenses {
              license {
                name 'The Apache License, Version 2.0'
                url 'http://www.apache.org/licenses/LICENSE-2.0.txt'
              }
            }

            developers {
              developer {
                id 'smenon'
                name 'Sudhir Menon'
                email 'smenon@snappydata.io'
              }
            }
          }
        }
      }
    }
  }
}

// apply common test and misc configuration
gradle.taskGraph.whenReady { graph ->

  String dunitSingle = System.getProperty('dunit.single')
  if (dunitSingle == null || dunitSingle.length() == 0) {
    dunitSingle = rootProject.hasProperty('dunit.single') ?
        rootProject.property('dunit.single') : null
  }
  String dunitSecSingle = System.getProperty('dunitSecurity.single')
  if (dunitSecSingle == null || dunitSecSingle.length() == 0) {
    dunitSecSingle = rootProject.hasProperty('dunitSecurity.single') ?
        rootProject.property('dunitSecurity.single') : null
  }

  def allTasks = subprojects.collect { it.tasks }.flatten()
  allTasks.each { task ->
    if (task instanceof Tar) {
      def tar = (Tar)task
      tar.compression = Compression.GZIP
      tar.extension = 'tar.gz'
    } else if (task instanceof Jar) {
      def pack = (Jar)task
      if (pack.name == 'packageTests') {
        pack.from(pack.project.sourceSets.test.output.classesDirs, pack.project.sourceSets.test.resources.srcDirs)
      }
    } else if (task instanceof Test) {
      def test = (Test)task
      test.configure {

        if (test.name == 'dunitTest') {
          includes.clear()
          excludes.clear()
          if (dunitSingle == null || dunitSingle.length() == 0) {
            def dunitTests = testClassesDirs.asFileTree.matching {
              includes = [ '**/*DUnitTest.class', '**/*DUnit.class' ]
              excludes = [ '**/*Suite.class', '**/*DUnitSecurityTest.class', '**/NCJ*DUnit.class',
                  '**/BackwardCompatabilityPart*DUnit.class', '**/*Perf*DUnit.class', '**/ListAggDUnit.class',
                  '**/SingleHop*TransactionDUnit.class', '**/*Parallel*AsyncEvent*DUnit.class', '**/pivotal/gemfirexd/wan/**/*DUnit.class' ]
            }
            FileTree includeTestFiles = dunitTests
            int dunitFrom = rootProject.hasProperty('dunit.from') ?
                getLast(includeTestFiles, rootProject.property('dunit.from')) : 0
            int dunitTo = rootProject.hasProperty('dunit.to') ?
                getLast(includeTestFiles, rootProject.property('dunit.to')) : includeTestFiles.size()

            int begin = dunitFrom != -1 ? dunitFrom : 0
            int end = dunitTo != -1 ? dunitTo : includeTestFiles.size()
            def filteredSet = includeTestFiles.drop(begin).take(end-begin+1).collect {f -> "**/" + f.name}
            if (begin != 0 || end != includeTestFiles.size()) {
              println("Picking tests :")
              filteredSet.each { a -> println(a) }
            }
            include filteredSet
          } else {
            include dunitSingle
          }
        } else if (test.name == 'dunitSecurityTest') {
          includes.clear()
          excludes.clear()
          if (!rootProject.hasProperty('snappydata.enterprise')) {
            excludes = [ '**/*Suite.class', '**/*DUnitSecurityTest.class', '**/*DUnitTest.class', '**/*DUnit.class' ]
          } else if (dunitSecSingle == null || dunitSecSingle.length() == 0) {
            def dunitSecurityTests = testClassesDirs.asFileTree.matching {
              includes = [ '**/*DUnitSecurityTest.class' ]
              excludes = [ '**/*Suite.class', '**/*DUnitTest.class', '**/*DUnit.class' ]
            }
            FileTree includeTestFiles = dunitSecurityTests
            int dunitFrom = rootProject.hasProperty('dunitSecurity.from') ?
                getLast(includeTestFiles, rootProject.property('dunitSecurity.from')) : 0
            int dunitTo = rootProject.hasProperty('dunitSecurity.to') ?
                getLast(includeTestFiles, rootProject.property('dunitSecurity.to')) : includeTestFiles.size()

            int begin = dunitFrom != -1 ? dunitFrom : 0
            int end = dunitTo != -1 ? dunitTo : includeTestFiles.size()
            def filteredSet = includeTestFiles.drop(begin).take(end-begin+1).collect {f -> "**/" + f.name}
            if (begin != 0 || end != includeTestFiles.size()) {
              println("Picking tests :")
              filteredSet.each { a -> println(a) }
            }
            include filteredSet
          } else {
            include dunitSecSingle
          }
        }

        String logLevel = System.getProperty('logLevel')
        if (logLevel != null && logLevel.length() > 0) {
          systemProperties 'gemfire.log-level'           : logLevel,
                           'logLevel'                    : logLevel
        }
        logLevel = System.getProperty('securityLogLevel')
        if (logLevel != null && logLevel.length() > 0) {
          systemProperties 'gemfire.security-log-level'  : logLevel,
                           'securityLogLevel'            : logLevel
        }

        environment 'SNAPPY_HOME': snappyProductDir,
          'APACHE_SPARK_HOME': sparkProductDir,
          'APACHE_SPARK_CURRENT_HOME': sparkCurrentProductDir,
          'SNAPPY_DIST_CLASSPATH': test.classpath.asPath

        def failureCount = new java.util.concurrent.atomic.AtomicInteger(0)
        def progress = new File(workingDir, 'progress.txt')
        def output = new File(workingDir, 'output.txt')

        String eol = System.getProperty('line.separator')
        beforeTest { desc ->
          String now = now()
          progress << "${now} Starting test ${desc.className} ${desc.name}${eol}"
          output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
        }
        onOutput { desc, event ->
          String msg = event.message
          if (event.destination.toString() == 'StdErr') {
            msg = msg.replace(eol, "${eol}[error]  ")
          }
          output << msg
        }
        afterTest { desc, result ->
          String now = now()
          progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
          output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
          def exceptions = result.exceptions
          if (exceptions.size() > 0) {
            exceptions.each { t ->
              progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
              output << "${getStackTrace(t)}${eol}"
            }
            failureCount.incrementAndGet()
          }
        }
        doLast {
          def report = "${test.reports.html.destination}/index.html"
          boolean hasProgress = progress.exists()
          if (failureCount.get() > 0) {
            println()
            def failureMsg = "FAILED: There were ${failureCount.get()} failures.${eol}"
            if (hasProgress) {
              failureMsg    += "        See the progress report in: file://$progress${eol}"
            }
            failureMsg    += "        HTML report in: file://$report"
            throw new GradleException(failureMsg)
          } else if (hasProgress) {
            println()
            println("SUCCESS: See the progress report in: file://$progress")
            println("         HTML report in: file://$report")
            println()
          } else {
            println()
            println("SUCCESS: See the HTML report in: file://$report")
            println()
          }
        }
      }
    }
  }
}


task publishLocal {
  dependsOn subprojects.findAll { p -> p.name != 'snappydata-native' &&
    p.name != 'snappydata-store-prebuild' && p.name != 'snappydata-store' }.collect {
      it.getTasksByName('install', false).collect { it.path }
  }
}

task publishMaven {
  dependsOn subprojects.findAll { p -> p.name != 'snappydata-native' &&
    p.name != 'snappydata-store-prebuild' && p.name != 'snappy-store' &&
      p.name != 'snappydata-store' }.collect {
      it.getTasksByName('uploadArchives', false).collect { it.path }
  }
}

task product(type: Zip) {
  dependsOn ":snappy-cluster_${scalaBinaryVersion}:jar"
  dependsOn ":snappy-examples_${scalaBinaryVersion}:jar"
  dependsOn ":snappy-spark:snappy-spark-assembly_${scalaBinaryVersion}:sparkProduct"
  dependsOn ':snappy-launcher:jar'
  dependsOn ':jdbcJar'
  dependsOn ":snappy-v2connector_${scalaBinaryVersion}:jar"
  dependsOn ":snappy-encoders_${scalaBinaryVersion}:jar"

  def clusterProject = project(":snappy-cluster_${scalaBinaryVersion}")
  def launcherProject = project(':snappy-launcher')
  def v2connectorProject = project(":snappy-v2connector_${scalaBinaryVersion}")
  def encodersProject = project(":snappy-encoders_${scalaBinaryVersion}")
  def targetProject = clusterProject

  if (isEnterpriseProduct) {
    if (hasAqpProject) {
      dependsOn ":snappy-aqp_${scalaBinaryVersion}:jar"
      targetProject = aqpProject
    }

    if (hasJdbcConnectorProject){
       dependsOn ":snappy-jdbc-connector_${scalaBinaryVersion}:jar"
    }
    if (hasGemFireConnectorProject){
       dependsOn ":gemfire-connector:product"
    }
  }


  // create snappydata+spark combined python zip
  destinationDir = file("${snappyProductDir}/python/lib")
  archiveName = 'pyspark.zip'
  from("${project(':snappy-spark').projectDir}/python") {
    include 'pyspark/**/*'
  }
  from("${rootDir}/python") {
    include 'pyspark/**/*'
  }

  doFirst {
    // remove the spark pyspark.zip
    delete "${snappyProductDir}/python/lib/pyspark.zip"
  }
  doLast {
    def examplesProject = project(":snappy-examples_${scalaBinaryVersion}")
    String exampleArchiveName = "quickstart.jar"

    // copy all runtime dependencies of snappy-cluster, itself and AQP
    def targets = targetProject.configurations.runtime
    copy {
      from(targets) {
        // exclude antlr4 explicitly (runtime is still included)
        // that gets pulled by antlr gradle plugin
        exclude '**antlr4-4*.jar'
        // exclude scalatest included by spark-tags
        exclude '**scalatest*.jar'
        // exclude other test jars
        exclude '**junit*.jar'
        exclude '**mockito*.jar'
        exclude '**hamcrest-core*.jar'
        exclude '**test-interface*.jar'
        exclude '**scalacheck*.jar'
        if (rootProject.hasProperty('hadoop-provided')) {
          exclude 'hadoop-*.jar'
        }
      }
      from targetProject.jar.outputs
      into "${snappyProductDir}/jars"
    }
    // copy the launcher jar
    copy {
      from launcherProject.jar.destinationDir
      into "${snappyProductDir}/jars"
      include launcherProject.jar.archiveName
    }

    copy {
      from v2connectorProject.jar.destinationDir
      into "${snappyProductDir}/jars"
      include v2connectorProject.jar.archiveName
    }

    copy {
      from encodersProject.jar.destinationDir
      into "${snappyProductDir}/jars"
      include encodersProject.jar.archiveName
    }

    // create the RELEASE file
    def releaseFile = file("${snappyProductDir}/RELEASE")
    String buildFlags = ''
    if (rootProject.hasProperty('docker')) {
      buildFlags += ' -Pdocker'
    }
    if (rootProject.hasProperty('ganglia')) {
      buildFlags += ' -Pganglia'
    }
    if (rootProject.hasProperty('hadoop-provided')) {
      buildFlags += ' -Phadoop-provided'
    }
    String gitRevision = "${gitCmd} rev-parse --short HEAD".execute().text.trim()
    if (gitRevision.length() > 0) {
      gitRevision = " (git revision ${gitRevision})"
    }

    if (rootProject.hasProperty('hadoop-provided')) {
      releaseFile.append("SnappyData ${version}${gitRevision} " +
              "built with Hadoop ${hadoopVersion} but hadoop not bundled.\n")
    } else {
      releaseFile.append("SnappyData ${version}${gitRevision} built for Hadoop ${hadoopVersion}.\n")
    }
    releaseFile.append("Build flags:${buildFlags}\n")

    // copy LICENSE, README.md and doc files
    copy {
      from projectDir
      into snappyProductDir
      include 'LICENSE'
      if (!isEnterpriseProduct) {
        include 'NOTICE'
      }
      include 'README.md'
    }
    copy {
      from "${projectDir}/docs"
      into "${snappyProductDir}/docs"
    }

    copy {
      from "${rootDir}/python/pyspark/"
      include 'sql/**'
      include 'streaming/**'
      include 'test_support/**'
      into "${snappyProductDir}/python/pyspark"
    }

    // Next the remaining components of full product like examples etc
    // Spark portions already copied in the assembly:product dependency
    copy {
      from("${examplesProject.projectDir}/src/main/python")
      into "${snappyProductDir}/quickstart/python"
    }
    if (new File(rootDir, 'store/build.gradle').exists()) {
      // copy snappy-store shared libraries for JNI calls
      def storeCoreProject = project(':snappy-store:snappydata-store-core')
      copy {
        from "${storeCoreProject.projectDir}/lib"
        into "${snappyProductDir}/jars"
        exclude '*_sol*.so'
      }
      copy {
        from "${storeCoreProject.projectDir}/../quickstart"
        into "${snappyProductDir}/quickstart/store"
        exclude '.git*'
      }
    }
    if (isEnterpriseProduct) {
      if (hasAqpProject) {
        // copy enterprise shared libraries for optimized JNI calls
        copy {
          from aqpProject.projectDir.path + '/lib'
          into "${snappyProductDir}/jars"
        }
        copy {
          from aqpProject.projectDir
          into snappyProductDir
          include 'NOTICE'
          include '*EULA*'
        }
      }

      def jdbcConnectorProject = project(":snappy-jdbc-connector_${scalaBinaryVersion}")
      def gemfireConnectorProject = project(":gemfire-connector")
      def gfeConnectorProject = project(":gemfire-connector:connector_${scalaBinaryVersion}")
      def gfeFunctionProject = project(":gemfire-connector:gfeFunctions")
      if (hasJdbcConnectorProject) {
        copy {
          from jdbcConnectorProject.jar.destinationDir
          into "${snappyProductDir}/connectors"
          include "*.jar"
        }
      }
      if (hasGemFireConnectorProject) {
        copy {
          from gfeConnectorProject.jar.destinationDir
          into "${snappyProductDir}/connectors"
          include "*.jar"
        }
        copy {
          from gfeFunctionProject.jar.destinationDir
          into "${snappyProductDir}/connectors"
          include "*.jar"
        }
        copy {
          from "${gemfireConnectorProject.projectDir}/examples/quickstart/data"
          into "${snappyProductDir}/connectors"
          include "persons.jar"
        }
      }
    }
    copy {
      from "${examplesProject.buildDir}/libs"
      into "${snappyProductDir}/examples/jars"
      include "${examplesProject.jar.archiveName}"
      rename { filename -> exampleArchiveName }
    }
    copy {
      from("${clusterProject.projectDir}/bin")
      into "${snappyProductDir}/bin"
    }
    copy {
      from("${clusterProject.projectDir}/sbin")
      into "${snappyProductDir}/sbin"
    }
    copy {
      from("${clusterProject.projectDir}/conf")
      into "${snappyProductDir}/conf"
    }
    copy {
      from("${examplesProject.projectDir}/quickstart")
      into "${snappyProductDir}/quickstart"
    }
    copy {
      from("${examplesProject.projectDir}/src")
      into "${snappyProductDir}/quickstart/src"
    }
    copy {
      from("${clusterProject.projectDir}/benchmark")
      into "${snappyProductDir}/benchmark"
    }
  }
}

if (rootProject.hasProperty('copyToDir')) {
  task copyProduct(type: Copy, dependsOn: product) {
    from snappyProductDir
    into copyToDir
  }
}

// TODO: right now just copying over the product contents.
// Can flip it around and let distribution do all the work.

distributions {
  main {
    baseName = 'snappydata'
    contents {
      from { snappyProductDir }
    }
  }
}

ospackage {
  packageName = distributions.main.baseName
  version = version
  release = '1'
  os = LINUX

  maintainer = vendorName
  packageDescription = productName +
      ', the Spark Database. Stream - Transact - Analyze - Predict all in one cluster'
  summary = productName + ' Installer'

  user = 'snappydata'
  permissionGroup = 'snappydata'

  from(snappyProductDir) {
    into '/opt/' + packageName
  }

  if (rootProject.hasProperty('enablePublish')) {
    signingKeyId = rootProject.property('signing.keyId')
    signingKeyPassphrase = rootProject.property('signing.password')
    signingKeyRingFile = file(rootProject.property('signing.secretKeyRingFile'))
  }

  link('/usr/sbin/snappy-start-all.sh', "/opt/${packageName}/sbin/snappy-start-all.sh")
  link('/usr/sbin/snappy-stop-all.sh', "/opt/${packageName}/sbin/snappy-stop-all.sh")
  link('/usr/sbin/snappy-status-all.sh', "/opt/${packageName}/sbin/snappy-status-all.sh")
  link('/usr/bin/snappy', "/opt/${packageName}/bin/snappy")
  link('/usr/bin/snappy-sql', "/opt/${packageName}/bin/snappy-sql")
  link('/usr/bin/snappy-job.sh', "/opt/${packageName}/bin/snappy-job.sh")
}

buildRpm {
  dependsOn ':packageVSD'
  dependsOn ':packageZeppelinInterpreter'
  requires('glibc')
  requires('bash')
  requires('perl')
  requires('curl')
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without_hadoop'
  }

  preInstall file('release/preInstallRpm.sh')
}

buildDeb {
  dependsOn ':packageVSD'
  dependsOn ':packageZeppelinInterpreter'
  requires('libc6')
  requires('bash')
  requires('perl')
  requires('curl')
  recommends('java8-sdk')
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without-hadoop'
  }

  preInstall file('release/preInstallDeb.sh')
}

distTar {
  dependsOn product
  // also package VSD
  dependsOn ':packageVSD'
  dependsOn ':packageZeppelinInterpreter'
  classifier 'bin'
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without-hadoop-bin'
  }
}

distZip {
  dependsOn product
  // also package VSD
  dependsOn ':packageVSD'
  dependsOn ':packageZeppelinInterpreter'
  classifier 'bin'
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without-hadoop-bin'
  }
}

// disable distZip by default
assemble.dependsOn.clear()
assemble.dependsOn product, distTar

task distRpm {
  dependsOn product
  dependsOn buildRpm
}

task distDeb {
  dependsOn product
  dependsOn buildDeb
}

task jdbcJar {
  dependsOn ":snappy-jdbc_${scalaBinaryVersion}:shadowJar"

  doLast {
    def jdbcProject = project(":snappy-jdbc_${scalaBinaryVersion}")
    String jdbcName = "snappydata-jdbc_${scalaBinaryVersion}-${version}.jar"
    // copy the snappy-jdbc shadow jar into distributions
    copy {
      from jdbcProject.shadowJar.destinationDir
      into "${rootProject.buildDir}/distributions"
      include jdbcProject.shadowJar.archiveName
      rename { filename -> jdbcName }
    }
  }
}

task v2ConnectorJar {
  dependsOn ":snappy-v2connector_${scalaBinaryVersion}:shadowJar"

  doLast {
    def v2ConnectorProject = project(":snappy-v2connector_${scalaBinaryVersion}")
    String v2ConnectorName = "snappydata-v2connector_${scalaBinaryVersion}-${version}.jar"
    // copy the snappy-v2connector shadow jar into distributions
    copy {
      from v2ConnectorProject.shadowJar.destinationDir
      into "${rootProject.buildDir}/distributions"
      include v2ConnectorProject.shadowJar.archiveName
      rename { filename -> v2ConnectorName }
    }
  }
}

task encodersJar {
  dependsOn ":snappy-encoders_${scalaBinaryVersion}:shadowJar"

  doLast {
    def encodersProject = project(":snappy-encoders_${scalaBinaryVersion}")
    String encodersName = "snappydata-encoders_${scalaBinaryVersion}-${version}.jar"
    // copy the snappy-encoders shadow jar into distributions
    copy {
      from encodersProject.shadowJar.destinationDir
      into "${rootProject.buildDir}/distributions"
      include encodersProject.shadowJar.archiveName
      rename { filename -> encodersName }
    }
  }
}

task copyShadowJars {
  dependsOn jdbcJar
  dependsOn v2ConnectorJar
  dependsOn encodersJar
  dependsOn ":snappy-core_${scalaBinaryVersion}:shadowJar"

  doLast {
    def coreProject = project(":snappy-core_${scalaBinaryVersion}")
    String coreName = "snappydata-core_${scalaBinaryVersion}-${version}.jar"
    copy {
      from coreProject.shadowJar.destinationDir
      into "${rootProject.buildDir}/distributions"
      include coreProject.shadowJar.archiveName
      rename { filename -> coreName }
    }
  }
}

task distInstallers {
  dependsOn product
  dependsOn buildRpm
  dependsOn buildDeb
}

// use the task below to prepare final release bits
task distProduct {
  dependsOn product, distTar, distZip dependsOn distInstallers
  dependsOn copyShadowJars
}

task generateSources {
  dependsOn ':snappy-spark:generateSources', ':snappy-store:generateSources'
  // copy all resource files into build classes path because new versions of IDEA
  // do not include separate resources path in CLASSPATH if output path has been customized
  doLast {
    subprojects.collect { proj ->
      String resourcesDir = proj.sourceSets.main.output.resourcesDir
      if (file(resourcesDir).exists()) {
        def projOutDir = file("${proj.projectDir}/src/main/scala").exists()
          ? "${proj.sourceSets.main.java.outputDir}/../../scala/main"
          : proj.sourceSets.main.java.outputDir
        copy {
          from resourcesDir
          into projOutDir
        }
      }
      resourcesDir = proj.sourceSets.test.output.resourcesDir
      if (file(resourcesDir).exists()) {
        def projOutDir = file("${proj.projectDir}/src/test/scala").exists()
          ? "${proj.sourceSets.test.java.outputDir}/../../scala/test"
          : proj.sourceSets.test.java.outputDir
        copy {
          from resourcesDir
          into projOutDir
        }
      }
    }
  }
}

task cleanAll {
  dependsOn getTasksByName('clean', true).collect { it.path }
}
task buildAll {
  dependsOn getTasksByName('assemble', true).collect { it.path }
  dependsOn getTasksByName('product', true).collect { it.path }
  dependsOn getTasksByName('testClasses', true).collect { it.path }
  mustRunAfter cleanAll
}
task buildDtests {
  dependsOn "snappy-dtests_${scalaBinaryVersion}:buildDtests"
}
task checkAll {
  dependsOn ':snappy-spark:scalaStyle'
  if (rootProject.hasProperty('store')) {
    dependsOn ':snappy-store:check'
  }
  dependsOn ":snappy-core_${scalaBinaryVersion}:check"
  if (rootProject.hasProperty('spark')) {
    dependsOn ':snappy-spark:check'
  }
  dependsOn ":snappy-cluster_${scalaBinaryVersion}:check"
  dependsOn ":snappy-examples_${scalaBinaryVersion}:check"
  if (!rootProject.hasProperty('aqp.skip') && hasAqpProject && isEnterpriseProduct) {
    dependsOn ":snappy-aqp_${scalaBinaryVersion}:check"
  }
  if (!rootProject.hasProperty('connectors.skip') && hasGemFireConnectorProject && isEnterpriseProduct) {
    dependsOn ":gemfire-connector:check"
  }
  if (!rootProject.hasProperty('smoke.skip')) {
    dependsOn buildDtests, ":snappy-dtests_${scalaBinaryVersion}:check"
  }
  if (!rootProject.hasProperty('compatibility.skip')) {
    dependsOn ":snappy-compatibility-tests_${scalaBinaryVersion}:check"
  }
  mustRunAfter buildAll
}
task allReports(type: TestReport) {
  description 'Combines the test reports.'
  dependsOn cleanAllReports
  destinationDir = file("${testResultsBase}/combined-reports")
  mustRunAfter checkAll
}
gradle.taskGraph.whenReady { graph ->
  tasks.getByName('allReports').reportOn rootProject.subprojects.collect{ it.tasks.withType(Test) }.flatten()
}

def writeProperties(def parent, def name, def comment, def propsMap) {
  parent.exists() || parent.mkdirs()
  def writer = new File(parent, name).newWriter()
  def props = new Properties()
  propsMap.each { k, v -> props.setProperty(k, v.toString()) }
  try {
    props.store(writer, comment.toString())
    writer.flush()
  } finally {
    writer.close()
  }
}

int getLast(includeTestFiles, pattern) {
  includeTestFiles.findLastIndexOf {
    File f -> f.name.indexOf(pattern) >= 0
  }
}

task packageZeppelinInterpreter { doLast {
  String zeppelinInterpreterJarName = "snappydata-zeppelin_${scalaBinaryVersion}-${zeppelinInterpreterVersion}.jar"
  String zeppelinInterpreterDir = System.env.ZEPPELIN_INTERPRETER_DIR

  if (zeppelinInterpreterDir == null || zeppelinInterpreterDir.length() == 0) {
    zeppelinInterpreterDir = "${projectDir}/../zeppelin-interpreter"
  }

  String zeppelinInterpreterLibDir = "${zeppelinInterpreterDir}/build-artifacts/libs"
  if (file(zeppelinInterpreterDir).canWrite()) {
    exec {
      executable "${zeppelinInterpreterDir}/gradlew"
      workingDir = zeppelinInterpreterDir
      args 'clean', 'product', 'distTar'
    }
    println ''
    println "Copying Zeppelin Interpreter jar from ${zeppelinInterpreterLibDir} to ${snappyProductDir}/jars"
    println ''
    copy {
      from "${zeppelinInterpreterLibDir}"
      into "${snappyProductDir}/jars"
      include "${zeppelinInterpreterJarName}"
    }
  } else {
    println "Skipping including Zeppelin Interpreter jar due to unwritable ${zeppelinInterpreterDir}"
  }
} }

task packagePulse { doLast {
  String pulseWarName = "pulse-${pulseVersion}.war"
  String pulseDir = System.env.PULSEDIR

  if (pulseDir == null || pulseDir.length() == 0) {
    pulseDir = "${projectDir}/../pulse"
  }

  String pulseDistDir = "${pulseDir}/build-artifacts/linux/dist"
  if (file(pulseDir).canWrite()) {
    exec {
      executable "${pulseDir}/build.sh"
      workingDir = pulseDir
      args 'clean', 'build-all'
    }
    delete "${snappyProductDir}/jars/pulse.war"
    println ''
    println "Copying Pulse war from ${pulseDistDir} to ${snappyProductDir}/jars"
    println ''
    copy {
      from "${pulseDir}/build-artifacts/linux/dist"
      into "${snappyProductDir}/jars"
      include "${pulseWarName}"
      rename { filename -> 'pulse.war' }
    }
  } else {
    println "Skipping Pulse due to unwritable ${pulseDir}"
  }
} }

task packageVSD { doLast {
  String thirdparty = System.env.THIRDPARTYDIR
  String vsdDir = ''

  if (thirdparty == null || thirdparty.length() == 0) {
    vsdDir = "${projectDir}/../thirdparty/vsd"
  } else {
    vsdDir = "${thirdparty}/vsd"
  }

  String vsdDistDir = "${vsdDir}/70/vsd"
  if (file(vsdDistDir).canWrite()) {
    println ''
    println "Copying VSD from ${vsdDistDir} to ${snappyProductDir}/vsd"
    println ''
    delete "${snappyProductDir}/vsd"
    copy {
      from vsdDistDir
      into "${snappyProductDir}/vsd"
    }
  } else {
    println "Skipping VSD due to unwritable ${vsdDistDir}"
  }
} }

task sparkPackage {
  dependsOn ":snappy-core_${scalaBinaryVersion}:sparkPackage"
}

packagePulse.mustRunAfter product
packageVSD.mustRunAfter product
packageZeppelinInterpreter.mustRunAfter product

distTar.mustRunAfter clean, cleanAll, product
distZip.mustRunAfter clean, cleanAll, product
distRpm.mustRunAfter clean, cleanAll, product
distDeb.mustRunAfter clean, cleanAll, product
distInstallers.mustRunAfter clean, cleanAll, product, distRpm, distDeb
distProduct.mustRunAfter clean, cleanAll, product

task deleteDocsDir(type: Delete) {
  delete "${rootProject.buildDir}/docs"
}

task docs(type: ScalaDoc) {
  apply plugin: 'scala'

  scalaDocOptions.additionalParameters = [ '-J-Xmx2g', '-J-XX:ReservedCodeCacheSize=512m', '-J-Djava.net.preferIPv4Stack=true' ]

  dependsOn deleteDocsDir
  Set<String> allSource = []
  def docProjects = rootProject.subprojects.collectMany { project ->
    if ((project.plugins.hasPlugin('scala') || project.plugins.hasPlugin('java')) &&
            // skip gemfire-connector
            !project.path.contains('gemfire-connector') &&
            // jobserver depends on Apache Spark 1.5.x which causes conflicts
            !project.path.contains('snappy-store') &&
            !project.name.contains('jobserver') &&
            // below three will get filtered with the snappy-store path check itself
            // but still keeping it as when we would remove the snappy-store path filter
            // still the below three sub prejects should not be built.
            !project.name.contains('jgroups') &&
            !project.name.contains('gemfire-examples') &&
            !project.name.contains('trove') &&
            !project.name.contains('kafka') &&
            // exclude tests
            !project.name.contains('tests')) {
      allSource.addAll(project.sourceSets.main.allJava.findAll {
        !it.getPath().matches('.*/internal/.*') && !it.getPath().contains('com/gemstone/gemfire/cache/operations/')
      })

      if (project.plugins.hasPlugin('scala')) {
        allSource.addAll(project.sourceSets.main.allScala.findAll {
          !it.getPath().matches('.*org/apache/spark/sql/execution/joins/HashedRelation.*') &&
          !it.getPath().matches('.*org/apache/spark/sql/execution/debug/package.*')
        })
      }
      [ project ]
    } else []
  }
  source = allSource
  classpath = files(docProjects.collect { project ->
   project.sourceSets.main.compileClasspath
  })
  destinationDir = file("${rootProject.buildDir}/docs")
}
task publishDocs(type:Exec) {
  dependsOn docs
  //on linux
  commandLine './publish-site.sh'
}

// It runs test script from product dir. Hence if running the target individually make sure
// to run product target first
task checkPython(type:Exec) {
  String wdir = "${testResultsBase}/python"
  delete wdir
  file(wdir).mkdirs()
  workingDir  wdir
  copy {
    from "${rootDir}/python/pyspark/test_supprt"
    into wdir
  }
  environment 'PYTHONPATH', "${snappyProductDir}/python/lib/py4j-0.10.4-src.zip:${snappyProductDir}/python"
  environment 'SPARK_HOME', "${snappyProductDir}"
  commandLine 'python', '-u', "${rootDir}/python/run-snappy-tests.py"
}

task precheckin {
  dependsOn cleanAll, buildAll, checkAll, allReports, docs
}

if (rootProject.hasProperty('trackBuildTime') ) {
  buildtimetracker {
    reporters {
      summary {
            ordered false
            threshold 5000
            barstyle "unicode"
      }
    }
  }
}

// log build output to buildOutput.log in addition to console output

def buildOutput = new File("${rootDir}/buildOutput.log")
// delete build output file if it has become large
if (buildOutput.length() > 1000000) {
  delete buildOutput
}
def gradleLogger = new org.gradle.api.logging.StandardOutputListener() {
  void onOutput(CharSequence output) {
    buildOutput << output
  }
}
def loggerService = gradle.services.get(LoggingOutputInternal)
loggerService.addStandardOutputListener(gradleLogger)
loggerService.addStandardErrorListener(gradleLogger)

println()
println('-------------------------------------------------')
println("Starting new build on ${buildDate}")
println('-------------------------------------------------')
println()
