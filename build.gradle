import groovy.json.JsonSlurper

apply plugin: 'wrapper'
apply plugin: 'distribution'

if (JavaVersion.current().isJava8Compatible()) {
  allprojects {
    tasks.withType(Javadoc) {
      options.addStringOption('Xdoclint:none', '-quiet')
    }
  }
}

buildscript {
  repositories {
    maven { url "https://plugins.gradle.org/m2" }
    mavenCentral()
  }
  dependencies {
    classpath "com.github.maiflai:gradle-scalatest:0.10"
    classpath "org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.10:0.8.2"
    classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.3'
  }
}

allprojects {
  // We want to see all test results.  This is equivalatent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  repositories {
    mavenCentral()
    maven { url "https://oss.sonatype.org/content/repositories/snapshots" }
    maven { url "http://repository.snappydata.io:8089/repository/internal" }
    maven { url "https://repository.apache.org/content/repositories/releases" }
    maven { url "https://repository.jboss.org/nexus/content/repositories/releases" }
    maven { url "https://repo.eclipse.org/content/repositories/paho-releases" }
    maven { url "https://repository.cloudera.com/artifactory/cloudera-repos" }
    maven { url "https://oss.sonatype.org/content/repositories/orgspark-project-1113" }
    maven { url "http://repository.mapr.com/maven" }
    maven { url "https://repo.spring.io/libs-release" }
    maven { url "http://maven.twttr.com" }
    maven { url "http://repository.apache.org/snapshots" }
  }

  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'
  apply plugin: 'idea'
  apply plugin: 'eclipse'

  group = 'io.snappydata'
  version = '0.2.1-PREVIEW'

  // apply compiler options
  compileJava.options.encoding = 'UTF-8'
  compileJava.options.compilerArgs << '-Xlint:all,-serial,-path'
  javadoc.options.charSet = 'UTF-8'

  gradle.taskGraph.whenReady( { graph ->
    tasks.withType(Tar).each { tar ->
      tar.compression = Compression.GZIP
      tar.extension = 'tar.gz'
    }
  })

  ext {
    scalaBinaryVersion = '2.10'
    scalaVersion = scalaBinaryVersion + '.6'
    sparkVersion = '1.6.0-BETA'
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.12'
    junitVersion = '4.11'
    hadoopVersion = '2.4.1'
    gemfireXDVersion = '1.5.0-BETA'
    buildFlags = ''
    createdBy = System.getProperty("user.name")
  }

  if (!buildRoot.isEmpty()) {
    buildDir = new File(buildRoot, 'scala-' + scalaBinaryVersion + '/' +  project.path.replace(':', '/'))
  } else {
    // default output directory like in sbt/maven
    buildDir = 'build-artifacts/scala-' + scalaBinaryVersion
  }

  ext {
    testResultsBase = "${rootProject.buildDir}/tests/snappy"
    snappyProductDir = "${rootProject.buildDir}/snappy"
  }
}

def getProcessId() {
  def name = java.lang.management.ManagementFactory.getRuntimeMXBean().getName()
  return name[0..name.indexOf("@") - 1]
}

def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

// Skip snappy-spark, snappy-aqp and spark-jobserver that have their own
// scalaStyle configuration. Skip snappy-store that will not use it.
configure(subprojects.findAll {!(it.name ==~ /snappy-spark.*/ ||
      it.name ==~ /snappy-store.*/ ||
      it.name ==~ /snappy-aqp.*/ ||
      it.name ==~ /spark-jobserver.*/)}) {
  scalaStyle {
    configLocation = "${rootProject.projectDir}/scalastyle-config.xml"
    inputEncoding = 'UTF-8'
    outputEncoding = 'UTF-8'
    outputFile = "${buildDir}/scalastyle-output.xml"
    includeTestSourceDirectory = false
    source = 'src/main/scala'
    testSource = 'src/test/scala'
    failOnViolation = true
    failOnWarning = false
  }
}

def cleanIntermediateFiles(def projectName) {
  def projDir = "${project(projectName).projectDir}"
  delete "${projDir}/metastore_db"
  delete "${projDir}/warehouse"
  delete "${projDir}/datadictionary"
  delete fileTree(projDir) {
    include 'BACKUPGFXD-DEFAULT-DISKSTORE**', 'locator*.dat'
  }
}
task cleanScalaTest << {
  def workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanJUnit << {
  def workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanDUnit << {
  def workingDir = "${testResultsBase}/dunit"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanAllReports << {
  def workingDir = "${testResultsBase}/combined-reports"
  delete workingDir
  file(workingDir).mkdirs()
}

task publishDocs(type:Exec) {
  dependsOn ':snappy-tools_' + scalaBinaryVersion + ':docs'
  //on linux
  commandLine './publish-site.sh'
}

subprojects {
  // the run task for a selected sub-project
  task run(type:JavaExec) {
    if (!project.hasProperty('mainClass')) {
      main = 'io.snappydata.app.SparkSQLTest'
    } else {
      main = mainClass
    }
    if (project.hasProperty('params')) {
      args = params.split(",") as List
    }
    classpath = sourceSets.main.runtimeClasspath + sourceSets.test.runtimeClasspath
    jvmArgs '-Xmx2g', '-XX:MaxPermSize=512m'
  }

  task scalaTest(type: Test) {
    actions = [ new com.github.maiflai.ScalaTestAction() ]
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    maxHeapSize '4g'
    jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:+CMSClassUnloadingEnabled',  '-XX:MaxPermSize=512m', '-ea'
    testLogging.exceptionFormat = 'full'

    List<String> suites = []
    extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
    extensions.add("suite", { String name -> suites.add(name) } )
    extensions.add("suites", { String... name -> suites.addAll(name) } )

    // running a single scala suite
    if (rootProject.hasProperty('singleSuite')) {
      suite singleSuite
    }
    workingDir = "${testResultsBase}/scalatest"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test {
    maxParallelForks = (2 * Runtime.getRuntime().availableProcessors())
    maxHeapSize '2g'
    jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:+CMSClassUnloadingEnabled',  '-XX:MaxPermSize=384m', '-ea'
    testLogging.exceptionFormat = 'full'

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }

  gradle.taskGraph.whenReady({ graph ->
    tasks.withType(Jar).each { pack ->
      if (pack.name == 'packageTests') {
        pack.from(pack.project.sourceSets.test.output.classesDir)
      }
    }
    tasks.withType(Test).each { test ->
      test.configure {
        environment 'SNAPPY_HOME': snappyProductDir,
          'SNAPPY_DIST_CLASSPATH': "${sourceSets.test.runtimeClasspath.asPath}"

        def eol = System.getProperty('line.separator')
        beforeTest { desc ->
          def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
          def progress = new File(workingDir, "progress.txt")
          def output = new File(workingDir, "output.txt")
          progress << "${now} Starting test ${desc.className} ${desc.name}${eol}"
          output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
        }
        onOutput { desc, event ->
          def output = new File(workingDir, "output.txt")
          def msg = event.message
          if (event.destination.toString() == 'StdErr') {
            msg = msg.replace("\n", "\n[error]  ")
          }
          output << msg
        }
        afterTest { desc, result ->
          def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
          def progress = new File(workingDir, "progress.txt")
          def output = new File(workingDir, "output.txt")
          progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
          output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
          result.exceptions.each { t ->
            progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
            output << "${getStackTrace(t)}${eol}"
          }
        }
      }
    }
  })

  // apply default manifest
  if (rootProject.hasProperty('enablePublish')) {
    createdBy = "SnappyData Build Team"
  }
  jar {
    manifest {
      attributes(
        "Manifest-Version"  : "1.0",
        "Created-By"        : createdBy,
        "Title"             : rootProject.name,
        "Version"           : version,
        "Vendor"            : "SnappyData, Inc."
      )
    }
  }

  configurations {
    provided {
      description 'a dependency that is provided externally at runtime'
      visible true
    }

    nospark {
      description 'a dependency for creating jar without spark dependencies'
      visible true
      transitive true
    }

    testOutput {
      extendsFrom testCompile
      description 'a dependency that exposes test artifacts'
    }
    /*
    all {
      resolutionStrategy {
        // fail eagerly on version conflict (includes transitive dependencies)
        // e.g. multiple different versions of the same dependency (group and name are equal)
        failOnVersionConflict()
      }
    }
    */
  }

  task packageTests(type: Jar, dependsOn: testClasses) {
    description 'Assembles a jar archive of test classes.'
    classifier = 'tests'
  }
  artifacts {
    testOutput packageTests
  }

  idea {
    module {
      scopes.PROVIDED.plus += [ configurations.provided ]
    }
  }

  sourceSets {
    main.compileClasspath += configurations.provided
    main.runtimeClasspath -= configurations.provided
    test.compileClasspath += configurations.provided
    test.runtimeClasspath += configurations.provided
  }

  javadoc.classpath += configurations.provided

  dependencies {
    compile 'log4j:log4j:' + log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
  }
}

// maven publish tasks
subprojects {

  apply plugin: 'signing'

  task packageSources(type: Jar, dependsOn: classes) {
    classifier = 'sources'
    from sourceSets.main.allSource
  }
  task packageDocs(type: Jar, dependsOn: javadoc) {
    classifier = 'javadoc'
    from javadoc
  }
  if (rootProject.hasProperty('enablePublish')) {
    signing {
      sign configurations.archives
    }

    uploadArchives {
      repositories {
        mavenDeployer {
          beforeDeployment { MavenDeployment deployment -> signing.signPom(deployment) }

          repository(url: "https://oss.sonatype.org/service/local/staging/deploy/maven2/") {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }
          snapshotRepository(url: "https://oss.sonatype.org/content/repositories/snapshots/") {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }

          pom.project {
            name 'SnappyData'
            packaging 'jar'
            // optionally artifactId can be defined here
            description 'SnappyData distributed data store and execution engine'
            url 'http://www.snappydata.io'

            scm {
              connection 'scm:git:https://github.com/SnappyDataInc/snappydata.git'
              developerConnection 'scm:git:https://github.com/SnappyDataInc/snappydata.git'
              url 'https://github.com/SnappyDataInc/snappydata'
            }

            licenses {
              license {
                name 'The Apache License, Version 2.0'
                url 'http://www.apache.org/licenses/LICENSE-2.0.txt'
              }
            }

            developers {
              developer {
                id 'smenon'
                name 'Sudhir Menon'
                email 'smenon@snappydata.io'
              }
            }
          }
        }
      }
    }
  }
}

task publishLocal {
  dependsOn subprojects.findAll { p -> p.name != 'gemfirexd-native' &&
    p.name != 'gemfirexd-prebuild' && p.name != 'gemfirexd' &&
    p.name != 'gemfirexd-tests' }.collect {
    it.getTasksByName('install', false).collect { it.path }
  }
}

task publishMaven {
  dependsOn subprojects.findAll { p -> p.name != 'gemfirexd-native' &&
    p.name != 'gemfirexd-prebuild' && p.name != 'snappy-store' &&
    p.name != 'gemfirexd' && p.name != 'gemfirexd-tests' }.collect {
    it.getTasksByName('uploadArchives', false).collect { it.path }
  }
}


task generateSources {
  dependsOn ':snappy-spark:snappy-spark-streaming-flume-sink_' + scalaBinaryVersion + ':generateAvroJava'
  dependsOn ':snappy-store:generateSources'
}

task product {
  dependsOn ":snappy-tools_${scalaBinaryVersion}:shadowJar"
  dependsOn ':snappy-examples:jar'
  if (new File(rootDir, "snappy-aqp/build.gradle").exists()) {
    dependsOn ':snappy-aqp:jar'
  }

  doFirst {
    delete snappyProductDir
    file("${snappyProductDir}/lib").mkdirs()
  }
  doLast {
    // create the RELEASE file
    def release = file("${snappyProductDir}/RELEASE")
    def gitCommitId = "git rev-parse HEAD".execute().text.trim()
    release << "SnappyData ${project.version} ${gitCommitId} built for Hadoop $hadoopVersion\n"
    release << "Build flags: ${buildFlags}\n"

    // copy LICENSE, README.md and doc files
    copy {
      from projectDir
      into snappyProductDir
      include 'LICENSE'
      include 'README.md'
    }
    copy {
      from "${projectDir}/docs"
      into "${snappyProductDir}/docs"
    }

    def toolsProject = project(":snappy-tools_${scalaBinaryVersion}")
    def examplesProject = project(':snappy-examples')
    def baseName = 'snappydata-assembly'
    def archiveName = "${baseName}_${scalaBinaryVersion}-${version}-hadoop${hadoopVersion}.jar"
    def exampleArchiveName = "quickstart-${version}.jar"
    copy {
      from("${toolsProject.buildDir}/libs")
      into "${snappyProductDir}/lib"
      include "${toolsProject.shadowJar.archiveName}"
      rename { filename -> archiveName }
    }
    // copy datanucleus jars specifically since they don't work as part of fat jar
    // copy hbase jar as required for GFXD HDFS support (needs to be explicitly added to SPARK_DIST_CLASSPATH)
    // copy bin, sbin, data etc from spark
    if (new File(rootDir, "snappy-spark/build.gradle").exists()) {
      copy {
        from project(":snappy-spark:snappy-spark-hive_${scalaBinaryVersion}").configurations.runtime.filter {
          it.getName().contains('datanucleus')
        }
        from project(":snappy-store:gemfire-core").configurations.provided.filter {
          it.getName().contains('hbase')
        }
        into "${snappyProductDir}/lib"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/bin"
        into "${snappyProductDir}/bin"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/sbin"
        into "${snappyProductDir}/sbin"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/conf"
        into "${snappyProductDir}/conf"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/python"
        into "${snappyProductDir}/python"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/data"
        into "${snappyProductDir}/data"
      }

      def sparkR = "${project(':snappy-spark').projectDir}/R/lib/SparkR"
      if (file(sparkR).exists()) {
        copy {
          from sparkR
          into "${snappyProductDir}/R/lib"
        }
      }
    } else {
      copy {
        from project(':snappy-examples').configurations.testRuntime.filter {
          it.getName().contains('datanucleus')
        }
        into "${snappyProductDir}/lib"
      }
      def snappySparkJar = project(':snappy-examples').configurations.testRuntime.findResult {
        it.getName().startsWith("snappy-spark-${sparkVersion}") ? it.getPath() : null
      }
      if (snappySparkJar != null) {
        copy {
          from zipTree(snappySparkJar)
          into snappyProductDir
          include 'bin/**'
          include 'sbin/**'
          include 'conf/**'
          include 'python/**'
          include 'data/**'
          include 'R/**'
        }
      }
    }
    // copy GemFireXD shared libraries for optimized JNI calls
    if (new File(rootDir, "snappy-store/build.gradle").exists()) {
      copy {
        from "${project(':snappy-store:gemfirexd-core').projectDir}/lib"
        into "${snappyProductDir}/lib"
      }
    }
    // copy AQP jar
    if (new File(rootDir, "snappy-aqp/build.gradle").exists()) {
      def aqpProject = project(':snappy-aqp')
      copy {
        from("${aqpProject.buildDir}/libs")
        into "${snappyProductDir}/lib"
        include "${aqpProject.jar.archiveName}"
      }
    } else {
      copy {
        from project(':snappy-examples').configurations.testRuntime.filter {
          it.getName().contains('snappy-aqp')
        }
        into "${snappyProductDir}/lib"
      }
    }
    copy {
      from "${examplesProject.buildDir}/libs"
      into "${snappyProductDir}/lib"
      include "${examplesProject.jar.archiveName}"
      rename { filename -> exampleArchiveName }
    }
    copy {
      from("${toolsProject.projectDir}/bin")
      into "${snappyProductDir}/bin"
    }
    copy {
      from("${toolsProject.projectDir}/sbin")
      into "${snappyProductDir}/sbin"
    }
    copy {
      from("${toolsProject.projectDir}/conf")
      into "${snappyProductDir}/conf"
    }
    copy {
      from("${examplesProject.projectDir}/quickstart")
      into "${snappyProductDir}/quickstart"
    }
  }
}

// TODO: right now just copying over the product contents.
// Can flip it around and let distribution do all the work.

distributions {
  main {
    baseName = 'snappydata'
    contents {
      from { snappyProductDir }
    }
  }
}
distTar {
  dependsOn product
  classifier 'bin'
}
distZip {
  dependsOn product
  classifier 'bin'
}

def copyTestsCommonResources(def bdir) {
  def outdir = "${bdir}/resources/test"
  file(outdir).mkdirs()

  copy {
    from "${rootDir}/tests-common/src/main/resources"
    into outdir
  }
}

def runScript(def execName, def param) {
def stdout = new ByteArrayOutputStream()
    exec{
      executable "${execName}"
      args  (param)
      standardOutput = stdout;
    }
  return "$stdout"
}

def runSQLScript(def fileName) {
  println("Executing ${fileName}")
  def queryoutput = runScript("${snappyProductDir}/bin/snappy-shell",
          ["run", "-file=${snappyProductDir}/quickstart/scripts/${fileName}"]);
  println "${queryoutput}"
  if (queryoutput.contains("ERROR") || queryoutput.contains("Failed")) {
    throw new GradleException("Failed to run ${fileName}")
  }
}
def writeToFile(def fileName) {
  new File("$fileName").withWriter { out ->
    out.println "localhost"
    out.println "localhost"
  }
}

def runSubmitQuery(def jobName, def appName) {
  println "Running job $jobName"
  def exampleArchiveName = "quickstart-${version}.jar"
  def submitjobairline = runScript("${snappyProductDir}/bin/snappy-job.sh",
      ["submit", "--lead", "localhost:8090", "--app-name", appName + System.currentTimeMillis(),
       "--class", jobName, "--app-jar",
       "${snappyProductDir}/lib/${exampleArchiveName}"]);

  println "${submitjobairline}"

  def jsonStr = (submitjobairline.charAt(2) == '{') ? submitjobairline.substring(2) :
      submitjobairline.substring(4)
  def json = new JsonSlurper().parseText(jsonStr)
  def jobid = ""
  json.each {
    k, v ->
      if (k == "result") {
        if (v instanceof groovy.json.internal.LazyMap) {
          jobid = v.get("jobId")
        }
      }
  }

  def status = "RUNNING"
  while (status == "RUNNING") {
    Thread.sleep(3000)
    def statusjobairline = runScript("${snappyProductDir}/bin/snappy-job.sh",
        ["status", "--lead", "localhost:8090", "--job-id", "${jobid}"]);
    println "${statusjobairline}"

    def statusjson = new JsonSlurper().parseText("${statusjobairline}")
    statusjson.each {
      k, v ->
        if (k == "status") {
          println("Current status of job: " + v)
          status = v
        }
    }
  }
  if (status == "ERROR") {
    throw new GradleException("Failed to submit queries")
  }
}

task runQuickstart(dependsOn: product) {
  mustRunAfter 'buildAll'
  def exampleArchiveName = "quickstart-${version}.jar"
  doLast {
    try {
      writeToFile("${snappyProductDir}/conf/servers")
      def startoutput = runScript("${snappyProductDir}/sbin/snappy-start-all.sh", []);
      println "${startoutput}"
      if (!startoutput.contains("Distributed system now has 4 members")) {
        throw new GradleException("Failed to start Snappy cluster")
      }
      runSQLScript("create_and_load_column_table.sql")

      runSQLScript("create_and_load_row_table.sql")
      
      runSQLScript("create_and_load_sample_table.sql")

      runSQLScript("status_queries.sql")

      runSQLScript("olap_queries.sql")

      runSQLScript("oltp_queries.sql")

      runSQLScript("olap_queries.sql")
      
      runSQLScript("olap_approx_queries.sql")

      runSubmitQuery("io.snappydata.examples.CreateAndLoadAirlineDataJob", "createJob")

      runSubmitQuery("io.snappydata.examples.AirlineDataJob", "processjob")

      def startSparkResult = runScript("${snappyProductDir}/sbin/start-all.sh", [])
      println "${startSparkResult}"

      def airlineappresult = runScript("${snappyProductDir}/bin/spark-submit",
              ["--class", "io.snappydata.examples.AirlineDataSparkApp", "--master", "spark://localhost:7077",
               "--conf", "snappydata.store.locators=localhost:10334", "--conf", "spark.ui.port=4041",
               "${snappyProductDir}/lib/${exampleArchiveName}"]);

      println "${airlineappresult}"
      if (airlineappresult.toLowerCase().contains("exception")) {
        throw new GradleException("Failed to submit AirlineDataSparkApp")
      }

    } finally {
      println runScript("${snappyProductDir}/sbin/snappy-stop-all.sh", []);
      println runScript("${snappyProductDir}/sbin/stop-all.sh", []);
      def conffile = new File("${snappyProductDir}/conf/servers")
      if (conffile.exists())
        conffile.delete()
    }
  }
}

task copyResourcesAll << {
  copyTestsCommonResources(project(":snappy-core_${scalaBinaryVersion}").buildDir)
  copyTestsCommonResources(project(":snappy-tools_${scalaBinaryVersion}").buildDir)
  copyTestsCommonResources(project(':snappy-dunits').buildDir)
  if (new File(rootDir, "snappy-aqp/build.gradle").exists()) {
    copyTestsCommonResources(project(":snappy-aqp").buildDir)
  }
}


task cleanAll {
  dependsOn getTasksByName('clean', true).collect { it.path }
}
task buildAll {
  dependsOn getTasksByName('assemble', true).collect { it.path }
  dependsOn getTasksByName('testClasses', true).collect { it.path }
  dependsOn distTar, distZip
  mustRunAfter cleanAll
}
task checkAll {
  dependsOn  ":snappy-core_${scalaBinaryVersion}:check", ":snappy-tools_${scalaBinaryVersion}:check", ':snappy-dunits:test'
  if (new File(rootDir, "snappy-aqp/build.gradle").exists()) {
    dependsOn ':snappy-aqp:check'
  }
  if (project.hasProperty('spark')) {
    dependsOn ':snappy-spark:check'
  }
  if (project.hasProperty('store')) {
    dependsOn ':snappy-store:check'
  }
  mustRunAfter buildAll
}
task allReports(type: TestReport) {
  description 'Combines the test reports.'
  dependsOn cleanAllReports
  destinationDir = file("${testResultsBase}/combined-reports")
  mustRunAfter checkAll
}
gradle.taskGraph.whenReady({ graph ->
  tasks.getByName('allReports').reportOn rootProject.subprojects.collect{ it.tasks.withType(Test) }.flatten()
})


task precheckin {
  dependsOn cleanAll, buildAll, checkAll, allReports, runQuickstart
  dependsOn ':snappy-spark:scalaStyle', ":snappy-tools_${scalaBinaryVersion}:docs"
}
