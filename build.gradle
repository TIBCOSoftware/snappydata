/*
 * Copyright (c) 2017-2019 TIBCO Software Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */

import org.gradle.api.tasks.testing.logging.*
import org.gradle.internal.logging.*

buildscript {
  repositories {
    maven { url 'https://plugins.gradle.org/m2' }
    mavenCentral()
  }
  dependencies {
    classpath 'io.snappydata:gradle-scalatest:0.25'
    classpath 'org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.11:0.9.0'
    classpath 'com.github.jengelman.gradle.plugins:shadow:5.2.0'
    classpath 'de.undercouch:gradle-download-task:3.4.3'
    classpath 'net.rdrei.android.buildtimetracker:gradle-plugin:0.11.+'
    classpath 'com.netflix.nebula:gradle-ospackage-plugin:5.2.+'
    // classpath 'org.owasp:dependency-check-gradle:6.5.3'
  }
}

apply plugin: 'wrapper'
apply plugin: 'distribution'
apply plugin: 'nebula.ospackage-base'
apply plugin: "nebula.ospackage"
// apply plugin: 'org.owasp.dependencycheck'

// def isEnterpriseProduct = rootProject.hasProperty('snappydata.enterprise')

allprojects {
  // We want to see all test results.  This is equivalent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  tasks.withType(Javadoc) {
    options.addStringOption('Xdoclint:none', '-quiet')
    /*
    if (javax.tools.ToolProvider.getSystemDocumentationTool().isSupportedOption("--allow-script-in-comments") == 0) {
      options.addBooleanOption("-allow-script-in-comments", true)
    }
    */
  }

  repositories {
    mavenCentral()
    maven { url 'https://packages.atlassian.com/maven-3rdparty' }
    maven { url 'https://repository.cloudera.com/artifactory/cloudera-repos' }
    maven { url 'https://app.camunda.com/nexus/content/repositories/public' }
    maven { url "https://repo.spring.io/libs-release" }
  }

  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'
  apply plugin: 'com.github.johnrengelman.shadow'
  apply plugin: 'idea'
  apply plugin: "build-time-tracker"

  group = 'io.snappydata'
  version = '1.3.1'

  // apply compiler options
  tasks.withType(JavaCompile) {
    options.encoding = 'UTF-8'
    options.incremental = true
    options.compilerArgs << '-Xlint:-serial,-path,-deprecation,-unchecked,-rawtypes'
    options.compilerArgs << '-XDignore.symbol.file'
    options.fork = true
    options.forkOptions.javaHome = file(System.properties['java.home'])
    options.forkOptions.jvmArgs = [ '-J-Xmx2g', '-J-Xms2g', '-J-XX:ReservedCodeCacheSize=512m', '-J-Djava.net.preferIPv4Stack=true' ]
  }
  tasks.withType(ScalaCompile) {
    options.fork = true
    options.forkOptions.jvmArgs = [ '-Xmx2g', '-Xms2g', '-XX:ReservedCodeCacheSize=512m', '-Djava.net.preferIPv4Stack=true' ]
    // scalaCompileOptions.optimize = true
    // scalaCompileOptions.useAnt = false
    scalaCompileOptions.deprecation = false
    scalaCompileOptions.additionalParameters = [ '-feature' ]
    options.encoding = 'UTF-8'
  }

  jar.duplicatesStrategy = DuplicatesStrategy.EXCLUDE

  javadoc.options.charSet = 'UTF-8'

  ext {
    productName = 'SnappyData'
    vendorName = 'TIBCO Software Inc.'
    scalaBinaryVersion = '2.11'
    scalaVersion = scalaBinaryVersion + '.8'
    sparkVersion = '2.1.3'
    snappySparkVersion = '2.1.3.2'
    sparkDistName = "spark-${sparkVersion}-bin-hadoop2.7"
    sparkCurrentVersion = '2.4.8'
    sparkCurrentDistName = "spark-${sparkCurrentVersion}-bin-hadoop2.7"
    sparkJobServerVersion = '0.6.2.12'
    snappySparkMetricsLibVersion = '2.0.0.1'
    log4j2Version = '2.17.1'
    slf4jVersion = '1.7.32'
    junitVersion = '4.12'
    mockitoVersion = '1.10.19'
    hadoopVersion = '3.2.0'
    awsSdkVersion = '1.11.375'
    gcsHadoop3ConnectorVersion = 'hadoop3-2.1.2'
    sparkAvroVersion = '4.0.0'
    sparkXmlVersion = '0.4.1'
    scalatestVersion = '2.2.6'
    py4jVersion = '0.10.7'
    jettyVersion = '9.4.44.v20210927'
    guavaVersion = '14.0.1'
    fastutilVersion = '8.5.6'
    kryoVersion = '4.0.1'
    thriftVersion = '0.9.3'
    jacksonVersion = '2.13.1'
    jacksonDatabindVersion = '2.13.1'
    hiveVersion = '1.21.2.7.0.3.2-3'
    metricsVersion = '4.0.3'
    metrics2Version = '2.2.0'
    janinoVersion = '3.0.8'
    derbyVersion = '10.14.2.0'
    parboiledVersion = '2.1.8'
    tomcatJdbcVersion = '10.0.10'
    hikariCPVersion = '2.7.9'
    twitter4jVersion = '4.0.7'
    objenesisVersion = '3.0.1'
    rabbitMqVersion = '4.9.1'
    akkaVersion = '2.3.16'
    sprayVersion = '1.3.4'
    sprayJsonVersion = '1.3.5'
    sprayShapelessVersion = '1.3.3'
    sprayTestkitVersion = '1.3.4'
    jodaVersion = '2.1.2'
    jodaTimeVersion = '2.10.1'
    slickVersion = '2.1.0'
    h2Version = '1.3.176'
    commonsIoVersion = '2.6'
    commonsPoolVersion = '1.6'
    dbcpVersion = '1.4'
    shiroVersion = '1.2.6'
    flywayVersion = '3.2.1'
    typesafeConfigVersion = '1.3.3'
    mssqlVersion = '7.0.0.jre8'
    antlr2Version = '2.7.7'
    eclipseCollectionsVersion = '10.4.0'

    pegdownVersion = '1.6.0'
    snappyStoreVersion = '1.6.6'
    snappydataVersion = version
    zeppelinInterpreterVersion = '0.8.2.1'

    buildFlags = ''
    createdBy = System.getProperty('user.name')
    osArch = System.getProperty('os.arch')
    osName = org.gradle.internal.os.OperatingSystem.current()
    osFamilyName = osName.getFamilyName().replace(' ', '').toLowerCase()
    osVersion = System.getProperty('os.version')
    buildDate = new Date().format('yyyy-MM-dd HH:mm:ss Z')
    buildDateShort = ''
    if (rootProject.hasProperty('withDate')) {
      buildDateShort = "${new Date().format('yyyyMMdd')}_"
    }
    devEdition = ''
    if (rootProject.hasProperty('dev')) {
      devEdition = "-dev"
    }
    buildNumber = new Date().format('MMddyy')
    jdkVersion = System.getProperty('java.version')

    gitCmd = "git --git-dir=${rootDir}/.git --work-tree=${rootDir}"
    gitBranch = "${gitCmd} rev-parse --abbrev-ref HEAD".execute().text.trim()
    commitId = "${gitCmd} rev-parse HEAD".execute().text.trim()
    sourceDate = "${gitCmd} log -n 1 --format=%ai".execute().text.trim()
    buildIdPrefix = System.env.USER + ' '

    sparkDistDir = "${project.gradle.gradleUserHomeDir}/sparkDist"
    sparkProductDir = "${sparkDistDir}/${sparkDistName}"
    sparkCurrentProductDir = "${sparkDistDir}/${sparkCurrentDistName}"
  }

  if (!buildRoot.isEmpty()) {
    buildDir = new File(buildRoot, 'scala-' + scalaBinaryVersion + '/' +  project.path.replace(':', '/'))
  } else {
    // default output directory suffix like in sbt/maven
    buildDir = 'build-artifacts/scala-' + scalaBinaryVersion
  }
  if (rootProject.hasProperty('enablePublish')) {
    buildIdPrefix = "${vendorName} "
  }
  if (rootProject.hasProperty('sparkDistDir')) {
    sparkDistDir = rootProject.property('sparkDistDir')
    sparkProductDir = "${sparkDistDir}/${sparkDistName}"
    sparkCurrentProductDir = "${sparkDistDir}/${sparkCurrentDistName}"
  }

  ext {
    testResultsBase = "${rootProject.buildDir}/tests/snappy"
    snappyProductDir = "${rootProject.buildDir}/snappy"
  }

  // force same output directory for IDEA and gradle
  idea {
    module {
      def projOutDir = file("${projectDir}/src/main/scala").exists()
        ? "${project.sourceSets.main.java.outputDir}/../../scala/main"
        : project.sourceSets.main.java.outputDir
      def projTestOutDir = file("${projectDir}/src/test/scala").exists()
        ? "${project.sourceSets.test.java.outputDir}/../../scala/test"
        : project.sourceSets.test.java.outputDir
      outputDir file(projOutDir)
      testOutputDir file(projTestOutDir)
    }
  }
}


def hasAqpProject = new File(rootDir, 'aqp/build.gradle').exists()
def aqpProject = project(":snappy-aqp_${scalaBinaryVersion}")
def hasJdbcConnectorProject = new File(rootDir, 'snappy-connectors/jdbc-stream-connector/build.gradle').exists()
def hasGemFireConnectorProject = new File(rootDir, 'snappy-connectors/gemfire-connector/build.gradle').exists()

if (!hasAqpProject) {
  throw new GradleException('Project repository snappy-aqp not found under aqp/.')
}

if (!hasJdbcConnectorProject) {
  throw new GradleException('Project JDBC Stream Connector inside repository snappy-connectors not found.')
}

// set python2 for pyspark if python3 version is an unsupported one
String sparkPython = 'python'
def checkResult = exec {
  ignoreExitValue = true
  commandLine 'sh', '-c', 'python --version 2>/dev/null | grep -Eq "( 3\\.[0-7])|( 2\\.)"'
}
if (checkResult.exitValue != 0) {
  checkResult = exec {
    ignoreExitValue = true
    commandLine 'sh', '-c', 'python2 --version >/dev/null 2>&1'
  }
  if (checkResult.exitValue == 0) {
    sparkPython = 'python2'
  }
}

static def getProcessId() {
  String name = java.lang.management.ManagementFactory.getRuntimeMXBean().getName()
  return name[0..name.indexOf('@') - 1]
}

static def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

// Skip snappy-spark and spark-jobserver that have their own
// scalaStyle configuration. Skip snappy-store that will not use it.
configure(subprojects.findAll {!(it.name ==~ /snappy-spark.*/ ||
      it.name ==~ /snappy-store.*/ ||
      it.name ==~ /spark-jobserver.*/)}) {
  scalaStyle {
    configLocation = "${rootProject.projectDir}/scalastyle-config.xml"
    inputEncoding = 'UTF-8'
    outputEncoding = 'UTF-8'
    outputFile = "${buildDir}/scalastyle-output.xml"
    includeTestSourceDirectory = true
    source = 'src/main/scala'
    testSource = 'src/test/scala'
    failOnViolation = true
    failOnWarning = false
  }
}

def cleanIntermediateFiles(String projectName) {
  def projDir = "${project(projectName).projectDir}"
  delete "${projDir}/metastore_db"
  delete "${projDir}/warehouse"
  delete "${projDir}/datadictionary"
  delete fileTree(projDir) {
    include 'BACKUPGFXD-DEFAULT-DISKSTORE**', 'locator*.dat'
  }
}

static def now() {
  return new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
}

task cleanScalaTest { doLast {
  String workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanJUnit { doLast {
  String workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanRecoveryTest { doLast {
  String workingDir = "${testResultsBase}/recoveryTest"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanDUnit { doLast {
  String workingDir = "${testResultsBase}/dunit"
  delete workingDir
  file(workingDir).mkdirs()
  // clean spark cluster directories
  delete "${snappyProductDir}/work", "${snappyProductDir}/logs"
  delete "${sparkProductDir}/work", "${sparkProductDir}/logs"
  delete "${sparkCurrentProductDir}/work", "${sparkCurrentProductDir}/logs"
} }
task cleanSecurityDUnit { doLast {
  String workingDir = "${testResultsBase}/dunit-security"
  delete workingDir
  file(workingDir).mkdirs()
  // clean spark cluster directories
  delete "${snappyProductDir}/work", "${snappyProductDir}/logs"
  delete "${sparkProductDir}/work", "${sparkProductDir}/logs"
  delete "${sparkCurrentProductDir}/work", "${sparkCurrentProductDir}/logs"
} }
task cleanAllReports { doLast {
  String workingDir = "${testResultsBase}/combined-reports"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanQuickstart { doLast {
  String workingDir = "${testResultsBase}/quickstart"
  delete workingDir
  file(workingDir).mkdirs()
} }

subprojects {

  int maxWorkers = project.hasProperty('org.gradle.workers.max') ?
                   project.property('org.gradle.workers.max') as int :
                   Runtime.getRuntime().availableProcessors()

  // the run task for a selected sub-project
  task run(type: JavaExec) {
    if (!project.hasProperty('mainClass')) {
      main = 'io.snappydata.app.SparkSQLTest'
    } else {
      main = mainClass
    }
    if (project.hasProperty('params')) {
      args = params.split(',') as List
    }
    classpath = sourceSets.main.runtimeClasspath + sourceSets.test.runtimeClasspath
    jvmArgs '-Xmx2g', '-Xms2g'
  }

  task scalaTest(type: Test) {
    actions = [ new com.github.maiflai.ScalaTestAction() ]
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    minHeapSize '4g'
    maxHeapSize '4g'
    jvmArgs '-ea', '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:MaxNewSize=1g',
            '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled', '-Xss4m', '-XX:ReservedCodeCacheSize=1g'
    // for benchmarking
    // minHeapSize '12g'
    // maxHeapSize '12g'
    // jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:MaxNewSize=2g',
    //        '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled', '-Xss4m', '-XX:ReservedCodeCacheSize=1g'

    testLogging.exceptionFormat = TestExceptionFormat.FULL
    testLogging.events = TestLogEvent.values() as Set

    extensions.add(com.github.maiflai.ScalaTestAction.TAGS, new org.gradle.api.tasks.util.PatternSet())
    List<String> suites = []
    extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
    extensions.add('suite', { String name -> suites.add(name) } )
    extensions.add('suites', { String... name -> suites.addAll(name) } )

    def result = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTRESULT, result)
    extensions.add('testResult', { String name -> result.setLength(0); result.append(name) } )

    def output = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTOUTPUT, output)
    extensions.add('testOutput', { String name -> output.setLength(0); output.append(name) })

    def errorOutput = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTERROR, errorOutput)
    extensions.add('testError', { String name -> errorOutput.setLength(0); errorOutput.append(name) })

    // running a single scala suite
    if (rootProject.hasProperty('singleSuite')) {
      suite singleSuite
    }
    workingDir = "${testResultsBase}/scalatest"

    // testResult '/dev/tty'
    testOutput "${workingDir}/output.txt"
    testError "${workingDir}/error.txt"
    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }

  test {
    maxParallelForks = maxWorkers
    maxHeapSize '2g'
    jvmArgs '-ea', '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC',
            '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled'
    testLogging.exceptionFormat = TestExceptionFormat.FULL

    def single = System.getProperty('junit.single')
    if (single == null || single.length() == 0) {
      single = rootProject.hasProperty('junit.single') ?
          rootProject.property('junit.single') : null
    }
    if (single == null || single.length() == 0) {
      include '**/*.class'
      exclude '**/*TestBase.class'
      exclude '**/*DUnit*.class'
    } else {
      include single
    }

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      progress << "${eol}${now} ========== STARTING JUNIT TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
  }

  task configureDUnitTest(dependsOn: testClasses) {
    String dunitSingle = System.getProperty('dunit.single')
    if (dunitSingle == null || dunitSingle.length() == 0) {
      dunitSingle = rootProject.hasProperty('dunit.single') ?
          rootProject.property('dunit.single') : null
    }
    doLast {
      tasks.named('dunitTest').configure {
        includes.clear()
        excludes.clear()
        if (dunitSingle == null || dunitSingle.length() == 0) {
          def dunitTests = testClassesDirs.asFileTree.matching {
            includes = [ '**/*DUnitTest.class', '**/*DUnit.class' ]
            excludes = [
                '**/*Suite.class', '**/*DUnitSecurityTest.class', '**/NCJ*DUnit.class', '**/*HDFS*DUnit*.class',
                '**/BackwardCompatabilityPart*DUnit.class', '**/*Perf*DUnit.class', '**/ListAggDUnit.class',
                '**/SingleHop*TransactionDUnit.class', '**/*Parallel*AsyncEvent*DUnit.class',
                '**/pivotal/gemfirexd/wan/**/*DUnit.class', '**/*DUnitRecoveryTest.class',
                '**/offheap/**/*DUnit*.class', '**/*OffHeap*DUnit*.class', '**/*Offheap*DUnit*.class'
            ]
          }
          FileTree includeTestFiles = dunitTests
          int dunitFrom = rootProject.hasProperty('dunit.from') ?
              getLast(includeTestFiles, rootProject.property('dunit.from')) : 0
          int dunitTo = rootProject.hasProperty('dunit.to') ?
              getLast(includeTestFiles, rootProject.property('dunit.to')) : includeTestFiles.size()

          int begin = dunitFrom != -1 ? dunitFrom : 0
          int end = dunitTo != -1 ? dunitTo : includeTestFiles.size()
          def filteredSet = includeTestFiles.drop(begin).take(end-begin+1).collect {f -> "**/" + f.name}
          if (begin != 0 || end != includeTestFiles.size()) {
            println("Picking DUNIT tests for ${project.path}")
            filteredSet.each { a -> println(a) }
          }
          include filteredSet
        } else {
          include dunitSingle
        }
      }
    }
  }

  task dunitTest(type: Test) {
    dependsOn ':cleanDUnit'
    dependsOn ':product'
    dependsOn ':copyShadowJars'
    dependsOn configureDUnitTest

    // maxParallelForks = Math.max(Math.sqrt(maxWorkers + 1) + 1 as int, 2)
    maxParallelForks = 1
    minHeapSize '1536m'
    maxHeapSize '1536m'

    // limit netty buffer arenas to avoid occasional OOMEs with 1.5g heap
    int numArenas = Math.min(8, Runtime.getRuntime().availableProcessors())
    jvmArgs = ['-XX:+HeapDumpOnOutOfMemoryError',
               '-XX:+UseParNewGC', '-XX:+UseConcMarkSweepGC',
               '-XX:CMSInitiatingOccupancyFraction=50',
               '-XX:+CMSClassUnloadingEnabled', '-ea',
               '-Dspark.sql.codegen.cacheSize=1000',
               '-Dspark.ui.retainedStages=1000',
               '-Dspark.ui.retainedJobs=1000',
               '-Dspark.sql.ui.retainedExecutions=500',
               '-Dio.netty.allocator.pageSize=8192',
               '-Dio.netty.allocator.maxOrder=10',
               "-Dio.netty.allocator.numHeapArenas=${numArenas}",
               "-Dio.netty.allocator.numDirectArenas=${numArenas}"]

    workingDir = "${testResultsBase}/dunit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    systemProperties 'java.net.preferIPv4Stack': 'true',
                     'SNAPPY_HOME': snappyProductDir

    int numTestClasses = 0
    def testCount = new java.util.concurrent.atomic.AtomicInteger(0)

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      numTestClasses = getCandidateClassFiles().getFiles().size()
      progress << "${eol}${now} ========== STARTING DUNIT TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
    beforeSuite { desc ->
      if (desc.className != null) {
        def count = testCount.incrementAndGet()
        println "${now()} Start ${desc.className} ($count/$numTestClasses)"
      }
    }
    afterSuite { desc, result ->
      if (desc.className != null) {
        println "${now()} END ${desc.className}"
      }
    }
  }

  task configureRecoveryTest(dependsOn: testClasses) {
    String recoverySingle = System.getProperty('recovery.single')
    if (recoverySingle == null || recoverySingle.length() == 0) {
      recoverySingle = rootProject.hasProperty('recovery.single') ?
          rootProject.property('recovery.single') : null
    }
    doLast {
      tasks.named('recoveryTest').configure {
        includes.clear()
        excludes.clear()
        if (recoverySingle == null || recoverySingle.length() == 0) {
          def recoveryTests = testClassesDirs.asFileTree.matching {
            includes = [ '**/*DUnitRecoveryTest.class' ]
            excludes = [
                '**/*Suite.class', '**/*DUnitSecurityTest.class', '**/NCJ*DUnit.class', '**/*HDFS*DUnit*.class',
                '**/BackwardCompatabilityPart*DUnit.class', '**/*Perf*DUnit.class', '**/ListAggDUnit.class',
                '**/SingleHop*TransactionDUnit.class', '**/*Parallel*AsyncEvent*DUnit.class',
                '**/pivotal/gemfirexd/wan/**/*DUnit.class', '**/*DUnitTest.class', '**/*DUnit.class'
            ]
          }
          FileTree includeTestFiles = recoveryTests
          include includeTestFiles.collect {f -> "**/" + f.name}
        } else {
          include recoverySingle
        }
      }
    }
  }

  task recoveryTest(type: Test) {
    dependsOn ':cleanRecoveryTest'
    dependsOn ':product'
    dependsOn ':copyShadowJars'
    dependsOn configureRecoveryTest

    maxParallelForks = 1
    minHeapSize '4g'
    maxHeapSize '4g'

    // limit netty buffer arenas to avoid occasional OOMEs with 1.5g heap
    int numArenas = Math.min(8, Runtime.getRuntime().availableProcessors())
    jvmArgs = ['-XX:+HeapDumpOnOutOfMemoryError',
               '-XX:+UseParNewGC', '-XX:+UseConcMarkSweepGC',
               '-XX:CMSInitiatingOccupancyFraction=50',
               '-XX:+CMSClassUnloadingEnabled', '-ea',
               '-Dspark.sql.codegen.cacheSize=1000',
               '-Dspark.ui.retainedStages=1000',
               '-Dspark.ui.retainedJobs=1000',
               '-Dspark.sql.ui.retainedExecutions=500',
               '-Dio.netty.allocator.pageSize=8192',
               '-Dio.netty.allocator.maxOrder=10',
               "-Dio.netty.allocator.numHeapArenas=${numArenas}",
               "-Dio.netty.allocator.numDirectArenas=${numArenas}"]

    workingDir = "${testResultsBase}/recoveryTest"
    systemProperties 'java.net.preferIPv4Stack': 'true',
                     'SNAPPY_HOME': snappyProductDir,
                     'RECOVERY_TEST_DIR': workingDir

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    int numTestClasses = 0
    def testCount = new java.util.concurrent.atomic.AtomicInteger(0)

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      numTestClasses = getCandidateClassFiles().getFiles().size()
      progress << "${eol}${now} ========== STARTING RECOVERY TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
    beforeSuite { desc ->
      if (desc.className != null) {
        def count = testCount.incrementAndGet()
        println "${now()} Start ${desc.className} ($count/$numTestClasses)"
      }
    }
    afterSuite { desc, result ->
      if (desc.className != null) {
        println "${now()} END ${desc.className}"
      }
    }
  }

  task configureDUnitSecurityTest(dependsOn: testClasses) {
    String dunitSecSingle = System.getProperty('dunitSecurity.single')
    if (dunitSecSingle == null || dunitSecSingle.length() == 0) {
      dunitSecSingle = rootProject.hasProperty('dunitSecurity.single') ?
          rootProject.property('dunitSecurity.single') : null
    }
    doLast {
      tasks.named('dunitSecurityTest').configure {
        includes.clear()
        excludes.clear()
        if (dunitSecSingle == null || dunitSecSingle.length() == 0) {
          def dunitSecurityTests = testClassesDirs.asFileTree.matching {
            includes = [ '**/*DUnitSecurityTest.class' ]
            excludes = [ '**/*Suite.class', '**/*DUnitTest.class', '**/*DUnit.class', '**/*DUnitRecoveryTest.class' ]
          }
          FileTree includeTestFiles = dunitSecurityTests
          int dunitFrom = rootProject.hasProperty('dunitSecurity.from') ?
              getLast(includeTestFiles, rootProject.property('dunitSecurity.from')) : 0
          int dunitTo = rootProject.hasProperty('dunitSecurity.to') ?
              getLast(includeTestFiles, rootProject.property('dunitSecurity.to')) : includeTestFiles.size()

          int begin = dunitFrom != -1 ? dunitFrom : 0
          int end = dunitTo != -1 ? dunitTo : includeTestFiles.size()
          def filteredSet = includeTestFiles.drop(begin).take(end-begin+1).collect {f -> "**/" + f.name}
          if (begin != 0 || end != includeTestFiles.size()) {
            println("Picking SECURITY tests for ${project.path}")
            filteredSet.each { a -> println(a) }
          }
          include filteredSet
        } else {
          include dunitSecSingle
        }
      }
    }
  }

  task dunitSecurityTest(type: Test) {
    dependsOn ':cleanSecurityDUnit'
    dependsOn ':product'
    dependsOn ':copyShadowJars'
    dependsOn configureDUnitSecurityTest

    maxParallelForks = 1
    minHeapSize '1536m'
    maxHeapSize '1536m'

    // limit netty buffer arenas to avoid occasional OOMEs with 1.5g heap
    int numArenas = Math.min(8, Runtime.getRuntime().availableProcessors())
    jvmArgs = ['-XX:+HeapDumpOnOutOfMemoryError',
               '-XX:+UseParNewGC', '-XX:+UseConcMarkSweepGC',
               '-XX:CMSInitiatingOccupancyFraction=50',
               '-XX:+CMSClassUnloadingEnabled', '-ea',
               '-Dspark.sql.codegen.cacheSize=1000',
               '-Dspark.ui.retainedStages=1000',
               '-Dspark.ui.retainedJobs=1000',
               '-Dspark.sql.ui.retainedExecutions=500',
               '-Dio.netty.allocator.pageSize=8192',
               '-Dio.netty.allocator.maxOrder=10',
               "-Dio.netty.allocator.numHeapArenas=${numArenas}",
               "-Dio.netty.allocator.numDirectArenas=${numArenas}"]

    workingDir = "${testResultsBase}/dunit-security"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    systemProperties 'java.net.preferIPv4Stack': 'true',
                     'SNAPPY_HOME': snappyProductDir

    doFirst {
      String eol = System.getProperty('line.separator')
      String now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
      def progress = new File(workingDir, 'progress.txt')
      progress << "${eol}${now} ========== STARTING SECURITY DUNIT TEST SUITE FOR ${project.name} ==========${eol}${eol}"
    }
  }

  // apply default manifest
  if (rootProject.hasProperty('enablePublish')) {
    createdBy = vendorName
  }
  jar {
    manifest {
      attributes(
        'Manifest-Version'  : '1.0',
        'Created-By'        : createdBy,
        'Title'             : rootProject.name,
        'Version'           : archiveVersion.get(),
        'Vendor'            : vendorName
      )
    }
  }

  configurations {
    testOutput {
      extendsFrom testCompile
      description 'a dependency that exposes test artifacts'
    }
    shadowInclude {
      description = 'a dependency that is included only in the shadow jar'
      canBeResolved = true
      canBeConsumed = false
    }
    /*
    all {
      resolutionStrategy {
        // fail eagerly on version conflict (includes transitive dependencies)
        // e.g. multiple different versions of the same dependency (group and name are equal)
        failOnVersionConflict()
      }
    }
    */
  }

  // force versions for some dependencies that get pulled multiple times
  configurations.all {
    resolutionStrategy.force "com.google.guava:guava:${guavaVersion}",
      "org.apache.derby:derby:${derbyVersion}",
      "org.apache.hadoop:hadoop-annotations:${hadoopVersion}",
      "org.apache.hadoop:hadoop-auth:${hadoopVersion}",
      "org.apache.hadoop:hadoop-client:${hadoopVersion}",
      "org.apache.hadoop:hadoop-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-hdfs:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-app:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-core:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-jobclient:${hadoopVersion}",
      "org.apache.hadoop:hadoop-mapreduce-client-shuffle:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-api:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-client:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-server-common:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-server-nodemanager:${hadoopVersion}",
      "org.apache.hadoop:hadoop-yarn-server-web-proxy:${hadoopVersion}"
    exclude(group: 'org.mortbay.jetty', module: 'servlet-api')
  }
  if (it.name != 'gemfire-shared') {
    configurations.all {
      exclude(group: 'log4j', module: 'log4j')
      exclude(group: 'org.slf4j', module: 'slf4j-log4j12')
    }
  }
  /*
  configurations.testRuntime {
    // below is included indirectly by hadoop deps and conflicts with embedded 1.5.7 apacheds
    exclude(group: 'org.apache.directory.server', module: 'apacheds-kerberos-codec')
    exclude(group: 'org.apache.directory.server', module: 'apacheds-i18n')
  }
  */

  task packageTests(type: Jar, dependsOn: testClasses) {
    description 'Assembles a jar archive of test classes.'
    archiveClassifier.set('tests')
  }
  artifacts {
    testOutput packageTests
  }

  dependencies {
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    testCompile "junit:junit:${junitVersion}"
  }
}

// maven publish tasks
subprojects {

  apply plugin: 'signing'

  task packageSources(type: Jar, dependsOn: classes) {
    archiveClassifier.set('sources')
    from sourceSets.main.allSource
  }
  task packageDocs(type: Jar, dependsOn: javadoc) {
    archiveClassifier.set('javadoc')
    from javadoc
  }
  if (rootProject.hasProperty('enablePublish')) {
    signing {
      useGpgCmd()
      sign configurations.archives
    }

    uploadArchives {
      repositories {
        mavenDeployer {
          beforeDeployment { MavenDeployment deployment -> signing.signPom(deployment) }

          repository(url: 'https://oss.sonatype.org/service/local/staging/deploy/maven2/') {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }
          snapshotRepository(url: 'https://oss.sonatype.org/content/repositories/snapshots/') {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }

          pom.project {
            name 'SnappyData'
            packaging 'jar'
            // optionally artifactId can be defined here
            description 'SnappyData distributed data store and execution engine'
            url 'https://github.com/TIBCOSoftware/snappydata'

            scm {
              connection 'scm:git:https://github.com/TIBCOSoftware/snappydata.git'
              developerConnection 'scm:git:https://github.com/TIBCOSoftware/snappydata.git'
              url 'https://github.com/TIBCOSoftware/snappydata'
            }

            licenses {
              license {
                name 'The Apache License, Version 2.0'
                url 'http://www.apache.org/licenses/LICENSE-2.0.txt'
              }
            }

            developers {
              developer {
                id 'smenon'
                name 'Sudhir Menon'
                email 'sumenon@tibco.com'
              }
            }
          }
        }
      }
    }
  }
}

// apply common test and misc configuration
gradle.taskGraph.whenReady { graph ->

  def allTasks = subprojects.collect { it.tasks }.flatten()
  allTasks.each { task ->
    if (task instanceof Tar) {
      def tar = (Tar)task
      tar.compression = Compression.GZIP
      tar.archiveExtension.set('tar.gz')
    } else if (task instanceof Jar) {
      def pack = (Jar)task
      if (pack.name == 'packageTests') {
        pack.from(pack.project.sourceSets.test.output.classesDirs, pack.project.sourceSets.test.resources.srcDirs)
      }
    } else if (task instanceof Test) {
      def test = (Test)task
      test.configure {

        String logLevel = System.getProperty('logLevel')
        if (logLevel != null && logLevel.length() > 0) {
          systemProperties 'gemfire.log-level'           : logLevel,
                           'logLevel'                    : logLevel
        }
        logLevel = System.getProperty('securityLogLevel')
        if (logLevel != null && logLevel.length() > 0) {
          systemProperties 'gemfire.security-log-level'  : logLevel,
                           'securityLogLevel'            : logLevel
        }

        systemProperties 'SNAPPY_HOME': snappyProductDir,
            'APACHE_SPARK_HOME': sparkProductDir,
            'APACHE_SPARK_CURRENT_HOME': sparkCurrentProductDir,
            'gemfire.DUnitLauncher.CHILD_HEAPSIZE': '2g'
        environment 'PYSPARK_PYTHON': sparkPython,
            'PYSPARK_DRIVER_PYTHON': sparkPython,
            'SPARK_TESTING': '1',
            'SNAPPY_DIST_CLASSPATH': test.classpath.asPath

        def failureCount = new java.util.concurrent.atomic.AtomicInteger(0)
        def progress = new File(workingDir, 'progress.txt')
        def output = new File(workingDir, 'output.txt')

        String eol = System.getProperty('line.separator')
        beforeTest { desc ->
          String now = now()
          progress << "${now} Starting test ${desc.className} ${desc.name}${eol}"
          output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
        }
        onOutput { desc, event ->
          String msg = event.message
          if (event.destination.toString() == 'StdErr') {
            msg = msg.replace(eol, "${eol}[error]  ")
          }
          output << msg
        }
        afterTest { desc, result ->
          String now = now()
          progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
          output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
          def exceptions = result.exceptions
          if (exceptions.size() > 0) {
            exceptions.each { t ->
              progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
              output << "${getStackTrace(t)}${eol}"
            }
            failureCount.incrementAndGet()
          }
        }
        doLast {
          def report = "${test.reports.html.destination}/index.html"
          boolean hasProgress = progress.exists()
          if (failureCount.get() > 0) {
            println()
            def failureMsg = "FAILED: There were ${failureCount.get()} failures.${eol}"
            if (hasProgress) {
              failureMsg    += "        See the progress report in: file://$progress${eol}"
            }
            failureMsg    += "        HTML report in: file://$report"
            throw new GradleException(failureMsg)
          } else if (hasProgress) {
            println()
            println("SUCCESS: See the progress report in: file://$progress")
            println("         HTML report in: file://$report")
            println()
          } else {
            println()
            println("SUCCESS: See the HTML report in: file://$report")
            println()
          }
        }
      }
    }
  }
}


task publishLocal {
  dependsOn subprojects.findAll { p -> p.name != 'snappydata-native' &&
    p.name != 'snappydata-store-prebuild' && p.name != 'snappydata-store' }.collect {
      it.getTasksByName('install', false).collect { it.path }
  }
}

task publishMaven {
  dependsOn subprojects.findAll { p -> p.name != 'snappydata-native' &&
    p.name != 'snappydata-store-prebuild' && p.name != 'snappy-store' &&
      p.name != 'snappydata-store' }.collect {
      it.getTasksByName('uploadArchives', false).collect { it.path }
  }
}

task product(type: Zip) {
  dependsOn ":snappy-cluster_${scalaBinaryVersion}:jar"
  dependsOn ":snappy-examples_${scalaBinaryVersion}:jar"
  dependsOn ":snappy-spark:snappy-spark-assembly_${scalaBinaryVersion}:sparkProduct"
  dependsOn ':snappy-launcher:jar'
  dependsOn ':jdbcJar'

  def clusterProject = project(":snappy-cluster_${scalaBinaryVersion}")
  def launcherProject = project(':snappy-launcher')
  def targetProject = clusterProject

  dependsOn ":snappy-aqp_${scalaBinaryVersion}:jar"
  targetProject = aqpProject
  dependsOn ":snappy-jdbc-connector_${scalaBinaryVersion}:jar"
  if (hasGemFireConnectorProject) {
    dependsOn ":gemfire-connector:product"
  }

  // create snappydata+spark combined python zip
  destinationDirectory.set(file("${snappyProductDir}/python/lib"))
  archiveFileName.set('pyspark.zip')
  from("${project(':snappy-spark').projectDir}/python") {
    include 'pyspark/**/*'
  }
  from("${rootDir}/python") {
    include 'pyspark/**/*'
  }

  doFirst {
    // remove the spark pyspark.zip
    delete "${snappyProductDir}/python/lib/pyspark.zip"
  }
  doLast {
    def examplesProject = project(":snappy-examples_${scalaBinaryVersion}")
    String exampleArchiveName = "quickstart.jar"

    // copy all runtime dependencies of snappy-cluster, itself and AQP
    def targets = targetProject.configurations.runtime
    copy {
      from(targets) {
        // exclude antlr4 explicitly (runtime is still included)
        // that gets pulled by antlr gradle plugin
        exclude '**antlr4-4*.jar'
        // exclude scalatest included by spark-tags
        exclude '**scalatest*.jar'
        // exclude other test jars
        exclude '**junit*.jar'
        exclude '**mockito*.jar'
        exclude '**hamcrest-core*.jar'
        exclude '**test-interface*.jar'
        exclude '**scalacheck*.jar'
        if (rootProject.hasProperty('hadoop-provided')) {
          exclude 'hadoop-*.jar'
        }
      }
      from targetProject.jar.outputs
      into "${snappyProductDir}/jars"
    }
    // copy the launcher jar
    copy {
      from launcherProject.jar.destinationDir
      into "${snappyProductDir}/jars"
      include launcherProject.jar.archiveName
    }

    // create the RELEASE file
    def releaseFile = file("${snappyProductDir}/RELEASE")
    String buildFlags = ''
    if (rootProject.hasProperty('docker')) {
      buildFlags += ' -Pdocker'
    }
    if (rootProject.hasProperty('ganglia')) {
      buildFlags += ' -Pganglia'
    }
    if (rootProject.hasProperty('hadoop-provided')) {
      buildFlags += ' -Phadoop-provided'
    }
    String gitRevision = "${gitCmd} rev-parse --short HEAD".execute().text.trim()
    if (gitRevision.length() > 0) {
      gitRevision = " (git revision ${gitRevision})"
    }

    if (rootProject.hasProperty('hadoop-provided')) {
      releaseFile.append("SnappyData ${archiveVersion.get()}${gitRevision} " +
              "built with Hadoop ${hadoopVersion} but hadoop not bundled.\n")
    } else {
      releaseFile.append("SnappyData ${archiveVersion.get()}${gitRevision} built for Hadoop ${hadoopVersion}.\n")
    }
    releaseFile.append("Build flags:${buildFlags}\n")

    // copy LICENSE, README.md and doc files
    copy {
      from projectDir
      into snappyProductDir
      include 'LICENSE'
      // if (!isEnterpriseProduct) {
      include 'NOTICE'
      // }
      include 'README.md'
    }
    copy {
      from "${projectDir}/docs"
      into "${snappyProductDir}/docs"
    }

    copy {
      from "${rootDir}/python/pyspark/"
      include 'sql/**'
      include 'streaming/**'
      include 'test_support/**'
      into "${snappyProductDir}/python/pyspark"
    }

    // Next the remaining components of full product like examples etc
    // Spark portions already copied in the assembly:product dependency
    copy {
      from("${examplesProject.projectDir}/src/main/python")
      into "${snappyProductDir}/quickstart/python"
    }
    if (new File(rootDir, 'store/build.gradle').exists()) {
      // copy snappy-store shared libraries for JNI calls
      def storeCoreProject = project(':snappy-store:snappydata-store-core')
      copy {
        from "${storeCoreProject.projectDir}/lib"
        into "${snappyProductDir}/jars"
        exclude '*_sol*.so'
      }
      copy {
        from "${storeCoreProject.projectDir}/../quickstart"
        into "${snappyProductDir}/quickstart/store"
        exclude '.git*'
      }
    }
    if (hasAqpProject) {
      // copy enterprise shared libraries for optimized JNI calls
      copy {
        from aqpProject.projectDir.path + '/lib'
        into "${snappyProductDir}/jars"
      }
    }

    def jdbcConnectorProject = project(":snappy-jdbc-connector_${scalaBinaryVersion}")
    def gemfireConnectorProject = hasGemFireConnectorProject ? project(":gemfire-connector") : null
    def gfeConnectorProject = hasGemFireConnectorProject ? project(":gemfire-connector:connector_${scalaBinaryVersion}") : null
    def gfeFunctionProject = hasGemFireConnectorProject ? project(":gemfire-connector:gfeFunctions") : null

    copy {
      from jdbcConnectorProject.jar.destinationDir
      into "${snappyProductDir}/connectors"
      include "*.jar"
    }

    if (hasGemFireConnectorProject) {
      copy {
        from gfeConnectorProject.jar.destinationDir
        into "${snappyProductDir}/connectors"
        include "*.jar"
      }
      copy {
        from gfeFunctionProject.jar.destinationDir
        into "${snappyProductDir}/connectors"
        include "*.jar"
      }
      copy {
        from "${gemfireConnectorProject.projectDir}/examples/quickstart/data"
        into "${snappyProductDir}/connectors"
        include "persons.jar"
      }
    }

    copy {
      from "${examplesProject.buildDir}/libs"
      into "${snappyProductDir}/examples/jars"
      include "${examplesProject.jar.archiveName}"
      rename { filename -> exampleArchiveName }
    }
    copy {
      from("${clusterProject.projectDir}/bin")
      into "${snappyProductDir}/bin"
    }
    copy {
      from("${clusterProject.projectDir}/sbin")
      into "${snappyProductDir}/sbin"
    }
    copy {
      from("${clusterProject.projectDir}/conf")
      into "${snappyProductDir}/conf"
    }
    copy {
      from("${examplesProject.projectDir}/quickstart")
      into "${snappyProductDir}/quickstart"
    }
    copy {
      from("${examplesProject.projectDir}/src")
      into "${snappyProductDir}/quickstart/src"
    }
    copy {
      from("${clusterProject.projectDir}/benchmark")
      into "${snappyProductDir}/benchmark"
    }

    /* (preferred one is the standard %jdbc interpreter in Zeppelin)
    if (rootProject.hasProperty('enablePublish')) {
      packageZeppelinInterpreter()
    }
    */

    if (rootProject.hasProperty('R.enable')) {
      def targetRDir = "${snappyProductDir}/R"
      copy {
        from("${project(":snappy-spark").projectDir}/R")
        into targetRDir
      }

      exec {
        environment 'SPARK_HOME': snappyProductDir,
            'NO_TESTS': '1',
            'CLEAN_INSTALL': '1'
        workingDir targetRDir
        commandLine "${targetRDir}/check-cran.sh"
      }

      fileTree(targetRDir).exclude('lib').visit { delete it.file }
    }
  }
}

if (rootProject.hasProperty('copyToDir')) {
  task copyProduct(type: Copy, dependsOn: product) {
    from snappyProductDir
    into copyToDir
  }
}

distributions {
  main {
    baseName = 'snappydata'
    contents {
      from { snappyProductDir }
    }
  }
}

ospackage {
  packageName = distributions.main.baseName
  version = version
  release = '1'
  os = LINUX

  maintainer = vendorName
  packageDescription = productName +
      ', the Spark Database. Stream - Transact - Analyze - Predict all in one cluster'
  summary = productName + ' Installer'

  user = 'snappydata'
  permissionGroup = 'snappydata'

  from(snappyProductDir) {
    into '/opt/' + packageName
  }

  if (rootProject.hasProperty('enablePublish')) {
    // signingKeyId = rootProject.property('signing.keyId')
    // signingKeyPassphrase = rootProject.property('signing.password')
    // signingKeyRingFile = file(rootProject.property('signing.secretKeyRingFile'))
  }

  link('/usr/sbin/snappy-start-all.sh', "/opt/${packageName}/sbin/snappy-start-all.sh")
  link('/usr/sbin/snappy-stop-all.sh', "/opt/${packageName}/sbin/snappy-stop-all.sh")
  link('/usr/sbin/snappy-status-all.sh', "/opt/${packageName}/sbin/snappy-status-all.sh")
  link('/usr/bin/snappy', "/opt/${packageName}/bin/snappy")
  link('/usr/bin/snappy-sql', "/opt/${packageName}/bin/snappy-sql")
  link('/usr/bin/snappy-job.sh', "/opt/${packageName}/bin/snappy-job.sh")
}

buildRpm {
  dependsOn product
  requires('glibc')
  requires('bash')
  requires('perl')
  requires('curl')
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without_hadoop'
  }

  preInstall file('release/preInstallRpm.sh')
}

buildDeb {
  dependsOn product
  requires('libc6')
  requires('bash')
  requires('perl')
  requires('curl')
  recommends('java8-sdk')
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without-hadoop'
  }

  preInstall file('release/preInstallDeb.sh')
}

distTar {
  // if (isEnterpriseProduct) {
  //   archiveName = 'TIB_compute' + devEdition + '_' + version + '_' + buildDateShort + osFamilyName + '.tar.gz'
  // } else {
  classifier 'bin'
  // }
  dependsOn product
  compression = Compression.GZIP
  archiveExtension.set('tar.gz')
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without-hadoop-bin'
  }
}

distZip {
  classifier 'bin'
  dependsOn product
  if (rootProject.hasProperty('hadoop-provided')) {
    classifier 'without-hadoop-bin'
  }
}

assemble.dependsOn.clear()
assemble.dependsOn product

task distRpm {
  dependsOn product
  dependsOn buildRpm
}

task distDeb {
  dependsOn product
  dependsOn buildDeb
}

task jdbcJar {
  dependsOn ":snappy-jdbc_${scalaBinaryVersion}:shadowJar"

  doLast {
    def jdbcProject = project(":snappy-jdbc_${scalaBinaryVersion}")
    String jdbcName = "snappydata-jdbc_${scalaBinaryVersion}-${version}.jar"
    // copy the snappy-jdbc shadow jar into distributions
    copy {
      from jdbcProject.shadowJar.destinationDir
      into "${rootProject.buildDir}/distributions"
      include jdbcProject.shadowJar.archiveName
      rename { filename -> jdbcName }
    }
  }
}

task copyShadowJars {
  dependsOn jdbcJar
  dependsOn ":snappy-core_${scalaBinaryVersion}:shadowJar"

  doLast {
    def coreProject = project(":snappy-core_${scalaBinaryVersion}")
    String coreName = "snappydata-core_${scalaBinaryVersion}-${version}.jar"
    copy {
      from coreProject.shadowJar.destinationDir
      into "${rootProject.buildDir}/distributions"
      include coreProject.shadowJar.archiveName
      rename { filename -> coreName }
    }
  }
}

task distInstallers {
  dependsOn product
  dependsOn buildRpm
  dependsOn buildDeb
}

// use the task below to prepare final release bits
task distProduct {
  dependsOn product, copyShadowJars, distTar, distZip
  dependsOn distInstallers
}

task generateSources {
  dependsOn ':snappy-spark:generateSources', ':snappy-store:generateSources'
  // copy all resource files into build classes path because new versions of IDEA
  // do not include separate resources path in CLASSPATH if output path has been customized
  doLast {
    subprojects.forEach { proj ->
      String resourcesDir = proj.sourceSets.main.output.resourcesDir
      if (file(resourcesDir).exists()) {
        def projOutDir = file("${proj.projectDir}/src/main/scala").exists()
          ? "${proj.sourceSets.main.java.outputDir}/../../scala/main"
          : proj.sourceSets.main.java.outputDir
        copy {
          from resourcesDir
          into projOutDir
        }
      }
      resourcesDir = proj.sourceSets.test.output.resourcesDir
      if (file(resourcesDir).exists()) {
        def projOutDir = file("${proj.projectDir}/src/test/scala").exists()
          ? "${proj.sourceSets.test.java.outputDir}/../../scala/test"
          : proj.sourceSets.test.java.outputDir
        copy {
          from resourcesDir
          into projOutDir
        }
      }
    }
  }
}

task cleanAll {
  dependsOn getTasksByName('clean', true).collect { it.path }
}
task buildAll {
  dependsOn product, distTar
  dependsOn getTasksByName('assemble', true).collect { it.path }
  dependsOn getTasksByName('product', true).collect { it.path }
  dependsOn getTasksByName('testClasses', true).collect { it.path }
  dependsOn copyShadowJars
  mustRunAfter cleanAll
}
task buildDtests {
  dependsOn "snappy-dtests_${scalaBinaryVersion}:buildDtests"
}
task checkAll {
  dependsOn ":snappy-aqp_${scalaBinaryVersion}:scalaStyle"
  dependsOn ":snappy-jdbc-connector_${scalaBinaryVersion}:scalaStyle"
  dependsOn ':snappy-spark:scalaStyle'
  if (rootProject.hasProperty('store')) {
    dependsOn ':snappy-store:check'
  }
  dependsOn ":snappy-core_${scalaBinaryVersion}:check"
  if (rootProject.hasProperty('spark')) {
    dependsOn ':snappy-spark:check'
  }
  if (!rootProject.hasProperty('cluster.skip')) {
    dependsOn ":snappy-cluster_${scalaBinaryVersion}:check"
  }
  dependsOn ":snappy-examples_${scalaBinaryVersion}:check"
  if (!rootProject.hasProperty('aqp.skip')) {
    dependsOn ":snappy-aqp_${scalaBinaryVersion}:check"
  }
  if (hasGemFireConnectorProject && !rootProject.hasProperty('connectors.skip')) {
    dependsOn ":gemfire-connector:check"
  }
  if (!rootProject.hasProperty('smoke.skip')) {
    dependsOn buildDtests, ":snappy-dtests_${scalaBinaryVersion}:check"
  }
  if (!rootProject.hasProperty('compatibility.skip')) {
    dependsOn ":snappy-compatibility-tests_${scalaBinaryVersion}:check"
  }
  mustRunAfter buildAll
}
task allReports(type: TestReport) {
  description 'Combines the test reports.'
  dependsOn cleanAllReports
  destinationDir = file("${testResultsBase}/combined-reports")
  mustRunAfter checkAll
}
gradle.taskGraph.whenReady { graph ->
  tasks.getByName('allReports').reportOn rootProject.subprojects.collect{ it.tasks.withType(Test) }.flatten()
}

static void writeProperties(File parent, String name, def comment, def propsMap) {
  parent.exists() || parent.mkdirs()
  def writer = new File(parent, name).newWriter()
  def props = new Properties()
  propsMap.each { k, v -> props.setProperty(k, v.toString()) }
  try {
    props.store(writer, comment.toString())
    writer.flush()
  } finally {
    writer.close()
  }
}

static int getLast(includeTestFiles, pattern) {
  includeTestFiles.findLastIndexOf {
    File f -> f.name.indexOf(pattern) >= 0
  }
}

def packageZeppelinInterpreter() {
  String zeppelinInterpreterJarName = "snappydata-zeppelin_${scalaBinaryVersion}-${zeppelinInterpreterVersion}.jar"
  String zeppelinInterpreterDir = System.env.ZEPPELIN_INTERPRETER_DIR

  if (zeppelinInterpreterDir == null || zeppelinInterpreterDir.length() == 0) {
    zeppelinInterpreterDir = "${projectDir}/../zeppelin-interpreter"
  }

  String zeppelinInterpreterLibDir = "${zeppelinInterpreterDir}/build-artifacts/libs"
  if (file(zeppelinInterpreterDir).canWrite()) {
    exec {
      executable "${zeppelinInterpreterDir}/gradlew"
      workingDir = zeppelinInterpreterDir
      args "clean", "product", "distTar", "-PenablePublish"
    }
    println ''
    println "Copying Zeppelin Interpreter jar from ${zeppelinInterpreterLibDir} to ${snappyProductDir}/jars"
    println ''
    copy {
      from "${zeppelinInterpreterLibDir}"
      into "${snappyProductDir}/jars"
      include "${zeppelinInterpreterJarName}"
    }
  } else {
    println "Skipping including Zeppelin Interpreter jar due to unwritable ${zeppelinInterpreterDir}"
  }
}

task sparkPackage {
  dependsOn ":snappy-core_${scalaBinaryVersion}:sparkPackage"
}

product.mustRunAfter clean, cleanAll
distTar.mustRunAfter clean, cleanAll, product
distZip.mustRunAfter clean, cleanAll, product
distRpm.mustRunAfter clean, cleanAll, product
distDeb.mustRunAfter clean, cleanAll, product
distInstallers.mustRunAfter clean, cleanAll, product, distRpm, distDeb
distProduct.mustRunAfter clean, cleanAll, product

task deleteDocsDir(type: Delete) {
  delete "${rootProject.buildDir}/docs"
}

task docs(type: ScalaDoc) {
  apply plugin: 'scala'

  scalaDocOptions.additionalParameters = [ '-J-Xmx7g', '-J-XX:ReservedCodeCacheSize=512m', '-J-Djava.net.preferIPv4Stack=true' ]

  dependsOn deleteDocsDir
  mustRunAfter buildAll

  Set<String> allSource = []
  def docProjects = rootProject.subprojects.collectMany { project ->
    if ((project.plugins.hasPlugin('scala') || project.plugins.hasPlugin('java')) &&
            // skip gemfire-connector
            !project.path.contains('gemfire-connector') &&
            // jobserver depends on Apache Spark 1.5.x which causes conflicts
            !project.path.contains('snappy-store') &&
            !project.name.contains('jobserver') &&
            // below three will get filtered with the snappy-store path check itself
            // but still keeping it as when we would remove the snappy-store path filter
            // still the below three sub prejects should not be built.
            !project.name.contains('jgroups') &&
            !project.name.contains('gemfire-examples') &&
            !project.name.contains('trove') &&
            !project.name.contains('kafka') &&
            !project.name.contains('encoders') &&
            // exclude tests
            !project.name.contains('tests')) {
      allSource.addAll(project.sourceSets.main.allJava.findAll {
        !it.getPath().matches('.*/internal/.*') && !it.getPath().contains('com/gemstone/gemfire/cache/operations/')
      })

      if (project.plugins.hasPlugin('scala')) {
        allSource.addAll(project.sourceSets.main.allScala.findAll {
          !it.getPath().matches('.*org/apache/spark/sql/execution/joins/HashedRelation.*') &&
          !it.getPath().matches('.*org/apache/spark/sql/execution/debug/package.*') &&
          !it.getPath().matches('.*org/apache/spark/sql/store/CodeGeneration.*')
        })
      }
      [ project ]
    } else []
  }
  source = allSource
  classpath = files(docProjects.collect { project ->
   project.sourceSets.main.compileClasspath
  })
  destinationDir = file("${rootProject.buildDir}/docs")
}

task publishDocs(type: Exec) {
  dependsOn product, docs
  //on linux
  commandLine './publish-site.sh'
}


// It runs test script from product dir. Hence if running the target individually make sure
// to run product target first
task checkPython(type: Exec) {
  String wdir = "${testResultsBase}/python"
  delete wdir
  file(wdir).mkdirs()
  workingDir  wdir
  copy {
    from "${rootDir}/python/pyspark/test_supprt"
    into wdir
  }
  environment 'PYTHONPATH': "${snappyProductDir}/python/lib/py4j-${py4jVersion}-src.zip:${snappyProductDir}/python",
      'SPARK_HOME': "${snappyProductDir}"
  commandLine 'python', '-u', "${rootDir}/python/run-snappy-tests.py"
}

task precheckin {
  dependsOn cleanAll, buildAll, checkAll, allReports, docs
}

if (rootProject.hasProperty('trackBuildTime') ) {
  buildtimetracker {
    reporters {
      summary {
            ordered false
            threshold 5000
            barstyle "unicode"
      }
    }
  }
}

// log build output to buildOutput.log in addition to console output

def buildOutput = new File("${rootDir}/buildOutput.log")
// delete build output file if it has become large
if (buildOutput.length() > 1000000) {
  delete buildOutput
}
def gradleLogger = new org.gradle.api.logging.StandardOutputListener() {
  void onOutput(CharSequence output) {
    buildOutput << output
  }
}
def loggerService = gradle.services.get(LoggingOutputInternal)
loggerService.addStandardOutputListener(gradleLogger)
loggerService.addStandardErrorListener(gradleLogger)

println()
println('-------------------------------------------------')
println("Starting new build on ${buildDate}")
println('-------------------------------------------------')
println()
