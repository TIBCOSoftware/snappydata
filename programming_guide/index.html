<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="SnappyData Team">
  
  <title>Programming Guide - SnappyData Documentation</title>
  

  <link rel="shortcut icon" href="../favicon.ico">
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../css/toc.css" rel="stylesheet">
  <link href="../css/menu.css" rel="stylesheet">
  <link href="../css/images.css" rel="stylesheet">
  <link href="../css/style.css" rel="stylesheet">
  <link href="../extra.css" rel="stylesheet">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Programming Guide";
    var mkdocs_page_input_path = "programming_guide.md";
    var mkdocs_page_url = "/programming_guide/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 
  <script src="../js/menu.js"></script>

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> SnappyData Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Overview</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../quickstart/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../install/">Download and Install</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../howto/">How Tos</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../architecture/">Architecture</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../configuration/">Configuring the Cluster</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Configuring the Cluster</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../configuring_cluster/configuring_cluster/">Configuration</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../configuring_cluster/property_description/">SnappyData Properties</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Programming Guide</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#overview">Overview</a></li>
                
            
                <li class="toctree-l3"><a href="#snappysession-and-snappystreamingcontext">SnappySession and SnappyStreamingContext</a></li>
                
                    <li><a class="toctree-l4" href="#to-create-a-snappysession">To Create a SnappySession</a></li>
                
                    <li><a class="toctree-l4" href="#to-create-a-snappystreamingcontext">To Create a SnappyStreamingContext</a></li>
                
            
                <li class="toctree-l3"><a href="#snappydata-jobs">SnappyData Jobs</a></li>
                
                    <li><a class="toctree-l4" href="#submitting-jobs">Submitting Jobs</a></li>
                
                    <li><a class="toctree-l4" href="#running-python-applications">Running Python Applications</a></li>
                
                    <li><a class="toctree-l4" href="#streaming-jobs">Streaming Jobs</a></li>
                
            
                <li class="toctree-l3"><a href="#managing-jar-files">Managing JAR Files</a></li>
                
            
                <li class="toctree-l3"><a href="#using-snappydata-shell">Using SnappyData Shell</a></li>
                
            
                <li class="toctree-l3"><a href="#using-the-spark-shell-and-spark-submit">Using the Spark Shell and spark-submit</a></li>
                
            
                <li class="toctree-l3"><a href="#using-jdbc-with-snappydata">Using JDBC with SnappyData</a></li>
                
            
                <li class="toctree-l3"><a href="#multiple-language-binding-using-thrift-protocol">Multiple Language Binding using Thrift Protocol</a></li>
                
            
                <li class="toctree-l3"><a href="#building-snappydata-applications-using-spark-api">Building SnappyData Applications using Spark API</a></li>
                
                    <li><a class="toctree-l4" href="#snappysession-usage">SnappySession Usage</a></li>
                
                    <li><a class="toctree-l4" href="#create-row-tables-using-api-update-the-contents-of-row-table">Create Row Tables using API, Update the Contents of Row Table</a></li>
                
                    <li><a class="toctree-l4" href="#snappystreamingcontext-usage">SnappyStreamingContext Usage</a></li>
                
            
                <li class="toctree-l3"><a href="#tables-in-snappydata">Tables in SnappyData</a></li>
                
                    <li><a class="toctree-l4" href="#row-and-column-tables">Row and Column Tables</a></li>
                
            
                <li class="toctree-l3"><a href="#stream-processing-using-sql">Stream processing using SQL</a></li>
                
                    <li><a class="toctree-l4" href="#spark-streaming-overview">Spark Streaming Overview</a></li>
                
                    <li><a class="toctree-l4" href="#snappydata-streaming-extensions-over-spark">SnappyData Streaming Extensions over Spark</a></li>
                
                    <li><a class="toctree-l4" href="#working-with-stream-tables">Working with Stream Tables</a></li>
                
                    <li><a class="toctree-l4" href="#stream-sql-through-snappy-sql">Stream SQL through snappy-sql</a></li>
                
                    <li><a class="toctree-l4" href="#schemadstream">SchemaDStream</a></li>
                
                    <li><a class="toctree-l4" href="#registering-continuous-queries">Registering Continuous Queries</a></li>
                
                    <li><a class="toctree-l4" href="#dynamic-ad-hoc-continuous-queries">Dynamic (ad-hoc) Continuous Queries</a></li>
                
            
                <li class="toctree-l3"><a href="#user-defined-functions-udf-and-user-defined-aggregate-functions-udaf">User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)</a></li>
                
                    <li><a class="toctree-l4" href="#create-user-defined-function">Create User Defined Function</a></li>
                
                    <li><a class="toctree-l4" href="#create-user-defined-aggregate-functions">Create User Defined Aggregate Functions</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../deployment/">Affinity Modes</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../best_practices/">Best practices</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Best practices</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../best_practices/capacity_planning/">Setting up your Cluster </a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../best_practices/design_schema/">Designing your Database and Schema</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../aqp/">Synopsis Data Engine (SDE)</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../aqp_aws/">Using iSight-Cloud</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../monitoring/monitoring/">Managing and Monitoring</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../reference/">Reference Guides</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Reference Guides</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../sql_reference/">SQL Reference Guide</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>SQL Reference Guide</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-statements/">CREATE Statements</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>CREATE Statements</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-diskstore/">CREATE DISKSTORE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-function/">CREATE FUNCTION</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-index/">CREATE INDEX</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-schema/">CREATE SCHEMA</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-table/">CREATE TABLE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-sample-table/">CREATE SAMPLE TABLE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-external-table/">CREATE EXTERNAL TABLE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-temporary-table/">CREATE TEMPORARY TABLE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/create-stream-table/">CREATE STREAM TABLE</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/delete/">DELETE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/drop-statements/">DROP Statements</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>DROP Statements</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/drop-diskstore/">DROP DISKSTORE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/drop-function/">DROP FUNCTION</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/drop-index/">DROP INDEX</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/drop-table/">DROP TABLE</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/insert/">INSERT</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/put-into/">PUT INTO</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/select/">SELECT</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/set-schema/">SET SCHEMA</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/truncate-table/">TRUNCATE TABLE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/sql_reference/update/">UPDATE</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/inbuilt_system_procedures/system-procedures/">Inbuilt System Procedures</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>Inbuilt System Procedures</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/inbuilt_system_procedures/dump-stacks/">DUMP_STACKS</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/inbuilt_system_procedures/rebalance-all-buckets/">REBALANCE_ALL_BUCKETS</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/inbuilt_system_procedures/set_critical_heap_percentage/">SET_CRITICAL_HEAP_PERCENTAGE</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/inbuilt_system_procedures/set-trace-flag/">SET_TRACE_FLAG</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/system_tables/system_tables/">System Tables</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>System Tables</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/system_tables/members/">MEMBERS</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/system_tables/memoryanalytics/">MEMORYANALYTICS</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/system_tables/sysdiskstores/">SYSDISKSTORES</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/command_line_utilities/store-launcher/">Command Line Utilities</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>Command Line Utilities</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/command_line_utilities/store-backup/">backup</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/command_line_utilities/store-list-missing-disk-stores/">list-missing-disk-stores</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/command_line_utilities/store-revoke-missing-disk-stores/">revoke-missing-disk-store</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/command_line_utilities/store-run/">run</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/command_line_utilities/store-version/">version</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/store_command_reference/">Snappy-SQL Shell Interactive Commands</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>Snappy-SQL Shell Interactive Commands</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/connect_client/">connect client</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/connect/">connect</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/describe/">describe</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/disconnect/">disconnect</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/elapsedtime/">elapsedtime</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/exit/">exit</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/maximumdisplaywidth/">MaximumDisplayWidth</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/run/">run</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/set_connection/">set connection</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/interactive_commands/show/">show</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/config_parameters/">Configuration Parameters</a>
        
    </li>

        
            
    <ul class="subnav">
    <li><span>Configuration Parameters</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/ack-severe-alert-threshold/">ack-severe-alert-threshold</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/ack-wait-threshold/">ack-wait-threshold</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/archive-disk-space-limit/">archive-disk-space-limit</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/archive-file-size-limit/">archive-file-size-limit</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/bind-address/">bind-address</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/enable-network-partition-detection/">enable-network-partition-detection</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/enable-stats/">enable-stats</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/enable-time-statistics/">enable-time-statistics</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/enforce-unique-host/">enforce-unique-host</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/init-scripts/">init-scripts</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/locators/">locators</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/log-file/">log-file</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/log-level/">log-level</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/member-timeout/">member-timeout</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/redundancy-zone/">redundancy-zone</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/start-locator/">start-locator</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/statistic-archive-file/">statistic-archive-file</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/statistic-sample-rate/">statistic-sample-rate</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/statistic-sampling-enabled/">statistic-sampling-enabled</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/sys-disk-dir/">sys-disk-dir</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../reference/configuration_parameters/user/">user</a>
        
    </li>

        
    </ul>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../apidocsintro/">API Documentation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../setting_up_odbc_driver-tableau_desktop/">Setting Up SnappyData ODBC Driver and Tableau Desktop</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../additional_docs/">Additional Information</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../techsupport/">Contact and Support</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../LICENSE/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">SnappyData Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Programming Guide</li>
<!--    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/SnappyDataInc/snappydata" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li> !-->
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h3 id="overview">Overview</h3>
<p>SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). 
All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames.<br />
We therefore recommend that you understand the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#overview">concepts in SparkSQL</a> 
and the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">DataFrame API</a>. You can also store and manage arbitrary 
RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still 
evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced <a href="#markdown_link_row_and_column_tables">here</a>.</p>
<p>In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered 
to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. 
Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk.</p>
<p><a id="snappysession"></a></p>
<h2 id="snappysession-and-snappystreamingcontext">SnappySession and SnappyStreamingContext</h2>
<p><a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappySession">SnappySession</a> is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's <a href="http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.sql.SparkSession">SparkSession</a> to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame.
Similarly, <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.streaming.SnappyStreamingContext">SnappyStreamingContext</a> is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's
<a href="http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.streaming.StreamingContext">Streaming Context</a>.</p>
<p>Also SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see <a href="../deployment/">SnappyData Spark Affinity modes</a>.</p>
<p>If you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession.</p>
<h3 id="to-create-a-snappysession">To Create a SnappySession</h3>
<p><strong>Scala </strong></p>
<pre><code class="scala"> val spark: SparkSession = SparkSession
         .builder
         .appName(&quot;SparkApp&quot;)
         .master(&quot;master_url&quot;)
         .getOrCreate

 val snappy = new SnappySession(spark.sparkContext)
</code></pre>

<p><strong>Java</strong></p>
<pre><code class="Java"> SparkSession spark = SparkSession
       .builder()
       .appName(&quot;SparkApp&quot;)
       .master(&quot;master_url&quot;)
       .getOrCreate();

 JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
 SnappySession snappy = new SnappySession(spark.sparkContext());
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="Python"> from pyspark.sql.snappy import SnappySession
 from pyspark import SparkContext, SparkConf

 conf = SparkConf().setAppName(appName).setMaster(master)
 sc = SparkContext(conf=conf)
 snappy = SnappySession(sc)
</code></pre>

<h3 id="to-create-a-snappystreamingcontext">To Create a SnappyStreamingContext</h3>
<p><strong>Scala</strong></p>
<pre><code class="scala"> val spark: SparkSession = SparkSession
         .builder
         .appName(&quot;SparkApp&quot;)
         .master(&quot;master_url&quot;)
         .getOrCreate
 val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))
</code></pre>

<p><strong>Java</strong></p>
<pre><code class="Java"> SparkSession spark = SparkSession
     .builder()
     .appName(&quot;SparkApp&quot;)
     .master(&quot;master_url&quot;)
     .getOrCreate();

 JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());

 Duration batchDuration = Milliseconds.apply(500);
 JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext(jsc, batchDuration);
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="Python"> from pyspark.streaming.snappy.context import SnappyStreamingContext
 from pyspark import SparkContext, SparkConf

 conf = SparkConf().setAppName(appName).setMaster(master)
 sc = SparkContext(conf=conf)
 duration = .5
 snsc = SnappyStreamingContext(sc, duration)
</code></pre>

<p>If you are in the Embedded Mode, applications typically submit Jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext. 
These jobs are the primary mechanism to interact with SnappyData using the Spark API. 
A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.</p>
<p>The implementation of the <em>runSnappyJob</em> function from SnappySQLJob uses a SnappySession to interact with the SnappyData store to process and store tables.
The implementation of <em>runSnappyJob</em> from SnappyStreamingJob uses a SnappyStreamingContext to create streams and manage the streaming context.
The jobs are submitted to the lead node of SnappyData over REST API using a <em>spark-submit</em> like utility.</p>
<h2 id="snappydata-jobs">SnappyData Jobs</h2>
<p>To create a job that can be submitted through the job server, the job must implement the <strong>SnappySQLJob</strong> or <strong>SnappyStreamingJob</strong> trait. Your job is displayed as:</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">class SnappySampleJob implements SnappySQLJob {
  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/
  def runSnappyJob(snappy: SnappySession, jobConfig: Config): Any

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  def isValidJob(snappy: SnappySession, config: Config): SnappyJobValidation
}
</code></pre>

<p><strong>Java</strong></p>
<pre><code class="java">class SnappySampleJob extends SnappySQLJob {
  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/
  public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation}

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate}
}

</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">class SnappyStreamingSampleJob implements SnappyStreamingJob {
  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/
  def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation
}
</code></pre>

<p><strong>Java</strong></p>
<pre><code class="java">class SnappyStreamingSampleJob extends JavaSnappyStreamingJob {
  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/
  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)
  {//validate}
}
</code></pre>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <em>Job</em> traits are simply extensions of the <em>SparkJob</em> implemented by <a href="https://github.com/spark-jobserver/spark-jobserver">Spark JobServer</a>. </p>
</div>
<ul>
<li>
<p><code>runSnappyJob</code> contains the implementation of the Job.
The <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappySession">SnappySession</a>/<a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.streaming.SnappyStreamingContext">SnappyStreamingContext</a> is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.</p>
</li>
<li>
<p><code>isValidJob</code> allows for an initial validation of the context and any provided configuration.
    If the context and configuration can run the job, returning <code>spark.jobserver.SnappyJobValid</code> allows the job to execute, otherwise returning <code>spark.jobserver.SnappyJobInvalid&lt;reason&gt;</code> prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an "HTTP/1.1 400 Bad Request" status code. Validate helps you prevent running jobs that eventually fail due to a  missing or wrong configuration, and saves both time and resources.</p>
</li>
</ul>
<p>See <a href="https://github.com/SnappyDataInc/snappydata/tree/master/examples/src/main/scala/io/snappydata/examples">examples</a> for Spark and Spark streaming jobs. </p>
<p>SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs.</p>
<h3 id="submitting-jobs">Submitting Jobs</h3>
<p>The following command submits <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/io/snappydata/examples/CreateAndLoadAirlineDataJob.scala">CreateAndLoadAirlineDataJob</a>. This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its <code>runJob</code> method. 
The program is compiled into a jar file (<strong>quickstart.jar</strong>) and submitted to jobs server as shown below.</p>
<pre><code class="bash">$ bin/snappy-job.sh submit  \
    --lead hostNameOfLead:8090  \
    --app-name airlineApp \
    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \
    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar
</code></pre>

<p>The utility <code>snappy-job.sh</code> submits the job and returns a JSON that has a Job Id of this job.</p>
<ul>
<li>
<p><code>--lead</code>: Specifies the host name of the lead node along with the port on which it accepts jobs (8090)</p>
</li>
<li>
<p><code>--app-name</code>: Specifies the name given to the submitted application</p>
</li>
<li>
<p><code>--class</code>: Specifies the name of the class that contains implementation of the Spark job to be run</p>
</li>
<li>
<p><code>--app-jar</code>: Specifies the jar file that packages the code for Spark job</p>
</li>
</ul>
<p>The status returned by the utility is displayed below:</p>
<pre><code class="json">{
  &quot;status&quot;: &quot;STARTED&quot;,
  &quot;result&quot;: {
    &quot;jobId&quot;: &quot;321e5136-4a18-4c4f-b8ab-f3c8f04f0b48&quot;,
    &quot;context&quot;: &quot;snappyContext1452598154529305363&quot;
  }
}
</code></pre>

<p>This Job ID can be used to query the status of the running job. </p>
<pre><code class="bash">$ bin/snappy-job.sh status  \
    --lead hostNameOfLead:8090  \
    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48

{
  &quot;duration&quot;: &quot;17.53 secs&quot;,
  &quot;classPath&quot;: &quot;io.snappydata.examples.CreateAndLoadAirlineDataJob&quot;,
  &quot;startTime&quot;: &quot;2016-01-12T16:59:14.746+05:30&quot;,
  &quot;context&quot;: &quot;snappyContext1452598154529305363&quot;,
  &quot;result&quot;: &quot;See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out&quot;,
  &quot;status&quot;: &quot;FINISHED&quot;,
  &quot;jobId&quot;: &quot;321e5136-4a18-4c4f-b8ab-f3c8f04f0b48&quot;
}
</code></pre>

<p>Once the tables are created, they can be queried by running another job. Please refer to <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/io/snappydata/examples/AirlineDataJob.scala">AirlineDataJob</a> for implementing the job. </p>
<pre><code class="bash">$ bin/snappy-job.sh submit  \
    --lead hostNameOfLead:8090  \
    --app-name airlineApp \
    --class  io.snappydata.examples.AirlineDataJob \
    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar
</code></pre>

<p>The status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results.</p>
<h3 id="running-python-applications">Running Python Applications</h3>
<p>Python users can submit a Python application using <code>spark-submit</code> in the SnappyData Connector mode. For example, run the command given below to submit a Python application:</p>
<pre><code class="bash">bin/spark-submit \
    --master local[*]  \
    --conf snappydata.connection=localhost:1527 \
    --conf spark.ui.port=4042  quickstart/python/AirlineDataPythonApp.py
</code></pre>

<p><code>snappydata.connection</code> property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster.</p>
<h3 id="streaming-jobs">Streaming Jobs</h3>
<p>An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying <code>--stream</code> as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. 
Alternatively, you can specify the name of an existing/pre-created streaming context as <code>--context &lt;context-name&gt;</code> with the <code>submit</code> command.</p>
<p>For example, <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/io/snappydata/examples/TwitterPopularTagsJob.scala">TwitterPopularTagsJob</a> can be submitted as follows. 
This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets.</p>
<pre><code class="bash">$ bin/snappy-job.sh submit  \
    --lead hostNameOfLead:8090  \
    --app-name airlineApp \
    --class  io.snappydata.examples.TwitterPopularTagsJob \
    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \
    --stream

{
  &quot;status&quot;: &quot;STARTED&quot;,
  &quot;result&quot;: {
    &quot;jobId&quot;: &quot;982ac142-3550-41e1-aace-6987cb39fec8&quot;,
    &quot;context&quot;: &quot;snappyStreamingContext1463987084945028747&quot;
  }
}
</code></pre>

<p>To start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context.</p>
<pre><code class="bash">$ bin/snappy-job.sh stop  \
    --lead hostNameOfLead:8090  \
    --job-id 982ac142-3550-41e1-aace-6987cb39fec8

$ bin/snappy-job.sh listcontexts  \
    --lead hostNameOfLead:8090
[&quot;snappyContext1452598154529305363&quot;, &quot;snappyStreamingContext1463987084945028747&quot;, &quot;snappyStreamingContext&quot;]

$ bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \
    --lead hostNameOfLead:8090
</code></pre>

<h2 id="managing-jar-files">Managing JAR Files</h2>
<p>SnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster.</p>
<p><strong>Installing a JAR</strong> </p>
<p>Use SQLJ.INSTALL_JAR procedure to install a JAR file</p>
<p>Syntax:</p>
<pre><code class="bash">SQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER)
</code></pre>

<ul>
<li>
<p>JAR_FILE_PATH  is the full path for the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers</p>
</li>
<li>
<p>QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.</p>
</li>
<li>
<p>DEPLOY: This argument is currently ignored.</p>
</li>
</ul>
<p><em>Example: Installing a JAR</em></p>
<pre><code class="bash">snappy&gt; call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0);
</code></pre>

<p><strong>Replacing a JAR</strong> </p>
<p>Use  SQLJ.REPLACE_JAR procedure to replace an installed JAR file</p>
<p>Syntax:</p>
<pre><code class="bash">SQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672))
</code></pre>

<ul>
<li>
<p>JAR_FILE_PATH  is full path for the JAR file. This path must be accessible to server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers.</p>
</li>
<li>
<p>QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.</p>
</li>
</ul>
<p><em>Example: Replacing a JAR</em></p>
<pre><code class="bash">CALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs')
</code></pre>

<p><strong>Removing a JAR</strong> </p>
<p>Use SQLJ.REMOVE_JAR  procedure to remove a JAR file</p>
<p>Syntax:</p>
<pre><code class="bash">SQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER)
</code></pre>

<ul>
<li>
<p>QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.</p>
</li>
<li>
<p>UNDEPLOY: This argument is currently ignored.</p>
</li>
</ul>
<p><em>Example: Removing a JAR</em></p>
<pre><code class="bash">CALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)
</code></pre>

<h2 id="using-snappydata-shell">Using SnappyData Shell</h2>
<p>The SnappyData SQL Shell (<em>snappy-sql</em>) provides a simple command line interface to the SnappyData cluster.
It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. 
Internally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.</p>
<pre><code>// from the SnappyData base directory  
$ cd quickstart/scripts  
$ ../../bin/snappy-sql
Version 2.0-BETA
snappy&gt; 

//Connect to the cluster as a client  
snappy&gt; connect client 'localhost:1527'; //It connects to the locator.

//Show active connections  
snappy&gt; show connections;

//Display cluster members by querying a system table  
snappy&gt; select id, kind, status, host, port from sys.members;

//or
snappy&gt; show members;

//Run a sql script. This particular script creates and loads a column table in the default schema  
snappy&gt; run 'create_and_load_column_table.sql';

//Run a sql script. This particular script creates and loads a row table in the default schema  
snappy&gt; run 'create_and_load_row_table.sql';
</code></pre>

<p>The complete list of commands available through <em>snappy_shell</em> can be found <a href="http://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/gfxd_commands/gfxd-launcher.html">here</a></p>
<h2 id="using-the-spark-shell-and-spark-submit">Using the Spark Shell and spark-submit</h2>
<p>SnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. 
You however may need to isolate the computational cluster for other reasons. For instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly.</p>
<p>To support such cases it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the <code>spark.snappydata.connection</code> property should be provided while starting the Spark-shell. </p>
<p>To run all SnappyData functionalities you need to create a <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappySession">SnappySession</a>.</p>
<pre><code class="bash">// from the SnappyData base directory  
# Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter.
$ bin/spark-shell  --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041
scala&gt;
#Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql
scala&gt; val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)
scala&gt; val airlineDF = snappy.table(&quot;airline&quot;).show
scala&gt; val resultset = snappy.sql(&quot;select * from airline&quot;)
Next changed command:
# Start the Spark standalone cluster from SnappyData base directory 
$ sbin/start-all.sh 
# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.
$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar
</code></pre>

<p>Any Spark application can also use the SnappyData as store and Spark as a computational engine by providing an extra <code>spark.snappydata.connection</code> property in the conf.</p>
<pre><code class="bash"># Start the Spark standalone cluster from SnappyData base directory 
$ sbin/start-all.sh 
# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.
$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar

# The results can be seen on the command line.
</code></pre>

<h2 id="using-jdbc-with-snappydata">Using JDBC with SnappyData</h2>
<p>SnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information, and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail. </p>
<pre><code class="java">
// 1527 is the default port a Locator or Server uses to listen for thin client connections
Connection c = DriverManager.getConnection (&quot;jdbc:snappydata://locatorHostName:1527/&quot;);
// While, clients typically just point to a locator, you could also directly point the 
//   connection at a server endpoint
</code></pre>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the <strong>io.snappydata.jdbc.ClientDriver</strong> class.</p>
</div>
<h2 id="multiple-language-binding-using-thrift-protocol">Multiple Language Binding using Thrift Protocol</h2>
<p>SnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData.
Thrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the <a href="https://thrift.apache.org/">Apache Thrift documentation</a>.</p>
<p>The JDBC driver for SnappyData that uses the <code>jdbc:snappydata://</code> URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore <code>jdbc:gemfirexd://</code> continues to use the deprecated DRDA protocol.</p>
<p>Likewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode (<code>snappy-start-all.sh rowstore</code>) the DRDA servers are started as before.</p>
<p>To explicitly start a DRDA server in SnappyData, you can use the <code>-drda-server-address</code> and <code>-drda-server-port</code> options for the <strong>bind address</strong> and <strong>port</strong> respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the <code>-thrift-server-address</code> and <code>-thrift-server-port</code> options.</p>
<p>Refer to the following documents for information on support provided by SnappyData:</br></p>
<ul>
<li>
<p><a href="https://github.com/SnappyDataInc/snappydata/blob/branch-0.9/cluster/README-thrift.md"><strong>About SnappyData Thrift</strong></a>: Contains detailed information about the feature and it's capabilities.</p>
</li>
<li>
<p><a href="https://github.com/SnappyDataInc/snappy-store/blob/branch-1.5.4/gemfirexd/shared/src/main/java/io/snappydata/thrift/common/snappydata.thrift"><strong>The Thrift Interface Definition Language (IDL)</strong></a>: This is a Thrift interface definition file for the SnappyData service.</p>
</li>
<li>
<p><a href="https://github.com/SnappyDataInc/snappy-store/blob/branch-1.5.4/gemfirexd/tools/src/test/java/io/snappydata/app/TestThrift.java"><strong>Example</strong></a>:
 Example of the Thrift definitions using the SnappyData Thrift IDL.</p>
</li>
</ul>
<h2 id="building-snappydata-applications-using-spark-api">Building SnappyData Applications using Spark API</h2>
<h3 id="snappysession-usage">SnappySession Usage</h3>
<h4 id="create-columnar-tables-using-api">Create Columnar Tables using API</h4>
<p>Other than <code>create</code> and <code>drop</code> table, rest are all based on the Spark SQL Data Source APIs.</p>
<h4 id="scala">Scala</h4>
<pre><code class="scala"> val props = Map(&quot;BUCKETS&quot; -&gt; &quot;2&quot;)// Number of partitions to use in the SnappyStore

 case class Data(COL1: Int, COL2: Int, COL3: Int)

 val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))
 val rdd = spark.sparkContext.parallelize(data, data.length).map(s =&gt; new Data(s(0), s(1), s(2)))

 val df = snappy.createDataFrame(rdd)

 // create a column table
 snappy.dropTable(&quot;COLUMN_TABLE&quot;, ifExists = true)

 // &quot;column&quot; is the table format (that is row or column)
 // dataDF.schema provides the schema for table
 snappy.createTable(&quot;COLUMN_TABLE&quot;, &quot;column&quot;, df.schema, props)
 // append dataDF into the table
 df.write.insertInto(&quot;COLUMN_TABLE&quot;)

 val results = snappy.sql(&quot;SELECT * FROM COLUMN_TABLE&quot;)
 println(&quot;contents of column table are:&quot;)
 results.foreach(r =&gt; println(r))
</code></pre>

<h4 id="java">Java</h4>
<pre><code class="Java"> Map&lt;String, String&gt; props1 = new HashMap&lt;&gt;();
 props1.put(&quot;buckets&quot;, &quot;11&quot;);

 JavaRDD&lt;Row&gt; jrdd = jsc.parallelize(Arrays.asList(
  RowFactory.create(1, 2, 3),
  RowFactory.create(7, 8, 9),
  RowFactory.create(9, 2, 3),
  RowFactory.create(4, 2, 3),
  RowFactory.create(5, 6, 7)
 ));

 StructType schema = new StructType(new StructField[]{
  new StructField(&quot;col1&quot;, DataTypes.IntegerType, false, Metadata.empty()),
  new StructField(&quot;col2&quot;, DataTypes.IntegerType, false, Metadata.empty()),
  new StructField(&quot;col3&quot;, DataTypes.IntegerType, false, Metadata.empty()),
 });

 Dataset&lt;Row&gt; df = snappy.createDataFrame(jrdd, schema);

// create a column table
 snappy.dropTable(&quot;COLUMN_TABLE&quot;, true);

// &quot;column&quot; is the table format (that is row or column)
// dataDF.schema provides the schema for table
 snappy.createTable(&quot;COLUMN_TABLE&quot;, &quot;column&quot;, df.schema(), props1, false);
// append dataDF into the table
 df.write().insertInto(&quot;COLUMN_TABLE&quot;);

 Dataset&lt;Row&gt;  results = snappy.sql(&quot;SELECT * FROM COLUMN_TABLE&quot;);
 System.out.println(&quot;contents of column table are:&quot;);
 for (Row r : results.select(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;). collectAsList()) {
   System.out.println(r);
 }
</code></pre>

<h4 id="python">Python</h4>
<pre><code class="Python">from pyspark.sql.types import *

data = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]
rdd = sc.parallelize(data)
schema=StructType([StructField(&quot;col1&quot;, IntegerType()),
                   StructField(&quot;col2&quot;, IntegerType()),
                   StructField(&quot;col3&quot;, IntegerType())])

dataDF = snappy.createDataFrame(rdd, schema)

# create a column table
snappy.dropTable(&quot;COLUMN_TABLE&quot;, True)
#&quot;column&quot; is the table format (that is row or column)
#dataDF.schema provides the schema for table
snappy.createTable(&quot;COLUMN_TABLE&quot;, &quot;column&quot;, dataDF.schema, True, buckets=&quot;11&quot;)

#append dataDF into the table
dataDF.write.insertInto(&quot;COLUMN_TABLE&quot;)
results1 = snappy.sql(&quot;SELECT * FROM COLUMN_TABLE&quot;)

print(&quot;contents of column table are:&quot;)
results1.select(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;). show()
</code></pre>

<p>The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. 
For more details about the properties ('props1' map in above example) and <code>createTable</code> API refer to documentation for <a href="#tables-in-snappydata">row and column tables</a>.</p>
<h3 id="create-row-tables-using-api-update-the-contents-of-row-table">Create Row Tables using API, Update the Contents of Row Table</h3>
<pre><code class="scala">// create a row format table called ROW_TABLE
snappy.dropTable(&quot;ROW_TABLE&quot;, ifExists = true)
// &quot;row&quot; is the table format
// dataDF.schema provides the schema for table
val props2 = Map.empty[String, String]
snappy.createTable(&quot;ROW_TABLE&quot;, &quot;row&quot;, dataDF.schema, props2)

// append dataDF into the data
dataDF.write.insertInto(&quot;ROW_TABLE&quot;)

val results2 = snappy.sql(&quot;select * from ROW_TABLE&quot;)
println(&quot;contents of row table are:&quot;)
results2.foreach(println)

// row tables can be mutated
// for example update &quot;ROW_TABLE&quot; and set col3 to 99 where
// criteria &quot;col3 = 3&quot; is true using update API
snappy.update(&quot;ROW_TABLE&quot;, &quot;COL3 = 3&quot;, org.apache.spark.sql.Row(99), &quot;COL3&quot; )

val results3 = snappy.sql(&quot;SELECT * FROM ROW_TABLE&quot;)
println(&quot;contents of row table are after setting col3 = 99 are:&quot;)
results3.foreach(println)

// update rows using sql update statement
snappy.sql(&quot;UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99&quot;)
val results4 = snappy.sql(&quot;SELECT * FROM ROW_TABLE&quot;)
println(&quot;contents of row table are after setting col1 = 100 are:&quot;)
results4.foreach(println)
</code></pre>

<h3 id="snappystreamingcontext-usage">SnappyStreamingContext Usage</h3>
<p>SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.</p>
<p>Below example shows how to use the <code>SnappyStreamingContext</code> to apply a schema to existing DStream and then query the <code>SchemaDStream</code> with simple SQL. It also shows the ability of the SnappyStreamingContext to deal with SQL queries.</p>
<h4 id="scala_1">Scala</h4>
<pre><code class="scala"> import org.apache.spark.sql._
 import org.apache.spark.streaming._
 import scala.collection.mutable
 import org.apache.spark.rdd._
 import org.apache.spark.sql.types._
 import scala.collection.immutable.Map

 val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))
 val schema = StructType(List(StructField(&quot;id&quot;, IntegerType) ,StructField(&quot;text&quot;, StringType)))

 case class ShowCaseSchemaStream (loc:Int, text:String)

 snsc.snappyContext.dropTable(&quot;streamingExample&quot;, ifExists = true)
 snsc.snappyContext.createTable(&quot;streamingExample&quot;, &quot;column&quot;,  schema, Map.empty[String, String] , false)

 def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =&gt; ShowCaseSchemaStream( i, s&quot;Text$i&quot;))

 val dstream = snsc.queueStream[ShowCaseSchemaStream](
                 mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))

 val schemaDStream = snsc.createSchemaDStream(dstream )

 schemaDStream.foreachDataFrame(df =&gt; {
     df.write.format(&quot;column&quot;).
     mode(SaveMode.Append).
     options(Map.empty[String, String]).
     saveAsTable(&quot;streamingExample&quot;)    })

 snsc.start()
 snsc.sql(&quot;select count(*) from streamingExample&quot;).show
</code></pre>

<h4 id="java_1">Java</h4>
<pre><code class="java"> StructType schema = new StructType(new StructField[]{
     new StructField(&quot;id&quot;, DataTypes.IntegerType, false, Metadata.empty()),
     new StructField(&quot;text&quot;, DataTypes.StringType, false, Metadata.empty())
 });

 Map&lt;String, String&gt; props = Collections.emptyMap();
 jsnsc.snappySession().dropTable(&quot;streamingExample&quot;, true);
 jsnsc.snappySession().createTable(&quot;streamingExample&quot;, &quot;column&quot;, schema, props, false);

 Queue&lt;JavaRDD&lt;ShowCaseSchemaStream&gt;&gt; rddQueue = new LinkedList&lt;&gt;();// Define a JavaBean named ShowCaseSchemaStream
 rddQueue.add(rddList(jsc, 1, 10));
 rddQueue.add(rddList(jsc, 10, 20));
 rddQueue.add(rddList(jsc, 20, 30));

 //rddList methods is defined as
/* private static JavaRDD&lt;ShowCaseSchemaStream&gt; rddList(JavaSparkContext jsc, int start, int end){
    List&lt;ShowCaseSchemaStream&gt; objs = new ArrayList&lt;&gt;();
      for(int i= start; i&lt;=end; i++){
        objs.add(new ShowCaseSchemaStream(i, String.format(&quot;Text %d&quot;,i)));
      }
    return jsc.parallelize(objs);
 }*/

 JavaDStream&lt;ShowCaseSchemaStream&gt; dStream = jsnsc.queueStream(rddQueue);
 SchemaDStream schemaDStream = jsnsc.createSchemaDStream(dStream, ShowCaseSchemaStream.class);

 schemaDStream.foreachDataFrame(new VoidFunction&lt;Dataset&lt;Row&gt;&gt;() {
   @Override
   public void call(Dataset&lt;Row&gt; df) {
     df.write().insertInto(&quot;streamingExample&quot;);
   }
 });

 jsnsc.start();

 jsnsc.sql(&quot;select count(*) from streamingExample&quot;).show();
</code></pre>

<h4 id="python_1">Python</h4>
<pre><code>from pyspark.streaming.snappy.context import SnappyStreamingContext
from pyspark.sql.types import *

def  rddList(start, end):
  return sc.parallelize(range(start,  end)).map(lambda i : ( i, &quot;Text&quot; + str(i)))

def saveFunction(df):
   df.write.format(&quot;column&quot;).mode(&quot;append&quot;).saveAsTable(&quot;streamingExample&quot;)

schema=StructType([StructField(&quot;loc&quot;, IntegerType()),
                   StructField(&quot;text&quot;, StringType())])

snsc = SnappyStreamingContext(sc, 1)

dstream = snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])

snsc._snappycontext.dropTable(&quot;streamingExample&quot; , True)
snsc._snappycontext.createTable(&quot;streamingExample&quot;, &quot;column&quot;, schema)

schemadstream = snsc.createSchemaDStream(dstream, schema)
schemadstream.foreachDataFrame(lambda df: saveFunction(df))
snsc.start()
time.sleep(1)
snsc.sql(&quot;select count(*) from streamingExample&quot;).show()

</code></pre>

<!--
> Note: Above simple example uses local mode (i.e. development mode) to create tables and update data. In the production environment, users will want to deploy the SnappyData system as a unified cluster (default cluster model that consists of servers that embed colocated Spark executors and SnappyData stores, locators, and a job server enabled lead node) or as a split cluster (where Spark executors and SnappyData stores form independent clusters). Refer to the  [deployment](deployment.md) chapter for all the supported deployment modes and the [configuration](configuration.md) chapter for configuring the cluster. This mode is supported in both Java and Scala. Support for Python is yet not added.-->

<p><a id="markdown_link_row_and_column_tables"></a></p>
<h2 id="tables-in-snappydata">Tables in SnappyData</h2>
<h3 id="row-and-column-tables">Row and Column Tables</h3>
<p>Column tables organize and manage data in memory in compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.</p>
<p>Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates.</p>
<p>Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.</p>
<h4 id="ddl-and-dml-syntax-for-tables">DDL and DML Syntax for Tables</h4>
<pre><code class="sql">CREATE TABLE [IF NOT EXISTS] table_name
   (
  COLUMN_DEFININTION
   )
USING row | column
OPTIONS (
COLOCATE_WITH 'table_name',  // Default none
PARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.
BUCKETS  'NumPartitions', // Default 113
REDUNDANCY        '1' ,
EVICTION_BY LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,
OVERFLOW 'true',
PERSISTENCE  ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE,
DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore
EXPIRE TIMETOLIVE in seconds',
COLUMN_BATCH_SIZE '32000000',
COLUMN_MAX_DELTA_ROWS '10000',
)
[AS select_statement];

DROP TABLE [IF EXISTS] table_name
</code></pre>

<p>Refer to the <a href="../howto">How-Tos</a> section for more information on partitioning and collocating data.</p>
<p>For row format tables column definition can take underlying GemFire XD syntax to create a table. For example, note the PRIMARY KEY clause below.</p>
<pre><code class="scala">snappy.sql(&quot;CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT)
         USING row options(BUCKETS '5')&quot; )
</code></pre>

<p>For column table it is restricted to Spark syntax for column definition</p>
<pre><code class="scala">snappy.sql(&quot;CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')&quot; )
</code></pre>

<p>You can also define complex types (Map, Array and StructType) as columns for column tables.</p>
<pre><code class="scala">snappy.sql(&quot;CREATE TABLE tableName (
col1 INT , 
col2 Array&lt;Decimal&gt;, 
col3 Map&lt;Timestamp, Struct&lt;x: Int, y: String, z: Decimal(10,5)&gt;&gt;, 
col6 Struct&lt;a: Int, b: String, c: Decimal(10,5)&gt;
) USING column options(BUCKETS '5')&quot; )
</code></pre>

<p>To access the complex data from JDBC you can see <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/org/apache/spark/examples/snappydata/JDBCWithComplexTypes.scala">JDBCWithComplexTypes</a> for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.</p>
</div>
<h4 id="spark-api-for-managing-tables">Spark API for Managing Tables</h4>
<p><strong>Get a reference to <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappySession">SnappySession</a>:</strong></p>
<pre><code>val snappy: SnappySession = new SnappySession(spark.sparkContext)
</code></pre>
<p>Create a SnappyStore table using Spark APIs</p>
<pre><code>val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section
case class Data(col1: Int, col2: Int, col3: Int)
val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))
val rdd = sparkContext.parallelize(data, data.length).map(s =&gt; new Data(s(0), s(1), s(2)))
val dataDF = snappy.createDataFrame(rdd)
snappy.createTable("column_table", "column", dataDF.schema, props)
//or create a row format table
snappy.createTable("row_table", "row", dataDF.schema, props)
</code></pre>
<p><strong>Drop a SnappyStore table using Spark APIs</strong>:</p>
<pre><code>snappy.dropTable(tableName, ifExists = true)
</code></pre>
<p><a id="ddl"></a></p>
<h4 id="ddl-extensions-to-snappystore-tables">DDL extensions to SnappyStore Tables</h4>
<p>The below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions. </p>
<ul>
<li>
<p>COLOCATE_WITH: The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist.</p>
</li>
<li>
<p>PARTITION_BY: Use the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning. If not specified, it is a replicated table.</br> Column and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally on a generated scheme.</br> The default number of storage partitions (BUCKETS) is 113 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.</p>
</li>
<li>
<p>BUCKETS: The optional BUCKETS attribute specifies the fixed number of "buckets," the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.</p>
</li>
<li>
<p>REDUNDANCY: Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.</p>
</li>
<li>
<p>EVICTION_BY: Use the EVICTION_BY clause to evict rows automatically from in-memory table, based on different criteria. </br>For column tables, the default eviction setting is LRUHEAPPERCENT and the default action is to overflow to disk. You can also specify the OVERFLOW parameter along with the EVICTION_BY clause. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables no such defaults are set. Row tables allow all the eviction settings.</p>
</div>
</li>
<li>
<p>OVERFLOW: If it is set to <strong>false</strong> the evicted rows are destroyed. If set to <strong>true</strong> it overflows to a local SnappyStore disk store.
    When you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table).</p>
</li>
<li>
<p>PERSISTENCE:  When you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.</p>
</li>
<li>
<p>DISKSTORE: The disk directory where you want to persist the table data. For more information, <a href="http://rowstore.docs.snappydata.io/docs/reference/language_ref/ref-create-diskstore.html#create-diskstore">refer to this document</a>.</p>
</li>
<li>
<p>EXPIRE: You can use the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured TTL.</p>
</li>
<li>
<p>COLUMN_BATCH_SIZE: The default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M)</p>
</li>
<li>
<p>COLUMN_MAX_DELTA_ROWS: The maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by <code>COLUMN_BATCH_SIZE</code> (and thus limits size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The default value is 10000. </br> </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following corresponding SQLConf properties for <code>COLUMN_BATCH_SIZE</code> and <code>COLUMN_MAX_DELTA_ROWS</code> are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL): </p>
<ul>
<li>
<p><code>snappydata.column.batchSize</code> - explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit <code>COLUMN_BATCH_SIZE</code> specification, then this is inherited for that table property. </p>
</li>
<li>
<p><code>snappydata.column.maxDeltaRows</code> - maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit <code>COLUMN_MAX_DELTA_ROWS</code> specification, then this is inherited for that table property. </p>
</li>
</ul>
</div>
</li>
</ul>
<p>Refer to the <a href="http://rowstore.docs.snappydata.io/docs/reference/sql-language-reference.html">SQL Reference Guide</a> for information on the extensions.</p>
<h4 id="restrictions-on-column-tables">Restrictions on Column Tables</h4>
<ul>
<li>
<p>Column tables cannot specify any primary key, unique key constraints</p>
</li>
<li>
<p>Index on column table is not supported</p>
</li>
<li>
<p>Option EXPIRE is not applicable for column tables</p>
</li>
<li>
<p>Option EVICTION_BY with value LRUCOUNT is not applicable for column tables</p>
</li>
</ul>
<h4 id="dml-operations-on-tables">DML Operations on Tables</h4>
<pre><code>    INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;
    INSERT INTO TABLE tablename1 select_statement1 FROM from_statement;
    INSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;
    UPDATE tablename SET column = value [, column = value ...] [WHERE expression]
    PUT INTO tableName (column, ...) VALUES (value, ...)
    DELETE FROM tablename1 [WHERE expression]
    TRUNCATE TABLE tablename1;
</code></pre>

<h4 id="api-extensions-provided-in-snappycontext">API Extensions Provided in SnappyContext</h4>
<p>Several APIs have been added in <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappySession">SnappySession</a> to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.</p>
<pre><code>    //  Applicable for both row and column tables
    def insert(tableName: String, rows: Row*): Int .

    // Only for row tables
    def put(tableName: String, rows: Row*): Int
    def update(tableName: String, filterExpr: String, newColumnValues: Row, 
               updateColumns: String*): Int
    def delete(tableName: String, filterExpr: String): Int
</code></pre>

<p><strong>Usage SnappySession.insert()</strong>: Insert one or more [[org.apache.spark.sql.Row]] into an existing table</p>
<pre><code>    val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),
                   Seq(5, 6, 7), Seq(1,100,200))
    data.map { r =&gt;
      snappy.insert(&quot;tableName&quot;, Row.fromSeq(r))
    }
</code></pre>

<p><strong>Usage SnappySession.put()</strong>: Upsert one or more [[org.apache.spark.sql.Row]] into an existing table</p>
<pre><code>    val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),
                   Seq(5, 6, 7), Seq(1,100,200))
    data.map { r =&gt;
      snappy.put(tableName, Row.fromSeq(r))
    }
</code></pre>

<p><strong>Usage SnappySession.update()</strong>: Update all rows in table that match passed filter expression</p>
<pre><code>    snappy.update(tableName, &quot;ITEMREF = 3&quot; , Row(99) , &quot;ITEMREF&quot; )
</code></pre>

<p><strong>Usage SnappySession.delete()</strong>: Delete all rows in table that match passed filter expression</p>
<pre><code>    snappy.delete(tableName, &quot;ITEMREF = 3&quot;)
</code></pre>

<h4 id="stringcharvarchar-data-types">String/CHAR/VARCHAR Data Types</h4>
<p>SnappyData supports CHAR and VARCHAR datatypes in addition to Spark's String datatype. For performance reasons, it is recommended that you use either CHAR or VARCHAR type, if your column data fits in maximum CHAR size (254) or VARCHAR size (32768), respectively. For larger column data size, String type should be used as we store its data in CLOB format internally.</p>
<p><strong>Create a table with columns of CHAR and VARCHAR datatype using SQL</strong>:</p>
<pre><code class="Scala">CREATE TABLE tableName (Col1 char(25), Col2 varchar(100)) using row;
</code></pre>

<p><strong>Create a table with columns of CHAR and VARCHAR datatype using API</strong>:</p>
<pre><code class="Scala">    import org.apache.spark.sql.collection.Utils
    import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}

    val snappy: SnappySession = new SnappySession(spark.sparkContext)

    // define schema for table
    val varcharSize = 100
    val charSize = 25
    val schema = StructType(Array(
      StructField(&quot;col_varchar&quot;, StringType, false, Utils.varcharMetadata(varcharSize)),
      StructField(&quot;col_char&quot;, StringType, false, Utils.charMetadata(charSize))
    ))

    // create the table
    snappy.createTable(tableName, &quot;row&quot;, schema, Map.empty[String, String])
</code></pre>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>STRING columns are handled differently when queried over a JDBC connection.</p>
</div>
<p>To ensure optimal performance for SELECT queries executed over JDBC connection (more specifically, those that get routed to lead node), the data of STRING columns is returned in VARCHAR format, by default. This also helps the data visualization tools to render the data effectively.
<br/>However, if the STRING column size is larger than VARCHAR limit (32768), you can enforce the returned data format to be in CLOB in following ways:</p>
<p>Using the system property <code>spark-string-as-clob</code> when starting the lead node(s). This applies to all the STRING columns in all the tables in cluster.</p>
<pre><code>bin/snappy leader start -locators:localhost:10334 -J-Dspark-string-as-clob=true
</code></pre>

<p>Defining the column(s) itself as CLOB, either using SQL or API. In the example below, we define the column 'Col2' to be CLOB.</p>
<pre><code>CREATE TABLE tableName (Col1 INT, Col2 CLOB, Col3 STRING, Col4 STRING);
</code></pre>

<pre><code class="Scala">    import org.apache.spark.sql.collection.Utils
    import org.apache.spark.sql.types.{StringType, StructField, StructType}

    val snappy: SnappySession = new SnappySession(spark.sparkContext)

    // Define schema for table
    val schema = StructType(Array(
      // The parameter Utils.stringMetadata() ensures that this column is rendered as CLOB
      StructField(&quot;Col2&quot;, StringType, false, Utils.stringMetadata())
    ))

    snappy.createTable(tableName, &quot;column&quot;, schema, Map.empty[String, String])
</code></pre>

<p>Using the query-hint `columnsAsClob in the SELECT query.</p>
<pre><code>SELECT * FROM tableName --+ columnsAsClob(*)
</code></pre>

<p>The usage of <code>*</code> above causes all the STRING columns in the table to be rendered as CLOB. You can also provide comma-separated specific column name(s) instead of <code>*</code> above so that data of only those column(s) is returned as CLOB.</p>
<pre><code>SELECT * FROM tableName --+ columnsAsClob(Col3,Col4)
</code></pre>

<h4 id="row-buffers-for-column-tables">Row Buffers for Column Tables</h4>
<p>Generally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored.</p>
<p>In SnappyData, the column table consists of two components, delta row buffer and column store. We try to support individual insert of single row, we store them in a delta row buffer which is write optimized and highly available.
Once the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store.
Any query on column table also takes into account the row cached buffer. By doing this, we ensure that the query does not miss any data.</p>
<h4 id="catalog-in-snappystore">Catalog in SnappyStore</h4>
<p>We use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us the ability to query underlying GemFireXD to reconstruct the meta store in case of a driver failover.</p>
<!--<mark>There are pending work towards unifying DRDA & Spark layer catalog, which will part of future releases. </mark>-->

<h4 id="sql-reference-to-the-syntax">SQL Reference to the Syntax</h4>
<p>Refer to the <a href="http://rowstore.docs.snappydata.io/docs/reference/sql-language-reference.html">SQL Reference Guide</a> for information on the syntax.</p>
<h2 id="stream-processing-using-sql">Stream processing using SQL</h2>
<p>SnappyDatas streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. 
Here is a brief overview of <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark streaming</a> from the Spark Streaming guide. </p>
<h3 id="spark-streaming-overview">Spark Streaming Overview</h3>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <strong>map</strong>, <strong>reduce</strong>, <strong>join</strong> and <strong>window</strong>.</p>
<p>Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> and <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">graph processing</a> algorithms on data streams.</p>
<p><img alt="Spark Streaming architecture" src="http://spark.apache.org/docs/latest/img/streaming-arch.png" /></p>
<p>Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.</p>
<p><img alt="Spark Streaming data flow" src="http://spark.apache.org/docs/latest/img/streaming-flow.png" /></p>
<p>Spark Streaming provides a high-level abstraction called <em>discretized stream</em> or <em>DStream</em>, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of <a href="https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/rdd/RDD.html">RDDs</a>.</p>
<p>Additional details on the Spark Streaming concepts and programming is covered <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">here</a>.</p>
<h3 id="snappydata-streaming-extensions-over-spark">SnappyData Streaming Extensions over Spark</h3>
<p>We offer the following enhancements over Spark Streaming: </p>
<ol>
<li>
<p><strong>Manage Streams declaratively</strong>: Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. </p>
</li>
<li>
<p><strong>SQL based stream processing</strong>: With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. </p>
</li>
<li>
<p><strong>Continuous queries and time windows</strong>: Similar to popular stream processing products, applications can register continuous queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query. </p>
</li>
<li>
<p><strong>OLAP optimizations</strong>: By integrating and collocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store.</p>
</li>
<li>
<p><strong>Approximate stream analytics</strong>: When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.</p>
</li>
</ol>
<h3 id="working-with-stream-tables">Working with Stream Tables</h3>
<p>SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.</p>
<pre><code class="SQL">// DDL for creating a stream table
CREATE STREAM TABLE [IF NOT EXISTS] table_name
(COLUMN_DEFINITION)
USING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'
OPTIONS (
// multiple stream source specific options
  storagelevel '',
  rowConverter '',
  topics '',
  kafkaParams '',
  consumerKey '',
  consumerSecret '',
  accessToken '',
  accessTokenSecret '',
  hostname '',
  port '',
  directory ''
)

// DDL for dropping a stream table
DROP TABLE [IF EXISTS] table_name

// Initialize StreamingContext
STREAMING INIT &lt;batchInterval&gt; [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]

// Start streaming
STREAMING START

// Stop streaming
STREAMING STOP
</code></pre>

<p>For example to create a stream table using kafka source : </p>
<pre><code class="scala"> val spark: SparkSession = SparkSession
     .builder
     .appName(&quot;SparkApp&quot;)
     .master(&quot;local[4]&quot;)
     .getOrCreate

 val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))

 snsc.sql(&quot;create stream table streamTable (userId string, clickStreamLog string) &quot; +
     &quot;using kafka_stream options (&quot; +
     &quot;storagelevel 'MEMORY_AND_DISK_SER_2', &quot; +
     &quot;rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', &quot; +
     &quot;kafkaParams 'zookeeper.connect-&gt;localhost:2181;auto.offset.reset-&gt;smallest;group.id-&gt;myGroupId', &quot; +
     &quot;topics 'streamTopic:01')&quot;)

 // You can get a handle of underlying DStream of the table
 val dStream = snsc.getSchemaDStream(&quot;streamTable&quot;)

 // You can also save the DataFrames to an external table
 dStream.foreachDataFrame(_.write.insertInto(tableName))
</code></pre>

<p>The streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.</p>
<h3 id="stream-sql-through-snappy-sql">Stream SQL through snappy-sql</h3>
<p>Start a SnappyData cluster and connect through snappy-sql :</p>
<pre><code class="bash">//create a connection
snappy&gt; connect client 'localhost:1527';

// Initialize streaming with batchInterval of 2 seconds
snappy&gt; streaming init 2secs;

// Create a stream table
snappy&gt; create stream table streamTable (id long, text string, fullName string, country string,
        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',
        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');

// Start the streaming
snappy&gt; streaming start;

//Run ad-hoc queries on the streamTable on current batch of data
snappy&gt; select id, text, fullName from streamTable where text like '%snappy%'

// Drop the streamTable
snappy&gt; drop table streamTable;

// Stop the streaming
snappy&gt; streaming stop;
</code></pre>

<h3 id="schemadstream">SchemaDStream</h3>
<p>SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table.
Some of these ideas (especially naming our abstractions) were borrowed from <a href="https://github.com/Intel-bigdata/spark-streamingsql">Intel's Streaming SQL project</a>.</p>
<h3 id="registering-continuous-queries">Registering Continuous Queries</h3>
<pre><code class="scala">//You can join two stream tables and produce a result stream.
val resultStream = snsc.registerCQ(&quot;SELECT s1.id, s1.text FROM stream1 window (duration
    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id&quot;)

// You can also save the DataFrames to an external table
dStream.foreachDataFrame(_.write.insertInto(&quot;yourTableName&quot;))
</code></pre>

<h3 id="dynamic-ad-hoc-continuous-queries">Dynamic (ad-hoc) Continuous Queries</h3>
<p>Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.streaming.SnappyStreamingContext">SnappyStreamingContext</a> has started.</p>
<h2 id="user-defined-functions-udf-and-user-defined-aggregate-functions-udaf">User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)</h2>
<p>Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. 
The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Support for UDFs is available in SnappyData version 0.8 and future releases.</p>
</div>
<h3 id="create-user-defined-function">Create User Defined Function</h3>
<p>You can simply extend any one of the interfaces in the package <strong>org.apache.spark.sql.api.java</strong>. 
These interfaces can be included in your client application by adding <strong>snappy-spark-sql_2.11-2.0.3-2.jar</strong> to your classpath.</p>
<h4 id="define-a-udf-class"><strong>Define a UDF class</strong></h4>
<p>The number in the interfaces (UDF1 to UDF22) signifies the number of parameters an UDF can take.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, any UDF which can take more than 22 parameters is not supported.</p>
</div>
<pre><code>package some.package
import org.apache.spark.sql.api.java.UDF1

class StringLengthUDF extends UDF1[String, Int] {
 override def call(t1: String): Int = t1.length
}
</code></pre>

<p><a id= create_udf> </a></p>
<h4 id="create-the-udf-function"><strong>Create the UDF Function</strong></h4>
<p>After defining an UDF you can bundle the UDF class in a JAR file and create the function by using <code>./bin/snappy-shell</code> of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF.</p>
<pre><code>CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'
</code></pre>

<p>For example:</p>
<pre><code>CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'
[Rishi] we can modify the declaration section as above
</code></pre>

<p>You can write a JAVA or SCALA class to write an UDF implementation. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For input/output types: </br>
The framework always returns the Java types to the UDFs. So, if you are writing <code>scala.math.BigDecimal</code> as an input type or output type, an exception is reported. You can use <code>java.math.BigDecimal</code> in the SCALA code. </p>
</div>
<p><strong>Return Types to UDF program type mapping </strong></p>
<table>
<thead>
<tr>
<th>SnappyData Type</th>
<th>UDF Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>STRING</td>
<td>java.lang.String</td>
</tr>
<tr>
<td>INTEGER</td>
<td>java.lang.Integer</td>
</tr>
<tr>
<td>LONG</td>
<td>java.lang.Long</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>java.lang.Double</td>
</tr>
<tr>
<td>DECIMAL</td>
<td>java.math.BigDecimal</td>
</tr>
<tr>
<td>DATE</td>
<td>java.sql.Date</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>java.sql.Timestamp</td>
</tr>
<tr>
<td>FLOAT</td>
<td>java.lang.Float</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>java.lang.Boolean</td>
</tr>
<tr>
<td>SHORT</td>
<td>java.lang.Short</td>
</tr>
<tr>
<td>BYTE</td>
<td>java.lang.Byte</td>
</tr>
</tbody>
</table>
<h4 id="use-the-udf"><strong>Use the UDF</strong></h4>
<pre><code>select strnglen(string_column) from &lt;table&gt;
</code></pre>

<p>If you try to use an UDF on a different type of column, for example, an <strong>Int</strong> column an exception is reported.</p>
<h4 id="drop-the-function"><strong>Drop the Function</strong></h4>
<pre><code>DROP FUNCTION IF EXISTS udf_name
</code></pre>

<p>For example:</p>
<pre><code>DROP FUNCTION IF EXISTS app.strnglen
</code></pre>

<h4 id="modify-an-existing-udf"><strong>Modify an Existing UDF</strong></h4>
<p>1) Drop the existing UDF</p>
<p>2) Modify the UDF code and <a href="#create_udf">create a new UDF</a>. You can create the UDF with the same name as that of the dropped UDF.</p>
<h3 id="create-user-defined-aggregate-functions">Create User Defined Aggregate Functions</h3>
<p>SnappyData uses same interface as that of Spark to define a User Defined Aggregate Function  <code>org.apache.spark.sql.expressions.UserDefinedAggregateFunction</code>. For more information refer to this <a href="https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html">document</a>.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../deployment/" class="btn btn-neutral float-right" title="Affinity Modes">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../configuring_cluster/property_description/" class="btn btn-neutral" title="SnappyData Properties"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2017 SnappyData Inc.</p>
    
  </div>

<b>DISCLAIMER:</b>
This document is work-in-progress and will be progressively updated.

 <!-- Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. -->
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/SnappyDataInc/snappydata" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../configuring_cluster/property_description/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../deployment/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
